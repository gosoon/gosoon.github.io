<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kubernetes ç‰ˆæœ¬å¤šä¹…è¯¥å‡çº§ä¸€æ¬¡]]></title>
    <url>%2F2019%2F09%2F26%2Fk8s_release_version%2F</url>
    <content type="text"><![CDATA[kubernetes ç¤¾åŒºæ¯ä¸‰ä¸ªæœˆå‘å¸ƒä¸€ä¸ªæ–°ç‰ˆæœ¬ï¼Œå¯ä»¥è¯´å‘å¸ƒæ–°ç‰ˆæœ¬çš„é€Ÿåº¦éžå¸¸å¿«ï¼Œå½“ç„¶ï¼Œåœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ç‰ˆæœ¬å‡çº§çš„é€Ÿåº¦å¯èƒ½è·Ÿä¸ä¸Šæ–°ç‰ˆæœ¬å‘å¸ƒçš„é€Ÿåº¦ï¼Œé‚£ä¹ˆç¡®ä¿ç›®å‰ä½¿ç”¨çš„ç‰ˆæœ¬è¿˜å¤„äºŽç¤¾åŒºçš„ç»´æŠ¤é˜¶æ®µå°±éžå¸¸é‡è¦äº†ï¼Œkubernetes å®˜æ–¹å¯¹å„ä¸ªç‰ˆæœ¬æ”¯æŒçš„æ—¶é—´æ˜¯å¤šé•¿å‘¢ï¼Ÿ kubernetes å‘è¡Œç‰ˆé€šå¸¸æ”¯æŒ9ä¸ªæœˆï¼Œåœ¨æ­¤æœŸé—´ï¼Œå¦‚æžœå‘çŽ°ä¸¥é‡çš„bugæˆ–å®‰å…¨é—®é¢˜ï¼Œä¼šåœ¨å¯¹åº”çš„åˆ†æ”¯å‘å¸ƒè¡¥ä¸ç‰ˆæœ¬ã€‚æ¯”å¦‚ï¼Œå½“å‰ç‰ˆæœ¬ä¸º v1.10.1ï¼Œå½“ç¤¾åŒºä¿®å¤ä¸€äº› bug åŽï¼Œå°±ä¼šå‘å¸ƒ v1.10.2 ç‰ˆæœ¬ã€‚ å®˜æ–¹æ”¯æŒæ—¶é—´è¯´æ˜Žå¦‚ä¸‹ï¼š Kubernetes version Release month End-of-life-month v1.6.x March 2017 December 2017 v1.7.x June 2017 March 2018 v1.8.x September 2017 June 2018 v1.9.x December 2017 September 2018 v1.10.x March 2018 December 2018 v1.11.x June 2018 March 2019 v1.12.x September 2018 June 2019 v1.13.x December 2018 September 2019 v1.14.x March 2019 December 2019 v1.15.x June 2019 March 2020 v1.16.x September 2019 June 2020 åˆ°ç›®å‰ä¸ºæ­¢ï¼Œv1.13.x å·²ç»åœæ­¢æ”¯æŒäº†ï¼Œè¯·å°½å¿«å‡çº§è‡³é«˜ç‰ˆæœ¬ã€‚ kubernetes ç‰ˆæœ¬å‘å¸ƒæµç¨‹ ç¿»è¯‘è‡ªå®˜æ–¹æ–‡æ¡£ï¼šKubernetes Release Versioning è¯´æ˜Žï¼šKube X.Y.Z ä»£è¡¨ kubernetes å·²ç»å‘å¸ƒçš„ç‰ˆæœ¬ï¼ˆgit tagï¼‰ï¼Œè¿™ä¸ªç‰ˆæœ¬åŒ…å«æ‰€æœ‰çš„ç»„ä»¶ï¼šapiserver, kubelet, kubectl, etc. (X è¡¨ç¤ºä¸»ç‰ˆæœ¬å·, Y æ˜¯æ­¤ç‰ˆæœ¬å·, Z æ˜¯è¡¥ä¸ç‰ˆæœ¬ã€‚) ç‰ˆæœ¬å‘å¸ƒæ—¶é—´æ¬¡ç‰ˆæœ¬å‘å¸ƒè®¡åˆ’ä¸Žæ—¶é—´è¡¨ Kube X.Y.0-alpha.W, W &gt; 0 ( åˆ†æ”¯ï¼šmaster) Alpha ç‰ˆæœ¬å¤§çº¦æ¯ä¸¤å‘¨ç›´æŽ¥ä»Ž master åˆ†æ”¯å‘å¸ƒä¸€æ¬¡ã€‚ æ²¡æœ‰ cherrypick ç‰ˆæœ¬ã€‚å¦‚æœ‰æœ‰ä¸¥é‡çš„ bug è¢«ä¿®å¤ï¼Œå¯ä»¥åŸºäºŽ master åˆ†æ”¯æå‰åˆ›å»ºä¸€ä¸ªæ–°ç‰ˆæœ¬ã€‚ Kube X.Y.Z-beta.W (åˆ†æ”¯: release-X.Y) å½“ master å®Œæˆ Kube X.Y çš„åŠŸèƒ½åŽï¼Œåœ¨è· X.Y.0 å‘å¸ƒå‰ä¸¤å‘¨ä¼šåœæŽ‰ release-X.Y åˆ†æ”¯ï¼Œåªå°†ä¸€äº›æ¯”è¾ƒé‡è¦çš„ PR cherry-pick åˆ° X.Yã€‚ è¯¥åˆ†æ”¯ä¼šè¢«æ ‡è®°ä¸º X.Y.0-beta.0ï¼Œmaster åˆ†æ”¯ä¼šè¢«ç§»åˆ° X.Y+1.0-alpha.0ã€‚ å¦‚æžœ X.Y.0-beta.0 çš„åŠŸèƒ½æœ‰ç¼ºé™·ï¼Œè¿˜ä¼šå‘å¸ƒå…¶ä»–çš„ beta ç‰ˆæœ¬ (X.Y.0-beta.W | W &gt; 0) ã€‚ Kube X.Y.0 (åˆ†æ”¯: release-X.Y) æœ€ç»ˆçš„ release ç‰ˆæœ¬ä¼šæå‰ä¸¤å‘¨ä»Ž release-X.Y åˆ†æ”¯ä¸Šäº§ç”Ÿã€‚ åœ¨åŒä¸€åˆ†æ”¯çš„åŒä¸€ commit å¤„ä¹Ÿä¼šè¢«æ ‡è®°ä¸º X.Y.1-beta.0ã€‚ åœ¨ X.Y.0 å‘å¸ƒ 3-4 ä¸ªæœˆåŽä¼šå‘å¸ƒ X.(Y-1).0ã€‚ Kube X.Y.Z, Z &gt; 0 (åˆ†æ”¯: release-X.Y) å½“ cherrypick commits åˆ° release-X.Y åˆ†æ”¯æ—¶ï¼Œè‹¥æœ‰éœ€è¦ï¼Œä¹Ÿä¼šå‘å¸ƒç›¸åº”çš„è¡¥ä¸ç‰ˆæœ¬ ï¼ˆX.Y.Z-beta.Wï¼‰ã€‚ X.Y.Z æ˜¯ç›´æŽ¥ä»Ž release-X.Y åˆ†æ”¯ä¸Šäº§ç”Ÿçš„ï¼Œå½“ä½¿ç”¨ beta ç‰ˆæœ¬åœ¨æ›´æ–° pkg/version/base.go åŽä¼šè¢«æ ‡è®°ä¸º X.Y.Z+1-beta.0ã€‚ Kube X.Y.Z, Z &gt; 0 (åˆ†æ”¯: release-X.Y.Z) è¿™æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ tagï¼Œå¦‚æžœåœ¨ä¸Šä¸€ä¸ª release åˆ†æ”¯åŽæœ‰é‡å¤§çš„ bug è¢«ä¿®å¤ï¼Œä¼šæœ‰ä¸€ä¸ª X.Y.Z tagã€‚ release-X.Y.Z åˆ†æ”¯ä¼šè¢«åœæŽ‰ä»¥ç¡®ä¿è¡¥ä¸ç‰ˆæœ¬æ˜¯æœ€æ–°çš„ã€‚ å¦‚æžœè¿˜æœ‰é‡è¦ bug è¢«ä¿®å¤ä¼šå†æœ‰ä¸€ä¸ªè¡¥ä¸ç‰ˆæœ¬ X.Y.(Z+1)ã€‚ ä¸€èˆ¬ä¸ä¼šæœ‰è¡¥ä¸ç‰ˆæœ¬ï¼Œè¡¥ä¸ç‰ˆæœ¬ä»…ç”¨äºŽä¸€äº›é‡å¤§ bug çš„ä¿®å¤ã€‚ å¯ä»¥å‚è€ƒ#19849çœ‹çœ‹è¡¥ä¸ç‰ˆæœ¬çš„ä½œç”¨ã€‚ ä¸»ç‰ˆæœ¬æ—¶é—´çº¿ä¸»ç‰ˆæœ¬æš‚æ—¶æ²¡æœ‰é¢„æœŸå‘å¸ƒçš„æ—¶é—´ç‚¹ï¼Œä¹Ÿæ²¡æœ‰å…¬å¸ƒ 2.0.0 çš„æ ‡å‡†ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰å¯¹ä»»ä½•ç±»åž‹çš„ä¸å…¼å®¹æ›´æ”¹(ä¾‹å¦‚ï¼Œç»„ä»¶å‚æ•°æ›´æ”¹)ã€‚ä¹‹å‰è®¨è®ºè¿‡åœ¨å‘å¸ƒ 2.0.0 åŽ åˆ é™¤ v1 API group/versionï¼Œä½†ç›®å‰æ²¡æœ‰è¿™æ ·åšçš„è®¡åˆ’ã€‚ æ”¯æŒçš„ç»„ä»¶ç‰ˆæœ¬ä¸Žå…¼å®¹ç‰ˆæœ¬æˆ‘ä»¬å¸Œæœ›ç”¨æˆ·åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨ kubernetes æœ€ç¨³å®šçš„ç‰ˆæœ¬ï¼Œä½†å‡çº§ç‰ˆæœ¬éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå°¤å…¶æ˜¯å¯¹äºŽç”Ÿäº§çŽ¯å¢ƒä¸­çš„å…³é”®ç»„ä»¶ã€‚æˆ‘ä»¬ä¹Ÿå¸Œæœ›ç”¨æˆ·æ›´æ–°åˆ°æœ€æ–°çš„è¡¥ä¸ç‰ˆæœ¬ï¼Œè¡¥ä¸ç‰ˆæœ¬ä¸­åŒ…å«ä¸€äº›é‡è¦çš„ bugfixï¼Œå¸Œæœ›ç”¨æˆ·å°½å¿«å‡çº§ã€‚ kubernetes å¯¹å„ç»„ä»¶çš„ç‰ˆæœ¬ä¹Ÿæœ‰ä¸€å®šçš„å…¼å®¹æ€§ã€‚å…·ä½“çš„å…¼å®¹ç­–ç•¥æ˜¯ï¼š slaveç»„ä»¶å¯ä»¥ä¸Žmasterç»„ä»¶æœ€å¤šå»¶è¿Ÿä¸¤ä¸ªç‰ˆæœ¬(minor version)ï¼Œä½†æ˜¯ä¸èƒ½æ¯” master ç»„ä»¶æ–°ã€‚client ä¸èƒ½ä¸Ž master ç»„ä»¶è½åŽä¸€ä¸ªæ¬¡ç‰ˆæœ¬ï¼Œä½†æ˜¯å¯ä»¥é«˜ä¸€ä¸ªç‰ˆæœ¬ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š v1.3 çš„ master å¯ä»¥ä¸Ž v1.1ï¼Œv1.2ï¼Œv1.3 çš„ slave ç»„ä»¶ä¸€èµ·ä½¿ç”¨ï¼Œä¸Ž v1.2ï¼Œv1.3ï¼Œv1.4 client ä¸€èµ·ä½¿ç”¨ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸€æ¬¡â€œæ”¯æŒâ€ä¸‰ä¸ªæ¬¡ç‰ˆæœ¬ï¼Œâ€œæ”¯æŒâ€æ„å‘³ç€æˆ‘ä»¬å¸Œæœ›ç”¨æˆ·åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­è¿è¡Œè¯¥ç‰ˆæœ¬ï¼Œè™½ç„¶æˆ‘ä»¬å¯èƒ½å¯¹äºŽä¸åœ¨æ”¯æŒçš„ç‰ˆæœ¬è¿›è¡Œ bugfixã€‚ä¾‹å¦‚ï¼Œå½“ v1.3 å‘å¸ƒæ—¶ï¼Œå°†ä¸å†æ”¯æŒ v1.0ã€‚æ­¤å¤–æ–°ç‰ˆæœ¬æ¯ä¸‰ä¸ªæœˆå‘è¡Œä¸€æ¬¡ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€ä¸ªç‰ˆæœ¬ä»…æ”¯æŒ 9 ä¸ªæœˆã€‚ å‡çº§ç­–ç•¥ç”¨æˆ·å¯ä»¥ä½¿ç”¨æ»šåŠ¨æ–¹å¼å‡çº§ï¼Œä¸€æ¬¡å‡çº§ä¸€ä¸ªå°ç‰ˆæœ¬ï¼Œä¸å»ºè®®ç›´æŽ¥è·¨åº¦ä¸¤ä¸ªåŠä»¥ä¸Šå°ç‰ˆæœ¬ï¼Œå‡çº§æ—¶å…ˆå‡çº§ master å†å‡çº§ node èŠ‚ç‚¹ã€‚ ä»¥ä¸‹æ˜¯åœ¨å®žé™…å‡çº§è¿‡ç¨‹ä¸­çš„ä¸€äº›ç»éªŒï¼š é‡‘ä¸é›€éƒ¨ç½²ï¼šå³ç°åº¦å‡çº§ï¼Œè‹¥ä½¿ç”¨äºŒè¿›åˆ¶éƒ¨ç½²ï¼Œåˆ™åœ¨åŽŸæœ‰é›†ç¾¤ç›´æŽ¥æ›¿æ¢äºŒè¿›åˆ¶è¿›è¡Œå‡çº§ï¼Œè¿ç»´ä»£ä»·å°ï¼Œä¸ä¼šå¯¼è‡´æœåŠ¡ä¸­æ–­ï¼›è‹¥ä»¥ pod æ–¹å¼éƒ¨ç½²çš„ master ç»„ä»¶ç›´æŽ¥æ›¿æ¢é•œåƒè¿›è¡Œå‡çº§ï¼Œè‹¥ä»¥ deployment æ–¹å¼éƒ¨ç½² master ç»„ä»¶ï¼Œå¯¹äºŽ apiserver å¯ä»¥å‚è€ƒé˜¿é‡Œçš„ç»éªŒï¼Œè®¾ç½® maxSurge=3 çš„æ–¹å¼å‡çº§ï¼Œä»¥é¿å…å‡çº§è¿‡ç¨‹å¸¦æ¥çš„æ€§èƒ½æŠ–åŠ¨ï¼Œä½†æ‰€æœ‰çš„ node ç»„ä»¶ä¾ç„¶éœ€è¦æ›¿æ¢äºŒè¿›åˆ¶å‡çº§ã€‚ è“ç»¿éƒ¨ç½²ï¼šæ­å»ºä¸€å¥—æ–°çš„é›†ç¾¤ï¼Œè¿™ç§æ–¹å¼å‡çº§æ–¹å¼æ¯”è¾ƒéº»çƒ¦ï¼Œæ¶‰åŠåˆ°æ•°æ®è¿ç§»ï¼ŒIP æ›´æ¢æ“ä½œï¼Œå¯¹äºŽéƒ¨åˆ†ä¸šåŠ¡ä¸é€‚ç”¨ï¼Œé£Žé™©ä¸å¯æŽ§ã€‚ å¯ä»¥çœ‹åˆ°ï¼Œkubernetes ç¤¾åŒºçš„æ›´æ–°é€Ÿåº¦éžå¸¸å¿«ï¼Œåšå†³ä¸å»ºè®®è‡ªå·±ç»´æŠ¤ä¸€å¥— kubernetes ç‰ˆæœ¬ï¼Œæ¯æ¬¡å‡çº§å·¨éº»çƒ¦ï¼Œå°†æ‰€æœ‰ä¿®æ”¹è¿‡çš„ commit cherry-pick åˆ°æ¯ä¸ªæ–°ç‰ˆæœ¬ä¸Šï¼Œä¹Ÿå®¹æ˜“å‡ºé”™ï¼Œæœ‰äº›æ–°ç‰ˆæœ¬çš„æ”¹åŠ¨ä¹Ÿæ¯”è¾ƒå¤§ï¼Œä¹‹å‰ä¿®æ”¹è¿‡çš„åœ°æ–¹åœ¨æ–°ç‰ˆä¸­æœ‰å¯èƒ½å·²ç»è¢«ç§»é™¤æˆ–æ”¾åœ¨åˆ«çš„ä½ç½®äº†ã€‚ è¯¦ç»†çš„å‡çº§ç­–ç•¥å¯ä»¥å‚è€ƒï¼škubernetesé›†ç¾¤å‡çº§çš„æ­£ç¡®å§¿åŠ¿ã€‚ ç»“è®ºkubernetes æ¯ä¸‰ä¸ªæœˆå‘å¸ƒä¸€ä¸ªç‰ˆæœ¬ï¼Œç¤¾åŒºä»…ç»´æŠ¤æœ€æ–°çš„ä¸‰ä¸ªç‰ˆæœ¬ï¼Œä¸€ä¸ªç‰ˆæœ¬çš„ç»´æŠ¤æ—¶é—´ä¸º 9 ä¸ªæœˆï¼Œè¯·å°½é‡ä¿æŒç”Ÿäº§çŽ¯å¢ƒçš„ç‰ˆæœ¬åœ¨ç¤¾åŒºç»´æŠ¤èŒƒå›´å†…ï¼Œç‰ˆæœ¬å‡çº§æ—¶å°½é‡ä¿æŒå°ç‰ˆæœ¬æ»šåŠ¨å‡çº§ï¼Œä¸å»ºè®®è·¨å¤šä¸ªç‰ˆæœ¬å‡çº§ã€‚ å‚è€ƒï¼šhttps://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md]]></content>
      <tags>
        <tag>release version</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½¿ç”¨ kind éƒ¨ç½²å•æœºç‰ˆ kubernetes é›†ç¾¤]]></title>
    <url>%2F2019%2F09%2F06%2Fkind_deploy%2F</url>
    <content type="text"><![CDATA[kubernetes ä»Žä¸€å‘å¸ƒå¼€å§‹å…¶å­¦ä¹ é—¨æ§›å°±æ¯”è¾ƒé«˜ï¼Œé¦–å…ˆå°±æ˜¯éƒ¨ç½²éš¾ï¼Œç”¨æˆ·è¦æƒ³å­¦ä¹  kubernetes å¿…é¡»è¦è¿‡éƒ¨ç½²è¿™ä¸€å…³ï¼Œç¤¾åŒºä¹ŸæŽ¨å‡ºäº†å¤šä¸ªéƒ¨ç½²å·¥å…·å¸®åŠ©ç®€åŒ–é›†ç¾¤çš„éƒ¨ç½²ï¼Œç¤¾åŒºä¸­æŽ¨å‡ºçš„éƒ¨ç½²å·¥å…·ä¸»è¦ç›®æ ‡æœ‰ä¸¤å¤§ç±»ï¼Œéƒ¨ç½²æµ‹è¯•çŽ¯å¢ƒä¸Žç”Ÿäº§çŽ¯å¢ƒï¼Œæœ¬èŠ‚ä¸»è¦è®²è¿°æµ‹è¯•çŽ¯å¢ƒçš„éƒ¨ç½²ï¼Œç›®å‰ç¤¾åŒºå·²ç»æœ‰å¤šå¥—éƒ¨ç½²æ–¹æ¡ˆäº†ï¼š https://github.com/bsycorp/kind https://github.com/ubuntu/microk8s https://github.com/kinvolk/kube-spawn https://github.com/kubernetes/minikube https://github.com/danderson/virtuakube https://github.com/kubernetes-sigs/kubeadm-dind-cluster è€Œæœ¬æ–‡ä¸»è¦è®²è¿°ä½¿ç”¨ kindï¼ˆKubernetes In Dockerï¼‰éƒ¨ç½² k8s é›†ç¾¤ï¼Œå› ä¸º kind ä½¿ç”¨èµ·æ¥å®žåœ¨å¤ªç®€å•äº†ï¼Œç‰¹åˆ«é€‚ç”¨äºŽåœ¨æœ¬æœºéƒ¨ç½²æµ‹è¯•çŽ¯å¢ƒã€‚ kind çš„åŽŸç†å°±æ˜¯å°† k8s æ‰€éœ€è¦çš„æ‰€æœ‰ç»„ä»¶ï¼Œå…¨éƒ¨éƒ¨ç½²åœ¨ä¸€ä¸ª docker å®¹å™¨ä¸­ï¼Œåªéœ€è¦ä¸€ä¸ªé•œåƒå³å¯éƒ¨ç½²ä¸€å¥— k8s çŽ¯å¢ƒï¼Œå…¶åº•å±‚æ˜¯ä½¿ç”¨ kubeadm è¿›è¡Œéƒ¨ç½²ï¼ŒCRI ä½¿ç”¨ Containerdï¼ŒCNI ä½¿ç”¨ weaveã€‚ä¸‹é¢å°±æ¥çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ kind éƒ¨ç½²ä¸€å¥— kubernetes çŽ¯å¢ƒï¼Œåœ¨ä½¿ç”¨ kind å‰ä½ éœ€è¦ç¡®ä¿ç›®æ ‡æœºå™¨å·²ç»å®‰è£…äº† docker æœåŠ¡ã€‚ ä¸€ã€ä½¿ç”¨ kind éƒ¨ç½² k8s é›†ç¾¤ ä»¥ä¸‹å®‰è£…çŽ¯å¢ƒä¸º mac osã€‚ å®‰è£… kind ï¼š 123$ wget https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-darwin-amd64$ chmod +x kind-darwin-amd64$ mv kind-darwin-amd64 /usr/local/bin/kind ä½¿ç”¨ kind éƒ¨ç½² kubernetes é›†ç¾¤ï¼š 12345678910// é»˜è®¤çš„ cluster name ä¸º kindï¼Œå¯ä»¥ä½¿ç”¨ --name æŒ‡å®š$ kind create clusterCreating cluster &quot;kind&quot; ... âœ“ Ensuring node image (kindest/node:v1.15.3) ðŸ–¼ âœ“ Preparing nodes ðŸ“¦ âœ“ Creating kubeadm config ðŸ“œ âœ“ Starting control-plane ðŸ•¹ï¸ âœ“ Installing CNI ðŸ”Œ âœ“ Installing StorageClass ðŸ’¾Cluster creation complete. You can now use the cluster with: ä½¿ç”¨ kind create cluster å®‰è£…ï¼Œæ˜¯æ²¡æœ‰æŒ‡å®šä»»ä½•é…ç½®æ–‡ä»¶çš„å®‰è£…æ–¹å¼ã€‚ä»Žå®‰è£…æ‰“å°å‡ºçš„è¾“å‡ºæ¥çœ‹ï¼Œåˆ†ä¸º 6 æ­¥ï¼š å®‰è£…åŸºç¡€é•œåƒ kindest/node:v1.15.4ï¼Œè¿™ä¸ªé•œåƒé‡Œé¢åŒ…å«äº†æ‰€éœ€è¦çš„äºŒè¿›åˆ¶æ–‡ä»¶ã€é…ç½®æ–‡ä»¶ä»¥åŠ k8s å·¦å³ç»„ä»¶é•œåƒçš„ tar åŒ… å‡†å¤‡ nodeï¼Œæ£€æŸ¥çŽ¯å¢ƒã€å¯åŠ¨é•œåƒç­‰å·¥ä½œ ç”Ÿæˆ kubeadm çš„é…ç½®ï¼Œç„¶åŽä½¿ç”¨ kubeadm å®‰è£…ï¼Œå’Œç›´æŽ¥ä½¿ç”¨ kubeadm çš„æ­¥éª¤ç±»ä¼¼ å¯åŠ¨æœåŠ¡ éƒ¨ç½² CNI æ’ä»¶ï¼Œkind é»˜è®¤ä½¿ç”¨ weaveã€‚ åˆ›å»º StorageClassã€‚ 12345// æŸ¥çœ‹ kubeconfig path$ kind get kubeconfig-path/Users/feiyu/.kube/kind-config-kind$ export KUBECONFIG=&quot;$(kind get kubeconfig-path --name=&quot;kind&quot;)&quot; kind è¿˜æœ‰å¤šä¸ªå­å‘½ä»¤ï¼Œæ­¤å¤„ä¸å†ä¸€ä¸€è¯¦è§£ã€‚ 123456789101112// æŸ¥çœ‹é›†ç¾¤ä¿¡æ¯ï¼Œ$ kubectl cluster-infoKubernetes master is running at https://127.0.0.1:55387KubeDNS is running at https://127.0.0.1:55387/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use &apos;kubectl cluster-info dump&apos;.// æŸ¥çœ‹æœ¬åœ°çš„ kind å®¹å™¨$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe26545538cc7 kindest/node:v1.15.3 &quot;/usr/local/bin/entrâ€¦&quot; 15 minutes ago Up 15 minutes 55387/tcp, 127.0.0.1:55387-&gt;6443/tcp kind-control-plane å¯ä»¥çœ‹åˆ°ï¼Œkind å®¹å™¨æš´éœ²çš„ 6443 ç«¯å£æ˜ å°„åœ¨æœ¬æœºçš„ä¸€ä¸ªéšæœºç«¯å£(55387)ä¸Šã€‚ 1234567891011121314151617181920212223242526272829303132333435// æŸ¥çœ‹ node çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥çœ‹åˆ° cni ä¸º containerd$ kubectl describe node kind-control-plane... Container Runtime Version: containerd://1.2.6-0ubuntu1 Kubelet Version: v1.15.3 Kube-Proxy Version: v1.15.3PodCIDR: 10.244.0.0/24ExternalID: kind-control-plane...# è¿›å…¥ kind å®¹å™¨æŸ¥çœ‹ k8s çš„é…ç½®ï¼Œå’Œå•ç‹¬ä½¿ç”¨ kubeadm æ—¶ä¸€è‡´$ docker exec -it e26545538cc bashroot@kind-control-plane:~# ls /etc/kubernetes/admin.conf controller-manager.conf kubelet.conf manifests pki scheduler.confroot@kind-control-plane:~# ls /etc/kubernetes/manifests/etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml# æŸ¥çœ‹ cni é…ç½®root@kind-control-plane:/etc/kubernetes# cat /var/lib/kubelet/kubeadm-flags.envKUBELET_KUBEADM_ARGS=&quot;--container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --fail-swap-on=false --node-ip=172.17.0.2&quot;# æŸ¥çœ‹å®¹å™¨çš„çŠ¶æ€root@kind-control-plane:~# crictl podsPOD ID CREATED STATE NAME NAMESPACE ATTEMPTfc8700af77ca2 About an hour ago Ready coredns-5c98db65d4-bjxl2 kube-system 06378297d32811 About an hour ago Ready coredns-5c98db65d4-q2drh kube-system 0124b42a35e0d1 About an hour ago Ready kube-proxy-99nc9 kube-system 054b9511069534 About an hour ago Ready kindnet-xz8dp kube-system 061cb720ddece8 About an hour ago Ready etcd-kind-control-plane kube-system 04514b98de1a44 About an hour ago Ready kube-scheduler-kind-control-plane kube-system 09a29dbebc8dd1 About an hour ago Ready kube-controller-manager-kind-control-plane kube-system 0ab028c5f5a3e5 About an hour ago Ready kube-apiserver-kind-control-plane kube-system 0 åˆ é™¤é›†ç¾¤ï¼š 1$ kind delete cluster kind ä¹Ÿæ”¯æŒåˆ›å»ºå¤š master ä»¥åŠå¤š work èŠ‚ç‚¹çš„é›†ç¾¤ï¼Œéœ€è¦è‡ªå®šä¹‰ yaml é…ç½®ï¼š 12345678910111213# a cluster with 3 control-plane nodes and 3 workerskind: ClusterapiVersion: kind.sigs.k8s.io/v1alpha3nodes:- role: control-plane- role: control-plane- role: control-plane- role: worker- role: worker- role: worker// åˆ›å»ºé›†ç¾¤æŒ‡å®š config$ kind create cluster --config kind.yaml kind è¿˜æ”¯æŒè‡ªå®šä¹‰æ˜ å°„çš„ç«¯å£å·ã€æ”¯æŒä½¿ç”¨è‡ªå®šä¹‰é•œåƒä»“åº“ã€æ”¯æŒå¯ç”¨ Feature Gates ç­‰å¤šä¸ªåŠŸèƒ½ï¼Œè¯¦ç»†çš„ä½¿ç”¨è¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ quick-startã€‚ äºŒã€æœ¬åœ°æµ‹è¯•æ—¢ç„¶ kind ä¸èƒ½ç”¨ä½œç”Ÿäº§çŽ¯å¢ƒï¼Œé‚£æ€Žä¹ˆåœ¨æœ¬åœ°æµ‹è¯•æ—¶ä½¿ç”¨å‘¢ï¼Ÿç”±äºŽ k8s çš„æ–°ç‰ˆå·²ç»å…¨é¢å¯ç”¨äº† TLSï¼Œä¸å†æ”¯æŒéžå®‰å…¨ç«¯å£ï¼Œè®¿é—® APIServer çš„æŽ¥å£éƒ½éœ€è¦è®¤è¯ï¼Œä½†æ˜¯æœ¬åœ°æµ‹è¯•ä¸éœ€è¦é‚£ä¹ˆéº»çƒ¦ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œä¸ºåŒ¿åç”¨æˆ·è®¾ç½®è®¿é—®æƒé™å³å¯ã€‚ 1234567891011121314// ä¸ºåŒ¿åç”¨æˆ·å…³è” RBAC è§„åˆ™$ kubectl create clusterrolebinding system:anonymous --clusterrole=cluster-admin --user=system:anonymous// è¯·æ±‚ç›¸å…³çš„ API$ curl -k https://127.0.0.1:55387/api/v1/nodes&#123; &quot;kind&quot;: &quot;NodeList&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: &#123; &quot;selfLink&quot;: &quot;/api/v1/nodes&quot;, &quot;resourceVersion&quot;: &quot;11844&quot; &#125;, &quot;items&quot;: [ ...]]></content>
      <tags>
        <tag>kind</tag>
        <tag>deploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-on-kube-operator å¼€å‘(ä¸‰)]]></title>
    <url>%2F2019%2F09%2F01%2Fkube_on_kube_operator_3%2F</url>
    <content type="text"><![CDATA[kube-on-kube-operator å¼€å‘(ä¸€) kube-on-kube-operator å¼€å‘(äºŒ) æœ¬æ–‡æ˜¯ä»‹ç» kubernetes-operator å¼€å‘çš„ç¬¬ä¸‰ç¯‡ï¼Œå‰å‡ ç¯‡å·²ç»æåˆ°è¿‡ kubernetes-operator çš„ä¸»è¦ç›®æ ‡æ˜¯å®žçŽ°ä»¥ä¸‹ä¸‰ç§åœºæ™¯ä¸­çš„é›†ç¾¤ç®¡ç†ï¼š kube-on-kube kube-to-kube kube-to-cloud-kube ç›®å‰ç¬”è€…ä¸»è¦åœ¨å¼€å‘ kube-to-kubeï¼Œè¿™ä¸€èŠ‚ä¼šä»‹ç» kube-to-kube ä¸­å¦‚ä½•ä½¿ç”¨äºŒè¿›åˆ¶æ–¹å¼éƒ¨ç½²ä¸€ä¸ªé›†ç¾¤ï¼Œé—®ä»€ä¹ˆè¦å…ˆæ”¯æŒéƒ¨ç½²äºŒè¿›åˆ¶é›†ç¾¤å‘¢ï¼Œå¯ä»¥å‚è€ƒä¹‹å‰çš„æ–‡ç« ã€‚ç›®å‰ kubernetes-operator ä¸­éƒ¨ç½²é›†ç¾¤æ˜¯é€šè¿‡ ansible è°ƒç”¨ç¬”è€…å†™çš„ä¸€äº›è„šæœ¬éƒ¨ç½²çš„ï¼Œç”±äºŽ kubernetes äºŒè¿›åˆ¶æ–‡ä»¶æ¯”è¾ƒå¤§ï¼Œæš‚æ—¶ä»…æ”¯æŒç¦»çº¿éƒ¨ç½²ï¼Œéƒ¨ç½²å‰è¯·ä¸‹è½½å¥½æ‰€éœ€çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œç¬”è€…ä¹Ÿæä¾›äº†éƒ¨ç½² v1.14 éœ€è¦çš„æ‰€æœ‰äºŒè¿›åˆ¶æ–‡ä»¶ã€é•œåƒã€yaml ç­‰ã€‚ äºŒè¿›åˆ¶å®‰è£… kubernetes æœ€å›°éš¾çš„åœ°æ–¹å°±åœ¨äºŽå…¶å¤æ‚çš„è®¤è¯(Authentication)åŠé‰´æƒ(Authorization)æœºåˆ¶ï¼Œä¸Šç¯‡æ–‡ç« å·²ç»ä»‹ç»äº† kubernetes ä¸­çš„è®¤è¯ä¸Žé‰´æƒæœºåˆ¶ä»¥åŠå…¶ä¸­çš„è¯ä¹¦é“¾ï¼Œè‹¥å®‰è£…è¿‡ç¨‹ä¸­æœ‰ç–‘é—®è¯·å‚è€ƒ æµ…æž kubernetes çš„è®¤è¯ä¸Žé‰´æƒæœºåˆ¶ã€‚ ä½¿ç”¨ kubernetes-operator ç®¡ç†é›†ç¾¤æ—¶é¦–é€‰éœ€è¦æœ‰ä¸€ä¸ªå…ƒé›†ç¾¤ï¼Œå…ƒé›†ç¾¤å¯ä»¥ä½¿ç”¨ minkube æˆ–è€… kind éƒ¨ç½²ä¸€ä¸ªå•æœºç‰ˆé›†ç¾¤ï¼Œç„¶åŽå°† kubernetes-operator éƒ¨ç½²åˆ°è¯¥é›†ç¾¤ä¸­å†é€šè¿‡åˆ›å»º CR æ¥éƒ¨ç½²ä¸€ä¸ªä¸šåŠ¡é›†ç¾¤ï¼Œæœ€åŽä½¿ç”¨è¯¥ä¸šåŠ¡é›†ç¾¤ä½œä¸ºå…ƒé›†ç¾¤å³å¯ï¼Œæˆ–è€…ä¹Ÿå¯ä»¥ä½¿ç”¨ kubernetes-operator ä¸­éƒ¨ç½²ä¸šåŠ¡é›†ç¾¤çš„æ–¹å¼æ¥éƒ¨ç½²å…ƒé›†ç¾¤ã€‚ éƒ¨ç½²é›†ç¾¤å‰è¯·å…ˆå…‹éš† https://github.com/gosoon/kubernetes-operator å’Œ https://github.com/gosoon/kubernetes-utils é¡¹ç›®ï¼Œéƒ¨ç½²é›†ç¾¤æ‰€éœ€è¦çš„ä¸€äº›å·¥å…·ã€é…ç½®ä»¥åŠ bin æ–‡ä»¶éƒ½å­˜æ”¾åœ¨è¿™ä¸¤ä¸ªé¡¹ç›®ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå·±çš„é…ç½®ã€‚ å‡†å¤‡çŽ¯å¢ƒç¦ç”¨é˜²ç«å¢™ï¼š 12$ systemctl stop firewalld$ systemctl disable firewalld ç¦ç”¨ SELinuxï¼š 12$ setenforce 0$ sed -i &apos;s/^SELINUX=enforcing$/SELINUX=permissive/&apos; /etc/selinux/config å…³é—­ swapï¼š 1swapoff -a ä¿®æ”¹å†…æ ¸å‚æ•°ï¼š 12345cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system é…ç½® CA åŠåˆ›å»º TLS è¯ä¹¦å®‰è£…è¯ä¹¦ç”Ÿæˆå·¥å…·ï¼Œæœ¬æ–‡ä½¿ç”¨ cfssl 1$ cp kubernetes-utils/scripts/bin/certs/* /usr/bin/ etcd123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475$ cat &lt;&lt; EOF &gt; etcd-root-ca-csr.json&#123; &quot;CN&quot;: &quot;etcd-root-ca&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;ca&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;&#125;EOF$ cat &lt;&lt; EOF &gt; etcd-gencert.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125;&#125;EOF$ cat &lt;&lt; EOF &gt; etcd-csr.json&#123; &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;CN&quot;: &quot;etcd&quot;&#125;EOF$ cat &lt;&lt; EOF &gt; config-etcd-peer.json&#123; &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;CN&quot;: &quot;etcd&quot;&#125;EOF 1234567891011121314151617181920$ cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson -bare output/ca// æŒ‡å®š etcd hostsï¼Œetcd server å’Œ etcd peer ä¸­å¿…é¡»åŒ…å«æ‰€æœ‰ etcd çš„ hostsï¼Œegï¼šETCD_HOSTS=&quot;10.0.4.15ï¼Œ10.0.2.15&quot;# etcd server$ cfssl gencert \ -ca=output/ca.pem \ -ca-key=output/ca-key.pem \ -config=ca-config.json \ -hostname=127.0.0.1,$&#123;ETCD_HOSTS&#125; \ -profile=server \ server.json | cfssljson -bare output/etcd-server # etcd peer$ cfssl gencert \ -ca=output/ca.pem \ -ca-key=output/ca-key.pem \ -config=ca-config.json \ -hostname=127.0.0.1,$&#123;ETCD_HOSTS&#125; \ -profile=peer \ server.json | cfssljson -bare output/etcd-peer ç”Ÿæˆè¯ä¹¦åŽåè§£ etcd server å’Œ peer è¯ä¹¦æ ¡éªŒ ip æ˜¯å¦æ­£ç¡®ï¼š 1$ cfssl certinfo -cert etcd-peer.pem masterç”±äºŽ master ç»„ä»¶çš„ CSR é…ç½®ä¸Ž kubernetes ä¸­çš„è®¤è¯ä¸Žé‰´æƒç›¸å…³è”ï¼Œéœ€è¦ä¸¥æ ¼æŒ‰ç…§ kubernetes ä¸­é»˜è®¤çš„ RBAC è¿›è¡Œé…ç½®ï¼Œæ¯ä¸ªç»„ä»¶éƒ½æœ‰é»˜è®¤çš„ user æˆ–è€… groupã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209$ cat &lt;&lt; EOF &gt; ca-csr.json&#123; &quot;CN&quot;: &quot;Kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;Kubernetes&quot;, &quot;OU&quot;: &quot;Shanghai&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF$ cat &lt;&lt; EOF &gt; ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [&quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot;], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125;EOF// kube-apiserver csr$ cat &lt;&lt; EOF &gt; kube-apiserver-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;Kubernetes&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// kube-controller-manager csr$ cat &lt;&lt; EOF &gt; kube-controller-manager-csr.json&#123; &quot;CN&quot;: &quot;system:kube-controller-manager&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:kube-controller-manager&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// kube-scheduler csr$ cat &lt;&lt; EOF &gt; kube-scheduler-csr.json&#123; &quot;CN&quot;: &quot;system:kube-scheduler&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:kube-scheduler&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// kubelet csrï¼Œè¯·æ›¿æ¢ nodeName$ cat &lt;&lt; EOF &gt; kubelet-csr.json&#123; &quot;CN&quot;: &quot;system:node:&lt;nodeName&gt;&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:nodes&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// apiserver client csr$ cat &lt;&lt; EOF &gt; apiserver-kubelet-client-csr.json&#123; &quot;CN&quot;: &quot;system:kubelet-api-admin&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// kube-proxy csr$ cat &lt;&lt; EOF &gt; kube-proxy-csr.json&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:node-proxier&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// kubectl csr$ cat &lt;&lt; EOF &gt; admin-csr.json&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF$ cfssl gencert -initca ca-csr.json | cfssljson -bare output/caä¸ºäº†ä¿è¯å®¢æˆ·ç«¯ä¸Ž Kubernetes API çš„è®¤è¯ï¼ŒKubernetes API Server å‡­è¯ä¸­å¿…éœ€åŒ…å« master çš„é™æ€ IP åœ°å€,åœ¨ hostname ä¸­æŒ‡å®š# apiserver$ cfssl gencert \ -ca=output/ca.pem \ -ca-key=output/ca-key.pem \ -config=ca-config.json \ -hostname=10.250.0.1,$&#123;MASTER_HOSTS&#125;,$&#123;MASTER_VIP&#125;,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc \ -profile=kubernetes \ kube-apiserver-csr.json | cfssljson -bare output/kube-apiserver# kubeletfor node in `echo $&#123;NODE_HOSTS&#125; | tr &apos;,&apos; &apos; &apos;`;do cfssl gencert \ -ca=output/ca.pem \ -ca-key=output/ca-key.pem \ -config=ca-config.json \ -hostname=$&#123;NODE_HOSTS&#125; \ -profile=kubernetes \ kubelet-csr.json | cfssljson -bare output/kubeletdone# other componentfor component in kube-controller-manager kube-scheduler kube-proxy apiserver-kubelet-client admin service-account;do cfssl gencert \ -ca=output/ca.pem \ -ca-key=output/ca-key.pem \ -config=ca-config.json \ -profile=kubernetes \ $&#123;component&#125;-csr.json | cfssljson -bare output/$&#123;component&#125;done ç”Ÿæˆ kubeconfig12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697// æ›¿æ¢ apiserver KUBE_APISERVER=&quot;https://10.0.4.15:6443&quot;CERTS_DIR=&quot;/etc/kubernetes/ssl&quot;# ç”Ÿæˆ kubectl é…ç½®æ–‡ä»¶echo &quot;Create kubectl kubeconfig...&quot;kubectl config set-cluster kubernetes \ --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=output/kubectl.kubeconfigkubectl config set-credentials &quot;system:masters&quot; \ --client-certificate=$&#123;CERTS_DIR&#125;/admin.pem \ --client-key=$&#123;CERTS_DIR&#125;/admin-key.pem \ --embed-certs=true \ --kubeconfig=output/kubectl.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=system:masters \ --kubeconfig=output/kubectl.kubeconfigkubectl config use-context default --kubeconfig=output/kubectl.kubeconfig# ç”Ÿæˆ kube-controller-manager é…ç½®æ–‡ä»¶echo &quot;Create kube-controller-manager kubeconfig...&quot;kubectl config set-cluster kubernetes \ --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=output/kube-controller-manager.kubeconfigkubectl config set-credentials &quot;system:kube-controller-manager&quot; \ --client-certificate=$&#123;CERTS_DIR&#125;/kube-controller-manager.pem \ --client-key=$&#123;CERTS_DIR&#125;/kube-controller-manager-key.pem \ --embed-certs=true \ --kubeconfig=output/kube-controller-manager.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=system:kube-controller-manager \ --kubeconfig=output/kube-controller-manager.kubeconfigkubectl config use-context default --kubeconfig=output/kube-controller-manager.kubeconfig# ç”Ÿæˆ kube-scheduler é…ç½®æ–‡ä»¶echo &quot;Create kube-scheduler kubeconfig...&quot;kubectl config set-cluster kubernetes \ --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=output/kube-scheduler.kubeconfigkubectl config set-credentials &quot;system:kube-scheduler&quot; \ --client-certificate=$&#123;CERTS_DIR&#125;/kube-scheduler.pem \ --client-key=$&#123;CERTS_DIR&#125;/kube-scheduler-key.pem \ --embed-certs=true \ --kubeconfig=output/kube-scheduler.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=system:kube-scheduler \ --kubeconfig=output/kube-scheduler.kubeconfigkubectl config use-context default --kubeconfig=output/kube-scheduler.kubeconfig# ç”Ÿæˆ kubelet é…ç½®æ–‡ä»¶,éœ€è¦æ·»åŠ å¯¹åº”çš„ nodeNameecho &quot;Create kubelet kubeconfig...&quot;kubectl config set-cluster kubernetes \ --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfigkubectl config set-credentials system:node:$&#123;node&#125; \ --client-certificate=$&#123;CERTS_DIR&#125;/kubelet.pem \ --client-key=$&#123;CERTS_DIR&#125;/kubelet-key.pem \ --embed-certs=true \ --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=system:node:$&#123;node&#125; \ --kubeconfig=$&#123;CERTS_DIR&#125;/kubelet-$&#123;node&#125;.kubeconfigkubectl config use-context default --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfig# ç”Ÿæˆ kube-proxy é…ç½®æ–‡ä»¶echo &quot;Create kube-proxy kubeconfig...&quot;kubectl config set-cluster kubernetes \ --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=output/kube-proxy.kubeconfigkubectl config set-credentials &quot;system:kube-proxy&quot; \ --client-certificate=$&#123;CERTS_DIR&#125;/kube-proxy.pem \ --client-key=$&#123;CERTS_DIR&#125;/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=output/kube-proxy.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=system:kube-proxy \ --kubeconfig=output/kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=output/kube-proxy.kubeconfig éƒ¨ç½²éƒ¨ç½² etcdæ‹·è´è¯ä¹¦æ–‡ä»¶ï¼š 1$ cp output/* /etc/etcd/ssl/ æ‹·è´ bin æ–‡ä»¶ï¼š 1$ cp kubernetes-utils/scripts/bin/etcd_v3.3.13/* /usr/bin/ æ‹·è´é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­çš„ ip éœ€è¦æ‰‹åŠ¨æ›¿æ¢æŽ‰ï¼š 1$ cp kubernetes-operator/scripts/config/etcd/etcd.conf /etc/etcd/ éƒ¨ç½² k8s master ç»„ä»¶æ‹·è´è¯ä¹¦æ–‡ä»¶ï¼š 1$ cp output/* /etc/kubernetes/ssl/ æ‹·è´ bin æ–‡ä»¶ï¼š 1$ cp kubernetes-utils/scripts/bin/kubernetes_v1.14.0/* /usr/bin/ æ‹·è´é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­çš„ ip éœ€è¦æ‰‹åŠ¨æ›¿æ¢æŽ‰ï¼š 1$ cp kubernetes-operator/scripts/config/master/* /etc/kubernetes/ éƒ¨ç½² k8s node ç»„ä»¶éƒ¨ç½² dockerï¼Œæ‹·è´ bin æ–‡ä»¶ï¼š 1$ cp kubernetes-utils/scripts/bin/docker-ce-18.06.1.ce/* /usr/bin/ æ‹·è´è¯ä¹¦æ–‡ä»¶ï¼š 1$ cp output/* /etc/kubernetes/ssl/ æ‹·è´é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­çš„ ip éœ€è¦æ‰‹åŠ¨æ›¿æ¢æŽ‰ï¼š 1$ cp kubernetes-operator/scripts/config/node/* /etc/kubernetes/ åˆ›å»º systemd æ–‡ä»¶æ‹·è´æ‰€æœ‰æœåŠ¡çš„ systemd æ–‡ä»¶ï¼š æ‹·è´é…ç½®æ–‡ä»¶ï¼Œé…ç½®æ–‡ä»¶ä¸­çš„ ip éœ€è¦æ‰‹åŠ¨æ›¿æ¢æŽ‰ï¼š 1$ cp kubernetes-operator/scripts/systemd/* /usr/lib/systemd/system/ å¯åŠ¨æœåŠ¡é¦–å…ˆå¯åŠ¨ etcd æœåŠ¡ï¼Œetcd æ‰€éƒ¨ç½²çš„å‡ ä¸ªèŠ‚ç‚¹éœ€è¦åŒæ—¶å¯åŠ¨ï¼Œå¦åˆ™æœåŠ¡ä¼šå¯åŠ¨å¤±è´¥ã€‚ ç„¶åŽä¾æ¬¡å¯åŠ¨ master ä¸Šçš„ç»„ä»¶å’Œ node ä¸Šçš„ç»„ä»¶ã€‚ æ€»ç»“æœ¬æ–‡ä¸»è¦è®²è¿°äº† kubernetes-operator ä¸­ kube-to-kube éƒ¨ç½²é›†ç¾¤çš„æ–¹å¼ï¼Œä»‹ç»äº†ä¸»è¦çš„éƒ¨ç½²æ­¥éª¤ï¼Œæ–‡ä¸­éƒ¨ç½²é›†ç¾¤æ‰€æœ‰çš„æ“ä½œéƒ½æä¾›äº†è„šæœ¬çš„æ–¹å¼ï¼šhttps://github.com/gosoon/kubernetes-operator/tree/master/scriptsã€‚ kube-to-kube çš„éƒ¨ç½²æ–¹å¼æš‚æ—¶æ˜¯ä»¥ ansible + è‡ªå®šä¹‰è„šæœ¬çš„æ–¹å¼éƒ¨ç½²ï¼Œéƒ¨ç½²æ–¹å¼ä¹Ÿåœ¨æŒç»­æ›´æ–°ä¸Žå®Œå–„ä¸­ã€‚æŽ¥ä¸‹æ¥ä¼šç»§ç»­å¼€å‘ kube-on-kube çš„éƒ¨ç½²æ–¹å¼ï¼Œkube-on-kube ä¼šå°†ä¸šåŠ¡é›†ç¾¤çš„ master ç»„ä»¶éƒ¨ç½²åœ¨å…ƒé›†ç¾¤ä¸­ï¼Œkube-on-kube æ–¹å¼æš‚æ—¶ä¼šé‡‡ç”¨å¯¹ kubeadm å°è£…çš„å½¢å¼è¿›è¡Œéƒ¨ç½²ã€‚ ç›¸å…³æŽ¨èkube-on-kube-operator å¼€å‘(äºŒ)kube-on-kube-operator å¼€å‘(ä¸€)]]></content>
      <tags>
        <tag>operator</tag>
        <tag>kube-on-kube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æµ…æž kubernetes çš„è®¤è¯ä¸Žé‰´æƒæœºåˆ¶]]></title>
    <url>%2F2019%2F08%2F18%2Fk8s_auth_rbac%2F</url>
    <content type="text"><![CDATA[ç¬”è€…æœ€åˆæŽ¥è§¦ kubernetes æ—¶ä½¿ç”¨çš„æ˜¯ v1.4 ç‰ˆæœ¬ï¼Œé›†ç¾¤é—´çš„é€šä¿¡ä»…ä½¿ç”¨ 8080 ç«¯å£ï¼Œè®¤è¯ä¸Žé‰´æƒæœºåˆ¶è¿˜æœªå¾—åˆ°å®Œå–„ï¼Œåˆ°åŽæ¥å¼€å§‹ä½¿ç”¨ static token ä½œä¸ºè®¤è¯æœºåˆ¶ï¼Œç›´åˆ° v1.6 æ—¶æ‰å¼€å§‹ä½¿ç”¨ TLS è®¤è¯ã€‚éšç€ç¤¾åŒºçš„å‘å±•ï¼Œkubernetes çš„è®¤è¯ä¸Žé‰´æƒæœºåˆ¶å·²ç»è¶Šæ¥è¶Šå®Œå–„ï¼Œæ–°ç‰ˆæœ¬å·²ç»å…¨é¢è¶‹äºŽ TLS + RBAC é…ç½®ï¼Œä½†å…¶è®¤è¯ä¸Žé‰´æƒæœºåˆ¶ä¹Ÿæžå…¶å¤æ‚ï¼Œæœ¬æ–‡å°†ä¼šå¸¦ä½ ä¸€æ­¥æ­¥äº†è§£ã€‚ kubernetes é›†ç¾¤çš„æ‰€æœ‰æ“ä½œåŸºæœ¬ä¸Šéƒ½æ˜¯é€šè¿‡ apiserver è¿™ä¸ªç»„ä»¶è¿›è¡Œçš„ï¼Œå®ƒæä¾› HTTP RESTful å½¢å¼çš„ API ä¾›é›†ç¾¤å†…å¤–å®¢æˆ·ç«¯è°ƒç”¨ã€‚kubernetes å¯¹äºŽè®¿é—® API æ¥è¯´æä¾›äº†ä¸‰ä¸ªæ­¥éª¤çš„å®‰å…¨æŽªæ–½ï¼šè®¤è¯ã€æŽˆæƒã€å‡†å…¥æŽ§åˆ¶ï¼Œå½“ç”¨æˆ·ä½¿ç”¨ kubectlï¼Œclient-go æˆ–è€… REST API è¯·æ±‚ apiserver æ—¶ï¼Œéƒ½è¦ç»è¿‡ä»¥ä¸Šä¸‰ä¸ªæ­¥éª¤çš„æ ¡éªŒã€‚è®¤è¯è§£å†³çš„é—®é¢˜æ˜¯è¯†åˆ«ç”¨æˆ·çš„èº«ä»½ï¼Œé‰´æƒæ˜¯ä¸ºäº†è§£å†³ç”¨æˆ·æœ‰å“ªäº›æƒé™ï¼Œå‡†å…¥æŽ§åˆ¶æ˜¯ä½œç”¨äºŽ kubernetes ä¸­çš„å¯¹è±¡ï¼Œé€šè¿‡åˆç†çš„æƒé™ç®¡ç†ï¼Œèƒ½å¤Ÿä¿è¯ç³»ç»Ÿçš„å®‰å…¨å¯é ã€‚è®¤è¯æŽˆæƒè¿‡ç¨‹åªå­˜åœ¨ HTTPS å½¢å¼çš„ API ä¸­ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æžœå®¢æˆ·ç«¯ä½¿ç”¨ HTTP è¿žæŽ¥åˆ° apiserverï¼Œæ˜¯ä¸ä¼šè¿›è¡Œè®¤è¯æŽˆæƒçš„ï¼Œç„¶è€Œ apiserver çš„éžå®‰å…¨è®¤è¯ç«¯å£ 8080 å·²ç»åœ¨ v1.12 ä¸­åºŸå¼ƒäº†ï¼Œæœªæ¥å°†å…¨é¢ä½¿ç”¨ HTTPSã€‚ é¦–å…ˆæ¥çœ‹ä¸€ä¸‹ kubernetes ä¸­çš„è®¤è¯ã€æŽˆæƒä»¥åŠè®¿é—®æŽ§åˆ¶æœºåˆ¶ã€‚ kubernetes çš„è®¤è¯æœºåˆ¶(Authentication)kubernetes ç›®å‰æ‰€æœ‰çš„è®¤è¯ç­–ç•¥å¦‚ä¸‹æ‰€ç¤ºï¼š X509 client certs Static Token File Bootstrap Tokens Static Password File Service Account Tokens OpenId Connect Tokens Webhook Token Authentication Authticating Proxy Anonymous requests User impersonation Client-go credential plugins å¯ä»¥çœ‹åˆ°ï¼Œkubernetes çš„è®¤è¯æœºåˆ¶éžå¸¸å¤šï¼Œè¦æƒ³ä¸€ä¸ªä¸ªæžæ¸…æ¥šä¹Ÿç»éžæ˜“äº‹ï¼Œæœ¬æ–‡ä»…åˆ†æžå‡ ä¸ªæ¯”è¾ƒé‡è¦ä¸”ä½¿ç”¨å¹¿æ³›çš„è®¤è¯æœºåˆ¶ã€‚ X509 client certsX509æ˜¯ä¸€ç§æ•°å­—è¯ä¹¦çš„æ ¼å¼æ ‡å‡†ï¼ŒçŽ°åœ¨ HTTPS ä¾èµ–çš„ SSL è¯ä¹¦ä½¿ç”¨çš„å°±æ˜¯ä½¿ç”¨çš„ X509 æ ¼å¼ã€‚X509 å®¢æˆ·ç«¯è¯ä¹¦è®¤è¯æ–¹å¼æ˜¯ kubernetes æ‰€æœ‰è®¤è¯ä¸­ä½¿ç”¨æœ€å¤šçš„ä¸€ç§ï¼Œç›¸å¯¹æ¥è¯´ä¹Ÿæ˜¯æœ€å®‰å…¨çš„ä¸€ç§ï¼Œkubernetes çš„ä¸€äº›éƒ¨ç½²å·¥å…· kubeadmã€minkube ç­‰éƒ½æ˜¯åŸºäºŽè¯ä¹¦çš„è®¤è¯æ–¹å¼ã€‚å®¢æˆ·ç«¯è¯ä¹¦è®¤è¯å«ä½œ TLS åŒå‘è®¤è¯ï¼Œä¹Ÿå°±æ˜¯æœåŠ¡å™¨å®¢æˆ·ç«¯äº’ç›¸éªŒè¯è¯ä¹¦çš„æ­£ç¡®æ€§ï¼Œåœ¨éƒ½æ­£ç¡®çš„æƒ…å†µä¸‹åè°ƒé€šä¿¡åŠ å¯†æ–¹æ¡ˆã€‚ç›®å‰æœ€å¸¸ç”¨çš„ X509 è¯ä¹¦åˆ¶ä½œå·¥å…·æœ‰ opensslã€cfssl ç­‰ã€‚ Service Account Tokensæœ‰äº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨ pod å†…éƒ¨è®¿é—® apiserverï¼ŒèŽ·å–é›†ç¾¤çš„ä¿¡æ¯ï¼Œç”šè‡³å¯¹é›†ç¾¤è¿›è¡Œæ”¹åŠ¨ã€‚é’ˆå¯¹è¿™ç§æƒ…å†µï¼Œkubernetes æä¾›äº†ä¸€ç§ç‰¹æ®Šçš„è®¤è¯æ–¹å¼ï¼šserviceaccountsã€‚ serviceaccounts æ˜¯é¢å‘ namespace çš„ï¼Œæ¯ä¸ª namespace åˆ›å»ºçš„æ—¶å€™ï¼Œkubernetes ä¼šè‡ªåŠ¨åœ¨è¿™ä¸ª namespace ä¸‹é¢åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ serviceaccountsï¼›å¹¶ä¸”è¿™ä¸ª serviceaccounts åªèƒ½è®¿é—®è¯¥ namespace çš„èµ„æºã€‚serviceaccounts å’Œ podã€serviceã€deployment ä¸€æ ·æ˜¯ kubernetes é›†ç¾¤ä¸­çš„ä¸€ç§èµ„æºï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥åˆ›å»ºè‡ªå·±çš„ serviceaccountsã€‚ serviceaccounts ä¸»è¦åŒ…å«äº†ä¸‰ä¸ªå†…å®¹ï¼šnamespaceã€token å’Œ caï¼Œæ¯ä¸ª serviceaccounts ä¸­éƒ½å¯¹åº”ä¸€ä¸ª secretsï¼Œnamespaceã€token å’Œ ca ä¿¡æ¯éƒ½æ˜¯ä¿å­˜åœ¨ secrets ä¸­ä¸”éƒ½é€šè¿‡ base64 ç¼–ç çš„ã€‚namespace æŒ‡å®šäº† pod æ‰€åœ¨çš„ namespaceï¼Œca ç”¨äºŽéªŒè¯ apiserver çš„è¯ä¹¦ï¼Œtoken ç”¨ä½œèº«ä»½éªŒè¯ï¼Œå®ƒä»¬éƒ½é€šè¿‡ mount çš„æ–¹å¼ä¿å­˜åœ¨ pod çš„æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œå…¶ä¸‰è€…éƒ½æ˜¯ä¿å­˜åœ¨ /var/run/secrets/kubernetes.io/serviceaccount/ç›®å½•ä¸‹ã€‚ å…³äºŽ serviceaccounts çš„é…ç½®å¯ä»¥å‚è€ƒå®˜æ–¹çš„ Configure Service Accounts for Pods æ–‡æ¡£ã€‚ è®¤è¯æœºåˆ¶çš„å®˜æ–¹æ–‡æ¡£ï¼Œè¯·å‚è€ƒï¼šhttps://kubernetes.io/docs/reference/access-authn-authz/authentication/ å°ç»“ï¼škubernetes ä¸­æœ‰å¤šç§è®¤è¯æ–¹å¼ï¼Œä¸Šé¢è®²äº†æœ€å¸¸ä½¿ç”¨çš„ä¸¤ç§è®¤è¯æ–¹å¼ï¼ŒX509 client certs è®¤è¯æ–¹å¼æ˜¯ç”¨åœ¨ä¸€äº›å®¢æˆ·ç«¯è®¿é—® apiserver ä»¥åŠé›†ç¾¤ç»„ä»¶ä¹‹é—´è®¿é—®æ—¶ä½¿ç”¨ï¼Œæ¯”å¦‚ kubectl è¯·æ±‚ apiserver æ—¶ã€‚serviceaccounts æ˜¯ç”¨åœ¨ pod ä¸­è®¿é—® apiserver æ—¶è¿›è¡Œè®¤è¯çš„ï¼Œæ¯”å¦‚ä½¿ç”¨è‡ªå®šä¹‰ controller æ—¶ã€‚ è®¤è¯è§£å†³çš„é—®é¢˜æ˜¯è¯†åˆ«ç”¨æˆ·çš„èº«ä»½ï¼Œé‚£ kubernetes ä¸­éƒ½æœ‰å“ªå‡ ç§ç”¨æˆ·ï¼Ÿç›®å‰ kubernetes ä¸­çš„ç”¨æˆ·åˆ†ä¸ºå†…éƒ¨ç”¨æˆ·å’Œå¤–éƒ¨ç”¨æˆ·ï¼Œå†…éƒ¨ç”¨æˆ·æŒ‡åœ¨ kubernetes é›†ç¾¤ä¸­çš„ pod è¦è®¿é—® apiserver æ—¶æ‰€ä½¿ç”¨çš„ï¼Œä¹Ÿå°±æ˜¯ serviceaccountsï¼Œå†…éƒ¨ç”¨æˆ·éœ€è¦åœ¨ kubernetes ä¸­åˆ›å»ºã€‚å¤–éƒ¨ç”¨æˆ·æŒ‡ kubectl ä»¥åŠä¸€äº›å®¢æˆ·ç«¯å·¥å…·è®¿é—® apiserver æ—¶æ‰€éœ€è¦è®¤è¯çš„ç”¨æˆ·ï¼Œæ­¤ç±»ç”¨æˆ·åµŒå…¥åœ¨å®¢æˆ·ç«¯çš„è¯ä¹¦ä¸­ã€‚ kubernetes çš„é‰´æƒæœºåˆ¶(Authorization)kubernetes ç›®å‰æ”¯æŒå¦‚ä¸‹å››ç§é‰´æƒæœºåˆ¶ï¼š Node ABAC RBAC Webhook ä¸‹é¢ä»…ä»‹ç»ä¸¤ç§æœ€å¸¸ä½¿ç”¨çš„é‰´æƒæœºåˆ¶ï¼š Nodeä»… v1.7 ç‰ˆæœ¬ä»¥ä¸Šæ”¯æŒ Node æŽˆæƒï¼Œé…åˆ NodeRestriction å‡†å…¥æŽ§åˆ¶æ¥é™åˆ¶ kubeletï¼Œä½¿å…¶ä»…å¯è®¿é—® nodeã€endpointã€podã€service ä»¥åŠ secretã€configmapã€pvã€pvc ç­‰ç›¸å…³çš„èµ„æºï¼Œåœ¨ apiserver ä¸­ä½¿ç”¨ä»¥ä¸‹é…ç½®æ¥å¼€å¯ node çš„é‰´æƒæœºåˆ¶ï¼š 123KUBE_ADMISSION_CONTROL=&quot;...,NodeRestriction,...&quot;KUBE_API_ARGS=&quot;...,--authorization-mode=Node,...&quot; RBACRBACï¼ˆRole-Based Access Controlï¼‰æ˜¯ kubernetes ä¸­è´Ÿè´£å®ŒæˆæŽˆæƒï¼Œæ˜¯åŸºäºŽè§’è‰²çš„è®¿é—®æŽ§åˆ¶ï¼Œé€šè¿‡è‡ªå®šä¹‰è§’è‰²å¹¶å°†è§’è‰²å’Œç‰¹å®šçš„ userï¼Œgroupï¼Œserviceaccounts å…³è”èµ·æ¥å·²è¾¾åˆ°æƒé™æŽ§åˆ¶çš„ç›®çš„ã€‚ RBAC ä¸­æœ‰ä¸‰ä¸ªæ¯”è¾ƒé‡è¦çš„æ¦‚å¿µï¼š Roleï¼šè§’è‰²ï¼Œå®ƒå…¶å®žæ˜¯ä¸€ç»„è§„åˆ™ï¼Œå®šä¹‰äº†ä¸€ç»„å¯¹ Kubernetes API å¯¹è±¡çš„æ“ä½œæƒé™ï¼› Subjectï¼šè¢«ä½œç”¨è€…ï¼ŒåŒ…æ‹¬ userï¼Œgroupï¼Œserviceaccountsï¼Œé€šä¿—æ¥è®²å°±æ˜¯è®¤è¯æœºåˆ¶ä¸­æ‰€è¯†åˆ«çš„ç”¨æˆ·ï¼› RoleBindingï¼šå®šä¹‰äº†â€œè¢«ä½œç”¨è€…â€å’Œâ€œè§’è‰²â€çš„ç»‘å®šå…³ç³»ï¼Œä¹Ÿå°±æ˜¯å°†ç”¨æˆ·ä»¥åŠæ“ä½œæƒé™è¿›è¡Œç»‘å®šï¼› RBAC å…¶å®žå°±æ˜¯é€šè¿‡åˆ›å»ºè§’è‰²(Roleï¼‰ï¼Œé€šè¿‡ RoleBinding å°†è¢«ä½œç”¨è€…ï¼ˆsubjectï¼‰å’Œè§’è‰²ï¼ˆRoleï¼‰è¿›è¡Œç»‘å®šã€‚ä¸‹å›¾æ˜¯ RBAC ä¸­çš„å‡ ç§ç»‘å®šå…³ç³»ï¼š é‰´æƒæœºåˆ¶çš„å®˜æ–¹æ–‡æ¡£ï¼Œè¯·å‚è€ƒï¼šhttps://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules å‡†å…¥æŽ§åˆ¶(Admission Control)å‡†å…¥æŽ§åˆ¶æ˜¯è¯·æ±‚çš„æœ€åŽä¸€ä¸ªæ­¥éª¤ï¼Œå‡†å…¥æŽ§åˆ¶æœ‰è®¸å¤šå†…ç½®çš„æ¨¡å—ï¼Œå¯ä»¥ä½œç”¨äºŽå¯¹è±¡çš„ â€œCREATEâ€ã€â€UPDATEâ€ã€â€DELETEâ€ã€â€CONNECTâ€ å››ä¸ªé˜¶æ®µã€‚åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ï¼Œå¦‚æžœä»»ä¸€å‡†å…¥æŽ§åˆ¶æ¨¡å—æ‹’ç»ï¼Œé‚£ä¹ˆè¯·æ±‚ç«‹åˆ»è¢«æ‹’ç»ã€‚ä¸€æ—¦è¯·æ±‚é€šè¿‡æ‰€æœ‰çš„å‡†å…¥æŽ§åˆ¶å™¨åŽå°±ä¼šå†™å…¥å¯¹è±¡å­˜å‚¨ä¸­ã€‚ å‡†å…¥æŽ§åˆ¶æ˜¯åœ¨ apiserver ä¸­è¿›è¡Œé…ç½®çš„ï¼š 1KUBE_ADMISSION_CONTROL=&quot;--enable-admission-plugins=NamespaceLifecycle,LimitRanger,...MutatingAdmissionWebhook,ValidatingAdmissionWebhook,NodeRestriction...&quot; å‡†å…¥æŽ§åˆ¶çš„é…ç½®æ˜¯æœ‰åºçš„ï¼Œä¸åŒçš„é¡ºåºä¼šå½±å“ kubernetes çš„æ€§èƒ½ï¼Œå»ºè®®ä½¿ç”¨å®˜æ–¹çš„é…ç½®ã€‚ è‹¥éœ€è¦å¯¹ kubernetes ä¸­çš„å¯¹è±¡åšä¸€äº›æ‰©å±•ï¼Œå¯ä»¥ä½¿ç”¨å‡†å…¥æŽ§åˆ¶ï¼Œæ¯”å¦‚ï¼šåˆ›å»º pod æ—¶æ·»åŠ  initContainer æˆ–è€…æ ¡éªŒå­—æ®µç­‰ã€‚å‡†å…¥æŽ§åˆ¶æœ€å¸¸ä½¿ç”¨çš„æ‰©å±•æ–¹å¼å°±æ˜¯ admission webhooksï¼Œä»¥å‰å†™è¿‡ä¸€ç¯‡ç±»ä¼¼çš„æ–‡ç« ï¼Œå¯ä»¥å‚è€ƒï¼šhttp://blog.tianfeiyu.com/2019/07/02/k8s_crd_verify/ã€‚ å‡†å…¥æŽ§åˆ¶æ›´è¯¦ç»†çš„æ–‡æ¡£ï¼Œè¯·å‚è€ƒï¼šhttps://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ å°ç»“ï¼šä¸Šæ–‡å·²ç»è¯´äº† kubernetes ä¸­æœ‰ä¸¤ç§ç”¨æˆ·ï¼Œä¸€ç§æ˜¯å†…ç½®ç”¨æˆ·è¢«ç§°ä¸º serviceaccountsï¼Œä¸€ç§å¤–éƒ¨ç”¨æˆ·ï¼ŒåµŒå…¥åœ¨å®¢æˆ·ç«¯çš„è¯ä¹¦ä¸­ï¼Œé‚£ä¹ˆ kubernetes ä¸­æœ‰å“ªäº›è¯ä¹¦é“¾ä»¥åŠå†…åµŒçš„ç”¨æˆ·å¦‚ä½•ä¸Ž RBAC ç»“åˆå‘¢ï¼Ÿ kubernetes ä¸­çš„è¯ä¹¦é“¾ç¬”è€…é€šè¿‡è‡ªå·±çš„ç ”ç©¶åŠå®žè·µç»éªŒå‘çŽ°ï¼Œåœ¨ç›®å‰ä¸»æµç‰ˆæœ¬çš„ kubernetes é›†ç¾¤ä¸­ï¼Œæœ‰å››æ¡é‡è¦çš„ CA è¯ä¹¦é“¾ï¼Œè€Œåœ¨å¤§å¤šæ•°ç”Ÿäº§çŽ¯å¢ƒä¸­ï¼Œåˆ™è‡³å°‘éœ€è¦ä¸¤æ¡ CA è¯ä¹¦é“¾ã€‚ apiserver CA è¯ä¹¦é“¾ï¼šä¸»è¦ç”¨äºŽ kubernetes å†…éƒ¨ç»„ä»¶äº’ç›¸è®¿é—®ä»¥åŠå¤–éƒ¨å®¢æˆ·ç«¯è®¿é—® apiserver ä½¿ç”¨ etcd CA è¯ä¹¦é“¾ï¼šä¸»è¦ç”¨äºŽ etcd èŠ‚ç‚¹ä¹‹é—´çš„è®¿é—®ä»¥åŠ apiserver è®¿é—® etcd ä½¿ç”¨ extension apiserver CA è¯ä¹¦é“¾ï¼šç”¨äºŽè®¿é—® extension apiserver ä½¿ç”¨ï¼Œæ¯”å¦‚ metrics-server kubelet CA ä¿¡ä»»é“¾ï¼šç”¨äºŽ apiserver è®¿é—® kubelet æ—¶ä½¿ç”¨ å…¶ä»–è¯ä¹¦é“¾ï¼šadmission webhook è¯ä¹¦é“¾ã€audit webhook è¯ä¹¦é“¾ï¼Œç”¨äºŽ apiserver è®¿é—® webhook æ—¶ä½¿ç”¨ ä»¥ä¸Šè¿™å‡ å¥— CA è¯ä¹¦é“¾ä¸­ï¼Œapiserver CA è¯ä¹¦é“¾å’Œ etcd CA è¯ä¹¦é“¾æ˜¯å¿…è¦çš„ã€‚extension apiserver çš„ CA è¯ä¹¦é“¾åªæœ‰åœ¨ä½¿ç”¨æ—¶æ‰ä¼šç”¨åˆ°ï¼Œä¸”ä¸å¯ä¸Ž apiserver CA è¯ä¹¦é“¾ç›¸åŒã€‚kubelet çš„ CA è¯ä¹¦é“¾ä¸æ˜¯å¿…è¦çš„ï¼Œæ ¹æ®éƒ¨ç½²çš„å®žé™…æƒ…å†µå¯ä»¥å’Œ apiserver CA è¯ä¹¦é“¾å…¬ç”¨ã€‚ è¯ä¹¦ä¸­çš„å†…åµŒç”¨æˆ·å¦‚ä½•ä¸Ž RBAC é…ç½®è¿›è¡Œç»“åˆè¯ä¹¦ä¸­çš„å†…åµŒç”¨æˆ·ä»¥ä¸‹æ˜¯ kubelet çš„è¯ä¹¦è¯·æ±‚æ–‡ä»¶ï¼ˆCSRï¼‰ï¼š 12345678910111213141516&#123; &quot;CN&quot;: &quot;system:node:&lt;nodeName&gt;&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:nodes&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125; â€œCNâ€ï¼šCommon Nameï¼Œä»Žè¯ä¹¦ä¸­æå–è¯¥å­—æ®µä½œä¸ºè¯·æ±‚çš„ç”¨æˆ·å (User Name)ï¼› â€œOâ€ï¼šOrganizationï¼Œä»Žè¯ä¹¦ä¸­æå–è¯¥å­—æ®µä½œä¸ºè¯·æ±‚ç”¨æˆ·æ‰€å±žçš„ç»„ (Group)ï¼› kubernetes ä½¿ç”¨ X509 è¯ä¹¦ä¸­ CN(Common Name) ä»¥åŠ O(Organization) å­—æ®µå¯¹åº” kubernetes ä¸­çš„ user å’Œ groupï¼Œå³ RBAC ä¸­çš„ subjectï¼Œè€Œ kubernetes ä¹Ÿä¸ºå¤šä¸ªç»„ä»¶å†…ç½®äº† Role ä»¥åŠ RoleBindingï¼Œå·§å¦™çš„å°† Authentication å’Œ RBAC Authorization ç»“åˆåˆ°äº†ä¸€èµ·ã€‚ æŸ¥çœ‹ kubernetes ä¸­å†…ç½®çš„ RBACï¼š 123$ kubectl get clusterrole$ kubectl get clusterrolebinding ä¸‹é¢æ˜¯ kubernetes ä¸­æ ¸å¿ƒç»„ä»¶å†…ç½®çš„ user å’Œ groupï¼Œåœ¨ä¸ºæ¯ä¸ªç»„ä»¶ç”Ÿæˆè¯ä¹¦æ—¶éœ€è¦åœ¨å…¶ CSR ä¸­ä½¿ç”¨å¯¹åº”çš„ CN å’Œ O å­—æ®µã€‚ è®¿é—® apiserver çš„å‡ ç§æ–¹å¼é€šè¿‡ä¸Šæ–‡å¯ä»¥çŸ¥é“è®¿é—® apiserver æ—¶éœ€è¦é€šè¿‡è®¤è¯ã€é‰´æƒä»¥åŠè®¿é—®æŽ§åˆ¶ä¸‰ä¸ªæ­¥éª¤ï¼Œè®¤è¯çš„æ–¹å¼å¯ä»¥ä½¿ç”¨ serviceaccounts å’Œ X509 è¯ä¹¦ï¼Œé‰´æƒçš„æ–¹å¼ä½¿ç”¨ RBACï¼Œè®¿é—®æŽ§åˆ¶è‹¥æ²¡æœ‰ç‰¹æ®Šéœ€æ±‚å¯ä»¥ä¸ä½¿ç”¨ã€‚ serviceaccounts æ˜¯ kubernetes é’ˆå¯¹ pod å†…è®¿é—® apiserver æä¾›çš„è®¤è¯æ–¹å¼ï¼Œé‚£å¯ä»¥ç”¨åœ¨å¤–éƒ¨ client ç«¯å—ï¼Ÿç­”æ¡ˆæ˜¯å¯ä»¥çš„ï¼Œserviceaccounts æœ€ç»ˆæ˜¯é€šè¿‡ ca + token çš„æ–¹å¼è®¿é—®çš„ï¼Œä½ åªè¦åˆ›å»ºä¸€ä¸ª serviceaccounts å¹¶ä»Žå¯¹åº”çš„ secrets ä¸­èŽ·å– ca + token å³å¯è®¿é—® apiserverã€‚é‚£ä½¿ç”¨è¯ä¹¦è®¤è¯çš„æ–¹å¼å¯ä»¥åœ¨ pod å†…è®¿é—® apiserver å—ï¼Ÿå½“ç„¶ä¹Ÿå¯ä»¥ï¼Œä¸è¿‡åˆ›å»ºè¯ä¹¦æ¯” serviceaccounts éº»çƒ¦ï¼Œè¯ä¹¦é»˜è®¤æ˜¯ç”¨äºŽå†…ç½®ç»„ä»¶è®¿é—® apiserver ä½¿ç”¨çš„ã€‚ä¸è®ºå“ªç§æ–¹å¼ï¼Œä½ éƒ½éœ€è¦ä¸ºå…¶åˆ›å»º RBAC é…ç½®ã€‚ æ‰€ä»¥åœ¨ TLS +RBAC æ¨¡å¼ä¸‹ï¼Œè®¿é—® apiserver ç›®å‰æœ‰ä¸¤ç§æ–¹å¼ï¼š ä½¿ç”¨ serviceaccounts + RBAC ï¼šéœ€è¦åˆ›å»º serviceaccounts ä»¥åŠå…³è”å¯¹åº”çš„ RBAC(ca + token + RBAC) ä½¿ç”¨è¯ä¹¦ + RBACï¼šéœ€è¦ç”¨åˆ° caã€clientã€client-key ä»¥åŠå…³è”å¯¹åº”çš„ RBAC(ca + client-key + client-cert + RBAC) æ€»ç»“æœ¬æ–‡ä¸»è¦è®²è¿°äº† kubernetes ä¸­çš„è®¤è¯(Authentication)ä»¥åŠé‰´æƒ(Authorization)æœºåˆ¶ï¼Œå…¶å¤æ‚æ€§ä¸»è¦ä½“çŽ°åœ¨éƒ¨ç½² kubernetes é›†ç¾¤æ—¶ç»„ä»¶ä¹‹é—´çš„è®¤è¯ä»¥åŠåœ¨é›†ç¾¤ä¸­ä¸ºé™„åŠ ç»„ä»¶é…ç½®æ­£ç¡®çš„æƒé™ï¼Œå¸Œæœ›é€šè¿‡æœ¬èŠ‚ä½ å¯ä»¥äº†è§£åˆ° kubernetes ä¸­çš„ç»„ä»¶éœ€è¦å“ªäº›æƒé™è®¤è¯ä»¥åŠå¦‚ä½•ä¸ºç›¸å…³ç»„ä»¶é…ç½®æ­£ç¡®çš„æƒé™ã€‚ å‚è€ƒï¼š Controlling Access to the Kubernetes APIï¼šhttps://kubernetes.io/docs/reference/access-authn-authz/controlling-access/ admission controllersï¼šhttps://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ kubelet é…ç½®æƒé™è®¤è¯ï¼šhttps://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ master-node communicationï¼šhttps://kubernetes.io/docs/concepts/architecture/master-node-communication/ kubernetes æ•°å­—è¯ä¹¦ä½“ç³»æµ…æžï¼šhttps://mp.weixin.qq.com/s/iXuDbPKjSc65t_9Y1--bog]]></content>
      <tags>
        <tag>Authentication</tag>
        <tag>Authorization</tag>
        <tag>RBAC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-on-kube-operator å¼€å‘(äºŒ)]]></title>
    <url>%2F2019%2F08%2F07%2Fkube_on_kube_operator_2%2F</url>
    <content type="text"><![CDATA[æœ¬æ–‡ä¸»è¦è®²è¿° kubernetes-operator çš„å¼€å‘è¿‡ç¨‹ï¼Œkubernetes-operator å·²ç»å¼€å‘äº†ä¸€ä¸ªå¤šæœˆï¼Œå…¶æ ¸å¿ƒåŠŸèƒ½å·²ç»å®žçŽ°ï¼Œå…¶ä¸­çš„æž¶æž„ä»¥åŠåŠŸèƒ½è®¾è®¡ä¸»è¦æ¥è‡ªäºŽä¸€äº›ç”Ÿäº§çŽ¯å¢ƒçš„ç»éªŒä»¥åŠè‡ªå·±ä»Žäº‹ kubernetes è¿ç»´å¼€å‘ä¸¤å¹´å¤šçš„ä¸€äº›å·¥ä½œç»éªŒï¼Œå¦‚æœ‰é—®é¢˜æœ›æŒ‡æ­£ã€‚ kubernetes-operator ç»„ä»¶ä»‹ç»kubernetes-operator ä¸­ä¸»è¦åŒ…å«ä¸€ä¸ªè‡ªå®šä¹‰çš„ controller å’Œä¸€ä¸ª HTTP Serverï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œcontroller ä¸»è¦æ˜¯ç›‘å¬ CRD çš„å˜åŒ–ä»¥åŠä½¿å…¶è¾¾åˆ°ç»ˆæ€ï¼ŒHTTP Server æä¾›äº†å¤šä¸ª RESTful APIï¼Œç”¨äºŽæ“ä½œ CRD(åˆ›å»ºã€åˆ é™¤ã€æ‰©ç¼©å®¹ã€æŽ¥æ”¶å›žè°ƒç­‰)ã€‚ é™¤æ­¤ä¹‹å¤–è¿˜æœ‰å…¶ä»–çš„ç»„ä»¶ï¼Œansibleinitã€precheckã€admission-webhookï¼Œansibleinit æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶ç”¨æ¥ä½œä¸ºå®¹å™¨å†…çš„ 1 å·è¿›ç¨‹ï¼Œä¼šè°ƒç”¨ ansible ç›¸å…³çš„å‘½ä»¤ä»¥åŠå¤„ç†ä¿¡å·ã€å­è¿›ç¨‹æ”¶å‰²ç­‰ã€‚precheck ä¸»è¦ç”¨äºŽåœ¨å¯¹é›†ç¾¤æ“ä½œå‰æ£€æŸ¥ç›®æ ‡å®¿ä¸»æœºçš„çŽ¯å¢ƒï¼Œç”±äºŽå¯¹é›†ç¾¤çš„æ“ä½œéœ€è¦è€—è´¹æ•°åç§’ï¼Œä¸ºäº†ä¿è¯æˆåŠŸçŽ‡éœ€è¦åœ¨éƒ¨ç½²å‰æ£€æŸ¥å®¿ä¸»çš„çŽ¯å¢ƒã€‚admission-webhook æš‚æ—¶ç”¨äºŽæ ¡éªŒ CR ä¸­å­—æ®µï¼Œæ¯”å¦‚é›†ç¾¤æ‰§è¡Œæ‰©å®¹æ“ä½œæ—¶ï¼Œmaster ç­‰å­—æ®µçš„å€¼è‚¯å®šæ˜¯ä¸èƒ½æ”¹å˜çš„ã€‚ kubernetes-operator çš„å¼€å‘ä¸‹é¢ä¸»è¦è®² kubernetes-operator ä¸­æ ¸å¿ƒç»„ä»¶çš„å¼€å‘ï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ æ­¥ï¼š å®šä¹‰ CRD ç”Ÿæˆä»£ç  å¼€å‘ controller å¼€å‘ RESTful API å®šä¹‰ CRDä¸‹é¢æ˜¯ CRD çš„å®šä¹‰ï¼Œkubernetes-operator ä¸­çš„è‡ªå®šä¹‰èµ„æºä¸º KubernetesClusterï¼Œé¡¹ç›®ä¸­ç®€ç§°ä¸º ecsã€‚ 123456789101112131415161718192021apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: kubernetesclusters.ecs.yun.comspec: group: ecs.yun.com names: kind: KubernetesCluster listKind: KubernetesClusterList plural: kubernetesclusters singular: kubernetescluster shortNames: - ecs scope: Namespaced subresources: status: &#123;&#125; version: v1 versions: - name: v1 served: true storage: true å°† CRD éƒ¨ç½²åˆ° kubernetes é›†ç¾¤ä¸­ï¼ŒCRD ä¸­çš„è‡ªå®šä¹‰èµ„æºKubernetesCluster(CR) å°±æˆä¸ºäº† kubernetes ä¸­çš„ä¸€ç§èµ„æºï¼Œå’Œ podã€deployment ç­‰ç±»ä¼¼ã€‚ ç”Ÿæˆä»£ç ç”Ÿæˆä»£ç å¯ä»¥å‚è€ƒä¸Šä¸€ç¯‡æ–‡ç« ä½¿ç”¨ code-generator ä¸º CustomResources ç”Ÿæˆä»£ç ï¼Œæ­¤å¤„ä¸å†è¯¦è§£ã€‚ å¼€å‘ controllerå¦‚ä¸‹æ‰€ç¤ºæ˜¯ controller æœ€ç®€å•çš„ä¸€ä¸ªå£°æ˜Žï¼š 12345for &#123; desired := getDesiredState() current := getCurrentState() makeChanges(desired, current)&#125; æ‰€æœ‰ controller ä¹Ÿéƒ½æ˜¯ä»¥æ­¤è¿›è¡Œæ¼”å˜çš„ï¼Œcontroller çš„ä»£ç æ¨¡å¼æˆ–è€…å¥—è·¯å¯ä»¥å‚è€ƒsample-controller æˆ–è€… kube-controller-manager ä¸­æ‰€æœ‰ controller çš„å®žçŽ°ã€‚ ä¸‹é¢æ˜¯ kubernetes-operator ä¸­ controller å®žçŽ°çš„ä¸€ä¸ªæµç¨‹å›¾ï¼š æ›´æ–° CR éƒ½æ˜¯å®¢æˆ·ç«¯çš„æ“ä½œï¼Œæ‰€ä»¥åœ¨è®¾è®¡æ—¶å®¢æˆ·ç«¯éƒ½æ˜¯æ“ä½œ annotation ä¸­çš„å­—æ®µï¼Œç„¶åŽ operator ç›‘å¬åˆ°ç›¸å…³çš„æ—¶é—´åŽä¼šè¿›è¡Œå¤„ç†ã€‚ä¾‹å¦‚ï¼Œå½“ç”¨æˆ·è¦åˆ›å»ºä¸€ä¸ªé›†ç¾¤æ—¶ï¼Œé¦–å…ˆå®¢æˆ·ç«¯å°† app.kubernetes.io/operationè®¾ç½®ä¸º creatingï¼Œæ­¤æ—¶ operator watch åˆ° CR å˜åŒ–åŽä¼šå¤„ç†æ–°å»ºé›†ç¾¤çš„æ“ä½œï¼Œoperator ä¼šåˆ›å»ºä¸€ä¸ªç”¨æ¥éƒ¨ç½²é›†ç¾¤çš„ jobï¼Œä»¥åŠåˆ›å»º configmap æ¥ä¿å­˜æœ¬æ¬¡çš„æ“ä½œè®°å½•ä»¥åŠå…³è”å¯¹åº”çš„ jobï¼Œä¹Ÿèƒ½ç”¨æ¥æŸ¥è¯¢æœ¬æ¬¡æ“ä½œçš„æ—¥å¿—ï¼Œç„¶åŽä¼šæ›´æ–° CR ä¸­ status.phase ä¸­çš„ Creating (æ–°åˆ›å»ºçš„ CR status.phase ä¸º â€œâ€)ï¼ŒæŽ¥ä¸‹æ¥ä¸º CR è®¾ç½® finalizersï¼Œæœ€åŽä¼šå¯åŠ¨ä¸€ä¸ª goroutine æ£€æµ‹ job çš„çŠ¶æ€ã€‚æ­¤æ—¶éœ€è¦ç­‰å¾… job çš„å®Œæˆä»¥åŠå›žè°ƒï¼Œè‹¥ job å¤±è´¥æˆ–è€…è¶…æ—¶éƒ½ä¼šè¢«æœ€åŽå¯åŠ¨çš„ goroutine æ£€æµ‹åˆ°ï¼Œjob æˆåŠŸä¸Žå¦éƒ½ä¼šè§¦å‘æ›´æ–° CR status.phase çš„æ“ä½œã€‚è‹¥ job æ‰§è¡Œå®ŒæˆæˆåŠŸå›žè°ƒï¼Œå®¢æˆ·ç«¯ä¼šæ›´æ–° app.kubernetes.io/operationä¸º create-finishedï¼Œå®¢æˆ·ç«¯æ›´æ–°å®ŒæˆåŽä¼šè§¦å‘ä¸€æ¬¡äº‹ä»¶ï¼Œç„¶åŽ operator ä¼šå°† status.phase æ›´æ–°ä¸º Running çŠ¶æ€ï¼Œå¦åˆ™ job å¼‚å¸¸ operator ä¼šç›´æŽ¥æ›´æ–° status.phase ä¸º Failedã€‚ å…³äºŽ CR ä¸­ app.kubernetes.io/operation å­—æ®µä»¥åŠ status.phase ä¸­æ‰€æœ‰çš„å®šä¹‰è¯·å‚è§ kubernetes-operator/pkg/enum/task.goã€‚ å¼€å‘ RESTful APIåœ¨å‰åŽç«¯åˆ†ç¦»çš„åœºæ™¯ä¸­ï¼ŒRESTful API çš„å¼€å‘ä»…éœ€è¦ä¸€ä¸ª route æ¡†æž¶å³å¯ï¼Œkubernetes-operator ä¸­ç”¨çš„æ˜¯ muxï¼Œå…·ä½“çš„ä»£ç åœ¨ kubernetes-operator/pkg/server ä¸‹ã€‚ æ€»ç»“æœ¬æ–‡ä¸»è¦è®²è¿°äº† kubernetes-operator ä¸­ä¸»è¦çš„æ¨¡å—ä»¥åŠ controller çš„å…·ä½“å®žçŽ°ï¼Œå…¶ä¸­è®¸å¤šç»†èŠ‚æš‚æœªæåŠåˆ°ï¼Œè¯¦ç»†çš„å®žçŽ°è¯·å‚è€ƒä»£ç ï¼Œè¯¥é¡¹ç›®åªæ˜¯ç¬”è€…åˆ©ç”¨ä¸šä½™æ—¶é—´è¿›è¡Œå¼€å‘çš„ï¼Œæ¯•ç«Ÿä¸ªäººç²¾åŠ›æœ‰é™ï¼Œç¬”è€…å½“å‰ä¸»è¦é›†ä¸­åœ¨ kube-on-kube çš„å¼€å‘ä¸Šï¼Œåœ¨é˜…è¯»æ–‡ç« æˆ–è€…ä»£ç çš„è¿‡ç¨‹ä¸­å¦‚æœ‰é—®é¢˜å¯ä»¥éšæ—¶ç•™è¨€ï¼Œç¬”è€…ä¼šæŒç»­è¿­ä»£ç‰ˆæœ¬ã€‚ä¸‹ä¸€ç¯‡æ–‡ç« ä¼šè®²è¿°å¦‚ä½•ä½¿ç”¨äºŒè¿›åˆ¶æ–‡ä»¶éƒ¨ç½² kubernetes é›†ç¾¤ã€‚ å‚è€ƒï¼š https://github.com/kubernetes/community/blob/8decfe4/contributors/devel/controllers.md https://github.com/kubernetes/sample-controller https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html https://www.cnblogs.com/gaorong/p/8854934.html https://yucs.github.io/2017/12/21/2017-12-21-operator/ ç›¸å…³æŽ¨èkube-on-kube-operator å¼€å‘(ä¸‰)kube-on-kube-operator å¼€å‘(ä¸€)]]></content>
      <tags>
        <tag>operator</tag>
        <tag>kube-on-kube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½¿ç”¨ code-generator ä¸º CustomResources ç”Ÿæˆä»£ç ]]></title>
    <url>%2F2019%2F08%2F06%2Fcode_generator%2F</url>
    <content type="text"><![CDATA[kubernetes é¡¹ç›®ä¸­æœ‰ç›¸å½“ä¸€éƒ¨åˆ†ä»£ç æ˜¯è‡ªåŠ¨ç”Ÿæˆçš„ï¼Œä¸»è¦æ˜¯ API çš„å®šä¹‰å’Œè°ƒç”¨æ–¹æ³•ï¼Œkubernetes é¡¹ç›®ä¸‹ k8s.io/kubernetes/hack/ ç›®å½•ä¸­ä»¥ update å¼€å¤´çš„å¤§éƒ¨åˆ†è„šæœ¬éƒ½æ˜¯ç”¨æ¥ç”Ÿæˆä»£ç çš„ã€‚code-generator æ˜¯å®˜æ–¹æä¾›çš„ä»£ç ç”Ÿæˆå·¥å…·ï¼Œåœ¨å®žçŽ°è‡ªå®šä¹‰ controller çš„æ—¶å€™éœ€è¦ç”¨åˆ° CRDï¼Œä¹Ÿéœ€è¦ä½¿ç”¨è¯¥å·¥å…·ç”Ÿæˆå¯¹ CRD æ“ä½œçš„ä»£ç ã€‚ è¦ç”Ÿæˆå“ªäº›ä»£ç åœ¨è‡ªå®šä¹‰ controller æ—¶éœ€è¦ç”¨åˆ° typed clientsetsï¼Œinformersï¼Œlisters å’Œ deep-copy ç­‰å‡½æ•°ï¼Œè¿™äº›å‡½æ•°éƒ½å¯ä»¥ä½¿ç”¨ code-generator æ¥ç”Ÿæˆï¼Œå…·ä½“çš„ä½œç”¨å¯ä»¥å‚è€ƒï¼škubernetes ä¸­ informer çš„ä½¿ç”¨ã€‚ code-generator é‡Œé¢åŒ…å«å¤šä¸ªç”Ÿæˆä»£ç çš„å·¥å…·ï¼Œä¸‹é¢æ˜¯éœ€è¦ç”¨åˆ°çš„å‡ ä¸ªï¼š deepcopy-genï¼šä¸ºæ¯ç§ç±»åž‹Tç”Ÿæˆæ–¹æ³•ï¼š func (t* T) DeepCopy() *Tï¼ŒCustomResources å¿…é¡»å®žçŽ°runtime.Object æŽ¥å£ä¸”è¦æœ‰ DeepCopy æ–¹æ³• client-genï¼šä¸º CustomResource APIGroups ç”Ÿæˆ typed clientsets informer-genï¼šä¸º CustomResources åˆ›å»º informersï¼Œç”¨æ¥ watch å¯¹åº” CRD æ‰€è§¦å‘çš„äº‹ä»¶ï¼Œä»¥ä¾¿å¯¹ CustomResources çš„å˜åŒ–è¿›è¡Œå¯¹åº”çš„å¤„ç† lister-genï¼šä¸º CustomResources åˆ›å»º listersï¼Œç”¨æ¥å¯¹ GET/List è¯·æ±‚æä¾›åªè¯»çš„ç¼“å­˜å±‚ é™¤äº†ä¸Šé¢å‡ ä¸ªå·¥å…·å¤–ï¼Œcode-generator ä¸­è¿˜æä¾›äº† conversion-genã€defaulter-genã€register-genã€set-genï¼Œè¿™äº›ç”Ÿæˆå™¨å¯ä»¥åº”ç”¨åœ¨å…¶ä»–åœºæ™¯ï¼Œæ¯”å¦‚æž„å»ºèšåˆ API æœåŠ¡æ—¶ä¼šç”¨åˆ°ä¸€äº›å†…éƒ¨çš„ç±»åž‹ï¼Œconversion-gen ä¼šä¸ºè¿™äº›å†…éƒ¨å’Œå¤–éƒ¨ç±»åž‹ä¹‹é—´åˆ›å»ºè½¬æ¢å‡½æ•°ï¼Œdefaulter-gen ä¼šå¤„ç†æŸäº›å­—æ®µçš„é»˜è®¤å€¼ã€‚ ä»£ç ç”Ÿæˆæ­¥éª¤ä½¿ç”¨ code-generator ç”Ÿæˆä»£ç è¿˜éœ€è¦ä»¥ä¸‹å‡ æ­¥ï¼š åˆ›å»ºæŒ‡å®šçš„ç›®å½•æ ¼å¼ åœ¨ä»£ç ä¸­ä½¿ç”¨ tag æ ‡æ³¨è¦ç”Ÿæˆå“ªäº›ä»£ç  é¦–å…ˆè¦åˆ›å»ºæŒ‡å®šçš„ç›®å½•æ ¼å¼ï¼Œç›®å½•çš„æ ¼å¼å¯ä»¥å‚è€ƒå®˜æ–¹æä¾›çš„ç¤ºä¾‹é¡¹ç›®ï¼šsample-controllerï¼Œä¸‹æ–‡ä¹Ÿä¼šè®²åˆ°ï¼Œç›®å½•ä¸­éœ€è¦åŒ…å«å¯¹åº” CustomResources çš„å®šä¹‰ä»¥åŠ group å’Œ version ä¿¡æ¯ã€‚ å…¶æ¬¡è¦åœ¨åœ¨ä»£ç ä¸­ä½¿ç”¨ tag æ ‡æ³¨è¦ç”Ÿæˆå“ªäº›ä»£ç ï¼Œtag æœ‰ä¸¤é’Ÿç±»åž‹ï¼Œå…¨å±€çš„å’Œå±€éƒ¨çš„ï¼Œæ‰€æœ‰ç±»åž‹çš„ deepcopy tag ä¼šé»˜è®¤å¯ç”¨ï¼Œæ›´å¤šå…³äºŽ tag çš„ä½¿ç”¨æ–¹æ³•å¯ä»¥å‚è€ƒï¼šKubernetes Deep Dive: Code Generation for CustomResourcesï¼Œä¹Ÿå¯ä»¥å‚è€ƒå®˜æ–¹çš„ç¤ºä¾‹ code-generator/_example ã€‚ å¼€å§‹ç”Ÿæˆä»£ç æœ¬æ–‡ä»¥è¯¥ CRD ä¸ºä¾‹å­è¿›è¡Œæ¼”ç¤ºï¼Œgroup ä¸ºecs.yun.com ï¼Œversion ä¸º v1ï¼š 123456789101112131415161718192021apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: kubernetesclusters.ecs.yun.comspec: group: ecs.yun.com names: kind: KubernetesCluster listKind: KubernetesClusterList plural: kubernetesclusters singular: kubernetescluster shortNames: - ecs scope: Namespaced subresources: status: &#123;&#125; version: v1 versions: - name: v1 served: true storage: true åˆ›å»ºæŒ‡å®šç›®å½•ç»“æž„ pkg/apis/${group}/${version}ï¼Œgroup å¯ä»¥å®šä¹‰ä¸€ä¸ª shortNamesï¼Œä¹Ÿå°±æ˜¯ CRD ä¸­çš„ shortNames 1$ mkdir -pv pkg/apis/ecs/v1 åˆ›å»º doc.goï¼š123456$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/doc.go// Package v1 contains API Schema definitions for the ecs v1 API group// +k8s:deepcopy-gen=package,register// +groupName=ecs.yun.compackage v1EOF åˆ›å»º register.goï¼š 123456789101112131415161718192021222324252627282930313233343536373839$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/register.gopackage v1import ( &quot;github.com/gosoon/kubernetes-operator/pkg/apis/ecs&quot; metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot; &quot;k8s.io/apimachinery/pkg/runtime&quot; &quot;k8s.io/apimachinery/pkg/runtime/schema&quot;)// SchemeGroupVersion is group version used to register these objectsvar SchemeGroupVersion = schema.GroupVersion&#123;Group: ecs.GroupName, Version: &quot;v1&quot;&#125;// Kind takes an unqualified kind and returns back a Group qualified GroupKindfunc Kind(kind string) schema.GroupKind &#123; return SchemeGroupVersion.WithKind(kind).GroupKind()&#125;// Resource takes an unqualified resource and returns a Group qualified GroupResourcefunc Resource(resource string) schema.GroupResource &#123; return SchemeGroupVersion.WithResource(resource).GroupResource()&#125;var ( SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) AddToScheme = SchemeBuilder.AddToScheme)// Adds the list of known types to Scheme.func addKnownTypes(scheme *runtime.Scheme) error &#123; scheme.AddKnownTypes(SchemeGroupVersion, &amp;KubernetesCluster&#123;&#125;, &amp;KubernetesClusterList&#123;&#125;, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil&#125;EOF åˆ›å»º types.goï¼Œè¯¥æ–‡ä»¶ä¸­ä¼šå®šä¹‰å¤šä¸ª tag 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/types.gopackage v1import ( metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;)// +genclient// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// KubernetesCluster is the Schema for the kubernetesclusters APItype KubernetesCluster struct &#123; metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;` Spec KubernetesClusterSpec `json:&quot;spec,omitempty&quot;` Status KubernetesClusterStatus `json:&quot;status,omitempty&quot;`&#125;// KubernetesClusterSpec defines the desired state of KubernetesClustertype KubernetesClusterSpec struct &#123; // Add custom validation using kubebuilder tags: // https://book.kubebuilder.io/beyond_basics/generating_crd.html TimeoutMins string `json:&quot;timeout_mins,omitempty&quot;` ClusterType string `json:&quot;clusterType,omitempty&quot;` ContainerCIDR string `json:&quot;containerCIDR,omitempty&quot;` ServiceCIDR string `json:&quot;serviceCIDR,omitempty&quot;` MasterList []Node `json:&quot;masterList&quot; tag:&quot;required&quot;` MasterVIP string `json:&quot;masterVIP,omitempty&quot;` NodeList []Node `json:&quot;nodeList&quot; tag:&quot;required&quot;` EtcdList []Node `json:&quot;etcdList,omitempty&quot;` Region string `json:&quot;region,omitempty&quot;` AuthConfig AuthConfig `json:&quot;authConfig,omitempty&quot;`&#125;// AuthConfig defines the nodes peer authenticationtype AuthConfig struct &#123; Username string `json:&quot;username,omitempty&quot;` Password string `json:&quot;password,omitempty&quot;` PrivateSSHKey string `json:&quot;privateSSHKey,omitempty&quot;`&#125;// KubernetesClusterStatus defines the observed state of KubernetesClustertype KubernetesClusterStatus struct &#123; // Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html Phase KubernetesOperatorPhase `json:&quot;phase,omitempty&quot;` // when job failed callback or job timeout used Reason string `json:&quot;reason,omitempty&quot;` // JobName is store each job name JobName string `json:&quot;jobName,omitempty&quot;` // Last time the condition transitioned from one status to another. LastTransitionTime metav1.Time `json:&quot;lastTransitionTime,omitempty&quot;`&#125;// +genclient:nonNamespaced// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// KubernetesClusterList contains a list of KubernetesClustertype KubernetesClusterList struct &#123; metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ListMeta `json:&quot;metadata,omitempty&quot;` Items []KubernetesCluster `json:&quot;items&quot;`&#125;// users// &quot;None,Creating,Running,Failed,Scaling&quot;type KubernetesOperatorPhase stringtype Node struct &#123; IP string `json:&quot;ip,omitempty&quot;`&#125;EOF æ‰§è¡Œå‘½ä»¤ç”Ÿæˆä»£ç ï¼š 1$ $GOPATH/src/k8s.io/code-generator/generate-groups.sh all github.com/gosoon/kubernetes-operator/pkg/client github.com/gosoon/kubernetes-operator/pkg/apis ecs:v1 generate-groups.sh éœ€è¦å››ä¸ªå‚æ•°ï¼š ç¬¬ä¸€ä¸ª å‚æ•°ï¼šallï¼Œä¹Ÿå°±æ˜¯è¦ç”Ÿæˆæ‰€æœ‰çš„æ¨¡å—ï¼Œclientsetï¼Œinformersï¼Œlisters ç¬¬äºŒä¸ªå‚æ•°ï¼šgithub.com/gosoon/test/pkg/client è¿™ä¸ªæ˜¯ä½ è¦ç”Ÿæˆä»£ç çš„ç›®å½•ï¼Œç›®å½•çš„åç§°ä¸€èˆ¬å®šä¹‰ä¸º client ç¬¬ä¸‰ä¸ªå‚æ•°ï¼šgithub.com/gosoon/test/pkg/apis è¿™ä¸ªç›®å½•æ˜¯å·²ç»åˆ›å»ºå¥½çš„æºç›®å½• ç¬¬å››ä¸ªå‚æ•°ï¼šâ€ecs:v1â€ æ˜¯ group å’Œ version ä¿¡æ¯ï¼Œecs æ˜¯ apis ä¸‹çš„ç›®å½•ï¼Œv1 æ˜¯ ecs ä¸‹é¢çš„ç›®å½• ç”Ÿæˆçš„ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950.â””â”€â”€ pkg â”œâ”€â”€ apis â”‚ â””â”€â”€ ecs â”‚ â””â”€â”€ v1 â”‚ â”œâ”€â”€ doc.go â”‚ â”œâ”€â”€ register.go â”‚ â”œâ”€â”€ types.go â”‚ â””â”€â”€ zz_generated.deepcopy.go â””â”€â”€ client â”œâ”€â”€ clientset â”‚ â””â”€â”€ versioned â”‚ â”œâ”€â”€ clientset.go â”‚ â”œâ”€â”€ doc.go â”‚ â”œâ”€â”€ fake â”‚ â”‚ â”œâ”€â”€ clientset_generated.go â”‚ â”‚ â”œâ”€â”€ doc.go â”‚ â”‚ â””â”€â”€ register.go â”‚ â”œâ”€â”€ scheme â”‚ â”‚ â”œâ”€â”€ doc.go â”‚ â”‚ â””â”€â”€ register.go â”‚ â””â”€â”€ typed â”‚ â””â”€â”€ ecs â”‚ â””â”€â”€ v1 â”‚ â”œâ”€â”€ doc.go â”‚ â”œâ”€â”€ ecs_client.go â”‚ â”œâ”€â”€ fake â”‚ â”‚ â”œâ”€â”€ doc.go â”‚ â”‚ â”œâ”€â”€ fake_ecs_client.go â”‚ â”‚ â””â”€â”€ fake_kubernetescluster.go â”‚ â”œâ”€â”€ generated_expansion.go â”‚ â””â”€â”€ kubernetescluster.go â”œâ”€â”€ informers â”‚ â””â”€â”€ externalversions â”‚ â”œâ”€â”€ ecs â”‚ â”‚ â”œâ”€â”€ interface.go â”‚ â”‚ â””â”€â”€ v1 â”‚ â”‚ â”œâ”€â”€ interface.go â”‚ â”‚ â””â”€â”€ kubernetescluster.go â”‚ â”œâ”€â”€ factory.go â”‚ â”œâ”€â”€ generic.go â”‚ â””â”€â”€ internalinterfaces â”‚ â””â”€â”€ factory_interfaces.go â””â”€â”€ listers â””â”€â”€ ecs â””â”€â”€ v1 â”œâ”€â”€ expansion_generated.go â””â”€â”€ kubernetescluster.go21 directories, 26 files CRD ä»¥åŠç”Ÿæˆçš„ä»£ç è§ï¼škubernetes-operatorã€‚ æ€»ç»“æœ¬é—®è®²è¿°äº†å¦‚ä½•ä½¿ç”¨ code-generator ç”Ÿæˆä»£ç ï¼Œè¦ä½¿ç”¨è‡ªå®šä¹‰ controller ä»£ç ç”Ÿæˆæ˜¯æœ€å¼€å§‹çš„ä¸€æ­¥ï¼Œä¸‹æ–‡ä¼šç»§ç»­è®²è¿°è‡ªå®šä¹‰ controller çš„è¯¦ç»†æ­¥éª¤ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥å…³æ³¨ç¬”è€… github çš„é¡¹ç›® kubernetes-operatorã€‚ å‚è€ƒï¼š https://github.com/kubernetes/sample-controller https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/ https://hexo.do1618.com/2018/04/04/Kubernetes-Deep-Dive-Code-Generation-for-CustomResources/ ç›¸å…³æŽ¨èkubernetes è‡ªå®šä¹‰èµ„æºï¼ˆCRDï¼‰çš„æ ¡éªŒ]]></content>
      <tags>
        <tag>code-generator</tag>
        <tag>crd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-on-kube-operator å¼€å‘(ä¸€)]]></title>
    <url>%2F2019%2F08%2F05%2Fkube_on_kube_operator_1%2F</url>
    <content type="text"><![CDATA[kubernetes å·²ç»æˆä¸ºå®¹å™¨æ—¶ä»£çš„åˆ†å¸ƒå¼æ“ä½œç³»ç»Ÿå†…æ ¸ï¼Œç›®å‰ä¹Ÿæ˜¯æ‰€æœ‰å…¬æœ‰äº‘æä¾›å•†çš„æ ‡é…ï¼Œåœ¨å›½å†…ï¼Œé˜¿é‡Œäº‘ã€è…¾è®¯äº‘ã€åŽä¸ºäº‘è¿™æ ·çš„å…¬æœ‰äº‘å¤§åŽ‚å•†éƒ½æ”¯æŒä¸€é”®éƒ¨ç½² kubernetes é›†ç¾¤ï¼Œè€Œ kubernetes é›†ç¾¤è‡ªåŠ¨åŒ–ç®¡ç†åˆ™æ˜¯è¿«åˆ‡éœ€è¦è§£å†³çš„é—®é¢˜ã€‚å¯¹äºŽå¤§éƒ¨åˆ†ä¸ç†Ÿæ‚‰ kubernetes è€Œè¦ä¸Šäº‘çš„å°ç™½ç”¨æˆ·å°±å¼ºçƒˆéœ€è¦ä¸€ä¸ªè¢«æ‰˜ç®¡åŠèƒ½è‡ªåŠ¨åŒ–è¿ç»´çš„é›†ç¾¤ï¼Œä»–ä»¬å¹³æ—¶åªæ˜¯è¿›è¡Œä¸šåŠ¡çš„éƒ¨ç½²ä¸Žå˜æ›´ï¼Œåªéœ€è¦å¯¹ kubernetes ä¸­éƒ¨åˆ†æ¦‚å¿µäº†è§£å³å¯ã€‚åŒæ ·åœ¨ç§æœ‰äº‘åœºæ™¯ä¸‹ï¼Œç¬”è€…æ‰€å¾…è¿‡çš„å‡ ä¸ªå¤§å°å…¬å¸ä¸€èˆ¬éƒ½ä¼šç»´æŠ¤å¤šå¥—é›†ç¾¤ï¼Œé›†ç¾¤çš„è¿ç»´å·¥ä½œå°±æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„æŒ‘æˆ˜ï¼Œåè§‚å„å¤§åŽ‚åŒæ ·è¦æœ‰æ•ˆå¯é çš„ç®¡ç†å¤§è§„æ¨¡é›†ç¾¤ï¼Œkube-on-kube-operator æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è§£å†³æ–¹æ¡ˆã€‚ æ‰€è°“ kube-on-kube-operatorï¼Œå°±æ˜¯å°† kubernetes è¿è¡Œåœ¨ kubernetes ä¸Šï¼Œç”¨ kubernetes æ‰˜ç®¡ kubernetes æ–¹å¼æ¥è‡ªåŠ¨åŒ–ç®¡ç†é›†ç¾¤ã€‚å’Œæ‰€æœ‰ operator çš„åŠŸèƒ½ç±»ä¼¼ï¼Œç³»ç»Ÿä¼šå®šæ—¶æ£€æµ‹é›†ç¾¤å½“å‰çŠ¶æ€ï¼Œåˆ¤æ–­æ˜¯å¦ä¸Žç›®æ ‡çŠ¶æ€ä¸€è‡´ï¼Œå‡ºçŽ°ä¸ä¸€è‡´æ—¶ï¼Œoperator ä¼šå‘èµ·ä¸€ç³»åˆ—æ“ä½œï¼Œé©±åŠ¨é›†ç¾¤è¾¾åˆ°ç›®æ ‡çŠ¶æ€ã€‚ä»Šå¹´ kubeCon ä¸Šï¼Œé›…è™Žæ—¥æœ¬ä¹Ÿåˆ†äº«äº†å…¶ç®¡ç†å¤§è§„æ¨¡ kubernetes é›†ç¾¤çš„æ–¹æ³•ï¼Œ4000 èŠ‚ç‚¹æž„å»ºäº† 400 ä¸ª kubernetes é›†ç¾¤ï¼ŒåŒæ ·é‡‡ç”¨çš„æ˜¯ kube-on-kube-operator æž¶æž„ï¼Œä»¥ kubernetes as a service çš„å½¢å¼ä½¿ç”¨ã€‚ kubernetes-operator è®¾è®¡å‚è€ƒè®°å¾— kube-on-kube-operator çš„æ¦‚å¿µæœ€åˆæ˜¯åœ¨åŽ»å¹´çš„ kubeCon China ä¸Šèš‚èšé‡‘æœæå‡ºæ¥çš„ï¼Œå…ˆçœ‹çœ‹èš‚èšé‡‘æœä»¥åŠè…¾è®¯äº‘ kube-on-kube-operator çš„è®¾è®¡æ€è·¯ï¼Œå…¶å®žè…¾è®¯äº‘çš„æž¶æž„å’Œèš‚èšé‡‘æœçš„æ˜¯ç±»ä¼¼çš„ã€‚ä»¥ä¸‹æ˜¯èš‚èšé‡‘æœçš„æž¶æž„è®¾è®¡å›¾ï¼š é¦–å…ˆéƒ¨ç½²ä¸€å¥— kubernetes å…ƒé›†ç¾¤ï¼Œé€šè¿‡å…ƒé›†ç¾¤éƒ¨ç½²ä¸šåŠ¡é›†ç¾¤ï¼Œä¸šåŠ¡é›†ç¾¤çš„ master ç»„ä»¶éƒ½åˆ†å¸ƒåœ¨ä¸€å°å®¿ä¸»ä¸Šï¼Œè¯¥å®¿ä¸»ä»¥ node èŠ‚ç‚¹çš„æ–¹å¼æŒ‚è½½åœ¨å…ƒé›†ç¾¤ä¸­ã€‚åœ¨ç§æœ‰äº‘åœºæ™¯ä¸‹ï¼Œè¿™æ ·çš„éƒ¨ç½²æ–¹å¼æœ‰ä¸€ä¸ªå¾ˆæ˜Žæ˜¾çš„é—®é¢˜å°±æ˜¯å…ƒé›†ç¾¤èŠ‚ç‚¹è·¨æœºæˆ¿ï¼Œè™½ç„¶ä¸šåŠ¡é›†ç¾¤çš„ master ä¸Ž node éƒ½æ˜¯åœ¨åŒä¸€ä¸ªæœºæˆ¿ï¼Œä½†æ˜¯å…ƒé›†ç¾¤ä¸­çš„ node èŠ‚ç‚¹å¤§éƒ¨åˆ†æ˜¯åˆ†å¸ƒåœ¨ä¸åŒçš„æœºæˆ¿ï¼Œæœ‰äº›å…¬å¸åœ¨ä¸åŒæœºæˆ¿ä¹‹é—´ä¼šæœ‰ç½‘ç»œä¸Šçš„é™åˆ¶ï¼Œæœ‰å¯èƒ½ç½‘ç»œä¸é€šæˆ–è€…åªèƒ½ä½¿ç”¨ä¸“çº¿è¿žæŽ¥ã€‚åœ¨å…¬æœ‰äº‘åœºæ™¯ä¸‹ï¼Œå…ƒé›†ç¾¤è‡ªå·±æœ‰ä¸€å¥—ç‹¬ç«‹çš„ vpc ç½‘ç»œï¼Œå®ƒè¦æ€Žä¹ˆå’Œç”¨æˆ·çš„ vpc ç»“ç‚¹è¿›è¡Œé€šä¿¡å‘¢ï¼Ÿè…¾è®¯äº‘çš„åšæ³•æ˜¯åˆ©ç”¨ vpc æä¾›çš„å¼¹æ€§ç½‘å¡èƒ½åŠ›ï¼Œå°†è¿™ä¸ªå¼¹æ€§ç½‘å¡ç›´æŽ¥ç»‘å®šåˆ°è¿è¡Œ apiserver çš„ pod ä¸­ï¼Œè¿è¡Œ master çš„è¿™ä¸ªpod æ—¢åŠ å…¥äº†å…ƒé›†ç¾¤çš„ vpcï¼ŒåˆåŠ å…¥äº†ç”¨æˆ·çš„ vpcï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ª pod åŒæ—¶åœ¨ä¸¤ä¸ªç½‘ç»œä¸­ï¼Œè¿™æ ·å°±å¯ä»¥å¾ˆå¥½çš„åŽ»å®žçŽ°å’Œç”¨æˆ· node ç›¸å…³çš„äº’é€šã€‚è¿™ç§æ–¹å¼éƒ½æ˜¯é€šè¿‡ kubernetes API åŽ»ç®¡ç† master ç»„ä»¶çš„ï¼Œmaster ç»„ä»¶çš„å‡çº§ä»¥åŠæ•…éšœè‡ªæ„ˆéƒ½å¯ä»¥é€šè¿‡ kubernetes æä¾›çš„æ–¹å¼å®žçŽ°ã€‚ kubernetes-operator è®¾è®¡ kubernetes-operator é¡¹ç›®åœ°å€ï¼šhttps://github.com/gosoon/kubernetes-operator ç›®å‰è¯¥é¡¹ç›®çš„ä¸»è¦ç›®æ ‡æ˜¯å®žçŽ°ä»¥ä¸‹ä¸‰ç§åœºæ™¯ä¸­çš„é›†ç¾¤ç®¡ç†ï¼š kube-on-kube kube-to-kube kube-to-cloud-kube kubernetes-operator ä¸ä»…æ˜¯è¦å®žçŽ° kube-on-kube æž¶æž„ï¼Œè¿˜æœ‰ kube-to-kubeï¼Œkube-to-cloud-kubeï¼Œkube-to-kube å³ kubernetes é›†ç¾¤ç®¡ç†ä¸šåŠ¡ç‹¬ç«‹çš„ kubernetes é›†ç¾¤ï¼Œä¸¤ä¸ªé›†ç¾¤ç›¸äº’ç‹¬ç«‹ã€‚kube-to-cloud-kube å³ kubernetes é›†ç¾¤ç®¡ç†å¤šäº‘çŽ¯å¢ƒä¸Šçš„ kubernetes é›†ç¾¤ã€‚ ä¸Šé¢æ˜¯é¡¹ç›®çš„æž¶æž„å›¾ï¼Œçº¢è‰²çš„çº¿æ®µè¡¨ç¤ºå¯¹é›†ç¾¤ç”Ÿå‘½å‘¨æœŸç®¡ç†çš„ä¸€ä¸ªæ“ä½œï¼Œæ¶‰åŠé›†ç¾¤çš„åˆ›å»ºã€åˆ é™¤ã€æ‰©ç¼©å®¹ã€å‡çº§ç­‰ï¼Œè“è‰²çº¿æ®µæ˜¯å¯¹é›†ç¾¤åº”ç”¨çš„æ“ä½œï¼Œé›†ç¾¤ä¸­åº”ç”¨çš„åˆ›å»ºã€åˆ é™¤ã€å‘å¸ƒæ›´æ–°ç­‰ï¼Œkubernetes-proxy æ˜¯ä¸€ä¸ª API ä»£ç†ï¼Œæ‰€æœ‰æ¶‰åŠ API çš„è°ƒç”¨éƒ½è¦é€šè¿‡ kubernetes-proxyã€‚å·¦è¾¹éƒ¨ç½²æœ‰ kubernetes-operator çš„æ˜¯å…ƒé›†ç¾¤ï¼Œkubernetes-operator ä½¿ç”¨ etcd ä»…å­˜å‚¨éƒ¨åˆ†é…ç½®ä¿¡æ¯ï¼Œå…¶ç®¡ç†ä¸šåŠ¡é›†ç¾¤çš„ç”Ÿå‘½å‘¨æœŸï¼Œæ”¯æŒä¸‰ç§é›†ç¾¤çš„åˆ›å»ºæ–¹å¼ï¼Œç¬¬ä¸€ç§æ–¹å¼å°±æ˜¯å¯ä»¥åˆ›å»ºå‡ºç±»ä¼¼èš‚èšé‡‘æœè¿™ç§ç›´æŽ¥å°†ä¸šåŠ¡é›†ç¾¤ master è¿è¡Œåœ¨å…ƒé›†ç¾¤ï¼Œnode èŠ‚ç‚¹åœ¨ä¸šåŠ¡é›†ç¾¤ï¼Œç¬¬äºŒç§æ˜¯ä»¥äºŒè¿›åˆ¶æ–¹å¼åˆ›å»ºä¸šåŠ¡é›†ç¾¤ï¼Œå…¶ä¸­ä¸šåŠ¡é›†ç¾¤çš„ master ä»¥åŠ node éƒ½æ˜¯åœ¨ä¸šåŠ¡é›†ç¾¤æ‰€åœ¨çš„æœºæˆ¿ï¼Œç¬¬ä¸‰ç§æ–¹å¼å°±æ˜¯åœ¨å„ç§å…¬æœ‰äº‘åŽ‚å•†åˆ›å»ºé›†ç¾¤ï¼Œä»¥ä¸€ç§ç»Ÿä¸€çš„æ–¹å¼ç®¡ç†å…¬æœ‰äº‘ä¸Šçš„é›†ç¾¤ï¼Œä¹Ÿå¯ä»¥ç§°ä½œèžåˆäº‘ã€‚ é¡¹ç›®ç»“æž„æ€»ä½“æ¥è¯´ï¼Œé¡¹ç›®æš‚æ—¶åˆ†ä¸ºä¸‰å¤§å—ï¼š kubernetes proxyï¼šæ”¯æŒ API é€ä¼ ã€è®¿é—®æŽ§åˆ¶ç­‰åŠŸèƒ½ï¼› æŽ§åˆ¶å™¨ï¼šä¹Ÿå°±æ˜¯ kubernetes-operatorï¼Œç®¡ç†ä¸šåŠ¡é›†ç¾¤çš„ç”Ÿå‘½å‘¨æœŸï¼› é›†ç¾¤éƒ¨ç½²æ¨¡å—ï¼šç”¨æ¥éƒ¨ç½²ä¸šåŠ¡é›†ç¾¤ï¼Œç›®å‰ä¸»è¦åœ¨å¼€å‘ç¬¬äºŒç§æ–¹å¼ä½¿ç”¨äºŒè¿›åˆ¶éƒ¨ç½²ä¸šåŠ¡é›†ç¾¤ï¼› kubernetes åº”ç”¨å®‰è£…æ¨¡å—ï¼šåœ¨æ–°å»ºå®Œæˆçš„é›†ç¾¤ä¸­éƒ¨ç½²ç›‘æŽ§ã€æ—¥å¿—é‡‡é›†ã€é•œåƒä»“åº“ã€helm ç­‰ç»„ä»¶ï¼› æŽ§åˆ¶å™¨æŽ§åˆ¶å™¨ä¹Ÿå°±æ˜¯ Operator + CRï¼Œç›®å‰å¼€å‘ operator çš„æ–¹å¼å·²çŸ¥çš„æœ‰ä¸‰ç§ï¼š è‡ªå®šä¹‰ controller çš„æ–¹å¼ï¼škube-controller-manager ä¸­æ‰€æœ‰çš„ controller å°±æ˜¯ä»¥è‡ªå®šä¹‰ controller çš„æ–¹å¼ï¼Œè¿™ç§æ–¹å¼æ˜¯æœ€åŽŸç”Ÿçš„æ–¹å¼ï¼Œéœ€è¦å¼€å‘è€…äº†è§£ kubernetes ä¸­çš„ä»£ç ç”Ÿæˆï¼Œinformer çš„ä½¿ç”¨ç­‰ã€‚ operator-sdk çš„æ–¹å¼ï¼šä¸€ä¸ªå¼€å‘ Operator çš„æ¡†æž¶ï¼Œå¯¹äºŽä¸€äº›ä¸ç†Ÿæ‚‰ kubernetes çš„å¯ä»¥ä½¿ç”¨ operator-sdk çš„æ–¹å¼ï¼Œè¿™ç§æ–¹å¼è®©å¼€å‘è€…æ›´æ³¨é‡ä¸šåŠ¡çš„å®žçŽ°ï¼Œä½†æ˜¯ä¸å¤Ÿçµæ´»ã€‚ kubebuilder çš„æ–¹å¼ï¼škubebuilder æ˜¯å¼€å‘ controller manager çš„æ¡†æž¶ï¼Œcontroller manager ä¼šç®¡ç†ä¸€ä¸ªæˆ–è€…å¤šä¸ª operatorã€‚ kubebuilder, operator-sdk éƒ½æ˜¯å¯¹controller-runtimeåšäº†å°è£…, controller runtimeåˆæ˜¯å¯¹client-go shardInfromer åšçš„å°è£…ï¼Œæœ¬è´¨ä¸Šå…¶å®žéƒ½ä¸€æ ·çš„ã€‚kubernetes-operator ä½¿ç”¨çš„æ˜¯è‡ªå®šä¹‰ controller çš„æ–¹å¼ï¼Œå¦‚æžœæƒ³è¦æ›´æ·±å…¥çš„å­¦ä¹  kubernetesï¼Œéžè‡ªå®šä¹‰ controller æ–¹å¼èŽ«å±žäº†ï¼Œkube-controller-manager ç»„ä»¶ä¸­çš„å„ç§ controller éƒ½æ˜¯ä½¿ç”¨è¿™ç§æ–¹å¼å¼€å‘çš„ï¼Œå®Œå…¨å¯ä»¥æŒ‰ç…§å®˜æ–¹è¿™ç§å¥—è·¯æ¥å¼€å‘ã€‚åœ¨ kubernetes ä¸­ï¼Œç›®å‰æœ‰ä¸¤ç§æ–¹å¼å¯ä»¥å®šä¹‰ä¸€ä¸ªæ–°å¯¹è±¡ï¼Œä¸€æ˜¯ CustomResourceDefinitionï¼ˆCRDï¼‰ã€äºŒæ˜¯ Aggregation ApiServerï¼ˆAAï¼‰ï¼Œå…¶ä¸­ CRD æ˜¯ç›¸å¯¹ç®€å•ä¹Ÿæ˜¯ç›®å‰åº”ç”¨æ¯”è¾ƒå¹¿çš„æ–¹æ³•ã€‚kubernetes-operator é‡‡ç”¨ CRD çš„æ–¹å¼ã€‚ é›†ç¾¤éƒ¨ç½²å…¶å®žé¡¹ç›®ä¸­æœ€éš¾çš„æ˜¯é›†ç¾¤éƒ¨ç½²è¿™ä¸€éƒ¨åˆ†ï¼Œéƒ¨ç½²é›†ç¾¤ç›®å‰æœ‰ä¸¤ç§æ–¹å¼ï¼ŒäºŒè¿›åˆ¶éƒ¨ç½²å’Œå®¹å™¨åŒ–éƒ¨ç½²ï¼Œä½†æ˜¯éƒ½æœ‰ä¸€äº›å¼€æºå·¥å…·çš„æ”¯æŒã€‚æ‰‹åŠ¨éƒ¨ç½²ä¸€ä¸ªäºŒè¿›åˆ¶é›†ç¾¤éœ€è¦ç†Ÿæ‚‰ docker çš„éƒ¨ç½²ã€etcd çš„éƒ¨ç½²ã€è§’è‰²è¯ä¹¦çš„åˆ›å»ºã€RBAC æŽˆæƒã€ç½‘ç»œé…ç½®ã€yaml æ–‡ä»¶ç¼–å†™ã€kubernetes é›†ç¾¤è¿ç»´ç­‰ç­‰ï¼Œæ€»ä¹‹æ‰‹åŠ¨éƒ¨ç½²ä¸€ä¸ªäºŒè¿›åˆ¶é›†ç¾¤æ˜¯éžå¸¸éº»çƒ¦çš„ï¼Œä½†æ˜¯è¦çœŸæ­£ä¼šç”¨ kubernetes æ˜¯é€ƒä¸äº†éƒ¨ç½²è¿™ä¸€æ­¥çš„ã€‚ç¬¬äºŒç§æ–¹å¼å°±æ˜¯ä»¥å®¹å™¨åŒ–çš„æ–¹å¼éƒ¨ç½²ï¼Œè¿™ç§éƒ¨ç½²æ–¹å¼ç›¸å¯¹æ¥è¯´æ¯”è¾ƒç®€å•ï¼Œæœ‰çŽ°æˆçš„å·¥å…·ç›´æŽ¥å‚»ç“œå¼æ“ä½œå°±èƒ½éƒ¨ç½²æˆåŠŸã€‚ä½†æ˜¯æˆ‘ç›®å‰é€‰æ‹©çš„æ˜¯ä½¿ç”¨äºŒè¿›åˆ¶çš„éƒ¨ç½²æ–¹å¼ï¼Œç”±äºŽè‡ªå·±è¿ç»´è¿‡äºŒè¿›åˆ¶çš„ kubernetes é›†ç¾¤ï¼Œå¯¹äºŽç§æœ‰äº‘åœºæ™¯ä¸€èˆ¬éƒ½æ˜¯ç›´æŽ¥å°†é›†ç¾¤éƒ¨ç½²åœ¨ç‰©ç†æœºä¸Šï¼Œä½œä¸ºç”Ÿäº§çŽ¯å¢ƒï¼Œè‡ªå·±è®¤ä¸ºå®¹å™¨åŒ–çš„æ–¹å¼éƒ¨ç½²è¿˜ä¸æ˜¯éžå¸¸æˆç†Ÿçš„ï¼Œç›®å‰å·¥ä½œè¿‡çš„å¤§å°å…¬å¸ä¸­ï¼Œç”Ÿäº§çŽ¯å¢ƒæš‚æ—¶æ²¡æœ‰ä»¥å®¹å™¨åŒ–çš„æ–¹å¼è¿è¡Œé›†ç¾¤ã€‚æ‰€ä»¥ kubernetes-operator ä¸­ç›®å‰ä¸»è¦æ”¯æŒçš„å°±æ˜¯ä½¿ç”¨äºŒè¿›åˆ¶éƒ¨ç½²é›†ç¾¤ã€‚ ç›®å‰æ¯”è¾ƒæˆç†Ÿçš„ç”¨äºŽç”Ÿäº§çŽ¯å¢ƒçš„ kubernetes é›†ç¾¤éƒ¨ç½²å·¥å…·æœ‰ï¼škubeadmã€kubesparyã€kopsã€rancherã€kubeasz ç­‰ã€‚kubeadmã€kubesparyã€kops éƒ½æ˜¯å®˜æ–¹å¼€æºçš„äº§å“ï¼Œkubeadm ä½¿ç”¨å®¹å™¨åŒ–çš„æ–¹å¼éƒ¨ç½²ï¼Œéœ€è¦æ‰‹åŠ¨æ‰§è¡Œä¸€äº›éƒ¨ç½²å‘½ä»¤ï¼Œæš‚æ—¶æ— æ³•å®Œå…¨è‡ªåŠ¨åŒ–éƒ¨ç½²ã€‚kubespary æ˜¯å¯¹ kubeadm çš„ä¸€å±‚å°è£…ï¼Œä½¿ç”¨ ansible + kubeadm çš„æ–¹å¼è‡ªåŠ¨åŒ–è¿›è¡Œéƒ¨ç½²ï¼Œæ®è¯´é˜¿é‡Œäº‘å°±æ˜¯ä½¿ç”¨ kubespary éƒ¨ç½²é›†ç¾¤çš„ã€‚åœ¨å…¬æœ‰äº‘çš„çŽ¯å¢ƒ(GCPã€AWS)é€šå¸¸ä½¿ç”¨ kops éƒ¨ç½²èµ·æ¥æ›´æ–¹ä¾¿äº›ã€‚kubeasz æ˜¯ä½¿ç”¨ ansible è‡ªåŠ¨åŒ–çš„æ–¹å¼éƒ¨ç½²äºŒè¿›åˆ¶é›†ç¾¤ï¼Œç›®å‰ä¹Ÿå·²ç»æ¯”è¾ƒæˆç†Ÿäº†ã€‚ åº”ç”¨å®‰è£… ç›‘æŽ§ï¼šå½“ç„¶æ˜¯ä½¿ç”¨ promethusï¼› æ—¥å¿—é‡‡é›†ï¼šä½¿ç”¨ filebeat æˆ–è€…åŸºäºŽ filebeat å°è£…çš„ä¸€äº›ç»„ä»¶å¦‚ logpilotï¼Œå…¶ä»–çš„è¿˜æœ‰ logkit ç­‰éƒ½å¯ä»¥å°è¯•ä½¿ç”¨ï¼› é•œåƒä»“åº“ï¼šå½“ç„¶æ˜¯ä½¿ç”¨ harborï¼› HPAï¼šç»„ä»¶ä»¥åŠåº”ç”¨çš„è‡ªåŠ¨æ‰©ç¼©å®¹ï¼› åº”ç”¨å®‰è£…ä½¿ç”¨ helm çš„æ–¹å¼è¿›è¡Œå®‰è£…ã€‚ é›†ç¾¤å‡çº§è‹¥ä»¥äºŒè¿›åˆ¶éƒ¨ç½²æœ€å¥½æ˜¯æ›¿æ¢äºŒè¿›åˆ¶æ–‡ä»¶çš„æ–¹å¼è¿›è¡Œå‡çº§ï¼Œè‹¥ä½¿ç”¨å®¹å™¨åŒ–éƒ¨ç½²ï¼Œmaster éƒ¨ç½²åœ¨å…ƒé›†ç¾¤ä¸­å¯ä»¥ä½¿ç”¨ kubernetes çš„æ»šåŠ¨æ–¹å¼å‡çº§å¦åˆ™è¦ä»¥ä¿®æ”¹ manifest æ–‡ä»¶çš„æ–¹å¼ã€‚ é›†ç¾¤å‡çº§åŒ…æ‹¬é…ç½®å’Œç‰ˆæœ¬çš„å‡çº§ï¼Œé›†ç¾¤éƒ¨ç½²å®ŒæˆåŽï¼Œmaster çš„é…ç½®æ”¹åŠ¨ä¸ä¼šå¾ˆé¢‘ç¹ï¼Œç”±äºŽè¦è¿›è¡Œæ€§èƒ½ä¸Šçš„ä¼˜åŒ–ä»¥åŠä¸šåŠ¡çš„æ”¯æŒï¼Œå¯¹äºŽ node ç»„ä»¶ä¸Šçš„é…ç½®å‡çº§è¿˜æ˜¯æ¯”è¾ƒå¤šçš„ã€‚å¯¹äºŽé›†ç¾¤çš„ç‰ˆæœ¬å‡çº§ï¼Œå‡çº§çš„éš¾åº¦ç³»æ•°éšç€ç‰ˆæœ¬çš„è·¨åº¦å¢žå¤§è€Œå¢žå¤§ï¼Œè‹¥æŒ‰ç…§å®˜æ–¹çš„å‡çº§æµç¨‹ï¼Œä¸€èˆ¬ä¸ä¼šå‡ºçŽ°å¼‚å¸¸ã€‚å‡çº§æ“ä½œä¸€èˆ¬éƒ½æ˜¯å…ˆå‡ master å†å‡ nodeï¼Œåœ¨å·¥ä½œä¸­ç»åŽ†çš„å‡ æ¬¡ç‰ˆæœ¬å‡çº§ä¸­ï¼Œæ¯æ¬¡å‡çº§å®Œ master åŽç†è®ºä¸Šä¸ä¼šå†å›žé€€äº†ï¼Œé™¤éžå‡çº§è¿‡ç¨‹ä¸­æœ‰é—®é¢˜ï¼Œå¦åˆ™å‡çº§å®ŒæˆåŽå·²ç»å¾ˆéš¾å›žé€€äº†ï¼Œmaster å‡çº§å®ŒæˆåŽ APIServer çš„ä¸€äº› API è¿˜æœ‰ pod çš„å­—æ®µéƒ½æœ‰å¯èƒ½æ”¹å˜ï¼Œmaster ç‰ˆæœ¬å›žé€€åŽä¸€äº›å·²å­˜åœ¨çš„åº”ç”¨å¯èƒ½ä¼šå¼‚å¸¸ï¼Œæˆ–è€…è¿˜å¯ä»¥å‚è€ƒ openshift çš„è“ç»¿å‡çº§æ–¹å¼ã€‚äºŒè¿›åˆ¶éƒ¨ç½²çš„é›†ç¾¤å°½é‡ä»¥æ›¿æ¢äºŒè¿›åˆ¶æ–‡ä»¶çš„æ–¹å¼è¿›è¡Œå‡çº§ï¼Œå¯¹äºŽå®¹å™¨åŒ–éƒ¨ç½²çš„é›†ç¾¤ï¼Œå¯ä»¥ç›´æŽ¥ä½¿ç”¨ kubernetes çš„æ»šåŠ¨æ–¹å¼å‡çº§æˆ–è€…æ˜¯ä¿®æ”¹ manifest æ–‡ä»¶çš„æ–¹å¼ã€‚ ç›®å‰èš‚èšé‡‘æœ kube-on-kube-operator æž¶æž„ä¸­åœ¨ä¸šåŠ¡é›†ç¾¤ä¸­ä¼šéƒ¨ç½²ä¸€ä¸ª node-operatorï¼Œnode-operator ä¼šè®°å½• master ç»„ä»¶çš„é•œåƒã€é»˜è®¤å¯åŠ¨å‚æ•°ç­‰ä¿¡æ¯ï¼Œå…¶ä½œç”¨å°±æ˜¯èŠ‚ç‚¹é…ç½®ç®¡ç†ã€é›†ç¾¤ç»„ä»¶å‡çº§ä»¥åŠèŠ‚ç‚¹æ•…éšœè‡ªæ„ˆï¼Œæœªæ¥åœ¨é¡¹ç›®ä¸­ä¹Ÿä¼šå®žçŽ°åŸºäºŽæ­¤çš„æ–¹å¼ã€‚ åŽæœŸè®¡åˆ’ æ”¯æŒéƒ¨ç½² k3sã€kubeedgeï¼š5G æ—¶ä»£ï¼Œè¾¹ç¼˜è®¡ç®—å°†æ˜¯éžå¸¸ç«çš„ï¼Œç›®å‰å„å¤§åŽ‚å•†ä¹Ÿéƒ½åœ¨æ­¤å¸ƒå±€ï¼Œæ‰€ä»¥æ”¯æŒéƒ¨ç½² k3sã€kubeedge è¿™äº›ä¸“é—¨æ”¯æŒè¾¹ç¼˜è®¡ç®—çš„äº§å“è¿˜æ˜¯éžå¸¸æœ‰å¿…è¦çš„ã€‚ æ”¯æŒä½¿ç”¨ kops éƒ¨ç½² æ”¯æŒéƒ¨ç½²å¤šç‰ˆæœ¬ k8s node-operator å¼€å‘ï¼Œæ”¯æŒé›†ç¾¤çš„é…ç½®ç®¡ç†ã€è‡ªåŠ¨åŒ–å‡çº§ã€æ•…éšœè‡ªæ„ˆç­‰åŠŸèƒ½ ç”¨æˆ·åŠæƒé™ç®¡ç†ï¼šæ“ä½œé›†ç¾¤ç”¨æˆ·çš„æƒé™å’Œ kubernetes ä¸­ RBAC è§„åˆ™ç»‘å®š Kubernetes-operator ä¸€äº›åŠŸèƒ½çš„æ‰©å±•å’Œå®Œå–„ å‚è€ƒï¼š è…¾è®¯äº‘å®¹å™¨æœåŠ¡TKEï¼šä¸€é”®éƒ¨ç½²å®žè·µ ä¸€å¹´æ—¶é—´æ‰“é€ å…¨çƒæœ€å¤§è§„æ¨¡ä¹‹ä¸€çš„Kubernetesé›†ç¾¤ï¼Œèš‚èšé‡‘æœæ€Žä¹ˆåšåˆ°çš„ï¼Ÿ https://github.com/gosoon/kubernetes-operator ç›¸å…³æŽ¨èkube-on-kube-operator å¼€å‘(ä¸‰)kube-on-kube-operator å¼€å‘(äºŒ)]]></content>
      <tags>
        <tag>operator</tag>
        <tag>kube-on-kube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[éƒ¨ç½²é«˜å¯ç”¨ kubernetes é›†ç¾¤]]></title>
    <url>%2F2019%2F07%2F12%2Fk8s_components_ha%2F</url>
    <content type="text"><![CDATA[kubernetes è™½ç„¶å…·æœ‰æ•…éšœè‡ªæ„ˆå’Œå®¹é”™èƒ½åŠ›ï¼Œä½†æŸäº›ç»„ä»¶çš„å¼‚å¸¸ä¼šå¯¼è‡´æ•´ä¸ªé›†ç¾¤ä¸å¯ç”¨ï¼Œç”Ÿäº§çŽ¯å¢ƒä¸­å°†å…¶éƒ¨ç½²ä¸ºé«˜å¯ç”¨è¿˜æ˜¯éžå¸¸æœ‰å¿…è¦çš„ï¼Œæœ¬æ–‡ä¼šä»‹ç»å¦‚ä½•æž„å»ºä¸€ä¸ªé«˜å¯ç”¨çš„ Kubernetes é›†ç¾¤ã€‚kuber-controller-manager å’Œ kube-scheduler çš„é«˜å¯ç”¨å®˜æ–¹å·²ç»å®žçŽ°äº†ï¼Œéƒ½æ˜¯é€šè¿‡ etcd å…¨å±€é”è¿›è¡Œé€‰ä¸¾å®žçŽ°çš„ï¼Œetcd æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ï¼Œå¼ºä¸€è‡´çš„ï¼ˆæ»¡è¶³ CAP çš„ CPï¼‰KV å­˜å‚¨ç³»ç»Ÿï¼Œå…¶å¤©ç„¶å…·å¤‡é«˜å¯ç”¨ã€‚è€Œ apiserver ä½œä¸ºæ•´ä¸ªç³»ç»Ÿçš„æ ¸å¿ƒï¼Œæ‰€æœ‰å¯¹æ•°æ®çš„ä¿®æ”¹æ“ä½œéƒ½æ˜¯é€šè¿‡ apiserver é—´æŽ¥æ“ä½œ etcd çš„ï¼Œæ‰€ä»¥ apiserver çš„é«˜å¯ç”¨å®žçŽ°æ˜¯æ¯”è¾ƒå…³é”®çš„ã€‚ kube-apiserver çš„é«˜å¯ç”¨é…ç½®apiserver æœ¬èº«æ˜¯æ— çŠ¶æ€çš„ï¼Œå¯ä»¥æ¨ªå‘æ‰©å±•ï¼Œå…¶å€ŸåŠ©å¤–éƒ¨è´Ÿè½½å‡è¡¡è½¯ä»¶é…ç½®é«˜å¯ç”¨ä¹Ÿç›¸å¯¹å®¹æ˜“ï¼Œå®žçŽ°æ–¹æ¡ˆæ¯”è¾ƒå¤šï¼Œä½†ä¸€èˆ¬ä¼šé‡‡ç”¨å¤–éƒ¨ç»„ä»¶ LVS æˆ– HAProxy çš„æ–¹å¼å®žçŽ°ï¼Œæˆ‘ä»¬ç”Ÿäº§çŽ¯å¢ƒæ˜¯é€šè¿‡ LVS å®žçŽ°çš„ã€‚apiserver çš„é«˜å¯ç”¨å¯ä»¥åˆ†ä¸ºé›†ç¾¤å¤–é«˜å¯ç”¨å’Œé›†ç¾¤å†…é«˜å¯ç”¨ã€‚é›†ç¾¤å¤–é«˜å¯ç”¨æŒ‡å¯¹äºŽç›´æŽ¥è°ƒç”¨ k8s API çš„å¤–éƒ¨ç”¨æˆ·ï¼ˆä¾‹å¦‚ kubectl ã€kubeletï¼‰ï¼Œå®¢æˆ·ç«¯éœ€è¦è°ƒç”¨ apiserver çš„ VIP ä»¥è¾¾åˆ°é«˜å¯ç”¨ï¼Œæ­¤å¤„ LVS çš„éƒ¨ç½²ä»¥åŠ VIP çš„é…ç½®ä¸å†è¯¦ç»†è¯´æ˜Žã€‚ é›†ç¾¤å†…çš„é«˜å¯ç”¨é…ç½®æ˜¯æŒ‡å¯¹äºŽéƒ¨ç½²åˆ°é›†ç¾¤ä¸­çš„ pod è®¿é—® kubernetesï¼Œkubernetes é›†ç¾¤åˆ›å»ºå®ŒæˆåŽé»˜è®¤ä¼šå¯åŠ¨ä¸€ä¸ªkubernetesçš„ service ä¾›é›†ç¾¤å†…çš„ pod è®¿é—®ï¼Œservice çš„ ClusterIP é»˜è®¤å€¼ä¸º 172.0.0.1 ï¼Œæ¯ä¸€ä¸ª service å¯¹è±¡ç”Ÿæˆæ—¶ï¼Œéƒ½ä¼šç”Ÿæˆä¸€ä¸ªç”¨äºŽæš´éœ²è¯¥å¯¹è±¡åŽç«¯å¯¹åº” pod çš„å¯¹è±¡ endpointsï¼Œendpoints ä¸­å¯ä»¥çœ‹åˆ° apiserver çš„å®žä¾‹ã€‚è®¿é—® kubernetes çš„ serviceï¼Œservice ä¼šå°†è¯·æ±‚è½¬å‘åˆ° endpoints ä¸­çš„ ip ä¸Šï¼Œæ­¤æ—¶è‹¥ service ä¸­çš„ endpoints ä¸­æ²¡æœ‰ IPï¼Œåˆ™è¡¨ç¤º apiserver æ— æ³•è®¿é—®ã€‚ 1234567$ kubectl get svc kubernetesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 172.0.0.1 &lt;none&gt; 443/TCP 21d$ kubectl get endpoints kubernetesNAME ENDPOINTS AGEkubernetes 10.0.2.15:6443, 10.0.2.16:6443 21d kubernetes v1.9 ä¹‹å‰ kube-apiserver service çš„é«˜å¯ç”¨ä¹Ÿå°±æ˜¯ master ip è¦åŠ å…¥åˆ° kubernetes service çš„ endpoints ä¸­å¿…é¡»è¦åœ¨å‚æ•°ä¸­æŒ‡å®š --apiserver-count çš„å€¼ï¼Œv1.9 å‡ºçŽ°äº†å¦å¤–ä¸€ä¸ªå‚æ•° --endpoint-reconciler-type è¦å–ä»£ä»¥å‰çš„ --apiserver-countï¼Œä½†æ˜¯æ­¤æ—¶è¯¥å‚æ•°é»˜è®¤æ˜¯ç¦ç”¨çš„ï¼ˆAlpha ç‰ˆæœ¬ï¼‰ï¼Œv1.10 ä¹Ÿæ˜¯é»˜è®¤ç¦ç”¨çš„ã€‚v1.11 ä¸­ --endpoint-reconciler-type å‚æ•°é»˜è®¤å¼€å¯äº†ï¼Œé»˜è®¤å€¼æ˜¯ leaseã€‚--apiserver-count å‚æ•°ä¼šåœ¨ v1.13 ä¸­è¢«ç§»é™¤ã€‚v1.11 å’Œ v1.12 ä¸­è¿˜å¯ä»¥ä½¿ç”¨ --apiserver-countï¼Œä½†å‰ææ˜¯éœ€è¦è®¾ç½® --endpoint-reconciler-type=master-countã€‚ä¹Ÿå°±æ˜¯è¯´åœ¨ v1.11 ä»¥åŠä¹‹åŽçš„ç‰ˆæœ¬ä¸­ apiserver ä¸­ä¸éœ€è¦è¿›è¡Œé…ç½®äº†ï¼Œå¯ç”¨äº†å‡ ä¸ª apiserver å®žä¾‹é»˜è®¤éƒ½ä¼šåŠ åˆ° å¯¹åº”çš„ endpoints ä¸­ã€‚ kube-controller-manager å’Œ kube-scheduler çš„é«˜å¯ç”¨é…ç½®kube-controller-manager å’Œ kube-scheduler æ˜¯ç”± leader election å®žçŽ°é«˜å¯ç”¨çš„ï¼Œé€šè¿‡å‘ apiserver ä¸­çš„ endpoint åŠ é”çš„æ–¹å¼æ¥è¿›è¡Œ leader electionï¼Œ å¯ç”¨ leader election éœ€è¦åœ¨ç»„ä»¶çš„é…ç½®ä¸­åŠ å…¥ä»¥ä¸‹å‡ ä¸ªå‚æ•°ï¼š 12345--leader-elect=true--leader-elect-lease-duration=15s--leader-elect-renew-deadline=10s--leader-elect-resource-lock=endpoints--leader-elect-retry-period=2s ç»„ä»¶å½“å‰çš„ leader ä¼šå†™åœ¨ endpoints çš„ holderIdentity å­—æ®µä¸­ï¼Œ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ç»„ä»¶å½“å‰çš„ leader: 123$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml $ kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml å…³äºŽ kube-controller-manager å’Œ kube-scheduler é«˜å¯ç”¨çš„å®žçŽ°ç»†èŠ‚å¯ä»¥å‚è€ƒä¹‹å‰å†™çš„ä¸€ç¯‡æ–‡ç« ï¼škubernets ä¸­ç»„ä»¶é«˜å¯ç”¨çš„å®žçŽ°æ–¹å¼ã€‚ etcd çš„é«˜å¯ç”¨é…ç½®etcd æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼é›†ç¾¤ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæœ‰çŠ¶æ€çš„æœåŠ¡ï¼Œå…¶å¤©ç”Ÿå°±æ˜¯é«˜å¯ç”¨çš„æž¶æž„ã€‚ä¸ºäº†é˜²æ­¢ etcd è„‘è£‚ï¼Œå…¶ç»„æˆ etcd é›†ç¾¤çš„ä¸ªæ•°ä¸€èˆ¬ä¸ºå¥‡æ•°ä¸ª(3 æˆ– 5 ä¸ªèŠ‚ç‚¹) ã€‚è‹¥ä½¿ç”¨ç‰©ç†æœºæ­å»º k8s é›†ç¾¤ï¼Œç†è®ºä¸Šé›†ç¾¤çš„è§„æ¨¡ä¹Ÿä¼šæ¯”è¾ƒå¤§ï¼Œæ­¤æ—¶ etcd ä¹Ÿåº”è¯¥ä½¿ç”¨ 3 ä¸ªæˆ–è€…5 ä¸ªèŠ‚ç‚¹éƒ¨ç½²ä¸€å¥—ç‹¬ç«‹è¿è¡Œçš„é›†ç¾¤ã€‚è‹¥æƒ³è¦å¯¹ etcd åšåˆ°è‡ªåŠ¨åŒ–è¿ç»´ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ etcd-operator å°† etcd é›†ç¾¤éƒ¨ç½²åœ¨ k8s ä¸­ã€‚ kubernetes ä¸­ç»„ä»¶é«˜å¯ç”¨éƒ¨ç½²çš„ä¸€ä¸ªæž¶æž„å›¾ï¼š æ€»ç»“æœ¬æ–‡ä¸»è¦ä»‹ç»å¦‚ä½•é…ç½®ä¸€ä¸ªé«˜å¯ç”¨ kubernetes é›†ç¾¤ï¼Œkubernetes æ–°ç‰ˆæœ¬å·²ç»è¶Šæ¥è¶Šè¶‹è¿‘å…¨é¢ TLS + RBAC é…ç½®ï¼Œè‹¥ kubernetes é›†ç¾¤è¿˜åœ¨ä½¿ç”¨ 8080 ç«¯å£ï¼Œæ­¤æ—¶æ¯ä¸ª master èŠ‚ç‚¹ä¸Šçš„ kube-controller-manager å’Œ kube-scheduler éƒ½æ˜¯é€šè¿‡ 8080 ç«¯å£è¿žæŽ¥ apiserverï¼Œè‹¥èŠ‚ç‚¹ä¸Šçš„ apiserver æŒ‚æŽ‰ï¼Œåˆ™ kube-controller-manager å’Œ kube-scheduler ä¹Ÿä¼šéšä¹‹æŒ‚æŽ‰ã€‚apiserver ä½œä¸ºé›†ç¾¤çš„æ ¸å¿ƒç»„ä»¶ï¼Œå…¶å¿…é¡»é«˜å¯ç”¨éƒ¨ç½²ï¼Œå…¶ä»–ç»„ä»¶å®žçŽ°é«˜å¯ç”¨ç›¸å¯¹å®¹æ˜“ã€‚ å‚è€ƒï¼š https://k8smeetup.github.io/docs/admin/high-availability/ ç›¸å…³æŽ¨èkubernetesä»Žå…¥é—¨åˆ°æ”¾å¼ƒ--k8såŸºæœ¬æ¦‚å¿µ]]></content>
      <tags>
        <tag>kubernetes</tag>
        <tag>HA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes è‡ªå®šä¹‰èµ„æºï¼ˆCRDï¼‰çš„æ ¡éªŒ]]></title>
    <url>%2F2019%2F07%2F02%2Fk8s_crd_verify%2F</url>
    <content type="text"><![CDATA[åœ¨ä»¥å‰çš„ç‰ˆæœ¬è‹¥è¦å¯¹ apiserver çš„è¯·æ±‚åšä¸€äº›è®¿é—®æŽ§åˆ¶ï¼Œå¿…é¡»ä¿®æ”¹ apiserver çš„æºä»£ç ç„¶åŽé‡æ–°ç¼–è¯‘éƒ¨ç½²ï¼Œéžå¸¸éº»çƒ¦ä¹Ÿä¸çµæ´»ï¼Œapiserver ä¹Ÿæ”¯æŒä¸€äº›åŠ¨æ€çš„å‡†å…¥æŽ§åˆ¶å™¨ï¼Œåœ¨ apiserver é…ç½®ä¸­çœ‹åˆ°çš„ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota ç­‰éƒ½æ˜¯ apiserver çš„å‡†å…¥æŽ§åˆ¶å™¨ï¼Œä½†è¿™äº›éƒ½æ˜¯ kubernetes ä¸­é»˜è®¤å†…ç½®çš„ã€‚åœ¨ v1.9 ä¸­ï¼Œkubernetes çš„åŠ¨æ€å‡†å…¥æŽ§åˆ¶å™¨åŠŸèƒ½ä¸­æ”¯æŒäº† Admission Webhooksï¼Œå³ç”¨æˆ·å¯ä»¥ä»¥æ’ä»¶çš„æ–¹å¼å¯¹ apiserver çš„è¯·æ±‚åšä¸€äº›è®¿é—®æŽ§åˆ¶ï¼Œè¦ä½¿ç”¨è¯¥åŠŸèƒ½éœ€è¦è‡ªå·±å†™ä¸€ä¸ª admission webhookï¼Œapiserver ä¼šåœ¨è¯·æ±‚é€šè¿‡è®¤è¯å’ŒæŽˆæƒä¹‹åŽã€å¯¹è±¡è¢«æŒä¹…åŒ–ä¹‹å‰æ‹¦æˆªè¯¥è¯·æ±‚ï¼Œç„¶åŽè°ƒç”¨ webhook å·²è¾¾åˆ°å‡†å…¥æŽ§åˆ¶ï¼Œæ¯”å¦‚ Istio ä¸­ sidecar çš„æ³¨å…¥å°±æ˜¯é€šè¿‡è¿™ç§æ–¹å¼å®žçŽ°çš„ï¼Œåœ¨åˆ›å»º Pod é˜¶æ®µ apiserver ä¼šå›žè°ƒ webhook ç„¶åŽå°† Sidecar ä»£ç†æ³¨å…¥è‡³ç”¨æˆ· Podã€‚ æœ¬æ–‡ä¸»è¦ä»‹ç»å¦‚ä½•ä½¿ç”¨ AdmissionWebhook å¯¹ CR çš„æ ¡éªŒï¼Œä¸€èˆ¬åœ¨å¼€å‘ operator è¿‡ç¨‹ä¸­ï¼Œéƒ½æ˜¯é€šè¿‡å¯¹ CR çš„æ“ä½œå®žçŽ°æŸä¸ªåŠŸèƒ½çš„ï¼Œè‹¥ CR ä¸è§„èŒƒå¯èƒ½ä¼šå¯¼è‡´æŸäº›é—®é¢˜ï¼Œæ‰€ä»¥å¯¹æäº¤ CR çš„æ ¡éªŒæ˜¯ä¸å¯é¿å…çš„ä¸€ä¸ªæ­¥éª¤ã€‚ kubernetes ç›®å‰æä¾›äº†ä¸¤ç§æ–¹å¼æ¥å¯¹ CR çš„æ ¡éªŒï¼Œè¯­æ³•æ ¡éªŒ(OpenAPI v3 schemaï¼‰ å’Œè¯­ä¹‰æ ¡éªŒ(validatingadmissionwebhookï¼‰ã€‚ CRD çš„ä¸€ä¸ªç¤ºä¾‹ï¼š 1234567891011121314151617181920212223242526272829apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt; name: kubernetesclusters.ecs.yun.comspec: # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt; group: ecs.yun.com # list of versions supported by this CustomResourceDefinition versions: - name: v1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt; plural: kubernetesclusters # singular name to be used as an alias on the CLI and for display singular: kubernetescluster # kind is normally the CamelCased singular type. Your resource manifests use this. kind: KubernetesCluster # listKind listKind: KubernetesClusterList # shortNames allow shorter string to match your resource on the CLI shortNames: - ecs CRD çš„ä¸€ä¸ªå¯¹è±¡ï¼š 1234567891011121314apiVersion: ecs.yun.com/v1kind: KubernetesClustermetadata: name: test-clusterspec: clusterType: kubernetes serviceCIDR: &apos;&apos; masterList: - ip: 192.168.1.10 nodeList: - ip: 192.168.1.11 privateSSHKey: &apos;&apos; scaleUp: 0 scaleDown: 0 ä¸€ã€OpenAPI v3 schemaOpenAPI æ˜¯é’ˆå¯¹ REST API çš„ API æè¿°æ ¼å¼ï¼Œä¹Ÿæ˜¯ä¸€ç§è§„èŒƒã€‚ 1234567891011121314151617181920212223242526272829303132333435apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: kubernetesclusters.ecs.yun.comspec: group: ecs.yun.com versions: - name: v1 served: true storage: true scope: Namespaced names: plural: kubernetesclusters singular: kubernetescluster kind: KubernetesCluster listKind: KubernetesClusterList shortNames: - ecs validation: openAPIV3Schema: properties: spec: type: object required: - clusterType - masterList - nodeList properties: clusterType: type: string scaleUp: type: integer scaleDown: type: integer minimum: 0 ä¸Šé¢æ˜¯ä½¿ç”¨ OpenAPI v3 æ£€éªŒçš„ä¸€ä¸ªä¾‹å­ï¼ŒOpenAPI v3 ä»…æ”¯æŒä¸€äº›ç®€å•çš„æ ¡éªŒè§„åˆ™ï¼Œå¯ä»¥æ ¡éªŒå‚æ•°çš„ç±»åž‹ï¼Œå‚æ•°å€¼çš„ç±»åž‹(æ”¯æŒæ­£åˆ™)ï¼Œæ˜¯å¦ä¸ºå¿…è¦å‚æ•°ç­‰ï¼Œä½†è‹¥è¦ä½¿ç”¨ä¸Žã€æˆ–ã€éžç­‰æ“ä½œå¯¹å¤šä¸ªå­—æ®µåŒæ—¶æ ¡éªŒè¿˜æ˜¯åšä¸åˆ°çš„ï¼Œæ‰€ä»¥é’ˆå¯¹ä¸€äº›ç‰¹å®šåœºæ™¯çš„æ ¡éªŒéœ€è¦ä½¿ç”¨ admission webhookã€‚ äºŒã€Admission Webhooksadmission control åœ¨ apiserver ä¸­è¿›è¡Œé…ç½®çš„ï¼Œä½¿ç”¨--enable-admission-plugins æˆ– --admission-controlè¿›è¡Œå¯ç”¨ï¼Œadmission control é…ç½®çš„æŽ§åˆ¶å™¨åˆ—è¡¨æ˜¯æœ‰é¡ºåºçš„ï¼Œè¶Šé å‰çš„è¶Šå…ˆæ‰§è¡Œï¼Œä¸€æ—¦æŸä¸ªæŽ§åˆ¶å™¨è¿”å›žçš„ç»“æžœæ˜¯reject çš„ï¼Œé‚£ä¹ˆæ•´ä¸ªå‡†å…¥æŽ§åˆ¶é˜¶æ®µç«‹åˆ»ç»“æŸï¼Œæ‰€ä»¥è¿™é‡Œçš„é…ç½®é¡ºåºæ˜¯æœ‰åºçš„ï¼Œå»ºè®®ä½¿ç”¨å®˜æ–¹çš„é¡ºåºé…ç½®ã€‚ åœ¨ v1.9 ä¸­ï¼Œadmission webhook æ˜¯é€šè¿‡åœ¨ --admission-control ä¸­é…ç½® ValidatingAdmissionWebhook æˆ– MutatingAdmissionWebhook æ¥æ”¯æŒä½¿ç”¨çš„ï¼Œä¸¤è€…åŒºåˆ«å¦‚ä¸‹ï¼š MutatingAdmissionWebhookï¼šå…è®¸åœ¨ webhook ä¸­å¯¹ object è¿›è¡Œ mutate ä¿®æ”¹ï¼Œä½†åŒ¹é…åˆ°çš„ webhook ä¸²è¡Œæ‰§è¡Œï¼Œå› ä¸ºæ¯ä¸ª webhook éƒ½å¯èƒ½ä¼š mutate objectã€‚ ValidatingAdmissionWebhook: ä¸å…è®¸åœ¨ webhook ä¸­å¯¹ Object è¿›è¡Œ mutate ä¿®æ”¹ï¼Œä»…è¿”å›ž true æˆ– falseã€‚ å¯ç”¨ admission webhook åŽï¼Œæ¯æ¬¡å¯¹ CR åš CRUD æ“ä½œæ—¶ï¼Œè¯·æ±‚å°±ä¼šè¢« apiserver æ‹¦ä½ï¼Œè‡³äºŽ CRUD ä¸­å“ªäº›è¯·æ±‚è¢«æ‹¦ä½éƒ½æ˜¯æå‰åœ¨ WebhookConfiguration ä¸­é…ç½®çš„ï¼Œç„¶åŽä¼šè°ƒç”¨ AdmissionWebhook è¿›è¡Œæ£€æŸ¥æ˜¯å¦ Admit é€šè¿‡ã€‚ ä¸‰ã€å¯ç”¨ Admission Webhooks åŠŸèƒ½ kubernetes ç‰ˆæœ¬ &gt;= v1.9 1ã€åœ¨ apiserver ä¸­å¼€å¯ admission webhooks åœ¨ v1.9 ç‰ˆæœ¬ä¸­ä½¿ç”¨çš„æ˜¯ï¼š 1--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota åœ¨ v1.10 ä»¥åŽä¼šå¼ƒç”¨ --admission-controlï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ --enable-admission-pluginsï¼š 1--enable-admission-plugins=NodeRestriction,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota å¯ç”¨ä¹‹åŽåœ¨ api-resources å¯ä»¥çœ‹åˆ°ï¼š 123# kubectl api-resources | grep admissionregistrationmutatingwebhookconfigurations admissionregistration.k8s.io false MutatingWebhookConfigurationvalidatingwebhookconfigurations admissionregistration.k8s.io false ValidatingWebhookConfiguration 2ã€å¯ç”¨ admissionregistration.k8s.io/v1alpha1 API 12// æ£€æŸ¥ API æ˜¯å¦å·²å¯ç”¨$ kubectl api-versions | grep admissionregistration.k8s.io è‹¥ä¸å­˜åœ¨åˆ™éœ€è¦åœ¨ apiserver çš„é…ç½®ä¸­æ·»åŠ --runtime-config=admissionregistration.k8s.io/v1alpha1ã€‚ å››ã€ç¼–å†™ Admission Webhook Serverwebhook å…¶å®žå°±æ˜¯ä¸€ä¸ª RESTful API é‡Œé¢åŠ ä¸Šè‡ªå·±çš„ä¸€äº›æ ¡éªŒé€»è¾‘ã€‚ å¯ä»¥å‚è€ƒå®˜æ–¹çš„ç¤ºä¾‹ï¼šhttps://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/webhook/main.goæˆ–è€…https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/apimachinery/webhook.go å®Œæ•´ä»£ç å‚è€ƒï¼šhttps://github.com/gosoon/admission-webhook äº”ã€éƒ¨ç½² Admission Webhook Serviceç”±äºŽ apiserver è°ƒç”¨ webhook æ—¶å¼ºåˆ¶ä½¿ç”¨ TLS è®¤è¯ï¼Œæ‰€ä»¥ WebhookConfiguration ä¸­ä¸€å®šè¦é…ç½® caBundleï¼Œä¹Ÿå°±æ˜¯éœ€è¦è‡ªå·±ç”Ÿæˆä¸€å¥—ç§æœ‰è¯ä¹¦ã€‚ ç”Ÿæˆè¯ä¹¦çš„æ–¹å¼æ¯”è¾ƒå¤šï¼Œä»¥ä¸‹ä½¿ç”¨ openssl ç”Ÿæˆï¼Œè„šæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š 1234567891011121314#!/bin/bash# Generate the CA cert and private keyopenssl req -nodes -new -x509 -days 365 -keyout ca.key -out ca.crt -subj &quot;/CN=admission-webhook CA&quot;# Generate the private key for the webhook serveropenssl genrsa -out admission-webhook-tls.key 2048# Generate a Certificate Signing Request (CSR) for the private key, and sign it with the private key of the CA.openssl req -new -key admission-webhook-tls.key -subj &quot;/CN=admission-webhook.ecs-system.svc&quot; \ | openssl x509 -days 365 -req -CA ca.crt -CAkey ca.key -CAcreateserial -out admission-webhook-tls.crt# Generate pemopenssl base64 -A &lt; ca.crt &gt; ca.pem ç”Ÿæˆè¯ä¹¦åŽå°† ca.pem ä¸­çš„å†…å®¹å¤åˆ¶åˆ° caBundle å¤„ã€‚ ValidatingWebhook yaml æ–‡ä»¶å¦‚ä¸‹ï¼š 1234567891011121314151617181920212223apiVersion: admissionregistration.k8s.io/v1beta1kind: ValidatingWebhookConfigurationmetadata: name: admission-webhookwebhooks: - name: admission-webhook.ecs-system.svc # å¿…é¡»ä¸º &lt;svc_name&gt;.&lt;svc_namespace&gt;.svc. failurePolicy: Ignore clientConfig: service: name: admission-webhook namespace: ecs-system path: /ecs/operator/cluster # webhook controller caBundle: xxx rules: - operations: # éœ€è¦æ ¡éªŒçš„æ–¹æ³• - CREATE - UPDATE apiGroups: # api group - ecs.yun.com apiVersions: # version - v1 resources: # resource - kubernetesclusters æ³¨æ„ failurePolicy å¯ä»¥ä¸º Ignoreæˆ–è€…Failï¼Œæ„å‘³ç€å¦‚æžœå’Œ webhook é€šä¿¡å‡ºçŽ°é—®é¢˜å¯¼è‡´è°ƒç”¨å¤±è´¥ï¼Œå°†æ ¹æ® failurePolicyå†³å®šå¿½ç•¥å¤±è´¥ï¼ˆadmitï¼‰è¿˜æ˜¯å‡†å…¥å¤±è´¥(reject)ã€‚ æœ€åŽå°† webhook éƒ¨ç½²åœ¨é›†ç¾¤ä¸­ã€‚ å‚è€ƒï¼šhttps://github.com/gosoon/admission-webhookhttps://banzaicloud.com/blog/k8s-admission-webhooks/http://blog.fatedier.com/2019/03/20/k8s-crd/https://my.oschina.net/jxcdwangtao/blog/1591681https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-usehttps://istio.io/zh/help/ops/setup/validation/https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/ ç›¸å…³æŽ¨èä½¿ç”¨ code-generator ä¸º CustomResources ç”Ÿæˆä»£ç ]]></content>
      <tags>
        <tag>crd</tag>
        <tag>admission control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½¿ç”¨ Go Modules ç®¡ç†ä¾èµ–]]></title>
    <url>%2F2019%2F06%2F22%2Fgolang_modules%2F</url>
    <content type="text"><![CDATA[Go Modules æ˜¯ Go è¯­è¨€çš„ä¸€ç§ä¾èµ–ç®¡ç†æ–¹å¼ï¼Œè¯¥ feature æ˜¯åœ¨ Go 1.11 ç‰ˆæœ¬ä¸­å‡ºçŽ°çš„ï¼Œç”±äºŽæœ€è¿‘åœ¨åšçš„é¡¹ç›®ä¸­ï¼Œå›¢é˜Ÿéƒ½å¼€å§‹ä½¿ç”¨ go module æ¥æ›¿ä»£ä»¥å‰çš„ Godepï¼ŒKubernetes ä¹Ÿä»Ž v1.15 å¼€å§‹é‡‡ç”¨ go module æ¥è¿›è¡ŒåŒ…ç®¡ç†ï¼Œæ‰€ä»¥æœ‰å¿…è¦äº†è§£ä¸€ä¸‹ go moduleã€‚go module ç›¸æ¯”äºŽåŽŸæ¥çš„ Godepï¼Œgo module åœ¨æ‰“åŒ…ã€ç¼–è¯‘ç­‰å¤šä¸ªçŽ¯èŠ‚ä¸Šæœ‰ç€æ˜Žæ˜¾çš„é€Ÿåº¦ä¼˜åŠ¿ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨ä»»æ„æ“ä½œç³»ç»Ÿä¸Šæ–¹ä¾¿çš„å¤çŽ°ä¾èµ–åŒ…ï¼Œæ›´é‡è¦çš„æ˜¯ go module æœ¬èº«çš„è®¾è®¡ä½¿å¾—è‡ªèº«è¢«å…¶ä»–é¡¹ç›®å¼•ç”¨å˜å¾—æ›´åŠ å®¹æ˜“ï¼Œè¿™ä¹Ÿæ˜¯ Kubernetes é¡¹ç›®å‘æ¡†æž¶åŒ–æ¼”è¿›çš„åˆä¸€ä¸ªé‡è¦ä½“çŽ°ã€‚ ä½¿ç”¨ go module ç®¡ç†ä¾èµ–åŽä¼šåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶ go.mod å’Œ go.sumã€‚ go.mod ä¸­ä¼šè®°å½•å½“å‰é¡¹ç›®çš„æ‰€ä¾èµ–ï¼Œæ–‡ä»¶æ ¼å¼å¦‚ä¸‹æ‰€ç¤ºï¼š 123456789module github.com/gosoon/audit-webhookgo 1.12require ( github.com/elastic/go-elasticsearch v0.0.0 github.com/gorilla/mux v1.7.2 github.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81) go.sumè®°å½•æ¯ä¸ªä¾èµ–åº“çš„ç‰ˆæœ¬å’Œå“ˆå¸Œå€¼ï¼Œæ–‡ä»¶æ ¼å¼å¦‚ä¸‹æ‰€ç¤ºï¼š 123456github.com/elastic/go-elasticsearch v0.0.0 h1:Pd5fqOuBxKxv83b0+xOAJDAkziWYwFinWnBO0y+TZaA=github.com/elastic/go-elasticsearch v0.0.0/go.mod h1:TkBSJBuTyFdBnrNqoPc54FN0vKf5c04IdM4zuStJ7xg=github.com/gorilla/mux v1.7.2 h1:zoNxOV7WjqXptQOVngLmcSQgXmgk4NMz1HibBchjl/I=github.com/gorilla/mux v1.7.2/go.mod h1:1lud6UwP+6orDFRuTfBEV8e9/aOM/c4fVVCaMa2zaAs=github.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81 h1:JP0LU0ajeawW2xySrbhDqtSUfVWohZ505Q4LXo+hCmg=github.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81/go.mod h1:1e0N9vBl2wPF6qYa+JCRNIZnhxSkXkOJfD2iFw3eOfg= ä¸€ã€å¦‚ä½•å¯ç”¨ go module åŠŸèƒ½(1) go ç‰ˆæœ¬ &gt;= v1.11 (2) è®¾ç½®GO111MODULEçŽ¯å¢ƒå˜é‡ è¦ä½¿ç”¨go module é¦–å…ˆè¦è®¾ç½®GO111MODULE=onï¼ŒGO111MODULE æœ‰ä¸‰ä¸ªå€¼ï¼Œoffã€onã€autoï¼Œoff å’Œ on å³å…³é—­å’Œå¼€å¯ï¼Œauto åˆ™ä¼šæ ¹æ®å½“å‰ç›®å½•ä¸‹æ˜¯å¦æœ‰ go.mod æ–‡ä»¶æ¥åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ modules åŠŸèƒ½ã€‚æ— è®ºä½¿ç”¨å“ªç§æ¨¡å¼ï¼Œmodule åŠŸèƒ½é»˜è®¤ä¸åœ¨ GOPATH ç›®å½•ä¸‹æŸ¥æ‰¾ä¾èµ–æ–‡ä»¶ï¼Œæ‰€ä»¥ä½¿ç”¨ modules åŠŸèƒ½æ—¶è¯·è®¾ç½®å¥½ä»£ç†ã€‚ åœ¨ä½¿ç”¨ go module æ—¶ï¼Œå°† GO111MODULE å…¨å±€çŽ¯å¢ƒå˜é‡è®¾ç½®ä¸º offï¼Œåœ¨éœ€è¦ä½¿ç”¨çš„æ—¶å€™å†å¼€å¯ï¼Œé¿å…åœ¨å·²æœ‰é¡¹ç›®ä¸­æ„å¤–å¼•å…¥ go moduleã€‚ 123$ echo export GO111MODULE=off &gt;&gt; ~/.zshrcor$ echo export GO111MODULE=off &gt;&gt; ~/.bashrc go mod å‘½ä»¤çš„ä½¿ç”¨ï¼š 12345678download download modules to local cache (ä¸‹è½½ä¾èµ–çš„moduleåˆ°æœ¬åœ°cache))edit edit go.mod from tools or scripts (ç¼–è¾‘go.modæ–‡ä»¶)graph print module requirement graph (æ‰“å°æ¨¡å—ä¾èµ–å›¾))init initialize new module in current directory (åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸‹åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„module, åˆ›å»ºgo.modæ–‡ä»¶))tidy add missing and remove unused modules (å¢žåŠ ä¸¢å¤±çš„moduleï¼ŒåŽ»æŽ‰æœªä½¿ç”¨çš„module)vendor make vendored copy of dependencies (å°†ä¾èµ–å¤åˆ¶åˆ°vendorä¸‹)verify verify dependencies have expected content (æ ¡éªŒä¾èµ–)why explain why packages or modules are needed (è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦ä¾èµ–) äºŒã€ä½¿ç”¨ go module åŠŸèƒ½å¯¹äºŽæ–°å»ºé¡¹ç›®ä½¿ç”¨ go moduleï¼š 1234567$ export GO111MODULE=on $ go mod init github.com/you/hello ...// go build ä¼šå°†é¡¹ç›®çš„ä¾èµ–æ·»åŠ åˆ° go.mod ä¸­$ go build å¯¹äºŽå·²æœ‰é¡¹ç›®è¦æ”¹ä¸ºä½¿ç”¨ go moduleï¼š 1234567$ export GO111MODULE=on// åˆ›å»ºä¸€ä¸ªç©ºçš„ go.mod æ–‡ä»¶$ go mod init .// æŸ¥æ‰¾ä¾èµ–å¹¶è®°å½•åœ¨ go.mod æ–‡ä»¶ä¸­$ go get ./... go.mod æ–‡ä»¶å¿…é¡»è¦æäº¤åˆ° git ä»“åº“ï¼Œä½† go.sum æ–‡ä»¶å¯ä»¥ä¸ç”¨æäº¤åˆ° git ä»“åº“(gi tå¿½ç•¥æ–‡ä»¶ .gitignore ä¸­è®¾ç½®ä¸€ä¸‹)ã€‚ ä¸‰ã€é¡¹ç›®çš„æ‰“åŒ…é¦–å…ˆéœ€è¦ä½¿ç”¨ go mod vendor å°†é¡¹ç›®æ‰€æœ‰çš„ä¾èµ–ä¸‹è½½åˆ°æœ¬åœ° vendor ç›®å½•ä¸­ç„¶åŽè¿›è¡Œç¼–è¯‘ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªå‚è€ƒï¼š 12345678910#!/bin/bashexport GO111MODULE=&quot;on&quot;export GOPROXY=&quot;https://goproxy.io&quot;export CGO_ENABLED=&quot;0&quot;export GOOS=&quot;linux&quot;export GOARCH=amd64go mod vendorgo build -ldflags &quot;-s -w&quot; -a -installsuffix cgo -o audit-webhook . å››ã€æ³¨æ„äº‹é¡¹1ã€ä¾èµ–ä¸‹è½½ go module é»˜è®¤ä¸åœ¨ GOPATH ç›®å½•ä¸‹æŸ¥æ‰¾ä¾èµ–æ–‡ä»¶ï¼Œå…¶é¦–å…ˆä¼šåœ¨$GOPATH/pkg/modä¸­æŸ¥æ‰¾æœ‰æ²¡æœ‰æ‰€éœ€è¦çš„ä¾èµ–ï¼Œæ²¡æœ‰çš„ç›´æŽ¥ä¼šè¿›è¡Œä¸‹è½½ã€‚å¯ä»¥ä½¿ç”¨ go mod downloadä¸‹è½½å¥½æ‰€éœ€è¦çš„ä¾èµ–ï¼Œä¾èµ–é»˜è®¤ä¼šä¸‹è½½åˆ°$GOPATH/pkg/modä¸­ï¼Œå…¶ä»–é¡¹ç›®ä¹Ÿä¼šä½¿ç”¨ç¼“å­˜çš„ moduleã€‚ 2ã€å›½å†…æ— æ³•è®¿é—®çš„ä¾èµ– ä½¿ç”¨ Go çš„å…¶ä»–åŒ…ç®¡ç†å·¥å…· godepã€govendorã€glideã€dep ç­‰éƒ½é¿å…ä¸äº†ç¿»å¢™çš„é—®é¢˜ï¼ŒGo Modules ä¹Ÿæ˜¯ä¸€æ ·ï¼Œä½†åœ¨go.modä¸­å¯ä»¥ä½¿ç”¨replaceå°†ç‰¹å®šçš„åº“æ›¿æ¢æˆå…¶ä»–åº“ï¼š 123replace ( golang.org/x/text v0.3.0 =&gt; github.com/golang/text v0.3.0) æˆ–è€…ä¹Ÿå¯ä»¥åœ¨å…¶ä»–æœºå™¨ä¸Šä½¿ç”¨ go mod downloadä¸‹è½½å¥½æ‰€éœ€è¦çš„ä¾èµ–ï¼Œç„¶åŽå†ä¼ è¾“åˆ°æœ¬æœºã€‚ å‚è€ƒï¼š https://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/2019-03-19-go-modules.md https://blog.golang.org/using-go-modules]]></content>
      <tags>
        <tag>go module</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet çŠ¶æ€ä¸ŠæŠ¥çš„æ–¹å¼]]></title>
    <url>%2F2019%2F06%2F09%2Fnode_status%2F</url>
    <content type="text"><![CDATA[åˆ†å¸ƒå¼ç³»ç»Ÿä¸­æœåŠ¡ç«¯ä¼šé€šè¿‡å¿ƒè·³æœºåˆ¶ç¡®è®¤å®¢æˆ·ç«¯æ˜¯å¦å­˜æ´»ï¼Œåœ¨ k8s ä¸­ï¼Œkubelet ä¹Ÿä¼šå®šæ—¶ä¸ŠæŠ¥å¿ƒè·³åˆ° apiserverï¼Œä»¥æ­¤åˆ¤æ–­è¯¥ node æ˜¯å¦å­˜æ´»ï¼Œè‹¥ node è¶…è¿‡ä¸€å®šæ—¶é—´æ²¡æœ‰ä¸ŠæŠ¥å¿ƒè·³ï¼Œå…¶çŠ¶æ€ä¼šè¢«ç½®ä¸º NotReadyï¼Œå®¿ä¸»ä¸Šå®¹å™¨çš„çŠ¶æ€ä¹Ÿä¼šè¢«ç½®ä¸º Nodelost æˆ–è€… Unknown çŠ¶æ€ã€‚kubelet è‡ªèº«ä¼šå®šæœŸæ›´æ–°çŠ¶æ€åˆ° apiserverï¼Œé€šè¿‡å‚æ•° --node-status-update-frequency æŒ‡å®šä¸ŠæŠ¥é¢‘çŽ‡ï¼Œé»˜è®¤æ˜¯ 10s ä¸ŠæŠ¥ä¸€æ¬¡ï¼Œkubelet ä¸æ­¢ä¸ŠæŠ¥å¿ƒè·³ä¿¡æ¯è¿˜ä¼šä¸ŠæŠ¥è‡ªèº«çš„ä¸€äº›æ•°æ®ä¿¡æ¯ã€‚ ä¸€ã€kubelet ä¸ŠæŠ¥å“ªäº›çŠ¶æ€ åœ¨ k8s ä¸­ï¼Œä¸€ä¸ª node çš„çŠ¶æ€åŒ…å«ä»¥ä¸‹å‡ ä¸ªä¿¡æ¯ï¼š Addresses Condition Capacity Info 1ã€Addressesä¸»è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªå­—æ®µï¼š HostNameï¼šHostname ã€‚å¯ä»¥é€šè¿‡ kubelet çš„ --hostname-override å‚æ•°è¿›è¡Œè¦†ç›–ã€‚ ExternalIPï¼šé€šå¸¸æ˜¯å¯ä»¥å¤–éƒ¨è·¯ç”±çš„ node IP åœ°å€ï¼ˆä»Žé›†ç¾¤å¤–å¯è®¿é—®ï¼‰ã€‚ InternalIPï¼šé€šå¸¸æ˜¯ä»…å¯åœ¨é›†ç¾¤å†…éƒ¨è·¯ç”±çš„ node IP åœ°å€ã€‚ 2ã€Conditionconditions å­—æ®µæè¿°äº†æ‰€æœ‰ Running nodes çš„çŠ¶æ€ã€‚ 3ã€Capacityæè¿° node ä¸Šçš„å¯ç”¨èµ„æºï¼šCPUã€å†…å­˜å’Œå¯ä»¥è°ƒåº¦åˆ°è¯¥ node ä¸Šçš„æœ€å¤§ pod æ•°é‡ã€‚ 4ã€Infoæè¿° node çš„ä¸€äº›é€šç”¨ä¿¡æ¯ï¼Œä¾‹å¦‚å†…æ ¸ç‰ˆæœ¬ã€Kubernetes ç‰ˆæœ¬ï¼ˆkubelet å’Œ kube-proxy ç‰ˆæœ¬ï¼‰ã€Docker ç‰ˆæœ¬ ï¼ˆå¦‚æžœä½¿ç”¨äº†ï¼‰å’Œç³»ç»Ÿç‰ˆæœ¬ï¼Œè¿™äº›ä¿¡æ¯ç”± kubelet ä»Ž node ä¸ŠèŽ·å–åˆ°ã€‚ ä½¿ç”¨ kubectl get node xxx -o yaml å¯ä»¥çœ‹åˆ° node æ‰€æœ‰çš„çŠ¶æ€çš„ä¿¡æ¯ï¼Œå…¶ä¸­ status ä¸­çš„ä¿¡æ¯éƒ½æ˜¯ kubelet éœ€è¦ä¸ŠæŠ¥çš„ï¼Œæ‰€ä»¥ kubelet ä¸æ­¢ä¸ŠæŠ¥å¿ƒè·³ä¿¡æ¯è¿˜ä¸ŠæŠ¥èŠ‚ç‚¹ä¿¡æ¯ã€èŠ‚ç‚¹ OOD ä¿¡æ¯ã€å†…å­˜ç£ç›˜åŽ‹åŠ›çŠ¶æ€ã€èŠ‚ç‚¹ç›‘æŽ§çŠ¶æ€ã€æ˜¯å¦è°ƒåº¦ç­‰ã€‚ äºŒã€kubelet çŠ¶æ€å¼‚å¸¸æ—¶çš„å½±å“å¦‚æžœä¸€ä¸ª node å¤„äºŽéž Ready çŠ¶æ€è¶…è¿‡ pod-eviction-timeoutçš„å€¼(é»˜è®¤ä¸º 5 åˆ†é’Ÿï¼Œåœ¨ kube-controller-manager ä¸­å®šä¹‰)ï¼Œåœ¨ v1.5 ä¹‹å‰çš„ç‰ˆæœ¬ä¸­ kube-controller-manager ä¼š force delete pod ç„¶åŽè°ƒåº¦è¯¥å®¿ä¸»ä¸Šçš„ pods åˆ°å…¶ä»–å®¿ä¸»ï¼Œåœ¨ v1.5 ä¹‹åŽçš„ç‰ˆæœ¬ä¸­ï¼Œkube-controller-manager ä¸ä¼š force delete podï¼Œpod ä¼šä¸€ç›´å¤„äºŽTerminating æˆ–Unknown çŠ¶æ€ç›´åˆ° node è¢«ä»Ž master ä¸­åˆ é™¤æˆ– kubelet çŠ¶æ€å˜ä¸º Readyã€‚åœ¨ node NotReady æœŸé—´ï¼ŒDaemonset çš„ Pod çŠ¶æ€å˜ä¸º Nodelostï¼ŒDeploymentã€Statefulset å’Œ Static Pod çš„çŠ¶æ€å…ˆå˜ä¸º NodeLostï¼Œç„¶åŽé©¬ä¸Šå˜ä¸º Unknownã€‚Deployment çš„ pod ä¼š recreateï¼ŒStatic Pod å’Œ Statefulset çš„ Pod ä¼šä¸€ç›´å¤„äºŽ Unknown çŠ¶æ€ã€‚ å½“ kubelet å˜ä¸º Ready çŠ¶æ€æ—¶ï¼ŒDaemonsetçš„podä¸ä¼šrecreateï¼Œæ—§podçŠ¶æ€ç›´æŽ¥å˜ä¸ºRunningï¼ŒDeploymentçš„åˆ™æ˜¯å°†kubeletè¿›ç¨‹åœæ­¢çš„Nodeåˆ é™¤ï¼ŒStatefulsetçš„Podä¼šé‡æ–°recreateï¼ŒStaic Pod ä¼šè¢«åˆ é™¤ã€‚ ä¸‰ã€kubelet çŠ¶æ€ä¸ŠæŠ¥çš„å®žçŽ°kubelet æœ‰ä¸¤ç§ä¸ŠæŠ¥çŠ¶æ€çš„æ–¹å¼ï¼Œç¬¬ä¸€ç§å®šæœŸå‘ apiserver å‘é€å¿ƒè·³æ¶ˆæ¯ï¼Œç®€å•ç†è§£å°±æ˜¯å¯åŠ¨ä¸€ä¸ª goroutine ç„¶åŽå®šæœŸå‘ APIServer å‘é€æ¶ˆæ¯ã€‚ ç¬¬äºŒä¸­è¢«ç§°ä¸º NodeLeaseï¼Œåœ¨ v1.13 ä¹‹å‰çš„ç‰ˆæœ¬ä¸­ï¼ŒèŠ‚ç‚¹çš„å¿ƒè·³åªæœ‰ NodeStatusï¼Œä»Ž v1.13 å¼€å§‹ï¼ŒNodeLease feature ä½œä¸º alpha ç‰¹æ€§å¼•å…¥ã€‚å½“å¯ç”¨ NodeLease feature æ—¶ï¼Œæ¯ä¸ªèŠ‚ç‚¹åœ¨â€œkube-node-leaseâ€åç§°ç©ºé—´ä¸­éƒ½æœ‰ä¸€ä¸ªå…³è”çš„â€œLeaseâ€å¯¹è±¡ï¼Œè¯¥å¯¹è±¡ç”±èŠ‚ç‚¹å®šæœŸæ›´æ–°ï¼ŒNodeStatus å’Œ NodeLease éƒ½è¢«è§†ä¸ºæ¥è‡ªèŠ‚ç‚¹çš„å¿ƒè·³ã€‚NodeLease ä¼šé¢‘ç¹æ›´æ–°ï¼Œè€Œåªæœ‰åœ¨ NodeStatus å‘ç”Ÿæ”¹å˜æˆ–è€…è¶…è¿‡äº†ä¸€å®šæ—¶é—´(é»˜è®¤å€¼ä¸º1åˆ†é’Ÿï¼Œnode-monitor-grace-period çš„é»˜è®¤å€¼ä¸º 40s)ï¼Œæ‰ä¼šå°† NodeStatus ä¸ŠæŠ¥ç»™ masterã€‚ç”±äºŽ NodeLease æ¯” NodeStatus æ›´è½»é‡çº§ï¼Œè¯¥ç‰¹æ€§åœ¨é›†ç¾¤è§„æ¨¡æ‰©å±•æ€§å’Œæ€§èƒ½ä¸Šæœ‰æ˜Žæ˜¾æå‡ã€‚æœ¬æ–‡ä¸»è¦åˆ†æžç¬¬ä¸€ç§ä¸ŠæŠ¥æ–¹å¼çš„å®žçŽ°ã€‚ kubernetes ç‰ˆæœ¬ ï¼šv1.13 kubelet ä¸ŠæŠ¥çŠ¶æ€çš„ä»£ç å¤§éƒ¨åˆ†åœ¨ kubernetes/pkg/kubelet/kubelet_node_status.go ä¸­å®žçŽ°ã€‚çŠ¶æ€ä¸ŠæŠ¥çš„åŠŸèƒ½æ˜¯åœ¨ kubernetes/pkg/kubelet/kubelet.go#Run æ–¹æ³•ä»¥ goroutine å½¢å¼ä¸­å¯åŠ¨çš„ï¼Œkubelet ä¸­å¤šä¸ªé‡è¦çš„åŠŸèƒ½éƒ½æ˜¯åœ¨è¯¥æ–¹æ³•ä¸­å¯åŠ¨çš„ã€‚ kubernetes/pkg/kubelet/kubelet.go#Run 123456789101112131415func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123; // ... if kl.kubeClient != nil &#123; // Start syncing node status immediately, this may set up things the runtime needs to run. go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop) go kl.fastStatusUpdateOnce() // ä¸€ç§æ–°çš„çŠ¶æ€ä¸ŠæŠ¥æ–¹å¼ // start syncing lease if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123; go kl.nodeLeaseController.Run(wait.NeverStop) &#125; &#125; // ... &#125; kl.syncNodeStatus ä¾¿æ˜¯ä¸ŠæŠ¥çŠ¶æ€çš„ï¼Œæ­¤å¤„ kl.nodeStatusUpdateFrequency ä½¿ç”¨çš„æ˜¯é»˜è®¤è®¾ç½®çš„ 10sï¼Œä¹Ÿå°±æ˜¯è¯´èŠ‚ç‚¹é—´åŒæ­¥çŠ¶æ€çš„å‡½æ•° kl.syncNodeStatus æ¯ 10s æ‰§è¡Œä¸€æ¬¡ã€‚ syncNodeStatus æ˜¯çŠ¶æ€ä¸ŠæŠ¥çš„å…¥å£å‡½æ•°ï¼Œå…¶åŽæ‰€è°ƒç”¨çš„å¤šä¸ªå‡½æ•°ä¹Ÿéƒ½æ˜¯åœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­å®žçŽ°çš„ã€‚ kubernetes/pkg/kubelet/kubelet_node_status.go#syncNodeStatus 1234567891011121314151617func (kl *Kubelet) syncNodeStatus() &#123; kl.syncNodeStatusMux.Lock() defer kl.syncNodeStatusMux.Unlock() if kl.kubeClient == nil || kl.heartbeatClient == nil &#123; return &#125; // æ˜¯å¦ä¸ºæ³¨å†ŒèŠ‚ç‚¹ if kl.registerNode &#123; // This will exit immediately if it doesn&apos;t need to do anything. kl.registerWithAPIServer() &#125; if err := kl.updateNodeStatus(); err != nil &#123; klog.Errorf(&quot;Unable to update node status: %v&quot;, err) &#125;&#125; syncNodeStatus è°ƒç”¨ updateNodeStatusï¼Œ ç„¶åŽåˆè°ƒç”¨ tryUpdateNodeStatus æ¥è¿›è¡Œä¸ŠæŠ¥æ“ä½œï¼Œè€Œæœ€ç»ˆè°ƒç”¨çš„æ˜¯ setNodeStatusã€‚è¿™é‡Œè¿˜è¿›è¡Œäº†åŒæ­¥çŠ¶æ€åˆ¤æ–­ï¼Œå¦‚æžœæ˜¯æ³¨å†ŒèŠ‚ç‚¹ï¼Œåˆ™æ‰§è¡Œ registerWithAPIServerï¼Œå¦åˆ™ï¼Œæ‰§è¡Œ updateNodeStatusã€‚ updateNodeStatus ä¸»è¦æ˜¯è°ƒç”¨ tryUpdateNodeStatus è¿›è¡ŒåŽç»­çš„æ“ä½œï¼Œè¯¥å‡½æ•°ä¸­å®šä¹‰äº†çŠ¶æ€ä¸ŠæŠ¥é‡è¯•çš„æ¬¡æ•°ï¼ŒnodeStatusUpdateRetry é»˜è®¤å®šä¹‰ä¸º 5 æ¬¡ã€‚ kubernetes/pkg/kubelet/kubelet_node_status.go#updateNodeStatus 1234567891011121314func (kl *Kubelet) updateNodeStatus() error &#123; klog.V(5).Infof(&quot;Updating node status&quot;) for i := 0; i &lt; nodeStatusUpdateRetry; i++ &#123; if err := kl.tryUpdateNodeStatus(i); err != nil &#123; if i &gt; 0 &amp;&amp; kl.onRepeatedHeartbeatFailure != nil &#123; kl.onRepeatedHeartbeatFailure() &#125; klog.Errorf(&quot;Error updating node status, will retry: %v&quot;, err) &#125; else &#123; return nil &#125; &#125; return fmt.Errorf(&quot;update node status exceeds retry count&quot;)&#125; tryUpdateNodeStatus æ˜¯ä¸»è¦çš„ä¸ŠæŠ¥é€»è¾‘ï¼Œå…ˆç»™ node è®¾ç½®çŠ¶æ€ï¼Œç„¶åŽä¸ŠæŠ¥ node çš„çŠ¶æ€åˆ° masterã€‚ kubernetes/pkg/kubelet/kubelet_node_status.go#tryUpdateNodeStatus 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func (kl *Kubelet) tryUpdateNodeStatus(tryNumber int) error &#123; opts := metav1.GetOptions&#123;&#125; if tryNumber == 0 &#123; util.FromApiserverCache(&amp;opts) &#125; // èŽ·å– node ä¿¡æ¯ node, err := kl.heartbeatClient.CoreV1().Nodes().Get(string(kl.nodeName), opts) if err != nil &#123; return fmt.Errorf(&quot;error getting node %q: %v&quot;, kl.nodeName, err) &#125; originalNode := node.DeepCopy() if originalNode == nil &#123; return fmt.Errorf(&quot;nil %q node object&quot;, kl.nodeName) &#125; podCIDRChanged := false if node.Spec.PodCIDR != &quot;&quot; &#123; if podCIDRChanged, err = kl.updatePodCIDR(node.Spec.PodCIDR); err != nil &#123; klog.Errorf(err.Error()) &#125; &#125; // è®¾ç½® node çŠ¶æ€ kl.setNodeStatus(node) now := kl.clock.Now() if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &amp;&amp; now.Before(kl.lastStatusReportTime.Add(kl.nodeStatusReportFrequency)) &#123; if !podCIDRChanged &amp;&amp; !nodeStatusHasChanged(&amp;originalNode.Status, &amp;node.Status) &#123; kl.volumeManager.MarkVolumesAsReportedInUse(node.Status.VolumesInUse) return nil &#125; &#125; // æ›´æ–° node ä¿¡æ¯åˆ° master // Patch the current status on the API server updatedNode, _, err := nodeutil.PatchNodeStatus(kl.heartbeatClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, node) if err != nil &#123; return err &#125; kl.lastStatusReportTime = now kl.setLastObservedNodeAddresses(updatedNode.Status.Addresses) // If update finishes successfully, mark the volumeInUse as reportedInUse to indicate // those volumes are already updated in the node&apos;s status kl.volumeManager.MarkVolumesAsReportedInUse(updatedNode.Status.VolumesInUse) return nil&#125; tryUpdateNodeStatus ä¸­è°ƒç”¨ setNodeStatus è®¾ç½® node çš„çŠ¶æ€ã€‚setNodeStatus ä¼šèŽ·å–ä¸€æ¬¡ node çš„æ‰€æœ‰çŠ¶æ€ï¼Œç„¶åŽä¼šå°† kubelet ä¸­ä¿å­˜çš„æ‰€æœ‰çŠ¶æ€æ”¹ä¸ºæœ€æ–°çš„å€¼ï¼Œä¹Ÿå°±æ˜¯ä¼šé‡ç½® node status ä¸­çš„æ‰€æœ‰å­—æ®µã€‚ kubernetes/pkg/kubelet/kubelet_node_status.go#setNodeStatus 12345678func (kl *Kubelet) setNodeStatus(node *v1.Node) &#123; for i, f := range kl.setNodeStatusFuncs &#123; klog.V(5).Infof(&quot;Setting node status at position %v&quot;, i) if err := f(node); err != nil &#123; klog.Warningf(&quot;Failed to set some node status fields: %s&quot;, err) &#125; &#125;&#125; setNodeStatus é€šè¿‡ setNodeStatusFuncs æ–¹æ³•è¦†ç›– node ç»“æž„ä½“ä¸­æ‰€æœ‰çš„å­—æ®µï¼ŒsetNodeStatusFuncs æ˜¯åœ¨ NewMainKubelet(pkg/kubelet/kubelet.go) ä¸­åˆå§‹åŒ–çš„ã€‚ kubernetes/pkg/kubelet/kubelet.go#NewMainKubelet 1234567 func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, // ... // Generating the status funcs should be the last thing we do, klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs() return klet, nil&#125; defaultNodeStatusFuncs æ˜¯ç”ŸæˆçŠ¶æ€çš„å‡½æ•°ï¼Œé€šè¿‡èŽ·å– node çš„æ‰€æœ‰çŠ¶æ€æŒ‡æ ‡åŽä½¿ç”¨å·¥åŽ‚å‡½æ•°ç”ŸæˆçŠ¶æ€ kubernetes/pkg/kubelet/kubelet_node_status.go#defaultNodeStatusFuncs 1234567891011121314151617181920212223242526272829303132333435363738func (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error &#123; // if cloud is not nil, we expect the cloud resource sync manager to exist var nodeAddressesFunc func() ([]v1.NodeAddress, error) if kl.cloud != nil &#123; nodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses &#125; var validateHostFunc func() error if kl.appArmorValidator != nil &#123; validateHostFunc = kl.appArmorValidator.ValidateHost &#125; var setters []func(n *v1.Node) error setters = append(setters, nodestatus.NodeAddress(kl.nodeIP, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc), nodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity, kl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent), nodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version), nodestatus.DaemonEndpoints(kl.daemonEndpoints), nodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList), nodestatus.GoRuntime(), ) if utilfeature.DefaultFeatureGate.Enabled(features.AttachVolumeLimit) &#123; setters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits)) &#125; setters = append(setters, nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent), nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent), nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent), nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors, validateHostFunc, kl.containerManager. Status, kl.recordNodeStatusEvent), nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse), nodestatus.RemoveOutOfDiskCondition(), // TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event // and record state back to the Kubelet runtime object. In the future, I&apos;d like to isolate // these side-effects by decoupling the decisions to send events and partial status recording // from the Node setters. kl.recordNodeSchedulableEvent, ) return setters&#125; defaultNodeStatusFuncs å¯ä»¥çœ‹åˆ° node ä¸ŠæŠ¥çš„æ‰€æœ‰ä¿¡æ¯ï¼Œä¸»è¦æœ‰ MemoryPressureConditionã€DiskPressureConditionã€PIDPressureConditionã€ReadyCondition ç­‰ã€‚æ¯ä¸€ç§ nodestatus éƒ½è¿”å›žä¸€ä¸ª settersï¼Œæ‰€æœ‰ setters çš„å®šä¹‰åœ¨ pkg/kubelet/nodestatus/setters.go æ–‡ä»¶ä¸­ã€‚ å¯¹äºŽäºŒæ¬¡å¼€å‘è€Œè¨€ï¼Œå¦‚æžœæˆ‘ä»¬éœ€è¦ APIServer æŽŒæ¡æ›´å¤šçš„ Node ä¿¡æ¯ï¼Œå¯ä»¥åœ¨æ­¤å¤„æ·»åŠ è‡ªå®šä¹‰å‡½æ•°ï¼Œä¾‹å¦‚ï¼Œä¸ŠæŠ¥ç£ç›˜ä¿¡æ¯ç­‰ã€‚ tryUpdateNodeStatus ä¸­æœ€åŽè°ƒç”¨ PatchNodeStatus ä¸ŠæŠ¥ node çš„çŠ¶æ€åˆ° masterã€‚ kubernetes/pkg/util/node/node.go#PatchNodeStatus 1234567891011121314// PatchNodeStatus patches node status.func PatchNodeStatus(c v1core.CoreV1Interface, nodeName types.NodeName, oldNode *v1.Node, newNode *v1.Node) (*v1.Node, []byte, error) &#123; // è®¡ç®— patch patchBytes, err := preparePatchBytesforNodeStatus(nodeName, oldNode, newNode) if err != nil &#123; return nil, nil, err &#125; updatedNode, err := c.Nodes().Patch(string(nodeName), types.StrategicMergePatchType, patchBytes, &quot;status&quot;) if err != nil &#123; return nil, nil, fmt.Errorf(&quot;failed to patch status %q for node %q: %v&quot;, patchBytes, nodeName, err) &#125; return updatedNode, patchBytes, nil&#125; åœ¨ PatchNodeStatus ä¼šè°ƒç”¨å·²æ³¨å†Œçš„é‚£äº›æ–¹æ³•å°†çŠ¶æ€æŠŠçŠ¶æ€å‘ç»™ APIServerã€‚ å››ã€æ€»ç»“æœ¬æ–‡ä¸»è¦è®²è¿°äº† kubelet ä¸ŠæŠ¥çŠ¶æ€çš„æ–¹å¼åŠå…¶å®žçŽ°ï¼Œnode çŠ¶æ€ä¸ŠæŠ¥çš„æ–¹å¼ç›®å‰æœ‰ä¸¤ç§ï¼Œæœ¬æ–‡ä»…åˆ†æžäº†ç¬¬ä¸€ç§çŠ¶æ€ä¸ŠæŠ¥çš„æ–¹å¼ã€‚åœ¨å¤§è§„æ¨¡é›†ç¾¤ä¸­ç”±äºŽèŠ‚ç‚¹æ•°é‡æ¯”è¾ƒå¤šï¼Œæ‰€æœ‰ node éƒ½é¢‘ç¹æŠ¥çŠ¶æ€å¯¹ etcd ä¼šæœ‰ä¸€å®šçš„åŽ‹åŠ›ï¼Œå½“ node ä¸Ž master é€šä¿¡æ—¶ç”±äºŽç½‘ç»œå¯¼è‡´å¿ƒè·³ä¸ŠæŠ¥å¤±è´¥ä¹Ÿä¼šå½±å“ node çš„çŠ¶æ€ï¼Œä¸ºäº†é¿å…ç±»ä¼¼é—®é¢˜çš„å‡ºçŽ°æ‰æœ‰ NodeLease æ–¹å¼ï¼Œå¯¹äºŽè¯¥åŠŸèƒ½çš„å®žçŽ°åŽæ–‡ä¼šç»§ç»­è¿›è¡Œåˆ†æžã€‚ å‚è€ƒï¼šhttps://www.qikqiak.com/post/kubelet-sync-node-status/https://www.jianshu.com/p/054450557818https://blog.csdn.net/shida_csdn/article/details/84286058https://kubernetes.io/docs/concepts/architecture/nodes/ ç›¸å…³æŽ¨èkubernets ä¸­äº‹ä»¶å¤„ç†æœºåˆ¶kubelet åˆ›å»º pod çš„æµç¨‹kubelet å¯åŠ¨æµç¨‹åˆ†æž]]></content>
      <tags>
        <tag>kubelet</tag>
        <tag>node status</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes ä¸­ informer çš„ä½¿ç”¨]]></title>
    <url>%2F2019%2F05%2F17%2Fclient-go_informer%2F</url>
    <content type="text"><![CDATA[ä¸€ã€kubernetes é›†ç¾¤çš„å‡ ç§è®¿é—®æ–¹å¼åœ¨å®žé™…å¼€å‘è¿‡ç¨‹ä¸­ï¼Œè‹¥æƒ³è¦èŽ·å– kubernetes ä¸­æŸä¸ªèµ„æºï¼ˆæ¯”å¦‚ podï¼‰çš„æ‰€æœ‰å¯¹è±¡ï¼Œå¯ä»¥ä½¿ç”¨ kubectlã€k8s REST APIã€client-go(ClientSetã€Dynamic Clientã€RESTClient ä¸‰ç§æ–¹å¼) ç­‰å¤šç§æ–¹å¼è®¿é—® k8s é›†ç¾¤èŽ·å–èµ„æºã€‚åœ¨ç¬”è€…çš„å¼€å‘è¿‡ç¨‹ä¸­ï¼Œæœ€åˆéƒ½æ˜¯ç›´æŽ¥è°ƒç”¨ k8s çš„ REST API æ¥èŽ·å–çš„ï¼Œä½¿ç”¨ kubectl get pod -v=9 å¯ä»¥ç›´æŽ¥çœ‹åˆ°è°ƒç”¨ k8s çš„æŽ¥å£ï¼Œç„¶åŽåœ¨ç¨‹åºä¸­ç›´æŽ¥è®¿é—®è¿˜æ˜¯æ¯”è¾ƒæ–¹ä¾¿çš„ã€‚ä½†æ˜¯éšç€é›†ç¾¤è§„æ¨¡çš„å¢žé•¿æˆ–è€…ä»Žå›½å†…èŽ·å–æµ·å¤– k8s é›†ç¾¤çš„æ•°æ®ï¼Œç›´æŽ¥è°ƒç”¨ k8s æŽ¥å£èŽ·å–æ‰€æœ‰ pod è¿˜æ˜¯æ¯”è¾ƒè€—æ—¶ï¼Œè¿™ä¸ªé—®é¢˜æœ‰å¤šç§è§£å†³æ–¹æ³•ï¼Œæœ€åˆæ˜¯ç›´æŽ¥ä½¿ç”¨ k8s åŽŸç”Ÿçš„ watch æŽ¥å£æ¥èŽ·å–çš„ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªä¼ªä»£ç ï¼š 12345678910111213141516171819202122232425262728293031323334const ( ADDED string = &quot;ADDED&quot; MODIFIED string = &quot;MODIFIED&quot; DELETED string = &quot;DELETED&quot; ERROR string = &quot;ERROR&quot;)type Event struct &#123; Type string `json:&quot;type&quot;` Object json.RawMessage `json:&quot;object&quot;`&#125;func main() &#123; resp, err := http.Get(&quot;http://apiserver:8080/api/v1/watch/pods?watch=yes&quot;) if err != nil &#123; // ... &#125; decoder := json.NewDecoder(resp.Body) for &#123; var event Event err = decoder.Decode(&amp;event) if err != nil &#123; // ... &#125; switch event.Type &#123; case ADDED, MODIFIED: // ... case DELETED: // ... case ERROR: // ... &#125; &#125;&#125; è°ƒç”¨ watch æŽ¥å£åŽä¼šå…ˆå°†æ‰€æœ‰çš„å¯¹è±¡ list ä¸€æ¬¡ï¼Œç„¶åŽ apiserver ä¼šå°†å˜åŒ–çš„æ•°æ®æŽ¨é€åˆ° client ç«¯ï¼Œå¯ä»¥çœ‹åˆ°æ¯æ¬¡å¯¹äºŽ watch åˆ°çš„äº‹ä»¶éƒ½éœ€è¦åˆ¤æ–­åŽè¿›è¡Œå¤„ç†ï¼Œç„¶åŽå°†å¤„ç†åŽçš„ç»“æžœå†™å…¥åˆ°æœ¬åœ°çš„ç¼“å­˜ä¸­ï¼ŒåŽŸç”Ÿçš„ watch æ“ä½œè¿˜æ˜¯éžå¸¸éº»çƒ¦çš„ã€‚åŽæ¥äº†è§£åˆ°å®˜æ–¹æŽ¨å‡ºä¸€ä¸ªå®¢æˆ·ç«¯å·¥å…· client-go ï¼Œclient-go ä¸­çš„ Informer å¯¹ watch æ“ä½œåšäº†å°è£…ï¼Œä½¿ç”¨èµ·æ¥éžå¸¸æ–¹ä¾¿ï¼Œä¸‹é¢ä¼šä¸»è¦ä»‹ç»ä¸€ä¸‹ client-go çš„ä½¿ç”¨ã€‚ äºŒã€Informer çš„æœºåˆ¶cient-go æ˜¯ä»Ž k8s ä»£ç ä¸­æŠ½å‡ºæ¥çš„ä¸€ä¸ªå®¢æˆ·ç«¯å·¥å…·ï¼ŒInformer æ˜¯ client-go ä¸­çš„æ ¸å¿ƒå·¥å…·åŒ…ï¼Œå·²ç»è¢« kubernetes ä¸­ä¼—å¤šç»„ä»¶æ‰€ä½¿ç”¨ã€‚æ‰€è°“ Informerï¼Œå…¶å®žå°±æ˜¯ä¸€ä¸ªå¸¦æœ‰æœ¬åœ°ç¼“å­˜å’Œç´¢å¼•æœºåˆ¶çš„ã€å¯ä»¥æ³¨å†Œ EventHandler çš„ clientï¼Œæœ¬åœ°ç¼“å­˜è¢«ç§°ä¸º Storeï¼Œç´¢å¼•è¢«ç§°ä¸º Indexã€‚ä½¿ç”¨ informer çš„ç›®çš„æ˜¯ä¸ºäº†å‡è½» apiserver æ•°æ®äº¤äº’çš„åŽ‹åŠ›è€ŒæŠ½è±¡å‡ºæ¥çš„ä¸€ä¸ª cache å±‚, å®¢æˆ·ç«¯å¯¹ apiserver æ•°æ®çš„ â€œè¯»å–â€ å’Œ â€œç›‘å¬â€ æ“ä½œéƒ½é€šè¿‡æœ¬åœ° informer è¿›è¡Œã€‚Informer å®žä¾‹çš„Lister()æ–¹æ³•å¯ä»¥ç›´æŽ¥æŸ¥æ‰¾ç¼“å­˜åœ¨æœ¬åœ°å†…å­˜ä¸­çš„æ•°æ®ã€‚ Informer çš„ä¸»è¦åŠŸèƒ½ï¼š åŒæ­¥æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜ æ ¹æ®å¯¹åº”çš„äº‹ä»¶ç±»åž‹ï¼Œè§¦å‘äº‹å…ˆæ³¨å†Œå¥½çš„ ResourceEventHandler 1ã€Informer ä¸­å‡ ä¸ªç»„ä»¶çš„ä½œç”¨Informer ä¸­ä¸»è¦æœ‰ Reflectorã€Delta FIFO Queueã€Local Storeã€WorkQueue å‡ ä¸ªç»„ä»¶ã€‚ä»¥ä¸‹æ˜¯ Informer çš„å·¥ä½œæµç¨‹å›¾ã€‚ æ ¹æ®æµç¨‹å›¾æ¥è§£é‡Šä¸€ä¸‹ Informer ä¸­å‡ ä¸ªç»„ä»¶çš„ä½œç”¨ï¼š Reflectorï¼šç§°ä¹‹ä¸ºåå°„å™¨ï¼Œå®žçŽ°å¯¹ apiserver æŒ‡å®šç±»åž‹å¯¹è±¡çš„ç›‘æŽ§(ListAndWatch)ï¼Œå…¶ä¸­åå°„å®žçŽ°çš„å°±æ˜¯æŠŠç›‘æŽ§çš„ç»“æžœå®žä¾‹åŒ–æˆå…·ä½“çš„å¯¹è±¡ï¼Œæœ€ç»ˆä¹Ÿæ˜¯è°ƒç”¨ Kubernetes çš„ List/Watch APIï¼› DeltaIFIFO Queueï¼šä¸€ä¸ªå¢žé‡é˜Ÿåˆ—ï¼Œå°† Reflector ç›‘æŽ§å˜åŒ–çš„å¯¹è±¡å½¢æˆä¸€ä¸ª FIFO é˜Ÿåˆ—ï¼Œæ­¤å¤„çš„ Delta å°±æ˜¯å˜åŒ–ï¼› LocalStoreï¼šå°±æ˜¯ informer çš„ cacheï¼Œè¿™é‡Œé¢ç¼“å­˜çš„æ˜¯ apiserver ä¸­çš„å¯¹è±¡(å…¶ä¸­æœ‰ä¸€éƒ¨åˆ†å¯èƒ½è¿˜åœ¨DeltaFIFO ä¸­)ï¼Œæ­¤æ—¶ä½¿ç”¨è€…å†æŸ¥è¯¢å¯¹è±¡çš„æ—¶å€™å°±ç›´æŽ¥ä»Ž cache ä¸­æŸ¥æ‰¾ï¼Œå‡å°‘äº† apiserver çš„åŽ‹åŠ›ï¼ŒLocalStore åªä¼šè¢« Lister çš„ List/Get æ–¹æ³•è®¿é—®ã€‚ WorkQueueï¼šDeltaIFIFO æ”¶åˆ°æ—¶é—´åŽä¼šå…ˆå°†æ—¶é—´å­˜å‚¨åœ¨è‡ªå·±çš„æ•°æ®ç»“æž„ä¸­ï¼Œç„¶åŽç›´æŽ¥æ“ä½œ Store ä¸­å­˜å‚¨çš„æ•°æ®ï¼Œæ›´æ–°å®Œ store åŽ DeltaIFIFO ä¼šå°†è¯¥äº‹ä»¶ pop åˆ° WorkQueue ä¸­ï¼ŒController æ”¶åˆ° WorkQueue ä¸­çš„äº‹ä»¶ä¼šæ ¹æ®å¯¹åº”çš„ç±»åž‹è§¦å‘å¯¹åº”çš„å›žè°ƒå‡½æ•°ã€‚ 2ã€Informer çš„å·¥ä½œæµç¨‹ Informer é¦–å…ˆä¼š list/watch apiserverï¼ŒInformer æ‰€ä½¿ç”¨çš„ Reflector åŒ…è´Ÿè´£ä¸Ž apiserver å»ºç«‹è¿žæŽ¥ï¼ŒReflector ä½¿ç”¨ ListAndWatch çš„æ–¹æ³•ï¼Œä¼šå…ˆä»Ž apiserver ä¸­ list è¯¥èµ„æºçš„æ‰€æœ‰å®žä¾‹ï¼Œlist ä¼šæ‹¿åˆ°è¯¥å¯¹è±¡æœ€æ–°çš„ resourceVersionï¼Œç„¶åŽä½¿ç”¨ watch æ–¹æ³•ç›‘å¬è¯¥ resourceVersion ä¹‹åŽçš„æ‰€æœ‰å˜åŒ–ï¼Œè‹¥ä¸­é€”å‡ºçŽ°å¼‚å¸¸ï¼Œreflector åˆ™ä¼šä»Žæ–­å¼€çš„ resourceVersion å¤„é‡çŽ°å°è¯•ç›‘å¬æ‰€æœ‰å˜åŒ–ï¼Œä¸€æ—¦è¯¥å¯¹è±¡çš„å®žä¾‹æœ‰åˆ›å»ºã€åˆ é™¤ã€æ›´æ–°åŠ¨ä½œï¼ŒReflector éƒ½ä¼šæ”¶åˆ°â€äº‹ä»¶é€šçŸ¥â€ï¼Œè¿™æ—¶ï¼Œè¯¥äº‹ä»¶åŠå®ƒå¯¹åº”çš„ API å¯¹è±¡è¿™ä¸ªç»„åˆï¼Œè¢«ç§°ä¸ºå¢žé‡ï¼ˆDeltaï¼‰ï¼Œå®ƒä¼šè¢«æ”¾è¿› DeltaFIFO ä¸­ã€‚ Informer ä¼šä¸æ–­åœ°ä»Žè¿™ä¸ª DeltaFIFO ä¸­è¯»å–å¢žé‡ï¼Œæ¯æ‹¿å‡ºä¸€ä¸ªå¯¹è±¡ï¼ŒInformer å°±ä¼šåˆ¤æ–­è¿™ä¸ªå¢žé‡çš„æ—¶é—´ç±»åž‹ï¼Œç„¶åŽåˆ›å»ºæˆ–æ›´æ–°æœ¬åœ°çš„ç¼“å­˜ï¼Œä¹Ÿå°±æ˜¯ storeã€‚ å¦‚æžœäº‹ä»¶ç±»åž‹æ˜¯ Addedï¼ˆæ·»åŠ å¯¹è±¡ï¼‰ï¼Œé‚£ä¹ˆ Informer ä¼šé€šè¿‡ Indexer çš„åº“æŠŠè¿™ä¸ªå¢žé‡é‡Œçš„ API å¯¹è±¡ä¿å­˜åˆ°æœ¬åœ°çš„ç¼“å­˜ä¸­ï¼Œå¹¶ä¸ºå®ƒåˆ›å»ºç´¢å¼•ï¼Œè‹¥ä¸ºåˆ é™¤æ“ä½œï¼Œåˆ™åœ¨æœ¬åœ°ç¼“å­˜ä¸­åˆ é™¤è¯¥å¯¹è±¡ã€‚ DeltaFIFO å† pop è¿™ä¸ªäº‹ä»¶åˆ° controller ä¸­ï¼Œcontroller ä¼šè°ƒç”¨äº‹å…ˆæ³¨å†Œçš„ ResourceEventHandler å›žè°ƒå‡½æ•°è¿›è¡Œå¤„ç†ã€‚ åœ¨ ResourceEventHandler å›žè°ƒå‡½æ•°ä¸­ï¼Œå…¶å®žåªæ˜¯åšäº†ä¸€äº›å¾ˆç®€å•çš„è¿‡æ»¤ï¼Œç„¶åŽå°†å…³å¿ƒå˜æ›´çš„ Object æ”¾åˆ° workqueue é‡Œé¢ã€‚ Controller ä»Ž workqueue é‡Œé¢å–å‡º Objectï¼Œå¯åŠ¨ä¸€ä¸ª worker æ¥æ‰§è¡Œè‡ªå·±çš„ä¸šåŠ¡é€»è¾‘ï¼Œä¸šåŠ¡é€»è¾‘é€šå¸¸æ˜¯è®¡ç®—ç›®å‰é›†ç¾¤çš„çŠ¶æ€å’Œç”¨æˆ·å¸Œæœ›è¾¾åˆ°çš„çŠ¶æ€æœ‰å¤šå¤§çš„åŒºåˆ«ï¼Œç„¶åŽå­œå­œä¸å€¦åœ°è®© apiserver å°†çŠ¶æ€æ¼”åŒ–åˆ°ç”¨æˆ·å¸Œæœ›è¾¾åˆ°çš„çŠ¶æ€ï¼Œæ¯”å¦‚ä¸º deployment åˆ›å»ºæ–°çš„ podsï¼Œæˆ–è€…æ˜¯æ‰©å®¹/ç¼©å®¹ deploymentã€‚ åœ¨workerä¸­å°±å¯ä»¥ä½¿ç”¨ lister æ¥èŽ·å– resourceï¼Œè€Œä¸ç”¨é¢‘ç¹çš„è®¿é—® apiserverï¼Œå› ä¸º apiserver ä¸­ resource çš„å˜æ›´éƒ½ä¼šåæ˜ åˆ°æœ¬åœ°çš„ cache ä¸­ã€‚ Informer åœ¨ä½¿ç”¨æ—¶éœ€è¦å…ˆåˆå§‹åŒ–ä¸€ä¸ª InformerFactoryï¼Œç›®å‰ä¸»è¦æŽ¨èä½¿ç”¨çš„æ˜¯ SharedInformerFactoryï¼ŒShared æŒ‡çš„æ˜¯åœ¨å¤šä¸ª Informer ä¸­å…±äº«ä¸€ä¸ªæœ¬åœ° cacheã€‚ Informer ä¸­çš„ ResourceEventHandler å‡½æ•°æœ‰ä¸‰ç§ï¼š 12345678// ResourceEventHandlerFuncs is an adaptor to let you easily specify as many or// as few of the notification functions as you want while still implementing// ResourceEventHandler.type ResourceEventHandlerFuncs struct &#123; AddFunc func(obj interface&#123;&#125;) UpdateFunc func(oldObj, newObj interface&#123;&#125;) DeleteFunc func(obj interface&#123;&#125;)&#125; è¿™ä¸‰ç§å‡½æ•°çš„å¤„ç†é€»è¾‘æ˜¯ç”¨æˆ·è‡ªå®šä¹‰çš„ï¼Œåœ¨åˆå§‹åŒ– controller æ—¶æ³¨å†Œå®Œ ResourceEventHandler åŽï¼Œä¸€æ—¦è¯¥å¯¹è±¡çš„å®žä¾‹æœ‰åˆ›å»ºã€åˆ é™¤ã€æ›´æ–°ä¸‰ä¸­æ“ä½œåŽå°±ä¼šè§¦å‘å¯¹åº”çš„ ResourceEventHandlerã€‚ ä¸‰ã€Informer ä½¿ç”¨ç¤ºä¾‹åœ¨å®žé™…çš„å¼€å‘å·¥ä½œä¸­ï¼ŒInformer ä¸»è¦ç”¨åœ¨ä¸¤å¤„ï¼š åœ¨è®¿é—® k8s apiserver çš„å®¢æˆ·ç«¯ä½œä¸ºä¸€ä¸ª client ç¼“å­˜å¯¹è±¡ä½¿ç”¨ï¼› åœ¨ä¸€äº›è‡ªå®šä¹‰ controller ä¸­ä½¿ç”¨ï¼Œæ¯”å¦‚ operator çš„å¼€å‘ï¼› 1ã€ä¸‹é¢æ˜¯ä¸€ä¸ªä½œä¸º client çš„ä½¿ç”¨ç¤ºä¾‹ï¼š12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package mainimport ( &quot;flag&quot; &quot;fmt&quot; &quot;log&quot; &quot;path/filepath&quot; corev1 &quot;k8s.io/api/core/v1&quot; &quot;k8s.io/apimachinery/pkg/labels&quot; &quot;k8s.io/apimachinery/pkg/util/runtime&quot; &quot;k8s.io/client-go/informers&quot; &quot;k8s.io/client-go/kubernetes&quot; &quot;k8s.io/client-go/tools/cache&quot; &quot;k8s.io/client-go/tools/clientcmd&quot; &quot;k8s.io/client-go/util/homedir&quot;)func main() &#123; var kubeconfig *string if home := homedir.HomeDir(); home != &quot;&quot; &#123; kubeconfig = flag.String(&quot;kubeconfig&quot;, filepath.Join(home, &quot;.kube&quot;, &quot;config&quot;), &quot;(optional) absolute path to the kubeconfig file&quot;) &#125; else &#123; kubeconfig = flag.String(&quot;kubeconfig&quot;, &quot;&quot;, &quot;absolute path to the kubeconfig file&quot;) &#125; flag.Parse() config, err := clientcmd.BuildConfigFromFlags(&quot;&quot;, *kubeconfig) if err != nil &#123; panic(err) &#125; // åˆå§‹åŒ– client clientset, err := kubernetes.NewForConfig(config) if err != nil &#123; log.Panic(err.Error()) &#125; stopper := make(chan struct&#123;&#125;) defer close(stopper) // åˆå§‹åŒ– informer factory := informers.NewSharedInformerFactory(clientset, 0) nodeInformer := factory.Core().V1().Nodes() informer := nodeInformer.Informer() defer runtime.HandleCrash() // å¯åŠ¨ informerï¼Œlist &amp; watch go factory.Start(stopper) // ä»Ž apiserver åŒæ­¥èµ„æºï¼Œå³ list if !cache.WaitForCacheSync(stopper, informer.HasSynced) &#123; runtime.HandleError(fmt.Errorf(&quot;Timed out waiting for caches to sync&quot;)) return &#125; // ä½¿ç”¨è‡ªå®šä¹‰ handler informer.AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: onAdd, UpdateFunc: func(interface&#123;&#125;, interface&#123;&#125;) &#123; fmt.Println(&quot;update not implemented&quot;) &#125;, // æ­¤å¤„çœç•¥ workqueue çš„ä½¿ç”¨ DeleteFunc: func(interface&#123;&#125;) &#123; fmt.Println(&quot;delete not implemented&quot;) &#125;, &#125;) // åˆ›å»º lister nodeLister := nodeInformer.Lister() // ä»Ž lister ä¸­èŽ·å–æ‰€æœ‰ items nodeList, err := nodeLister.List(labels.Everything()) if err != nil &#123; fmt.Println(err) &#125; fmt.Println(&quot;nodelist:&quot;, nodeList) &lt;-stopper&#125;func onAdd(obj interface&#123;&#125;) &#123; node := obj.(*corev1.Node) fmt.Println(&quot;add a node:&quot;, node.Name)&#125; SharedæŒ‡çš„æ˜¯å¤šä¸ª lister å…±äº«åŒä¸€ä¸ªcacheï¼Œè€Œä¸”èµ„æºçš„å˜åŒ–ä¼šåŒæ—¶é€šçŸ¥åˆ°cacheå’Œ listersã€‚è¿™ä¸ªè§£é‡Šå’Œä¸Šé¢å›¾æ‰€å±•ç¤ºçš„å†…å®¹çš„æ˜¯ä¸€è‡´çš„ï¼Œcacheæˆ‘ä»¬åœ¨Indexerçš„ä»‹ç»ä¸­å·²ç»åˆ†æžè¿‡äº†ï¼Œlister æŒ‡çš„å°±æ˜¯OnAddã€OnUpdateã€OnDelete è¿™äº›å›žè°ƒå‡½æ•°èƒŒåŽçš„å¯¹è±¡ã€‚ 2ã€ä»¥ä¸‹æ˜¯ä½œä¸º controller ä½¿ç”¨çš„ä¸€ä¸ªæ•´ä½“å·¥ä½œæµç¨‹(1) åˆ›å»ºä¸€ä¸ªæŽ§åˆ¶å™¨ ä¸ºæŽ§åˆ¶å™¨åˆ›å»º workqueue åˆ›å»º informer, ä¸º informer æ·»åŠ  callback å‡½æ•°ï¼Œåˆ›å»º lister (2) å¯åŠ¨æŽ§åˆ¶å™¨ å¯åŠ¨ informer ç­‰å¾…æœ¬åœ° cache sync å®ŒæˆåŽï¼Œ å¯åŠ¨ workers (3) å½“æ”¶åˆ°å˜æ›´äº‹ä»¶åŽï¼Œæ‰§è¡Œ callback ç­‰å¾…äº‹ä»¶è§¦å‘ ä»Žäº‹ä»¶ä¸­èŽ·å–å˜æ›´çš„ Object åšä¸€äº›å¿…è¦çš„æ£€æŸ¥ ç”Ÿæˆ object keyï¼Œä¸€èˆ¬æ˜¯ namespace/name çš„å½¢å¼ å°† key æ”¾å…¥ workqueue ä¸­ (4) worker loop ç­‰å¾…ä»Ž workqueue ä¸­èŽ·å–åˆ° itemï¼Œä¸€èˆ¬ä¸º object key ç”¨ object key é€šè¿‡ lister ä»Žæœ¬åœ° cache ä¸­èŽ·å–åˆ°çœŸæ­£çš„ object å¯¹è±¡ åšä¸€äº›æ£€æŸ¥ æ‰§è¡ŒçœŸæ­£çš„ä¸šåŠ¡é€»è¾‘ å¤„ç†ä¸‹ä¸€ä¸ª item ä¸‹é¢æ˜¯è‡ªå®šä¹‰ controller ä½¿ç”¨çš„ä¸€ä¸ªå‚è€ƒï¼š 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485var ( masterURL string kubeconfig string)func init() &#123; flag.StringVar(&amp;kubeconfig, &quot;kubeconfig&quot;, &quot;&quot;, &quot;Path to a kubeconfig. Only required if out-of-cluster.&quot;) flag.StringVar(&amp;masterURL, &quot;master&quot;, &quot;&quot;, &quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.&quot;)&#125;func main() &#123; flag.Parse() stopCh := signals.SetupSignalHandler() cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) if err != nil &#123; glog.Fatalf(&quot;Error building kubeconfig: %s&quot;, err.Error()) &#125; kubeClient, err := kubernetes.NewForConfig(cfg) if err != nil &#123; glog.Fatalf(&quot;Error building kubernetes clientset: %s&quot;, err.Error()) &#125; // æ‰€è°“ Informerï¼Œå…¶å®žå°±æ˜¯ä¸€ä¸ªå¸¦æœ‰æœ¬åœ°ç¼“å­˜å’Œç´¢å¼•æœºåˆ¶çš„ã€å¯ä»¥æ³¨å†Œ EventHandler çš„ client // informer watch apiserver,æ¯éš” 30 ç§’ resync ä¸€æ¬¡(list) kubeInformerFactory := informers.NewSharedInformerFactory(kubeClient, time.Second*30) controller := controller.NewController(kubeClient, kubeInformerFactory.Core().V1().Nodes()) // å¯åŠ¨ informer go kubeInformerFactory.Start(stopCh) // start controller if err = controller.Run(2, stopCh); err != nil &#123; glog.Fatalf(&quot;Error running controller: %s&quot;, err.Error()) &#125;&#125;// NewController returns a new network controllerfunc NewController( kubeclientset kubernetes.Interface, networkclientset clientset.Interface, networkInformer informers.NetworkInformer) *Controller &#123; // Create event broadcaster // Add sample-controller types to the default Kubernetes Scheme so Events can be // logged for sample-controller types. utilruntime.Must(networkscheme.AddToScheme(scheme.Scheme)) glog.V(4).Info(&quot;Creating event broadcaster&quot;) eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(glog.Infof) eventBroadcaster.StartRecordingToSink(&amp;typedcorev1.EventSinkImpl&#123;Interface: kubeclientset.CoreV1().Events(&quot;&quot;)&#125;) recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource&#123;Component: controllerAgentName&#125;) controller := &amp;Controller&#123; kubeclientset: kubeclientset, networkclientset: networkclientset, networksLister: networkInformer.Lister(), networksSynced: networkInformer.Informer().HasSynced, workqueue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;Networks&quot;), recorder: recorder, &#125; glog.Info(&quot;Setting up event handlers&quot;) // Set up an event handler for when Network resources change networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: controller.enqueueNetwork, UpdateFunc: func(old, new interface&#123;&#125;) &#123; oldNetwork := old.(*samplecrdv1.Network) newNetwork := new.(*samplecrdv1.Network) if oldNetwork.ResourceVersion == newNetwork.ResourceVersion &#123; // Periodic resync will send update events for all known Networks. // Two different versions of the same Network will always have different RVs. return &#125; controller.enqueueNetwork(new) &#125;, DeleteFunc: controller.enqueueNetworkForDelete, &#125;) return controller&#125; è‡ªå®šä¹‰ controller çš„è¯¦ç»†ä½¿ç”¨æ–¹æ³•å¯ä»¥å‚è€ƒï¼šk8s-controller-custom-resource å››ã€ä½¿ç”¨ä¸­çš„ä¸€äº›é—®é¢˜1ã€Informer äºŒçº§ç¼“å­˜ä¸­çš„åŒæ­¥é—®é¢˜è™½ç„¶ Informer å’Œ Kubernetes ä¹‹é—´æ²¡æœ‰ resync æœºåˆ¶ï¼Œä½† Informer å†…éƒ¨çš„è¿™ä¸¤çº§ç¼“å­˜ DeltaIFIFO å’Œ LocalStore ä¹‹é—´ä¼šå­˜åœ¨ resync æœºåˆ¶ï¼Œk8s ä¸­ kube-controller-manager çš„ StatefulSetController ä¸­ä½¿ç”¨äº†ä¸¤çº§ç¼“å­˜çš„ resync æœºåˆ¶ï¼ˆå¦‚ä¸‹å›¾æ‰€ç¤ºï¼‰ï¼Œæˆ‘ä»¬åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­å‘çŽ° sts åˆ›å»ºåŽè¿‡äº†å¾ˆä¹… pod æ‰ä¼šåˆ›å»ºï¼Œä¸»è¦æ˜¯ç”±äºŽ StatefulSetController çš„ä¸¤çº§ç¼“å­˜ä¹‹é—´ 30s ä¼šåŒæ­¥ä¸€æ¬¡ï¼Œç”±äºŽ StatefulSetController watch åˆ°å˜åŒ–åŽå°±ä¼šæŠŠå¯¹åº”çš„ sts æ”¾å…¥ DeltaIFIFO ä¸­ï¼Œä¸”æ¯éš”30sä¼šæŠŠ LocalStore ä¸­å…¨éƒ¨çš„ sts é‡æ–°å…¥ä¸€é DeltaIFIFOï¼Œå…¥é˜Ÿæ—¶ä¼šåšä¸€äº›å¤„ç†ï¼Œè¿‡æ»¤æŽ‰ä¸€äº›ä¸éœ€è¦é‡å¤å…¥é˜Ÿåˆ—çš„ stsï¼Œè‹¥é—´éš”çš„ 30s å†…æ²¡æœ‰å¤„ç†å®Œé˜Ÿåˆ—ä¸­æ‰€æœ‰çš„ stsï¼Œåˆ™å¾…å¤„ç†é˜Ÿåˆ—ä¸­å§‹ç»ˆå­˜åœ¨æœªå¤„ç†å®Œçš„ stsï¼Œå¹¶ä¸”åœ¨åŒæ­¥è¿‡ç¨‹ä¸­äº§ç”Ÿçš„ sts ä¼šåŠ çš„é˜Ÿåˆ—çš„å°¾éƒ¨ï¼Œæ–°åŠ å…¥é˜Ÿå°¾çš„ sts åªèƒ½ç­‰åˆ°å‰é¢çš„ sts å¤„ç†å®Œæˆï¼ˆä¹Ÿå°±æ˜¯ resync å®Œæˆï¼‰æ‰ä¼šè¢«å¤„ç†ï¼Œæ‰€ä»¥å¯¼è‡´çš„çŽ°è±¡å°±æ˜¯ sts åˆ›å»ºåŽè¿‡äº†å¾ˆä¹… pod æ‰ä¼šåˆ›å»ºã€‚ ä¼˜åŒ–çš„æ–¹æ³•å°±æ˜¯åŽ»æŽ‰äºŒçº§ç¼“å­˜çš„åŒæ­¥ç­–ç•¥ï¼ˆå°† setInformer.Informer().AddEventHandlerWithResyncPeriod() æ”¹ä¸º informer.AddEventHandler()ï¼‰æˆ–è€…è°ƒå¤§åŒæ­¥å‘¨æœŸï¼Œä½†æ˜¯åœ¨ç ”ç©¶ kube-controller-manager å…¶ä»– controller æ—¶å‘çŽ°å¹¶ä¸æ˜¯æ‰€æœ‰çš„ controller éƒ½æœ‰åŒæ­¥ç­–ç•¥ï¼Œç¤¾åŒºä¹Ÿæœ‰ç›¸å…³çš„ issue åé¦ˆäº†è¿™ä¸€é—®é¢˜ï¼ŒRemove resync period for sset controllerï¼Œç¤¾åŒºä¹Ÿä¼šåœ¨ä»¥åŽçš„ç‰ˆæœ¬ä¸­åŽ»æŽ‰ä¸¤çº§ç¼“å­˜ä¹‹é—´çš„ resync ç­–ç•¥ã€‚ k8s.io/kubernetes/pkg/controller/statefulset/stateful_set.go 2ã€ä½¿ç”¨ Informer å¦‚ä½•ç›‘å¬æ‰€æœ‰èµ„æºå¯¹è±¡ï¼Ÿä¸€ä¸ª Informer å®žä¾‹åªèƒ½ç›‘å¬ä¸€ç§ resourceï¼Œæ¯ä¸ª resource éœ€è¦åˆ›å»ºå¯¹åº”çš„ Informer å®žä¾‹ã€‚ 3ã€ä¸ºä»€ä¹ˆä¸æ˜¯ä½¿ç”¨ workqueueï¼Ÿå»ºè®®ä½¿ç”¨ RateLimitingQueueï¼Œå®ƒç›¸æ¯”æ™®é€šçš„ workqueue å¤šäº†ä»¥ä¸‹çš„åŠŸèƒ½: é™æµï¼šå¯ä»¥é™åˆ¶ä¸€ä¸ª item è¢« reenqueued çš„æ¬¡æ•°ã€‚ é˜²æ­¢ hot loopï¼šå®ƒä¿è¯äº†ä¸€ä¸ª item è¢« reenqueued åŽï¼Œä¸ä¼šé©¬ä¸Šè¢«å¤„ç†ã€‚ äº”ã€æ€»ç»“æœ¬æ–‡ä»‹ç»äº† client-go åŒ…ä¸­æ ¸å¿ƒç»„ä»¶ Informer çš„åŽŸç†ä»¥åŠä½¿ç”¨æ–¹æ³•ï¼ŒInformer ä¸»è¦åŠŸèƒ½æ˜¯ç¼“å­˜å¯¹è±¡åˆ°æœ¬åœ°ä»¥åŠæ ¹æ®å¯¹åº”çš„äº‹ä»¶ç±»åž‹è§¦å‘å·²æ³¨å†Œå¥½çš„ ResourceEventHandlerï¼Œå…¶ä¸»è¦ç”¨åœ¨è®¿é—® k8s apiserver çš„å®¢æˆ·ç«¯å’Œ operator ä¸­ã€‚ å‚è€ƒï¼š å¦‚ä½•ç”¨ client-go æ‹“å±• Kubernetes çš„ API https://www.kubernetes.org.cn/2693.html Kubernetes å¤§å’–ç§€å¾è¶…ã€Šä½¿ç”¨ client-go æŽ§åˆ¶åŽŸç”ŸåŠæ‹“å±•çš„ Kubernetes APIã€‹ Use prometheus conventions for workqueue metrics æ·±å…¥æµ…å‡ºkubernetesä¹‹client-goçš„workqueue https://gianarb.it/blog/kubernetes-shared-informer ç†è§£ K8S çš„è®¾è®¡ç²¾é«“ä¹‹ List-Watchæœºåˆ¶å’ŒInformeræ¨¡å— https://ranler.org/notes/file/528 Kubernetes Client-go Informer æºç åˆ†æž]]></content>
      <tags>
        <tag>client-go</tag>
        <tag>informer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½¿ç”¨æ’ä»¶æ‰©å±• kubectl]]></title>
    <url>%2F2019%2F05%2F16%2Fkubectl_plugin%2F</url>
    <content type="text"><![CDATA[ç”±äºŽç¬”è€…æ‰€ç»´æŠ¤çš„é›†ç¾¤è§„æ¨¡è¾ƒå¤§ï¼Œç»å¸¸éœ€è¦ä½¿ç”¨ kubectl æ¥æŽ’æŸ¥ä¸€äº›é—®é¢˜ï¼Œä½†æ˜¯ kubectl åŠŸèƒ½æœ‰é™ï¼Œæœ‰äº›æ“ä½œè¿˜æ˜¯éœ€è¦å†™ä¸€ä¸ªè„šæœ¬å¯¹ kubectl åšä¸€äº›å°è£…æ‰èƒ½è¾¾åˆ°ç›®çš„ã€‚æ¯”å¦‚æˆ‘ç»å¸¸åšçš„ä¸€ä¸ªæ“ä½œå°±æ˜¯æŽ’æŸ¥ä¸€ä¸‹çº¿ä¸Šå“ªäº›å®¿ä¸»çš„ cpu/memory request ä½¿ç”¨çŽ‡è¶…è¿‡æŸä¸ªé˜ˆå€¼ï¼Œkubectl å¹¶ä¸èƒ½ç›´æŽ¥çœ‹åˆ°ä¸€ä¸ª master ä¸‹æ‰€æœ‰å®¿ä¸»çš„ request ä½¿ç”¨çŽ‡ï¼Œä½†å¯ä»¥ä½¿ç”¨ kubectl describe node xxxæŸ¥çœ‹æŸä¸ªå®¿ä¸»æœºçš„ request ä½¿ç”¨çŽ‡ï¼Œæ‰€ä»¥åªå¥½å†™ä¸€ä¸ªè„šæœ¬æ¥æ‰«ä¸€éäº†ã€‚ 123456789#!/bin/bashecho -e &quot;node\tcpu_requets memory_requets&quot;for i in `kubectl get node | grep -v NAME | awk &apos;&#123;print $1&#125;&apos;`;do res=$(kubectl describe node $i | grep -A 3 &quot;Resource&quot;) cpu_requets=$(echo $&#123;res&#125; | awk &apos;&#123;print $9&#125;&apos; | awk -F &apos;%&apos; &apos;&#123;print $1&#125;&apos; | awk -F &apos;(&apos; &apos;&#123;print $2&#125;&apos;) memory_requets=$(echo $&#123;res&#125; | awk &apos;&#123;print $14&#125;&apos; | awk -F &apos;%&apos; &apos;&#123;print $1&#125;&apos; | awk -F &apos;(&apos; &apos;&#123;print $2&#125;&apos;) echo -e &quot;$i\t$&#123;cpu_requets&#125; \t$&#123;memory_requets&#125;&quot;done ç±»ä¼¼çš„éœ€æ±‚æ¯”è¾ƒå¤šï¼Œæ­¤å¤„ä¸ä¸€ä¸€åˆ—ä¸¾ï¼Œè¿™ç§æ“ä½œç»å¸¸éœ€è¦åšï¼Œè™½ç„¶å†™ä¸€ä¸ªè„šæœ¬ä¹Ÿèƒ½å®Œå…¨æžå®šï¼Œä½†ç¡®å®žæ¯”è¾ƒ lowï¼Œä¹Ÿä¸ä¾¿æä¾›ç»™åˆ«äººä½¿ç”¨ï¼ŒåŸºäºŽæ­¤äº†è§£åˆ°ç›®å‰å®˜æ–¹å¯¹ kubectl çš„æ’ä»¶æœºåˆ¶åšäº†ä¸€äº›æ”¹è¿›ï¼Œå¯¹ kubectl çš„æ‰©å±•ä¹Ÿæ¯”è¾ƒå®¹æ˜“ï¼Œæ‰€ä»¥ä¸‹æ–‡ä¼šå¸¦ä½ äº†è§£ä¸€ä¸‹ kubectl çš„æ‰©å±•åŠŸèƒ½ã€‚ ä¸€ã€ç¼–å†™ kubectl æ’ä»¶kubectl å‘½ä»¤ä»Ž v1.8.0 ç‰ˆæœ¬å¼€å§‹æ”¯æŒæ’ä»¶æœºåˆ¶ï¼Œä¹‹åŽçš„ç‰ˆæœ¬ä¸­æˆ‘ä»¬éƒ½å¯ä»¥å¯¹ kubectl å‘½ä»¤è¿›è¡Œæ‰©å±•ï¼Œkubernetes åœ¨ v1.12 ä»¥åŽæ’ä»¶å¯ä»¥ç›´æŽ¥æ˜¯ä»¥ kubectl- å¼€å¤´å‘½ä»¤çš„ä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œæ’ä»¶æœºåˆ¶åœ¨ v1.14 è¿›å…¥ GA çŠ¶æ€ï¼Œè¿™ç§æ”¹è¿›æ˜¯å¸Œæœ›ç”¨æˆ·ä»¥äºŒè¿›åˆ¶æ–‡ä»¶å½¢å¼å¯ä»¥æ‰©å±•è‡ªå·±çš„ kubectl å­å‘½ä»¤ã€‚å½“ç„¶ï¼Œkubectl æ’ä»¶æœºåˆ¶æ˜¯ä¸Žè¯­è¨€æ— å…³çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ä½ å¯ä»¥ç”¨ä»»ä½•è¯­è¨€ç¼–å†™æ’ä»¶ã€‚ å¦‚ kubernetes å®˜æ–¹æ–‡æ¡£ä¸­æè¿°ï¼Œåªè¦å°†äºŒè¿›åˆ¶æ–‡ä»¶æ”¾åœ¨ç³»ç»Ÿ PATH ä¸‹ï¼Œkubectl å³å¯è¯†åˆ«ï¼ŒäºŒè¿›åˆ¶æ–‡ä»¶ç±»ä¼¼ kubectl-foo-barï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨æ—¶ kubectl ä¼šåŒ¹é…æœ€é•¿çš„äºŒè¿›åˆ¶æ–‡ä»¶ã€‚ å®˜æ–¹å»ºè®®ä½¿ç”¨ k8s.io/cli-runtime åº“è¿›è¡Œç¼–å†™ï¼Œè‹¥ä½ çš„æ’ä»¶éœ€è¦æ”¯æŒä¸€äº›å‘½ä»¤è¡Œå‚æ•°ï¼Œå¯ä»¥å‚è€ƒä½¿ç”¨ï¼Œå®˜æ–¹ä¹Ÿç»™äº†ä¸€ä¸ªä¾‹å­ sample-cli-pluginã€‚ è¿˜æ˜¯å›žåˆ°æœ€åˆçš„é—®é¢˜ï¼Œå¯¹äºŽèŽ·å–ä¸€ä¸ªé›†ç¾¤å†™æ‰€æœ‰ node çš„èµ„æºä½¿ç”¨çŽ‡ï¼Œç¬”è€…åŸºäºŽä¹Ÿç¼–å†™äº†ä¸€ä¸ªç®€å•çš„æ’ä»¶ã€‚ 12345// å®‰è£…æ’ä»¶$ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o bin/kubectl-view-node-resource cmd/view-node-resource/main.go$ mv bin/kubectl-view-node-resource /usr/bin/ ä½¿ç”¨ kubectl plugin list æŸ¥çœ‹ PATH ä¸‹æœ‰å“ªäº›å¯ç”¨çš„æ’ä»¶ã€‚ 123456// æŸ¥çœ‹æ’ä»¶$ kubectl plugin listThe following kubectl-compatible plugins are available:/usr/bin/kubectl-view-node-resource 123456789101112131415161718192021// ä½¿ç”¨æ’ä»¶$ kubectl view node taints --helpA longer description that spans multiple lines and likely containsexamples and usage of using your application. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.Usage: view-node-taints [flags]Flags: --config string config file (default is $HOME/.view-node-taints.yaml) -h, --help help for view-node-taints -t, --toggle Help message for toggle$ kubectl view node resource Name PodCount CPURequests MemoryRequests CPULimits MemoryLimits 192.168.1.110 4 0 (0.00%) 6.4 (41.26%) 8 (100.00%) 16.0 (103.14%) æ­¤å¤–ï¼Œè¿˜å¼€å‘ä¸€ä¸ªæŸ¥çœ‹é›†ç¾¤ä¸‹æ‰€æœ‰ node taints çš„æ’ä»¶ï¼Œkubectl æ”¯æŒæŸ¥çœ‹å®¿ä¸»çš„ labelï¼Œä½†æ˜¯æ²¡æœ‰ç›´æŽ¥æŸ¥çœ‹æ‰€æœ‰å®¿ä¸» taints çš„å‘½ä»¤ï¼Œæ’ä»¶æ•ˆæžœå¦‚ä¸‹ï¼š 123$ kubectl view node taints Name Status Age Version Taints 192.168.1.110 Ready,SchedulingDisabled 49d v1.8.1-35+9406f9d9909c61-dirty enabledDiskSchedule=true:NoSchedule æ’ä»¶ä»£ç åœ°å€ï¼škubectl-plugin äºŒã€kubectl æ’ä»¶ç®¡ç†å·¥å…· krewä¸Šæ–‡è®²äº†å¦‚ä½•ç¼–å†™ä¸€ä¸ªæ’ä»¶ï¼Œä½†æ˜¯å®˜æ–¹ä¹Ÿæä¾›ä¸€ä¸ªæ’ä»¶åº“å¹¶æä¾›äº†ä¸€ä¸ªæ’ä»¶ç®¡ç†å·¥å…· krew ï¼Œkrew æ˜¯ kubectl æ’ä»¶çš„ç®¡ç†å™¨ï¼Œä½¿ç”¨ krew å¯ä»¥è½»æ¾çš„æŸ¥æ‰¾ã€å®‰è£…å’Œç®¡ç† kubectl æ’ä»¶ï¼Œå®ƒç±»ä¼¼äºŽ yumã€aptã€ dnfï¼Œkrew ä¹Ÿå¯ä»¥å¸®åŠ©ä½ å°†å·²å†™å¥½çš„æ’ä»¶åœ¨å¤šä¸ªå¹³å°ä¸Šæ‰“åŒ…å’Œåˆ†å‘ï¼Œkrew è‡ªå·±ä¹Ÿä½œä¸ºä¸€ä¸ª kubectl æ’ä»¶å­˜åœ¨ã€‚ krew ä»…æ”¯æŒåœ¨ v1.12 åŠä¹‹åŽçš„ç‰ˆæœ¬ä¸­ä½¿ç”¨ã€‚ 1ã€å®‰è£… krew 123456789$ ( set -x; cd &quot;$(mktemp -d)&quot; &amp;&amp; curl -fsSLO &quot;https://storage.googleapis.com/krew/v0.2.1/krew.&#123;tar.gz,yaml&#125;&quot; &amp;&amp; tar zxvf krew.tar.gz &amp;&amp; ./krew-&quot;$(uname | tr &apos;[:upper:]&apos; &apos;[:lower:]&apos;)_amd64&quot; install \ --manifest=krew.yaml --archive=krew.tar.gz)$ export PATH=&quot;$&#123;KREW_ROOT:-$HOME/.krew&#125;/bin:$PATH&quot; 2ã€krew çš„ä½¿ç”¨ 12345$ kubectl krew search # show all plugins$ kubectl krew install view-secret # install a plugin named &quot;view-secret&quot;$ kubectl view-secret default-token-4cwvh # use the plugin$ kubectl krew upgrade # upgrade installed plugins$ kubectl krew remove view-secret # uninstall a plugin è‹¥æƒ³è®©ä½ è‡ªå·±çš„æ’ä»¶åŠ å…¥åˆ° krew çš„ç´¢å¼•ä¸­ï¼Œå¯ä»¥å‚è€ƒï¼šhow to package and publish a plugin for krewã€‚ å‚è€ƒï¼š kubectl æ’ä»¶å‘½æ˜Žè§„èŒƒ https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/ https://github.com/gosoon/kubectl-plugin]]></content>
      <tags>
        <tag>kubectl plugin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[éƒ¨ç½² kubernetes å¯è§†åŒ–ç›‘æŽ§ç»„ä»¶]]></title>
    <url>%2F2019%2F04%2F22%2Fk8s_dashboard_prometheus%2F</url>
    <content type="text"><![CDATA[éšç€ kubernetes çš„å¤§è§„æ¨¡ä½¿ç”¨ï¼Œå¯¹ kubernetes ç»„ä»¶åŠå…¶ä¸Šè¿è¡ŒæœåŠ¡çš„ç›‘æŽ§ä¹Ÿæ˜¯éžå¸¸é‡è¦çš„ä¸€ä¸ªçŽ¯èŠ‚ï¼Œç›®å‰å¼€æºçš„ç›‘æŽ§ç»„ä»¶æœ‰å¾ˆå¤šç§ï¼Œä¾‹å¦‚ cAdvisorã€Heapsterã€metrics-serverã€kube-state-metricsã€Prometheus ç­‰ï¼Œå¯¹ç›‘æŽ§æ•°æ®çš„å¯è§†åŒ–æŸ¥çœ‹ç»„ä»¶æœ‰ Dashboardã€ Prometheusã€Grafana ç­‰ï¼Œæœ¬æ–‡ä¼šä»‹ç» kube-dashboard å’ŒåŸºäºŽ prometheus æ­å»ºæ•°æ®å¯è§†åŒ–ç›‘æŽ§ã€‚ kubernetes ç‰ˆæœ¬ï¼šv1.12 ä¸€ã€kubernetes-dashboard çš„éƒ¨ç½²1ã€åˆ›å»º kubernetes-dashboard1234567$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml$ kubectl get svc -n kube-system | grep kubernetes-dashboardkubernetes-dashboard ClusterIP 10.101.203.44 &lt;none&gt; 443/TCP 2h$ kubectl get pod -n kube-system | grep kubernetes-dashboardkubernetes-dashboard-65c76f6c97-8npsv 1/1 Running 0 2h æ‰€éœ€é•œåƒä¸‹è½½åœ°å€ï¼šk8s-system-images 2ã€ä½¿ç”¨ nodePort æ–¹å¼è®¿é—® kubernetes-dashboardnodeport çš„è®¿é—®æ–¹å¼è™½ç„¶æœ‰æ€§èƒ½æŸå¤±ä½†æ˜¯æ¯”è¾ƒç®€å•ï¼Œkubernetes-dashboard é»˜è®¤ä½¿ç”¨ clusterIP çš„æ–¹å¼æš´éœ²æœåŠ¡ï¼Œä¿®æ”¹ kubernetes-dashboard svc ä½¿ç”¨ nodePort æ–¹å¼ï¼š 123456789101112131415$ kubectl edit svc -n kube-system ... spec: clusterIP: 10.101.203.44 externalTrafficPolicy: Cluster ports: - nodePort: 8004 // æ·»åŠ  nodeport ç«¯å£ port: 443 protocol: TCP targetPort: 8443 selector: k8s-app: kubernetes-dashboard sessionAffinity: None type: NodePort // å°† ClusterIP ä¿®æ”¹ä¸º NodePort ... nodePort ç«¯å£é»˜è®¤ä¸º 30000-32767ï¼Œè‹¥ä½¿ç”¨å…¶ä»–ç«¯å£ï¼Œéœ€è¦ä¿®æ”¹ apiserver çš„å¯åŠ¨å‚æ•° --service-node-port-range æ¥æŒ‡å®š nodePort èŒƒå›´ï¼Œå¦‚ï¼š--service-node-port-range 8000-9000ã€‚ 3ã€åˆ›å»º kubernetes-dashboard ç®¡ç†å‘˜è§’è‰² kubernetes-dashboard-admin.yamlï¼š 123456789101112131415161718apiVersion: v1kind: ServiceAccountmetadata: name: dashboard-admin namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: dashboard-adminsubjects: - kind: ServiceAccount name: dashboard-admin namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io åˆ›å»ºè§’è‰²å¹¶èŽ·å– tokenï¼š 1234567891011121314151617$ kubectl apply -f kubernetes-dashboard-admin.yaml$ kubectl describe secrets `kubectl get secret -n kube-system | grep dashboard-admin | awk &apos;&#123;print $1&#125;&apos;` -n kube-systemName: dashboard-admin-token-hrhfdNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: 76805bdb-6047-11e9-ba0d-525400c322d9Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4taHJoZmQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNzY4MDViZGItNjA0Ny0xMWU5LWJhMGQtNTI1NDAwYzMyMmQ5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.hJJRyp_O4sGvIULj3BhqidCkkPnD4A2AtnpkXJoEPCALaaQHC8zhCA5-nDNlo2fiEggZ02UZPwiyGxKKFPC57UlKhjTf5zYcMIhELVXlj5FdBmjzCZcCHVFF4tj_rCoOFlZi6fQ3vNCcX8CtLxX_OsH1YXaFVuUmR1gYm97hbyuO382_k3tFIPXFP3QG8zUtc_7QMkeMNEakJZLCvkW8xdlaCuC-GVAMhZl5Kq1MSthuF-8HY7KaXhvqQzfD4DQZrdQ7vf_7NG3rdvhsj8nQ__TTe1W0RjqwkQuxg5YdE4gbAsxwJjkek-N0K9HfnZhkS9WosaUaUe9pZaGZ9akqyQ token æ˜¯è®¿é—® dashboard éœ€è¦ç”¨çš„ã€‚ è‹¥æ²¡æœ‰å®‰è£… kube-proxyï¼Œå¯ä»¥å‚è€ƒå®˜æ–¹æä¾›ä½¿ç”¨ kubectl proxy çš„æ–¹å¼è®¿é—®ï¼š 1$ kubectl proxy --address=IP --disable-filter=true è®¿é—® http://IP:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login å·²éƒ¨ç½² kube-proxy çš„å¯ç›´æŽ¥è®¿é—® https://IP:nodePort é€‰æ‹©ä»¤ç‰Œæ–¹å¼ä½¿ç”¨ä¸Šé¢ç”Ÿæˆçš„ token ç™»å½•ã€‚ Dashboard å¯ä»¥ä½¿ç”¨ Ingressã€Letâ€™s Encrypt ç­‰å¤šç§æ–¹å¼é…ç½® sslï¼Œå…³äºŽ ssl çš„è¯¦ç»†é…ç½®æ­¤å¤„ä¸è¿›è¡Œè¯¦è§£ã€‚ äºŒã€éƒ¨ç½² prometheusprometheus ä½œä¸º CNCF ç”Ÿæ€åœˆä¸­çš„é‡è¦ä¸€å‘˜ï¼Œå…¶æ´»è·ƒåº¦ä»…æ¬¡äºŽ Kubernetes, çŽ°å·²å¹¿æ³›ç”¨äºŽ Kubernetes é›†ç¾¤çš„ç›‘æŽ§ç³»ç»Ÿä¸­ã€‚prometheus çš„éƒ¨ç½²ç›¸å¯¹æ¯”è¾ƒç®€å•ï¼Œç¤¾åŒºå·²ç»æœ‰äº† kube-prometheusï¼Œkube-prometheus ä¼šéƒ¨ç½²åŒ…å« prometheus-operatorã€grafanaã€kube-state-metrics ç­‰å¤šä¸ªç»„ä»¶ã€‚ 123$ git clone https://github.com/coreos/kube-prometheus$ kubectl apply -f manifests/ ä¸ºäº†ä½¿ç”¨ç®€å•ï¼Œæˆ‘ä¹Ÿä¼šå°† prometheus å’Œ grafana çš„ç«¯å£ä¿®æ”¹ä¸º nodePort çš„æ–¹å¼è¿›è¡Œæš´éœ²ï¼š 12345678910111213141516171819202122232425262728$ kubectl edit svc prometheus-k8s -n monitoring$ kubectl edit svc grafana -n monitoring$ kubectl get svc -n monitoringNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEalertmanager-main NodePort 10.102.81.118 &lt;none&gt; 9093:8007/TCP 5d1halertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,6783/TCP 5d1hgrafana NodePort 10.96.19.82 &lt;none&gt; 3000:8006/TCP 5d1hkube-state-metrics ClusterIP None &lt;none&gt; 8443/TCP,9443/TCP 5d1hnode-exporter ClusterIP None &lt;none&gt; 9100/TCP 5d1hprometheus-adapter ClusterIP 10.107.103.58 &lt;none&gt; 443/TCP 5d1hprometheus-k8s NodePort 10.110.222.41 &lt;none&gt; 9090:8005/TCP 5d1hprometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 5d1hprometheus-operator ClusterIP None &lt;none&gt; 8080/TCP 5d1h$ kubectl get pod -n monitoringNAME READY STATUS RESTARTS AGEalertmanager-main-0 2/2 Running 0 4dalertmanager-main-1 2/2 Running 0 4dalertmanager-main-2 2/2 Running 0 4dgrafana-9d97dfdc7-qfjts 1/1 Running 0 4dkube-state-metrics-74d7dcd7dc-qfz5m 4/4 Running 0 3d11hnode-exporter-5cdl2 2/2 Running 0 4dprometheus-adapter-b7d894c9c-dvzzq 1/1 Running 0 4dprometheus-k8s-0 3/3 Running 1 2d2hprometheus-k8s-1 3/3 Running 1 4dprometheus-operator-77b8b97459-7qfxj 1/1 Running 0 4d ä¸Šé¢å‡ ä¸ªç»„ä»¶æˆåŠŸè¿è¡ŒåŽå°±å¯ä»¥åœ¨é¡µé¢è®¿é—® prometheus å’Œ ganfana ï¼š è¿›å…¥ grafana çš„ web ç«¯ï¼Œé»˜è®¤ç”¨æˆ·åå’Œå¯†ç å‡ä¸º adminï¼š grafana æ”¯æŒå¯¼å…¥å…¶ä»–çš„ Dashboardï¼Œåœ¨ grafana å®˜æ–¹ç½‘ç«™å¯ä»¥æœåˆ°å¤§é‡ä¸Ž k8s ç›¸å…³çš„ dashboardã€‚ ä¸‰ã€æ€»ç»“æœ¬æ–‡ä»‹ç»äº†å¯¹ kubernetes å’Œå®¹å™¨ç›‘æŽ§æ¯”è¾ƒæˆç†Ÿçš„ä¸¤ä¸ªæ–¹æ¡ˆï¼Œè™½ç„¶ç›®å‰å¼€æºçš„æ–¹æ¡ˆæ¯”è¾ƒå¤šï¼Œä½†æ˜¯è¦å½¢æˆé‡‡é›†ã€å­˜å‚¨ã€å±•ç¤ºã€æŠ¥è­¦ä¸€ä¸ªå®Œæˆçš„ä½“ç³»è¿˜éœ€è¦åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­ä¸æ–­æŽ¢ç´¢ä¸Žå®Œå–„ã€‚]]></content>
      <tags>
        <tag>kube-dashboard</tag>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes æŒ‡æ ‡é‡‡é›†ç»„ä»¶ metrics-server çš„éƒ¨ç½²]]></title>
    <url>%2F2019%2F04%2F14%2Fk8s_metrics_server%2F</url>
    <content type="text"><![CDATA[metrics-server æ˜¯ä¸€ä¸ªé‡‡é›†é›†ç¾¤ä¸­æŒ‡æ ‡çš„ç»„ä»¶ï¼Œç±»ä¼¼äºŽ cadvisorï¼Œåœ¨ v1.8 ç‰ˆæœ¬ä¸­å¼•å…¥ï¼Œå®˜æ–¹å°†å…¶ä½œä¸º heapster çš„æ›¿ä»£è€…ï¼Œmetric-server å±žäºŽ core metrics(æ ¸å¿ƒæŒ‡æ ‡)ï¼Œæä¾› API metrics.k8s.ioï¼Œä»…å¯ä»¥æŸ¥çœ‹ nodeã€pod å½“å‰ CPU/Memory/Storage çš„èµ„æºä½¿ç”¨æƒ…å†µï¼Œä¹Ÿæ”¯æŒé€šè¿‡ Metrics API çš„å½¢å¼èŽ·å–ï¼Œä»¥æ­¤æ•°æ®æä¾›ç»™ Dashboardã€HPAã€scheduler ç­‰ä½¿ç”¨ã€‚ ä¸€ã€å¼€å¯ API Aggregationç”±äºŽ metrics-server éœ€è¦æš´éœ² APIï¼Œä½† k8s çš„ API è¦ç»Ÿä¸€ç®¡ç†ï¼Œå¦‚ä½•å°† apiserver çš„è¯·æ±‚è½¬å‘ç»™ metrics-server ï¼Œè§£å†³æ–¹æ¡ˆå°±æ˜¯ä½¿ç”¨ kube-aggregator ï¼Œæ‰€ä»¥åœ¨éƒ¨ç½² metrics-server ä¹‹å‰ï¼Œéœ€è¦åœ¨ kube-apiserver ä¸­å¼€å¯ API Aggregationï¼Œå³å¢žåŠ ä»¥ä¸‹é…ç½®ï¼š 1234567--proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt--proxy-client-key-file=/etc/kubernetes/certs/proxy.key--requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt--requestheader-allowed-names=aggregator--requestheader-extra-headers-prefix=X-Remote-Extra---requestheader-group-headers=X-Remote-Group--requestheader-username-headers=X-Remote-User å¦‚æžœkube-proxyæ²¡æœ‰åœ¨Masterä¸Šé¢è¿è¡Œï¼Œè¿˜éœ€è¦é…ç½® 1--enable-aggregator-routing=true kube-aggregator çš„è¯¦ç»†è®¾è®¡æ–‡æ¡£è¯·å‚è€ƒï¼šconfigure-aggregation-layer äºŒã€éƒ¨ç½² metrics-server1ã€èŽ·å–é…ç½®æ–‡ä»¶12$ git clone https://github.com/kubernetes/kubernetes$ cd kubernetes/cluster/addons/metrics-server/ 2ã€ä¿®æ”¹ metrics-server é…ç½®å‚æ•°ä¿®æ”¹ resource-reader.yaml æ–‡ä»¶ï¼š 123456789101112rules:- apiGroups: - &quot;&quot; resources: - pods - nodes - nodes/stats #æ–°å¢žè¿™ä¸€è¡Œ - namespaces verbs: - get - list - watch ä¿®æ”¹ metrics-server-deployment.yamlæ–‡ä»¶: 123456789101112131415161718192021222324252627282930313233 ...... # metrics-server containers å¯åŠ¨å‚æ•°ä½œå¦‚ä¸‹ä¿®æ”¹ï¼š containers: - name: metrics-server image: k8s.gcr.io/metrics-server-amd64:v0.3.1 command: - /metrics-server - --metric-resolution=30s - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP # These are needed for GKE, which doesn&apos;t support secure communication yet. # Remove these lines for non-GKE clusters, and when GKE supports token-based auth. #- --kubelet-port=10255 #- --deprecated-kubelet-completely-insecure=true ...... # ä¿®æ”¹å¯åŠ¨å‚æ•°ï¼š command: - /pod_nanny - --config-dir=/etc/config - --cpu=80m - --extra-cpu=0.5m - --memory=80Mi - --extra-memory=8Mi - --threshold=5 - --deployment=metrics-server-v0.3.1 - --container=metrics-server - --poll-period=300000 - --estimator=exponential # Specifies the smallest cluster (defined in number of nodes) # resources will be scaled to. #- --minClusterSize=&#123;&#123; metrics_server_min_cluster_size &#125;&#125; 3ã€éƒ¨ç½²1kubectl apply -f . metrics-server çš„èµ„æºå ç”¨é‡ä¼šéšç€é›†ç¾¤ä¸­çš„ Pod æ•°é‡çš„ä¸æ–­å¢žé•¿è€Œä¸æ–­ä¸Šå‡ï¼Œå› æ­¤éœ€è¦ addon-resizer åž‚ç›´æ‰©ç¼© metrics-serverã€‚addon-resizer ä¾æ®é›†ç¾¤ä¸­èŠ‚ç‚¹çš„æ•°é‡çº¿æ€§åœ°æ‰©å±• metrics-serverï¼Œä»¥ä¿è¯å…¶èƒ½å¤Ÿæœ‰èƒ½åŠ›æä¾›å®Œæ•´çš„metrics API æœåŠ¡ï¼Œå…·ä½“å‚è€ƒï¼šaddon-resizerã€‚ æ‰€éœ€è¦çš„é•œåƒå¯ä»¥åœ¨ k8s-system-images ä¸­ä¸‹è½½ã€‚ æ£€æŸ¥æ˜¯å¦éƒ¨ç½²æˆåŠŸï¼š 12345$ kubectl get apiservices | grep metricsv1beta1.metrics.k8s.io kube-system/metrics-server True 2m$ kubectl get pod -n kube-systemmetrics-server-v0.3.1-65b6db6945-rpqwf 2/2 Running 0 20h ä¸‰ã€metrics-server çš„ä½¿ç”¨ç”±äºŽé‡‡é›†æ•°æ®é—´éš”ä¸º1åˆ†é’Ÿï¼Œç­‰å¾…æ•°åˆ†é’ŸåŽæŸ¥çœ‹æ•°æ®ï¼š 123456789101112131415$ kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%node1 108m 2% 1532Mi 40%$ kubectl top pod -n kube-systemNAME CPU(cores) MEMORY(bytes)coredns-576cbf47c7-8v6n8 2m 14Micoredns-576cbf47c7-qk7rk 2m 10Mietcd-node1 11m 80Mikube-apiserver-node1 17m 566Mikube-controller-manager-node1 17m 67Mikube-flannel-ds-amd64-8lvs2 2m 13Mikube-proxy-85lhl 3m 19Mikube-scheduler-node1 5m 16Mimetrics-server-v0.3.1-65b6db6945-rpqwf 2m 19Mi Metrics-server å¯ç”¨ API åˆ—è¡¨å¦‚ä¸‹ï¼š http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes/&lt;node-name&gt; http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/pods http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespace/&lt;namespace-name&gt;/pods/&lt;pod-name&gt; ç”±äºŽ k8s åœ¨ v1.10 åŽåºŸå¼ƒäº† 8080 ç«¯å£ï¼Œå¯ä»¥é€šè¿‡ä»£ç†æˆ–è€…ä½¿ç”¨è®¤è¯çš„æ–¹å¼è®¿é—®è¿™äº› APIï¼š12$ kubectl proxy$ curl http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes ä¹Ÿå¯ä»¥ç›´æŽ¥é€šè¿‡ kubectl å‘½ä»¤æ¥è®¿é—®è¿™äº› APIï¼Œæ¯”å¦‚ï¼š1234$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/&lt;node-name&gt;$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespace/&lt;namespace-name&gt;/pods/&lt;pod-name&gt;]]></content>
      <tags>
        <tag>metrics-server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernets ä¸­ç»„ä»¶é«˜å¯ç”¨çš„å®žçŽ°æ–¹å¼]]></title>
    <url>%2F2019%2F03%2F13%2Fk8s_leader_election%2F</url>
    <content type="text"><![CDATA[ç”Ÿäº§çŽ¯å¢ƒä¸­ä¸ºäº†ä¿éšœä¸šåŠ¡çš„ç¨³å®šæ€§ï¼Œé›†ç¾¤éƒ½éœ€è¦é«˜å¯ç”¨éƒ¨ç½²ï¼Œk8s ä¸­ apiserver æ˜¯æ— çŠ¶æ€çš„ï¼Œå¯ä»¥æ¨ªå‘æ‰©å®¹ä¿è¯å…¶é«˜å¯ç”¨ï¼Œkube-controller-manager å’Œ kube-scheduler ä¸¤ä¸ªç»„ä»¶é€šè¿‡ leader é€‰ä¸¾ä¿éšœé«˜å¯ç”¨ï¼Œå³æ­£å¸¸æƒ…å†µä¸‹ kube-scheduler æˆ– kube-manager-controller ç»„ä»¶çš„å¤šä¸ªå‰¯æœ¬åªæœ‰ä¸€ä¸ªæ˜¯å¤„äºŽä¸šåŠ¡é€»è¾‘è¿è¡ŒçŠ¶æ€ï¼Œå…¶å®ƒå‰¯æœ¬åˆ™ä¸æ–­çš„å°è¯•åŽ»èŽ·å–é”ï¼ŒåŽ»ç«žäº‰ leaderï¼Œç›´åˆ°è‡ªå·±æˆä¸ºleaderã€‚å¦‚æžœæ­£åœ¨è¿è¡Œçš„ leader å› æŸç§åŽŸå› å¯¼è‡´å½“å‰è¿›ç¨‹é€€å‡ºï¼Œæˆ–è€…é”ä¸¢å¤±ï¼Œåˆ™ç”±å…¶å®ƒå‰¯æœ¬åŽ»ç«žäº‰æ–°çš„ leaderï¼ŒèŽ·å– leader ç»§è€Œæ‰§è¡Œä¸šåŠ¡é€»è¾‘ã€‚ kubernetes ç‰ˆæœ¬ï¼š v1.12 ç»„ä»¶é«˜å¯ç”¨çš„ä½¿ç”¨k8s ä¸­å·²ç»ä¸º kube-controller-managerã€kube-scheduler ç»„ä»¶å®žçŽ°äº†é«˜å¯ç”¨ï¼Œåªéœ€åœ¨æ¯ä¸ªç»„ä»¶çš„é…ç½®æ–‡ä»¶ä¸­æ·»åŠ  --leader-elect=true å‚æ•°å³å¯å¯ç”¨ã€‚åœ¨æ¯ä¸ªç»„ä»¶çš„æ—¥å¿—ä¸­å¯ä»¥çœ‹åˆ° HA ç›¸å…³å‚æ•°çš„é»˜è®¤å€¼ï¼š 12345I0306 19:17:14.109511 161798 flags.go:33] FLAG: --leader-elect=&quot;true&quot;I0306 19:17:14.109513 161798 flags.go:33] FLAG: --leader-elect-lease-duration=&quot;15s&quot;I0306 19:17:14.109516 161798 flags.go:33] FLAG: --leader-elect-renew-deadline=&quot;10s&quot;I0306 19:17:14.109518 161798 flags.go:33] FLAG: --leader-elect-resource-lock=&quot;endpoints&quot;I0306 19:17:14.109520 161798 flags.go:33] FLAG: --leader-elect-retry-period=&quot;2s&quot; kubernetes ä¸­æŸ¥çœ‹ç»„ä»¶ leader çš„æ–¹æ³•ï¼š 12$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml &amp;&amp; kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml å½“å‰ç»„ä»¶ leader çš„ hostname ä¼šå†™åœ¨ annotation çš„ control-plane.alpha.kubernetes.io/leader å­—æ®µé‡Œã€‚ Leader Election çš„å®žçŽ°Leader Election çš„è¿‡ç¨‹æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªç«žäº‰åˆ†å¸ƒå¼é”çš„è¿‡ç¨‹ã€‚åœ¨ Kubernetes ä¸­ï¼Œè¿™ä¸ªåˆ†å¸ƒå¼é”æ˜¯ä»¥åˆ›å»º Endpoint èµ„æºçš„å½¢å¼è¿›è¡Œï¼Œè°å…ˆåˆ›å»ºäº†è¯¥èµ„æºï¼Œè°å°±å…ˆèŽ·å¾—é”ï¼Œä¹‹åŽä¼šå¯¹è¯¥èµ„æºä¸æ–­æ›´æ–°ä»¥ä¿æŒé”çš„æ‹¥æœ‰æƒã€‚ ä¸‹é¢å¼€å§‹è®²è¿° kube-controller-manager ä¸­ leader çš„ç«žäº‰è¿‡ç¨‹ï¼Œcm åœ¨åŠ è½½åŠé…ç½®å®Œå‚æ•°åŽå°±å¼€å§‹æ‰§è¡Œ run æ–¹æ³•äº†ã€‚ä»£ç åœ¨ k8s.io/kubernetes/cmd/kube-controller-manager/app/controllermanager.go ä¸­ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586// Run runs the KubeControllerManagerOptions. This should never exit.func Run(c *config.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;) error &#123; ... // kube-controller-manager çš„æ ¸å¿ƒ run := func(ctx context.Context) &#123; rootClientBuilder := controller.SimpleControllerClientBuilder&#123; ClientConfig: c.Kubeconfig, &#125; var clientBuilder controller.ControllerClientBuilder if c.ComponentConfig.KubeCloudShared.UseServiceAccountCredentials &#123; if len(c.ComponentConfig.SAController.ServiceAccountKeyFile) == 0 &#123; // It&apos;c possible another controller process is creating the tokens for us. // If one isn&apos;t, we&apos;ll timeout and exit when our client builder is unable to create the tokens. glog.Warningf(&quot;--use-service-account-credentials was specified without providing a --service-account-private-key-file&quot;) &#125; clientBuilder = controller.SAControllerClientBuilder&#123; ClientConfig: restclient.AnonymousClientConfig(c.Kubeconfig), CoreClient: c.Client.CoreV1(), AuthenticationClient: c.Client.AuthenticationV1(), Namespace: &quot;kube-system&quot;, &#125; &#125; else &#123; clientBuilder = rootClientBuilder &#125; controllerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done()) if err != nil &#123; glog.Fatalf(&quot;error building controller context: %v&quot;, err) &#125; saTokenControllerInitFunc := serviceAccountTokenControllerStarter&#123;rootClientBuilder: rootClientBuilder&#125;.startServiceAccountTokenController // åˆå§‹åŒ–åŠå¯åŠ¨æ‰€æœ‰çš„ controller if err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux); err != nil &#123; glog.Fatalf(&quot;error starting controllers: %v&quot;, err) &#125; controllerContext.InformerFactory.Start(controllerContext.Stop) close(controllerContext.InformersStarted) select &#123;&#125; &#125; // å¦‚æžœ LeaderElect å‚æ•°æœªé…ç½®,è¯´æ˜Ž controller-manager æ˜¯å•ç‚¹å¯åŠ¨çš„ï¼Œ // åˆ™ç›´æŽ¥è°ƒç”¨ run æ–¹æ³•æ¥å¯åŠ¨éœ€è¦è¢«å¯åŠ¨çš„æŽ§åˆ¶å™¨å³å¯ã€‚ if !c.ComponentConfig.Generic.LeaderElection.LeaderElect &#123; run(context.TODO()) panic(&quot;unreachable&quot;) &#125; // å¦‚æžœ LeaderElect å‚æ•°é…ç½®ä¸º true,è¯´æ˜Ž controller-manager æ˜¯ä»¥ HA æ–¹å¼å¯åŠ¨çš„ï¼Œ // åˆ™æ‰§è¡Œä¸‹é¢çš„ä»£ç è¿›è¡Œ leader é€‰ä¸¾ï¼Œé€‰ä¸¾å‡ºçš„ leader ä¼šå›žè°ƒ run æ–¹æ³•ã€‚ id, err := os.Hostname() if err != nil &#123; return err &#125; // add a uniquifier so that two processes on the same host don&apos;t accidentally both become active id = id + &quot;_&quot; + string(uuid.NewUUID()) // åˆå§‹åŒ–èµ„æºé” rl, err := resourcelock.New(c.ComponentConfig.Generic.LeaderElection.ResourceLock, &quot;kube-system&quot;, &quot;kube-controller-manager&quot;, c.LeaderElectionClient.CoreV1(), resourcelock.ResourceLockConfig&#123; Identity: id, EventRecorder: c.EventRecorder, &#125;) if err != nil &#123; glog.Fatalf(&quot;error creating lock: %v&quot;, err) &#125; // è¿›å…¥åˆ°é€‰ä¸¾çš„æµç¨‹ leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig&#123; Lock: rl, LeaseDuration: c.ComponentConfig.Generic.LeaderElection.LeaseDuration.Duration, RenewDeadline: c.ComponentConfig.Generic.LeaderElection.RenewDeadline.Duration, RetryPeriod: c.ComponentConfig.Generic.LeaderElection.RetryPeriod.Duration, Callbacks: leaderelection.LeaderCallbacks&#123; OnStartedLeading: run, OnStoppedLeading: func() &#123; glog.Fatalf(&quot;leaderelection lost&quot;) &#125;, &#125;, WatchDog: electionChecker, Name: &quot;kube-controller-manager&quot;, &#125;) panic(&quot;unreachable&quot;)&#125; 1ã€åˆå§‹åŒ–èµ„æºé”ï¼Œkubernetes ä¸­é»˜è®¤çš„èµ„æºé”ä½¿ç”¨ endpointsï¼Œä¹Ÿå°±æ˜¯ c.ComponentConfig.Generic.LeaderElection.ResourceLock çš„å€¼ä¸º â€œendpointsâ€ï¼Œåœ¨ä»£ç ä¸­æˆ‘å¹¶æ²¡æœ‰æ‰¾åˆ°å¯¹ ResourceLock åˆå§‹åŒ–çš„åœ°æ–¹ï¼Œåªçœ‹åˆ°äº†å¯¹è¯¥å‚æ•°çš„è¯´æ˜Žä»¥åŠæ—¥å¿—ä¸­é…ç½®çš„é»˜è®¤å€¼ï¼š â€‹åœ¨åˆå§‹åŒ–èµ„æºé”çš„æ—¶å€™è¿˜ä¼ å…¥äº† EventRecorderï¼Œå…¶ä½œç”¨æ˜¯å½“ leader å‘ç”Ÿå˜åŒ–çš„æ—¶å€™ä¼šå°†å¯¹åº”çš„ events å‘é€åˆ° apiserverã€‚ 2ã€rl èµ„æºé”è¢«ç”¨äºŽ controller-manager è¿›è¡Œ leader çš„é€‰ä¸¾ï¼ŒRunOrDie æ–¹æ³•ä¸­å°±æ˜¯ leader çš„é€‰ä¸¾è¿‡ç¨‹äº†ã€‚ 3ã€Callbacks ä¸­å®šä¹‰äº†åœ¨åˆ‡æ¢çŠ¶æ€åŽéœ€è¦æ‰§è¡Œçš„æ“ä½œï¼Œå½“æˆä¸º leader åŽä¼šæ‰§è¡Œ OnStartedLeading ä¸­çš„ run æ–¹æ³•ï¼Œrun æ–¹æ³•æ˜¯ controller-manager çš„æ ¸å¿ƒï¼Œrun æ–¹æ³•ä¸­ä¼šåˆå§‹åŒ–å¹¶å¯åŠ¨æ‰€åŒ…å«èµ„æºçš„ controllerï¼Œä»¥ä¸‹æ˜¯ kube-controller-manager ä¸­æ‰€æœ‰çš„ controllerï¼š 123456789101112131415161718192021222324252627282930313233343536373839func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc &#123; controllers := map[string]InitFunc&#123;&#125; controllers[&quot;endpoint&quot;] = startEndpointController controllers[&quot;replicationcontroller&quot;] = startReplicationController controllers[&quot;podgc&quot;] = startPodGCController controllers[&quot;resourcequota&quot;] = startResourceQuotaController controllers[&quot;namespace&quot;] = startNamespaceController controllers[&quot;serviceaccount&quot;] = startServiceAccountController controllers[&quot;garbagecollector&quot;] = startGarbageCollectorController controllers[&quot;daemonset&quot;] = startDaemonSetController controllers[&quot;job&quot;] = startJobController controllers[&quot;deployment&quot;] = startDeploymentController controllers[&quot;replicaset&quot;] = startReplicaSetController controllers[&quot;horizontalpodautoscaling&quot;] = startHPAController controllers[&quot;disruption&quot;] = startDisruptionController controllers[&quot;statefulset&quot;] = startStatefulSetController controllers[&quot;cronjob&quot;] = startCronJobController controllers[&quot;csrsigning&quot;] = startCSRSigningController controllers[&quot;csrapproving&quot;] = startCSRApprovingController controllers[&quot;csrcleaner&quot;] = startCSRCleanerController controllers[&quot;ttl&quot;] = startTTLController controllers[&quot;bootstrapsigner&quot;] = startBootstrapSignerController controllers[&quot;tokencleaner&quot;] = startTokenCleanerController controllers[&quot;nodeipam&quot;] = startNodeIpamController if loopMode == IncludeCloudLoops &#123; controllers[&quot;service&quot;] = startServiceController controllers[&quot;route&quot;] = startRouteController &#125; controllers[&quot;nodelifecycle&quot;] = startNodeLifecycleController controllers[&quot;persistentvolume-binder&quot;] = startPersistentVolumeBinderController controllers[&quot;attachdetach&quot;] = startAttachDetachController controllers[&quot;persistentvolume-expander&quot;] = startVolumeExpandController controllers[&quot;clusterrole-aggregation&quot;] = startClusterRoleAggregrationController controllers[&quot;pvc-protection&quot;] = startPVCProtectionController controllers[&quot;pv-protection&quot;] = startPVProtectionController controllers[&quot;ttl-after-finished&quot;] = startTTLAfterFinishedController return controllers&#125; OnStoppedLeading æ˜¯ä»Ž leader çŠ¶æ€åˆ‡æ¢ä¸º slave è¦æ‰§è¡Œçš„æ“ä½œï¼Œæ­¤æ–¹æ³•ä»…æ‰“å°äº†ä¸€æ¡æ—¥å¿—ã€‚ 12345678910func RunOrDie(ctx context.Context, lec LeaderElectionConfig) &#123; le, err := NewLeaderElector(lec) if err != nil &#123; panic(err) &#125; if lec.WatchDog != nil &#123; lec.WatchDog.SetLeaderElection(le) &#125; le.Run(ctx)&#125; åœ¨ RunOrDie ä¸­é¦–å…ˆè°ƒç”¨ NewLeaderElector åˆå§‹åŒ–äº†ä¸€ä¸ª LeaderElector å¯¹è±¡ï¼Œç„¶åŽæ‰§è¡Œ LeaderElector çš„ run æ–¹æ³•è¿›è¡Œé€‰ä¸¾ã€‚ 12345678910111213func (le *LeaderElector) Run(ctx context.Context) &#123; defer func() &#123; runtime.HandleCrash() le.config.Callbacks.OnStoppedLeading() &#125;() if !le.acquire(ctx) &#123; return // ctx signalled done &#125; ctx, cancel := context.WithCancel(ctx) defer cancel() go le.config.Callbacks.OnStartedLeading(ctx) le.renew(ctx)&#125; Run ä¸­é¦–å…ˆä¼šæ‰§è¡Œ acquire å°è¯•èŽ·å–é”ï¼ŒèŽ·å–åˆ°é”ä¹‹åŽä¼šå›žè°ƒ OnStartedLeading å¯åŠ¨æ‰€éœ€è¦çš„ controllerï¼Œç„¶åŽä¼šæ‰§è¡Œ renew æ–¹æ³•å®šæœŸæ›´æ–°é”ï¼Œä¿æŒ leader çš„çŠ¶æ€ã€‚ 12345678910111213141516171819202122func (le *LeaderElector) acquire(ctx context.Context) bool &#123; ctx, cancel := context.WithCancel(ctx) defer cancel() succeeded := false desc := le.config.Lock.Describe() glog.Infof(&quot;attempting to acquire leader lease %v...&quot;, desc) wait.JitterUntil(func() &#123; // å°è¯•åˆ›å»ºæˆ–è€…ç»­çº¦èµ„æºé” succeeded = le.tryAcquireOrRenew() // leader å¯èƒ½å‘ç”Ÿäº†æ”¹å˜ï¼Œåœ¨ maybeReportTransition æ–¹æ³•ä¸­ä¼š // æ‰§è¡Œç›¸åº”çš„ OnNewLeader() å›žè°ƒå‡½æ•°,ä»£ç ä¸­å¯¹ OnNewLeader() å¹¶æ²¡æœ‰åˆå§‹åŒ– le.maybeReportTransition() if !succeeded &#123; glog.V(4).Infof(&quot;failed to acquire lease %v&quot;, desc) return &#125; le.config.Lock.RecordEvent(&quot;became leader&quot;) glog.Infof(&quot;successfully acquired lease %v&quot;, desc) cancel() &#125;, le.config.RetryPeriod, JitterFactor, true, ctx.Done()) return succeeded&#125; åœ¨ acquire ä¸­é¦–å…ˆåˆå§‹åŒ–äº†ä¸€ä¸ª ctxï¼Œé€šè¿‡ wait.JitterUntil å‘¨æœŸæ€§çš„åŽ»è°ƒç”¨ le.tryAcquireOrRenew æ–¹æ³•æ¥èŽ·å–èµ„æºé”ï¼Œç›´åˆ°èŽ·å–ä¸ºæ­¢ã€‚å¦‚æžœèŽ·å–ä¸åˆ°é”ï¼Œåˆ™ä¼šä»¥ RetryPeriod ä¸ºé—´éš”ä¸æ–­å°è¯•ã€‚å¦‚æžœèŽ·å–åˆ°é”ï¼Œå°±ä¼šå…³é—­ ctx é€šçŸ¥ wait.JitterUntil åœæ­¢å°è¯•ï¼ŒtryAcquireOrRenew æ˜¯æœ€æ ¸å¿ƒçš„æ–¹æ³•ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758func (le *LeaderElector) tryAcquireOrRenew() bool &#123; now := metav1.Now() leaderElectionRecord := rl.LeaderElectionRecord&#123; HolderIdentity: le.config.Lock.Identity(), LeaseDurationSeconds: int(le.config.LeaseDuration / time.Second), RenewTime: now, AcquireTime: now, &#125; // 1ã€èŽ·å–å½“å‰çš„èµ„æºé” oldLeaderElectionRecord, err := le.config.Lock.Get() if err != nil &#123; if !errors.IsNotFound(err) &#123; glog.Errorf(&quot;error retrieving resource lock %v: %v&quot;, le.config.Lock.Describe(), err) return false &#125; // æ²¡æœ‰èŽ·å–åˆ°èµ„æºé”ï¼Œå¼€å§‹åˆ›å»ºèµ„æºé”ï¼Œè‹¥åˆ›å»ºæˆåŠŸåˆ™æˆä¸º leader if err = le.config.Lock.Create(leaderElectionRecord); err != nil &#123; glog.Errorf(&quot;error initially creating leader election record: %v&quot;, err) return false &#125; le.observedRecord = leaderElectionRecord le.observedTime = le.clock.Now() return true &#125; // 2ã€èŽ·å–èµ„æºé”åŽæ£€æŸ¥å½“å‰ id æ˜¯ä¸æ˜¯ leader if !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) &#123; le.observedRecord = *oldLeaderElectionRecord le.observedTime = le.clock.Now() &#125; // å¦‚æžœèµ„æºé”æ²¡æœ‰è¿‡æœŸä¸”å½“å‰ id ä¸æ˜¯ Leaderï¼Œç›´æŽ¥è¿”å›ž if le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;&amp; !le.IsLeader() &#123; glog.V(4).Infof(&quot;lock is held by %v and has not yet expired&quot;, oldLeaderElectionRecord.HolderIdentity) return false &#125; // 3ã€å¦‚æžœå½“å‰ id æ˜¯ Leaderï¼Œå°†å¯¹åº”å­—æ®µçš„æ—¶é—´æ”¹æˆå½“å‰æ—¶é—´ï¼Œå‡†å¤‡ç»­ç§Ÿ // å¦‚æžœæ˜¯éž Leader èŠ‚ç‚¹åˆ™æŠ¢å¤ºèµ„æºé” if le.IsLeader() &#123; leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions &#125; else &#123; leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1 &#125; // æ›´æ–°èµ„æº // å¯¹äºŽ Leader æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªç»­ç§Ÿçš„è¿‡ç¨‹ // å¯¹äºŽéž Leader èŠ‚ç‚¹ï¼ˆä»…åœ¨ä¸Šä¸€ä¸ªèµ„æºé”å·²ç»è¿‡æœŸï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´æ–°é”æ‰€æœ‰æƒçš„è¿‡ç¨‹ if err = le.config.Lock.Update(leaderElectionRecord); err != nil &#123; glog.Errorf(&quot;Failed to update lock: %v&quot;, err) return false &#125; le.observedRecord = leaderElectionRecord le.observedTime = le.clock.Now() return true&#125; ä¸Šé¢çš„è¿™ä¸ªå‡½æ•°çš„ä¸»è¦é€»è¾‘ï¼š 1ã€èŽ·å– ElectionRecord è®°å½•ï¼Œå¦‚æžœæ²¡æœ‰åˆ™åˆ›å»ºä¸€æ¡æ–°çš„ ElectionRecord è®°å½•ï¼Œåˆ›å»ºæˆåŠŸåˆ™è¡¨ç¤ºèŽ·å–åˆ°é”å¹¶æˆä¸º leader äº†ã€‚ 2ã€å½“èŽ·å–åˆ°èµ„æºé”åŽå¼€å§‹æ£€æŸ¥å…¶ä¸­çš„ä¿¡æ¯ï¼Œæ¯”è¾ƒå½“å‰ id æ˜¯ä¸æ˜¯ leader ä»¥åŠèµ„æºé”æœ‰æ²¡æœ‰è¿‡æœŸï¼Œå¦‚æžœèµ„æºé”æ²¡æœ‰è¿‡æœŸä¸”å½“å‰ id ä¸æ˜¯ Leaderï¼Œåˆ™ç›´æŽ¥è¿”å›žã€‚ 3ã€å¦‚æžœå½“å‰ id æ˜¯ Leaderï¼Œå°†å¯¹åº”å­—æ®µçš„æ—¶é—´æ”¹æˆå½“å‰æ—¶é—´ï¼Œæ›´æ–°èµ„æºé”è¿›è¡Œç»­ç§Ÿã€‚ 4ã€å¦‚æžœå½“å‰ id ä¸æ˜¯ Leader ä½†æ˜¯èµ„æºé”å·²ç»è¿‡æœŸäº†ï¼Œåˆ™æŠ¢å¤ºèµ„æºé”ï¼ŒæŠ¢å¤ºæˆåŠŸåˆ™æˆä¸º leader å¦åˆ™è¿”å›žã€‚ æœ€åŽæ˜¯ renew æ–¹æ³•ï¼š 1234567891011121314151617181920212223242526272829303132333435func (le *LeaderElector) renew(ctx context.Context) &#123; ctx, cancel := context.WithCancel(ctx) defer cancel() wait.Until(func() &#123; timeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline) defer timeoutCancel() // æ¯é—´éš” RetryPeriod å°±æ‰§è¡Œ tryAcquireOrRenew() // å¦‚æžœ tryAcquireOrRenew() è¿”å›ž false è¯´æ˜Žç»­ç§Ÿå¤±è´¥ err := wait.PollImmediateUntil(le.config.RetryPeriod, func() (bool, error) &#123; done := make(chan bool, 1) go func() &#123; defer close(done) done &lt;- le.tryAcquireOrRenew() &#125;() select &#123; case &lt;-timeoutCtx.Done(): return false, fmt.Errorf(&quot;failed to tryAcquireOrRenew %s&quot;, timeoutCtx.Err()) case result := &lt;-done: return result, nil &#125; &#125;, timeoutCtx.Done()) le.maybeReportTransition() desc := le.config.Lock.Describe() if err == nil &#123; glog.V(4).Infof(&quot;successfully renewed lease %v&quot;, desc) return &#125; // ç»­ç§Ÿå¤±è´¥ï¼Œè¯´æ˜Žå·²ç»ä¸æ˜¯ Leaderï¼Œç„¶åŽç¨‹åº panic le.config.Lock.RecordEvent(&quot;stopped leading&quot;) glog.Infof(&quot;failed to renew lease %v: %v&quot;, desc, err) cancel() &#125;, le.config.RetryPeriod, ctx.Done())&#125; èŽ·å–åˆ°é”ä¹‹åŽå®šæœŸè¿›è¡Œæ›´æ–°ï¼Œrenew åªæœ‰åœ¨èŽ·å–é”ä¹‹åŽæ‰ä¼šè°ƒç”¨ï¼Œå®ƒä¼šé€šè¿‡æŒç»­æ›´æ–°èµ„æºé”çš„æ•°æ®ï¼Œæ¥ç¡®ä¿ç»§ç»­æŒæœ‰å·²èŽ·å¾—çš„é”ï¼Œä¿æŒè‡ªå·±çš„ leader çŠ¶æ€ã€‚ Leader Election åŠŸèƒ½çš„ä½¿ç”¨ä»¥ä¸‹æ˜¯ä¸€ä¸ª demoï¼Œä½¿ç”¨ k8s ä¸­ k8s.io/client-go/tools/leaderelection è¿›è¡Œä¸€ä¸ªæ¼”ç¤ºï¼š 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package mainimport ( &quot;context&quot; &quot;flag&quot; &quot;fmt&quot; &quot;os&quot; &quot;time&quot; &quot;github.com/golang/glog&quot; &quot;k8s.io/api/core/v1&quot; &quot;k8s.io/client-go/kubernetes&quot; &quot;k8s.io/client-go/kubernetes/scheme&quot; v1core &quot;k8s.io/client-go/kubernetes/typed/core/v1&quot; &quot;k8s.io/client-go/tools/clientcmd&quot; &quot;k8s.io/client-go/tools/leaderelection&quot; &quot;k8s.io/client-go/tools/leaderelection/resourcelock&quot; &quot;k8s.io/client-go/tools/record&quot;)var ( masterURL string kubeconfig string)func init() &#123; flag.StringVar(&amp;kubeconfig, &quot;kubeconfig&quot;, &quot;&quot;, &quot;Path to a kubeconfig. Only required if out-of-cluster.&quot;) flag.StringVar(&amp;masterURL, &quot;master&quot;, &quot;&quot;, &quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.&quot;) flag.Set(&quot;logtostderr&quot;, &quot;true&quot;)&#125;func main() &#123; flag.Parse() defer glog.Flush() id, err := os.Hostname() if err != nil &#123; panic(err) &#125; // åŠ è½½ kubeconfig é…ç½® cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) if err != nil &#123; glog.Fatalf(&quot;Error building kubeconfig: %s&quot;, err.Error()) &#125; // åˆ›å»º kubeclient kubeClient, err := kubernetes.NewForConfig(cfg) if err != nil &#123; glog.Fatalf(&quot;Error building kubernetes clientset: %s&quot;, err.Error()) &#125; // åˆå§‹åŒ– eventRecorder eventBroadcaster := record.NewBroadcaster() eventRecorder := eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: &quot;test-1&quot;&#125;) eventBroadcaster.StartLogging(glog.Infof) eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeClient.CoreV1().Events(&quot;&quot;)&#125;) run := func(ctx context.Context) &#123; fmt.Println(&quot;run.........&quot;) select &#123;&#125; &#125; id = id + &quot;_&quot; + &quot;1&quot; rl, err := resourcelock.New(&quot;endpoints&quot;, &quot;kube-system&quot;, &quot;test&quot;, kubeClient.CoreV1(), resourcelock.ResourceLockConfig&#123; Identity: id, EventRecorder: eventRecorder, &#125;) if err != nil &#123; glog.Fatalf(&quot;error creating lock: %v&quot;, err) &#125; leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig&#123; Lock: rl, LeaseDuration: 15 * time.Second, RenewDeadline: 10 * time.Second, RetryPeriod: 2 * time.Second, Callbacks: leaderelection.LeaderCallbacks&#123; OnStartedLeading: run, OnStoppedLeading: func() &#123; glog.Info(&quot;leaderelection lost&quot;) &#125;, &#125;, Name: &quot;test-1&quot;, &#125;)&#125; åˆ†åˆ«ä½¿ç”¨å¤šä¸ª hostname åŒæ—¶è¿è¡ŒåŽå¹¶æµ‹è¯• leader åˆ‡æ¢ï¼Œå¯ä»¥åœ¨ events ä¸­çœ‹åˆ° leader åˆ‡æ¢çš„è®°å½•ï¼š 1234567891011# kubectl describe endpoints test -n kube-systemName: testNamespace: kube-systemLabels: &lt;none&gt;Annotations: control-plane.alpha.kubernetes.io/leader=&#123;&quot;holderIdentity&quot;:&quot;localhost_2&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2019-03-10T08:47:42Z&quot;,&quot;renewTime&quot;:&quot;2019-03-10T08:47:44Z&quot;,&quot;leaderTransitions&quot;:2&#125;Subsets:Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal LeaderElection 50s test-1 localhost_1 became leader Normal LeaderElection 5s test-2 localhost_2 became leader æ€»ç»“æœ¬æ–‡è®²è¿°äº† kube-controller-manager ä½¿ç”¨ HA çš„æ–¹å¼å¯åŠ¨åŽ leader é€‰ä¸¾è¿‡ç¨‹çš„å®žçŽ°è¯´æ˜Žï¼Œk8s ä¸­é€šè¿‡åˆ›å»º endpoints èµ„æºä»¥åŠå¯¹è¯¥èµ„æºçš„æŒç»­æ›´æ–°æ¥å®žçŽ°èµ„æºé”è½®è½¬çš„è¿‡ç¨‹ã€‚ä½†æ˜¯ç›¸å¯¹äºŽå…¶ä»–åˆ†å¸ƒå¼é”çš„å®žçŽ°ï¼Œæ™®éæ˜¯ç›´æŽ¥åŸºäºŽçŽ°æœ‰çš„ä¸­é—´ä»¶å®žçŽ°ï¼Œæ¯”å¦‚ redisã€zookeeperã€etcd ç­‰ï¼Œå…¶æ‰€æœ‰å¯¹é”çš„æ“ä½œéƒ½æ˜¯åŽŸå­æ€§çš„ï¼Œé‚£ k8s é€‰ä¸¾è¿‡ç¨‹ä¸­çš„åŽŸå­æ“ä½œæ˜¯å¦‚ä½•å®žçŽ°çš„ï¼Ÿk8s ä¸­çš„åŽŸå­æ“ä½œæœ€ç»ˆä¹Ÿæ˜¯é€šè¿‡ etcd å®žçŽ°çš„ï¼Œå…¶åœ¨åš update æ›´æ–°é”çš„æ“ä½œæ—¶é‡‡ç”¨çš„æ˜¯ä¹è§‚é”ï¼Œé€šè¿‡å¯¹æ¯” resourceVersion å®žçŽ°çš„ï¼Œè¯¦ç»†çš„å®žçŽ°ä¸‹èŠ‚å†è®²ã€‚ å‚è€ƒæ–‡æ¡£ï¼šAPI OVERVIEWSimple leader election with Kubernetes and Docker]]></content>
      <tags>
        <tag>leader-election</tag>
        <tag>component</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes é›†ç¾¤å‡çº§è‡³ v1.12 éœ€è¦æ³¨æ„çš„å‡ ä¸ªé—®é¢˜]]></title>
    <url>%2F2019%2F03%2F05%2Fk8s_v1.12%2F</url>
    <content type="text"><![CDATA[æœ€è¿‘æˆ‘ä»¬ç”Ÿäº§çŽ¯å¢ƒçš„é›†ç¾¤å¼€å§‹å‡çº§è‡³ v1.12 ç‰ˆæœ¬äº†ï¼Œä¹‹å‰çš„ç‰ˆæœ¬æ˜¯ v1.8ï¼Œç”±äºŽè·¨äº†å¤šä¸ªç‰ˆæœ¬ï¼Œé£Žé™©è¿˜æ˜¯æ¯”è¾ƒå¤§çš„ï¼Œå®˜æ–¹çš„å»ºè®®ä¹Ÿæ˜¯ä¸€ä¸ªä¸€ä¸ªç‰ˆæœ¬å‡çº§ï¼Œk8s æ¯ä¸‰ä¸ªæœˆå‡ºä¸€ä¸ªç‰ˆæœ¬ï¼Œé›†ç¾¤ä¸Šäº†è§„æ¨¡åŽå‡çº§å¤ªéº»çƒ¦ï¼Œé‰´äºŽæˆ‘ä»¬çœŸæ­£ä½¿ç”¨ k8s ä¸­çš„åŠŸèƒ½è¿˜æ˜¯æ¯”è¾ƒå°‘çš„ï¼Œè€¦åˆæ€§æ²¡æœ‰é‚£ä¹ˆå¤§ï¼Œæ‰€ä»¥é£Žé™©è¿˜æ˜¯ç›¸å¯¹å¯æŽ§ï¼Œæµ‹è¯•çŽ¯å¢ƒè¿è¡Œ v1.12 ä¸€æ®µæ—¶é—´åŽå‘çŽ°é—®é¢˜ä¸å¤§ï¼ŒäºŽæ˜¯å¼€å§‹å‡çº§ã€‚æ­¤å¤„è®°å½•å‡ ä¸ªå‡çº§è¿‡ç¨‹è¦æ³¨æ„çš„é—®é¢˜ï¼š 1ã€æ³¨æ„ k8s ä¸­ resource version çš„å˜åŒ–k8s ä¸­è®¸å¤š resouce éƒ½æ˜¯éšç€ k8s çš„ç‰ˆæœ¬å˜åŒ–è€Œå˜åŒ–çš„ï¼Œä¾‹å¦‚ï¼Œstatefulset åœ¨ v1.8 ç‰ˆæœ¬ä¸­ apiVersion æ˜¯ apps/v1beta1ï¼Œåœ¨ v1.12 ä¸­å˜ä¸ºäº† apps/v1ã€‚k8s æœ‰æŽ¥å£å¯ä»¥èŽ·å–åˆ°å½“å‰ç‰ˆæœ¬æ‰€æœ‰çš„ OpenAPI ï¼š å‚è€ƒæ–‡æ¡£ï¼šThe Kubernetes API è™½ç„¶ k8s ä¸­ resource version éƒ½æ˜¯å‘ä¸‹å…¼å®¹çš„ï¼Œä½†æ˜¯åœ¨å‡çº§å®ŒæˆåŽå°½é‡ä½¿ç”¨å½“å‰ç‰ˆæœ¬çš„ resource version é¿å…ä¸å¿…è¦çš„éº»çƒ¦ã€‚ 2ã€kubelet é…ç½®æ–‡ä»¶æ ¼å¼v1.8 ä¸­ kubelet çš„é…ç½®æ˜¯åœ¨ /etc/kubernetes/kubelet æ–‡ä»¶ä¸­çš„ KUBELET_ARGS åŽé¢æŒ‡å®šï¼Œä½†æ˜¯åœ¨ v1.12 ä¸­å¼€å§‹ä½¿ç”¨ config.yaml æ–‡ä»¶ï¼Œå³æ‰€æœ‰çš„é…ç½®éƒ½å¯ä»¥æ”¾åœ¨ yaml æ–‡ä»¶ä¸­ï¼Œç”±äºŽé…ç½®æ˜¯å…¼å®¹çš„ï¼Œæ‰€ä»¥æš‚æ—¶ä¹Ÿå¯ä»¥ç»§ç»­ç”¨ä»¥å‰çš„æ–¹å¼ï¼Œå…¶ä¸­æœ‰äº›å‚æ•°ä»…æ”¯æŒåœ¨ config.yaml æ–‡ä»¶ä¸­æŒ‡å®šã€‚ config.yaml æ–‡ä»¶çš„å®˜æ–¹è¯´æ˜Žï¼šSet Kubelet parameters via a config file ä¸€ä¸ªä¾‹å­ï¼š 12345678910111213141516171819202122232425262728apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0- podseventBurst: 10eventRecordQPS: 5evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0sfailSwapOn: truefileCheckFrequency: 20shealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0smaxOpenFiles: 1000000maxPods: 110nodeLeaseDurationSeconds: 40nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1port: 10250staticPodPath: /etc/kubernetes/manifests å°† kubelet é…ç½®æ–‡ä»¶ä¸­çš„ LOG_LEVEL å‚æ•°æ”¹ä¸ºå¤§äºŽç­‰äºŽ 5 å¯ä»¥çœ‹åˆ° config.yaml ä¸­é…ç½®çš„å®šä¹‰ï¼Œä»¥æ–¹ä¾¿æŽ’æŸ¥é—®é¢˜ï¼š å¯¹åº”çš„æ—¥å¿—è¾“å‡ºï¼š 123456789I0228 16:14:14.064292 191819 server.go:260] KubeletConfiguration: config.KubeletConfiguration&#123;TypeMeta:v1.TypeMeta&#123;Kind:&quot;&quot;, APIVersion:&quot;&quot;&#125;, StaticPodPath:&quot;&quot;, Sync Frequency:v1.Duration&#123;Duration:60000000000&#125;, FileCheckFrequency:v1.Duration&#123;Duration:20000000000&#125;,HTTPCheckFrequency:v1.Duration&#123;Duration:20000000000&#125;, StaticPodURL:&quot;&quot;, StaticPodURLHeader:map[string][]string(nil),Address:&quot;0.0.0.0&quot;, Port:10250, ... æ³¨æ„ï¼škubelet é…ç½®æ–‡ä»¶ä¸­ ARGS ä¸­å®šä¹‰çš„å‚æ•°ä¼šè¦†ç›– config.yaml ä¸­çš„å®šä¹‰ã€‚ 3ã€feature-gates ä¸­åŠŸèƒ½çš„ä½¿ç”¨v1.12 ä¸­ feature-gates ä¸­è®¸å¤šåŠŸèƒ½é»˜è®¤ä¸ºå¼€å¯çŠ¶æ€ï¼Œéœ€è¦æ ¹æ®å®žé™…åœºæ™¯é€‰æ‹©ï¼Œä¸å¿…è¦çš„åŠŸèƒ½åœ¨é…ç½®æ–‡ä»¶ä¸­å°†å…¶å…³é—­ã€‚ k8s å„ç‰ˆæœ¬ä¸­çš„ Feature åˆ—è¡¨ä»¥åŠæ˜¯å¦å¯ç”¨çŠ¶æ€å¯ä»¥åœ¨ Feature Gates ä¸­æŸ¥çœ‹ã€‚ 4ã€cadvisor çš„ä½¿ç”¨k8s åœ¨ 1.12 ä¸­å°† cadvisor ä»Ž kubelet ä¸­ç§»é™¤äº†ï¼Œè‹¥è¦ä½¿ç”¨ cadvisorï¼Œå®˜æ–¹å»ºè®®ä½¿ç”¨ DaemonSet è¿›è¡Œéƒ¨ç½²ã€‚ç”±äºŽæˆ‘ä»¬ä¸€ç›´ä»Ž cadvisor èŽ·å–å®¹å™¨çš„ç›‘æŽ§æ•°æ®ç„¶åŽæŽ¨é€åˆ°è‡ªæœ‰çš„ç›‘æŽ§ç³»ç»Ÿä¸­è¿›è¡Œå±•ç¤ºï¼Œæ‰€ä»¥ cadvisor è¿˜å¾—ç»§ç»­ä½¿ç”¨ã€‚ è¿™æ˜¯å®˜æ–¹æŽ¨èçš„ cadvisor éƒ¨ç½²æ–¹æ³•ï¼ŒcAdvisor Kubernetes Daemonsetï¼Œå…¶ä¸­ç”¨äº† k8s.gcr.io/cadvisor:v0.30.2 é•œåƒï¼Œåœ¨æˆ‘ä»¬çš„æµ‹è¯•çŽ¯å¢ƒä¸­ï¼Œè¯¥é•œåƒæ— æ³•å¯åŠ¨ï¼ŒæŠ¥é”™ /sys/fs/cgroup/cpuacct,cpu: no such file or directory, ç»æŸ¥ cadvisor v0.30.2 ç‰ˆæœ¬çš„é•œåƒä½¿ç”¨ cgroup v2ï¼Œv2 ç‰ˆæœ¬ä¸­å·²ç»æ²¡æœ‰äº† cpuacct subsystemï¼Œè€Œ linux kernel 4.5 ä»¥ä¸Šçš„ç‰ˆæœ¬æ‰æ”¯æŒ cgroup v2ï¼Œä¸Žæˆ‘ä»¬çš„å®žé™…åœºæ™¯ä¸å¤ªç›¸ç¬¦ï¼Œæœ€åŽæµ‹è¯•å‘çŽ° v0.28.0 çš„é•œåƒå¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚ ç”±äºŽè¦å…¼å®¹ä¹‹å‰çš„ä½¿ç”¨æ–¹å¼ï¼Œcadvisor åœ¨å®¿ä¸»æœºä¸Šéœ€è¦å¯åŠ¨ 4194 ç«¯å£ï¼Œä½†æ˜¯åˆ›å»ºå®¹å™¨åˆè¦ç»“åˆè‡ªèº«çš„ç½‘ç»œæ–¹æ¡ˆï¼Œæœ€ç»ˆæˆ‘ä»¬ä½¿ç”¨ hostnetwork çš„æ–¹å¼éƒ¨ç½²ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566apiVersion: apps/v1kind: DaemonSetmetadata: name: cadvisor namespace: kube-system labels: app: cadvisorspec: selector: matchLabels: name: cadvisor template: metadata: labels: name: cadvisor spec: hostNetwork: true tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule key: enabledDiskSchedule value: &quot;true&quot; effect: NoSchedule containers: - name: cadvisor image: k8s.gcr.io/cadvisor:v0.28.0 imagePullPolicy: IfNotPresent volumeMounts: - name: rootfs mountPath: /rootfs readOnly: true - name: var-run mountPath: /var/run readOnly: false - name: sys mountPath: /sys readOnly: true - name: docker mountPath: /var/lib/docker readOnly: true ports: - name: http containerPort: 4194 protocol: TCP readinessProbe: tcpSocket: port: 4194 initialDelaySeconds: 5 periodSeconds: 10 args: - --housekeeping_interval=10s - --port=4194 terminationGracePeriodSeconds: 30 volumes: - name: rootfs hostPath: path: / - name: var-run hostPath: path: /var/run - name: sys hostPath: path: /sys - name: docker hostPath: path: /var/lib/docker å®˜æ–¹å»ºè®®ä½¿ç”¨ kustomize è¿›è¡Œéƒ¨ç½²ï¼Œkustomize æ˜¯ k8s çš„ä¸€ä¸ªé…ç½®ç®¡ç†å·¥å…·ï¼Œæ­¤å¤„æš‚ä¸è¯¦ç»†è§£é‡Šã€‚ æ³¨æ„ï¼šè‹¥é›†ç¾¤ä¸­æœ‰æ‰“ taint çš„å®¿ä¸»ï¼Œéœ€è¦åœ¨ yaml æ–‡ä»¶ä¸­åŠ ä¸Šå¯¹åº”çš„ tolerationsã€‚]]></content>
      <tags>
        <tag>kubernetes v1.12</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernets ä¸­äº‹ä»¶å¤„ç†æœºåˆ¶]]></title>
    <url>%2F2019%2F02%2F26%2Fk8s_events%2F</url>
    <content type="text"><![CDATA[å½“é›†ç¾¤ä¸­çš„ node æˆ– pod å¼‚å¸¸æ—¶ï¼Œå¤§éƒ¨åˆ†ç”¨æˆ·ä¼šä½¿ç”¨ kubectl æŸ¥çœ‹å¯¹åº”çš„ eventsï¼Œé‚£ä¹ˆ events æ˜¯ä»Žä½•è€Œæ¥çš„ï¼Ÿå…¶å®ž k8s ä¸­çš„å„ä¸ªç»„ä»¶ä¼šå°†è¿è¡Œæ—¶äº§ç”Ÿçš„å„ç§äº‹ä»¶æ±‡æŠ¥åˆ° apiserverï¼Œå¯¹äºŽ k8s ä¸­çš„å¯æè¿°èµ„æºï¼Œä½¿ç”¨ kubectl describe éƒ½å¯ä»¥çœ‹åˆ°å…¶ç›¸å…³çš„ eventsï¼Œé‚£ k8s ä¸­åˆæœ‰å“ªå‡ ä¸ªç»„ä»¶éƒ½ä¸ŠæŠ¥ events å‘¢ï¼Ÿ åªè¦åœ¨ k8s.io/kubernetes/cmd ç›®å½•ä¸‹æš´åŠ›æœç´¢ä¸€ä¸‹å°±èƒ½çŸ¥é“å“ªäº›ç»„ä»¶ä¼šäº§ç”Ÿ eventsï¼š1$ grep -R -n -i &quot;EventRecorder&quot; . å¯ä»¥çœ‹å‡ºï¼Œcontroller-manageã€kube-proxyã€kube-schedulerã€kubelet éƒ½ä½¿ç”¨äº† EventRecorderï¼Œæœ¬æ–‡åªè®²è¿° kubelet ä¸­å¯¹ Events çš„ä½¿ç”¨ã€‚ 1ã€Events çš„å®šä¹‰events åœ¨ k8s.io/api/core/v1/types.go ä¸­è¿›è¡Œå®šä¹‰,ç»“æž„ä½“å¦‚ä¸‹æ‰€ç¤ºï¼š 12345678910111213141516171819type Event struct &#123; metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ObjectMeta `json:&quot;metadata&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;` InvolvedObject ObjectReference `json:&quot;involvedObject&quot; protobuf:&quot;bytes,2,opt,name=involvedObject&quot;` Reason string `json:&quot;reason,omitempty&quot; protobuf:&quot;bytes,3,opt,name=reason&quot;` Message string `json:&quot;message,omitempty&quot; protobuf:&quot;bytes,4,opt,name=message&quot;` Source EventSource `json:&quot;source,omitempty&quot; protobuf:&quot;bytes,5,opt,name=source&quot;` FirstTimestamp metav1.Time `json:&quot;firstTimestamp,omitempty&quot; protobuf:&quot;bytes,6,opt,name=firstTimestamp&quot;` LastTimestamp metav1.Time `json:&quot;lastTimestamp,omitempty&quot; protobuf:&quot;bytes,7,opt,name=lastTimestamp&quot;` Count int32 `json:&quot;count,omitempty&quot; protobuf:&quot;varint,8,opt,name=count&quot;` Type string `json:&quot;type,omitempty&quot; protobuf:&quot;bytes,9,opt,name=type&quot;` EventTime metav1.MicroTime `json:&quot;eventTime,omitempty&quot; protobuf:&quot;bytes,10,opt,name=eventTime&quot;` Series *EventSeries `json:&quot;series,omitempty&quot; protobuf:&quot;bytes,11,opt,name=series&quot;` Action string `json:&quot;action,omitempty&quot; protobuf:&quot;bytes,12,opt,name=action&quot;` Related *ObjectReference `json:&quot;related,omitempty&quot; protobuf:&quot;bytes,13,opt,name=related&quot;` ReportingController string `json:&quot;reportingComponent&quot; protobuf:&quot;bytes,14,opt,name=reportingComponent&quot;` ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;` ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;`&#125; å…¶ä¸­ InvolvedObject ä»£è¡¨å’Œäº‹ä»¶å…³è”çš„å¯¹è±¡ï¼Œsource ä»£è¡¨äº‹ä»¶æºï¼Œä½¿ç”¨ kubectl çœ‹åˆ°çš„äº‹ä»¶ä¸€èˆ¬åŒ…å« Typeã€Reasonã€Ageã€Fromã€Message å‡ ä¸ªå­—æ®µã€‚ k8s ä¸­ events ç›®å‰åªæœ‰ä¸¤ç§ç±»åž‹ï¼šâ€Normalâ€ å’Œ â€œWarningâ€ï¼š 2ã€EventBroadcaster çš„åˆå§‹åŒ–events çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸéƒ½ä¸Ž EventBroadcaster æœ‰å…³ï¼Œkubelet ä¸­å¯¹ EventBroadcaster çš„åˆå§‹åŒ–åœ¨k8s.io/kubernetes/cmd/kubelet/app/server.goä¸­ï¼š 1234567891011121314151617181920212223242526func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123; ... // event åˆå§‹åŒ– makeEventRecorder(kubeDeps, nodeName) ...&#125;func makeEventRecorder(kubeDeps *kubelet.Dependencies, nodeName types.NodeName) &#123; if kubeDeps.Recorder != nil &#123; return &#125; // åˆå§‹åŒ– EventBroadcaster eventBroadcaster := record.NewBroadcaster() // åˆå§‹åŒ– EventRecorder kubeDeps.Recorder = eventBroadcaster.NewRecorder(legacyscheme.Scheme, v1.EventSource&#123;Component: componentKubelet, Host: string(nodeName)&#125;) // è®°å½• events åˆ°æœ¬åœ°æ—¥å¿— eventBroadcaster.StartLogging(glog.V(3).Infof) if kubeDeps.EventClient != nil &#123; glog.V(4).Infof(&quot;Sending events to api server.&quot;) // ä¸ŠæŠ¥ events åˆ° apiserver eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeDeps.EventClient.Events(&quot;&quot;)&#125;) &#125; else &#123; glog.Warning(&quot;No api server defined - no events will be sent to API server.&quot;) &#125;&#125; Kubelet åœ¨å¯åŠ¨çš„æ—¶å€™ä¼šåˆå§‹åŒ–ä¸€ä¸ª EventBroadcasterï¼Œå®ƒä¸»è¦æ˜¯å¯¹æŽ¥æ”¶åˆ°çš„ events åšä¸€äº›åŽç»­çš„å¤„ç†(ä¿å­˜ã€ä¸ŠæŠ¥ç­‰ï¼‰ï¼ŒEventBroadcaster ä¹Ÿä¼šè¢« kubelet ä¸­çš„å…¶ä»–æ¨¡å—ä½¿ç”¨ï¼Œä»¥ä¸‹æ˜¯ç›¸å…³çš„å®šä¹‰ï¼Œå¯¹ events ç”Ÿæˆå’Œå¤„ç†çš„å‡½æ•°éƒ½å®šä¹‰åœ¨ k8s.io/client-go/tools/record/event.go ä¸­ï¼š 123456789101112131415type eventBroadcasterImpl struct &#123; *watch.Broadcaster sleepDuration time.Duration&#125;// EventBroadcaster knows how to receive events and send them to any EventSink, watcher, or log.type EventBroadcaster interface &#123; StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface StartRecordingToSink(sink EventSink) watch.Interface StartLogging(logf func(format string, args ...interface&#123;&#125;)) watch.Interface NewRecorder(scheme *runtime.Scheme, source v1.EventSource) EventRecorder&#125; EventBroadcaster æ˜¯ä¸ªæŽ¥å£ç±»åž‹ï¼Œè¯¥æŽ¥å£æœ‰ä»¥ä¸‹å››ä¸ªæ–¹æ³•ï¼š StartEventWatcher() ï¼š EventBroadcaster ä¸­çš„æ ¸å¿ƒæ–¹æ³•ï¼ŒæŽ¥æ”¶å„æ¨¡å—äº§ç”Ÿçš„ eventsï¼Œå‚æ•°ä¸ºä¸€ä¸ªå¤„ç† events çš„å‡½æ•°ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨ StartEventWatcher() æŽ¥æ”¶ events ç„¶åŽä½¿ç”¨è‡ªå®šä¹‰çš„ handle è¿›è¡Œå¤„ç† StartRecordingToSink() ï¼š è°ƒç”¨ StartEventWatcher() æŽ¥æ”¶ eventsï¼Œå¹¶å°†æ”¶åˆ°çš„ events å‘é€åˆ° apiserver StartLogging() ï¼šä¹Ÿæ˜¯è°ƒç”¨ StartEventWatcher() æŽ¥æ”¶ eventsï¼Œç„¶åŽä¿å­˜ events åˆ°æ—¥å¿— NewRecorder() ï¼šä¼šåˆ›å»ºä¸€ä¸ªæŒ‡å®š EventSource çš„ EventRecorderï¼ŒEventSource æŒ‡æ˜Žäº†å“ªä¸ªèŠ‚ç‚¹çš„å“ªä¸ªç»„ä»¶ eventBroadcasterImpl æ˜¯ eventBroadcaster å®žé™…çš„å¯¹è±¡ï¼Œåˆå§‹åŒ– EventBroadcaster å¯¹è±¡çš„æ—¶å€™ä¼šåˆå§‹åŒ–ä¸€ä¸ª Broadcasterï¼ŒBroadcaster ä¼šå¯åŠ¨ä¸€ä¸ª goroutine æŽ¥æ”¶å„ç»„ä»¶äº§ç”Ÿçš„ events å¹¶å¹¿æ’­åˆ°æ¯ä¸€ä¸ª watcherã€‚ 123func NewBroadcaster() EventBroadcaster &#123; return &amp;eventBroadcasterImpl&#123;watch.NewBroadcaster(maxQueuedEvents, watch.DropIfChannelFull), defaultSleepDuration&#125;&#125; å¯ä»¥çœ‹åˆ°ï¼Œkubelet åœ¨åˆå§‹åŒ–å®Œ EventBroadcaster åŽä¼šè°ƒç”¨ StartRecordingToSink() å’Œ StartLogging() ä¸¤ä¸ªæ–¹æ³•ï¼ŒStartRecordingToSink() å¤„ç†å‡½æ•°ä¼šå°†æ”¶åˆ°çš„ events è¿›è¡Œç¼“å­˜ã€è¿‡æ»¤ã€èšåˆè€ŒåŽå‘é€åˆ° apiserverï¼ŒStartLogging() ä»…å°† events ä¿å­˜åˆ° kubelet çš„æ—¥å¿—ä¸­ã€‚ 3ã€Events çš„ç”Ÿæˆä»Žåˆå§‹åŒ– EventBroadcaster çš„ä»£ç ä¸­å¯ä»¥çœ‹åˆ° kubelet åœ¨åˆå§‹åŒ–å®Œ EventBroadcaster åŽç´§æŽ¥ç€åˆå§‹åŒ–äº† EventRecorderï¼Œå¹¶å°†å·²ç»åˆå§‹åŒ–çš„ Broadcaster å¯¹è±¡ä½œä¸ºå‚æ•°ä¼ ç»™äº† EventRecorderï¼Œè‡³æ­¤ï¼ŒEventBroadcasterã€EventRecorderã€Broadcaster ä¸‰ä¸ªå¯¹è±¡äº§ç”Ÿäº†å…³è”ã€‚EventRecorder çš„ä¸»è¦åŠŸèƒ½æ˜¯ç”ŸæˆæŒ‡å®šæ ¼å¼çš„ eventsï¼Œä»¥ä¸‹æ˜¯ç›¸å…³çš„å®šä¹‰ï¼š 12345678910111213141516type recorderImpl struct &#123; scheme *runtime.Scheme source v1.EventSource *watch.Broadcaster clock clock.Clock&#125;type EventRecorder interface &#123; Event(object runtime.Object, eventtype, reason, message string) Eventf(object runtime.Object, eventtype, reason, messageFmt string, args ...interface&#123;&#125;) PastEventf(object runtime.Object, timestamp metav1.Time, eventtype, reason, messageFmt string, args ...interface&#123;&#125;) AnnotatedEventf(object runtime.Object, annotations map[string]string, eventtype, reason, messageFmt string, args ...interface&#123;&#125;)&#125; EventRecorder ä¸­åŒ…å«çš„å‡ ä¸ªæ–¹æ³•éƒ½æ˜¯äº§ç”ŸæŒ‡å®šæ ¼å¼çš„ eventsï¼ŒEvent() å’Œ Eventf() çš„åŠŸèƒ½ç±»ä¼¼ fmt.Println() å’Œ fmt.Printf()ï¼Œkubelet ä¸­çš„å„ä¸ªæ¨¡å—ä¼šè°ƒç”¨ EventRecorder ç”Ÿæˆ eventsã€‚recorderImpl æ˜¯ EventRecorder å®žé™…çš„å¯¹è±¡ã€‚EventRecorder çš„æ¯ä¸ªæ–¹æ³•ä¼šè°ƒç”¨ generateEventï¼Œåœ¨ generateEvent ä¸­åˆå§‹åŒ– events ã€‚ ä»¥ä¸‹æ˜¯ç”Ÿæˆ events çš„å‡½æ•°ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344func (recorder *recorderImpl) generateEvent(object runtime.Object, annotations map[string]string, timestamp metav1.Time, eventtype, reason, message string) &#123; ref, err := ref.GetReference(recorder.scheme, object) if err != nil &#123; glog.Errorf(&quot;Could not construct reference to: &apos;%#v&apos; due to: &apos;%v&apos;. Will not report event: &apos;%v&apos; &apos;%v&apos; &apos;%v&apos;&quot;, object, err, eventtype, reason, message) return &#125; if !validateEventType(eventtype) &#123; glog.Errorf(&quot;Unsupported event type: &apos;%v&apos;&quot;, eventtype) return &#125; event := recorder.makeEvent(ref, annotations, eventtype, reason, message) event.Source = recorder.source go func() &#123; // NOTE: events should be a non-blocking operation defer utilruntime.HandleCrash() // å‘é€äº‹ä»¶ recorder.Action(watch.Added, event) &#125;()&#125;func (recorder *recorderImpl) makeEvent(ref *v1.ObjectReference, annotations map[string]string, eventtype, reason, message string) *v1.Event &#123; t := metav1.Time&#123;Time: recorder.clock.Now()&#125; namespace := ref.Namespace if namespace == &quot;&quot; &#123; namespace = metav1.NamespaceDefault &#125; return &amp;v1.Event&#123; ObjectMeta: metav1.ObjectMeta&#123; Name: fmt.Sprintf(&quot;%v.%x&quot;, ref.Name, t.UnixNano()), Namespace: namespace, Annotations: annotations, &#125;, InvolvedObject: *ref, Reason: reason, Message: message, FirstTimestamp: t, LastTimestamp: t, Count: 1, Type: eventtype, &#125;&#125; åˆå§‹åŒ–å®Œ events åŽä¼šè°ƒç”¨ recorder.Action() å°† events å‘é€åˆ° Broadcaster çš„äº‹ä»¶æŽ¥æ”¶é˜Ÿåˆ—ä¸­, Action() æ˜¯ Broadcaster ä¸­çš„æ–¹æ³•ã€‚ ä»¥ä¸‹æ˜¯ Action() æ–¹æ³•çš„å®žçŽ°ï¼š 123func (m *Broadcaster) Action(action EventType, obj runtime.Object) &#123; m.incoming &lt;- Event&#123;action, obj&#125;&#125; 4ã€Events çš„å¹¿æ’­ä¸Šé¢å·²ç»è¯´äº†ï¼ŒEventBroadcaster åˆå§‹åŒ–æ—¶ä¼šåˆå§‹åŒ–ä¸€ä¸ª Broadcasterï¼ŒBroadcaster çš„ä½œç”¨å°±æ˜¯æŽ¥æ”¶æ‰€æœ‰çš„ events å¹¶è¿›è¡Œå¹¿æ’­ï¼ŒBroadcaster çš„å®žçŽ°åœ¨ k8s.io/apimachinery/pkg/watch/mux.go ä¸­ï¼ŒBroadcaster åˆå§‹åŒ–å®ŒæˆåŽä¼šåœ¨åŽå°å¯åŠ¨ä¸€ä¸ª goroutineï¼Œç„¶åŽæŽ¥æ”¶æ‰€æœ‰ä»Ž EventRecorder å‘é€è¿‡æ¥çš„ eventsï¼ŒBroadcaster ä¸­æœ‰ä¸€ä¸ª map ä¼šä¿å­˜æ¯ä¸€ä¸ªæ³¨å†Œçš„ watcherï¼Œ æŽ¥ç€å°† events å¹¿æ’­ç»™æ‰€æœ‰çš„ watcherï¼Œæ¯ä¸ª watcher éƒ½æœ‰ä¸€ä¸ªæŽ¥æ”¶æ¶ˆæ¯çš„ channelï¼Œwatcher å¯ä»¥é€šè¿‡å®ƒçš„ ResultChan() æ–¹æ³•ä»Ž channel ä¸­è¯»å–æ•°æ®è¿›è¡Œæ¶ˆè´¹ã€‚ ä»¥ä¸‹æ˜¯ Broadcaster å¹¿æ’­ events çš„å®žçŽ°ï¼š123456789101112131415161718192021222324252627282930313233func (m *Broadcaster) loop() &#123; for event := range m.incoming &#123; if event.Type == internalRunFunctionMarker &#123; event.Object.(functionFakeRuntimeObject)() continue &#125; m.distribute(event) &#125; m.closeAll() m.distributing.Done()&#125;// distribute sends event to all watchers. Blocking.func (m *Broadcaster) distribute(event Event) &#123; m.lock.Lock() defer m.lock.Unlock() if m.fullChannelBehavior == DropIfChannelFull &#123; for _, w := range m.watchers &#123; select &#123; case w.result &lt;- event: case &lt;-w.stopped: default: // Don&apos;t block if the event can&apos;t be queued. &#125; &#125; &#125; else &#123; for _, w := range m.watchers &#123; select &#123; case w.result &lt;- event: case &lt;-w.stopped: &#125; &#125; &#125;&#125; 5ã€Events çš„å¤„ç†é‚£ä¹ˆ watcher æ˜¯ä»Žä½•è€Œæ¥å‘¢ï¼Ÿæ¯ä¸€ä¸ªè¦å¤„ç† events çš„ client éƒ½éœ€è¦åˆå§‹åŒ–ä¸€ä¸ª watcherï¼Œå¤„ç† events çš„æ–¹æ³•æ˜¯åœ¨ EventBroadcaster ä¸­å®šä¹‰çš„ï¼Œä»¥ä¸‹æ˜¯ EventBroadcaster ä¸­å¯¹ events å¤„ç†çš„ä¸‰ä¸ªå‡½æ•°ï¼š 12345678910111213141516func (eventBroadcaster *eventBroadcasterImpl) StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface &#123; watcher := eventBroadcaster.Watch() go func() &#123; defer utilruntime.HandleCrash() for watchEvent := range watcher.ResultChan() &#123; event, ok := watchEvent.Object.(*v1.Event) if !ok &#123; // This is all local, so there&apos;s no reason this should // ever happen. continue &#125; eventHandler(event) &#125; &#125;() return watcher&#125; StartEventWatcher() é¦–å…ˆå®žä¾‹åŒ–ä¸€ä¸ª watcherï¼Œæ¯ä¸ª watcher éƒ½ä¼šè¢«å¡žå…¥åˆ° Broadcaster çš„ watcher åˆ—è¡¨ä¸­ï¼Œwatcher ä»Ž Broadcaster æä¾›çš„ channel ä¸­è¯»å– eventsï¼Œç„¶åŽå†è°ƒç”¨ eventHandler è¿›è¡Œå¤„ç†ï¼ŒStartLogging() å’Œ StartRecordingToSink() éƒ½æ˜¯å¯¹ StartEventWatcher() çš„å°è£…ï¼Œéƒ½ä¼šä¼ å…¥è‡ªå·±çš„å¤„ç†å‡½æ•°ã€‚ 123456func (eventBroadcaster *eventBroadcasterImpl) StartLogging(logf func(format string, args ...interface&#123;&#125;)) watch.Interface &#123; return eventBroadcaster.StartEventWatcher( func(e *v1.Event) &#123; logf(&quot;Event(%#v): type: &apos;%v&apos; reason: &apos;%v&apos; %v&quot;, e.InvolvedObject, e.Type, e.Reason, e.Message) &#125;)&#125; StartLogging() ä¼ å…¥çš„ eventHandler ä»…å°† events ä¿å­˜åˆ°æ—¥å¿—ä¸­ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839func (eventBroadcaster *eventBroadcasterImpl) StartRecordingToSink(sink EventSink) watch.Interface &#123; // The default math/rand package functions aren&apos;t thread safe, so create a // new Rand object for each StartRecording call. randGen := rand.New(rand.NewSource(time.Now().UnixNano())) eventCorrelator := NewEventCorrelator(clock.RealClock&#123;&#125;) return eventBroadcaster.StartEventWatcher( func(event *v1.Event) &#123; recordToSink(sink, event, eventCorrelator, randGen, eventBroadcaster.sleepDuration) &#125;)&#125;func recordToSink(sink EventSink, event *v1.Event, eventCorrelator *EventCorrelator, randGen *rand.Rand, sleepDuration time.Duration) &#123; eventCopy := *event event = &amp;eventCopy result, err := eventCorrelator.EventCorrelate(event) if err != nil &#123; utilruntime.HandleError(err) &#125; if result.Skip &#123; return &#125; tries := 0 for &#123; if recordEvent(sink, result.Event, result.Patch, result.Event.Count &gt; 1, eventCorrelator) &#123; break &#125; tries++ if tries &gt;= maxTriesPerEvent &#123; glog.Errorf(&quot;Unable to write event &apos;%#v&apos; (retry limit exceeded!)&quot;, event) break &#125; // ç¬¬ä¸€æ¬¡é‡è¯•å¢žåŠ éšæœºæ€§ï¼Œé˜²æ­¢ apiserver é‡å¯çš„æ—¶å€™æ‰€æœ‰çš„äº‹ä»¶éƒ½åœ¨åŒä¸€æ—¶é—´å‘é€äº‹ä»¶ if tries == 1 &#123; time.Sleep(time.Duration(float64(sleepDuration) * randGen.Float64())) &#125; else &#123; time.Sleep(sleepDuration) &#125; &#125;&#125; StartRecordingToSink() æ–¹æ³•å…ˆæ ¹æ®å½“å‰æ—¶é—´ç”Ÿæˆä¸€ä¸ªéšæœºæ•°å‘ç”Ÿå™¨ randGenï¼Œå¢žåŠ éšæœºæ•°æ˜¯ä¸ºäº†åœ¨é‡è¯•æ—¶å¢žåŠ éšæœºæ€§ï¼Œé˜²æ­¢ apiserver é‡å¯çš„æ—¶å€™æ‰€æœ‰çš„äº‹ä»¶éƒ½åœ¨åŒä¸€æ—¶é—´å‘é€äº‹ä»¶ï¼ŒæŽ¥ç€å®žä¾‹åŒ–ä¸€ä¸ªEventCorrelatorï¼ŒEventCorrelator ä¼šå¯¹äº‹ä»¶åšä¸€äº›é¢„å¤„ç†çš„å·¥ä½œï¼Œå…¶ä¸­åŒ…æ‹¬è¿‡æ»¤ã€èšåˆã€ç¼“å­˜ç­‰æ“ä½œï¼Œå…·ä½“ä»£ç ä¸åšè¯¦ç»†åˆ†æžï¼Œæœ€åŽå°† recordToSink() å‡½æ•°ä½œä¸ºå¤„ç†å‡½æ•°ï¼ŒrecordToSink() ä¼šå°†å¤„ç†åŽçš„ events å‘é€åˆ° apiserverï¼Œè¿™æ˜¯ StartEventWatcher() çš„æ•´ä¸ªå·¥ä½œæµç¨‹ã€‚ 6ã€Events ç®€å•å®žçŽ°äº†è§£å®Œ events çš„æ•´ä¸ªå¤„ç†æµç¨‹åŽï¼Œå¯ä»¥å‚è€ƒå…¶å®žçŽ°æ–¹å¼å†™ä¸€ä¸ª demoï¼Œè¦å®žçŽ°ä¸€ä¸ªå®Œæ•´çš„ events éœ€è¦åŒ…å«ä»¥ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š 1ã€äº‹ä»¶çš„äº§ç”Ÿ 2ã€äº‹ä»¶çš„å‘é€ 3ã€äº‹ä»¶å¹¿æ’­ 4ã€äº‹ä»¶ç¼“å­˜ 5ã€äº‹ä»¶è¿‡æ»¤å’Œèšåˆ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201package mainimport ( &quot;fmt&quot; &quot;sync&quot; &quot;time&quot;)// watcher queueconst queueLength = int64(1)// Events xxxtype Events struct &#123; Reason string Message string Source string Type string Count int64 Timestamp time.Time&#125;// EventBroadcaster xxxtype EventBroadcaster interface &#123; Event(etype, reason, message string) StartLogging() Interface Stop()&#125;// eventBroadcaster xxxtype eventBroadcasterImpl struct &#123; *Broadcaster&#125;func NewEventBroadcaster() EventBroadcaster &#123; return &amp;eventBroadcasterImpl&#123;NewBroadcaster(queueLength)&#125;&#125;func (eventBroadcaster *eventBroadcasterImpl) Stop() &#123; eventBroadcaster.Shutdown()&#125;// generate eventfunc (eventBroadcaster *eventBroadcasterImpl) Event(etype, reason, message string) &#123; events := &amp;Events&#123;Type: etype, Reason: reason, Message: message&#125; // send event to broadcast eventBroadcaster.Action(events)&#125;// ä»…å®žçŽ° StartLogging() çš„åŠŸèƒ½ï¼Œå°†æ—¥å¿—æ‰“å°func (eventBroadcaster *eventBroadcasterImpl) StartLogging() Interface &#123; // register a watcher watcher := eventBroadcaster.Watch() go func() &#123; for watchEvent := range watcher.ResultChan() &#123; fmt.Printf(&quot;%v\n&quot;, watchEvent) &#125; &#125;() go func() &#123; time.Sleep(time.Second * 4) watcher.Stop() &#125;() return watcher&#125;// --------------------// Broadcaster å®šä¹‰ä¸Žå®žçŽ°// æŽ¥æ”¶ events channel çš„é•¿åº¦const incomingQueuLength = 100type Broadcaster struct &#123; lock sync.Mutex incoming chan Events watchers map[int64]*broadcasterWatcher watchersQueue int64 watchQueueLength int64 distributing sync.WaitGroup&#125;func NewBroadcaster(queueLength int64) *Broadcaster &#123; m := &amp;Broadcaster&#123; incoming: make(chan Events, incomingQueuLength), watchers: map[int64]*broadcasterWatcher&#123;&#125;, watchQueueLength: queueLength, &#125; m.distributing.Add(1) // åŽå°å¯åŠ¨ä¸€ä¸ª goroutine å¹¿æ’­ events go m.loop() return m&#125;// Broadcaster æŽ¥æ”¶æ‰€äº§ç”Ÿçš„ eventsfunc (m *Broadcaster) Action(event *Events) &#123; m.incoming &lt;- *event&#125;// å¹¿æ’­ events åˆ°æ¯ä¸ª watcherfunc (m *Broadcaster) loop() &#123; // ä»Ž incoming channel ä¸­è¯»å–æ‰€æŽ¥æ”¶åˆ°çš„ events for event := range m.incoming &#123; // å‘é€ events åˆ°æ¯ä¸€ä¸ª watcher for _, w := range m.watchers &#123; select &#123; case w.result &lt;- event: case &lt;-w.stopped: default: &#125; &#125; &#125; m.closeAll() m.distributing.Done()&#125;func (m *Broadcaster) Shutdown() &#123; close(m.incoming) m.distributing.Wait()&#125;func (m *Broadcaster) closeAll() &#123; // TODO m.lock.Lock() defer m.lock.Unlock() for _, w := range m.watchers &#123; close(w.result) &#125; m.watchers = map[int64]*broadcasterWatcher&#123;&#125;&#125;func (m *Broadcaster) stopWatching(id int64) &#123; m.lock.Lock() defer m.lock.Unlock() w, ok := m.watchers[id] if !ok &#123; return &#125; delete(m.watchers, id) close(w.result)&#125;// è°ƒç”¨ Watch(ï¼‰æ–¹æ³•æ³¨å†Œä¸€ä¸ª watcherfunc (m *Broadcaster) Watch() Interface &#123; watcher := &amp;broadcasterWatcher&#123; result: make(chan Events, incomingQueuLength), stopped: make(chan struct&#123;&#125;), id: m.watchQueueLength, m: m, &#125; m.watchers[m.watchersQueue] = watcher m.watchQueueLength++ return watcher&#125;// watcher å®žçŽ°type Interface interface &#123; Stop() ResultChan() &lt;-chan Events&#125;type broadcasterWatcher struct &#123; result chan Events stopped chan struct&#123;&#125; stop sync.Once id int64 m *Broadcaster&#125;// æ¯ä¸ª watcher é€šè¿‡è¯¥æ–¹æ³•è¯»å– channel ä¸­å¹¿æ’­çš„ eventsfunc (b *broadcasterWatcher) ResultChan() &lt;-chan Events &#123; return b.result&#125;func (b *broadcasterWatcher) Stop() &#123; b.stop.Do(func() &#123; close(b.stopped) b.m.stopWatching(b.id) &#125;)&#125;// --------------------func main() &#123; eventBroadcast := NewEventBroadcaster() var wg sync.WaitGroup wg.Add(1) // producer event go func() &#123; defer wg.Done() time.Sleep(time.Second) eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;1&quot;) time.Sleep(time.Second * 2) eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;2&quot;) time.Sleep(time.Second * 3) eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;3&quot;) //eventBroadcast.Stop() &#125;() eventBroadcast.StartLogging() wg.Wait()&#125; æ­¤å¤„ä»…ç®€å•å®žçŽ°ï¼Œå°† EventRecorder å¤„ç† events çš„åŠŸèƒ½ç›´æŽ¥æ”¾åœ¨äº† EventBroadcaster ä¸­å®žçŽ°ï¼Œå¯¹ events çš„å¤„ç†æ–¹æ³•ä»…å®žçŽ°äº† StartLogging()ï¼ŒBroadcaster ä¸­çš„éƒ¨åˆ†åŠŸèƒ½æ˜¯ç›´æŽ¥å¤åˆ¶ k8s ä¸­çš„ä»£ç ï¼Œæœ‰ä¸€å®šçš„ç²¾ç®€ï¼Œå…¶å®žçŽ°å€¼å¾—å­¦ä¹ ï¼Œæ­¤å¤„å¯¹ EventCorrelator å¹¶æ²¡æœ‰è¿›è¡Œå®žçŽ°ã€‚ ä»£ç è¯·å‚è€ƒï¼šhttps://github.com/gosoon/k8s-learning-notes/tree/master/k8s-package/events 7ã€æ€»ç»“æœ¬æ–‡è®²è¿°äº† k8s ä¸­ events ä»Žäº§ç”Ÿåˆ°å±•ç¤ºçš„ä¸€ä¸ªå®Œæ•´è¿‡ç¨‹ï¼Œæœ€åŽä¹Ÿå®žçŽ°äº†ä¸€ä¸ªç®€å•çš„ demoï¼Œåœ¨æ­¤å°† kubelet å¯¹ events çš„æ•´ä¸ªå¤„ç†è¿‡ç¨‹å†æ¢³ç†ä¸‹ï¼Œå…¶ä¸­ä¸»è¦æœ‰ä¸‰ä¸ªå¯¹è±¡ EventBroadcasterã€EventRecorderã€Broadcasterï¼š 1ã€kubelet é¦–å…ˆä¼šåˆå§‹åŒ– EventBroadcaster å¯¹è±¡ï¼ŒåŒæ—¶ä¼šåˆå§‹åŒ–ä¸€ä¸ª Broadcaster å¯¹è±¡ã€‚ 2ã€kubelet é€šè¿‡ EventBroadcaster å¯¹è±¡çš„ NewRecorder() æ–¹æ³•åˆå§‹åŒ– EventRecorder å¯¹è±¡ï¼ŒEventRecorder å¯¹è±¡æä¾›çš„å‡ ä¸ªæ–¹æ³•ä¼šç”Ÿæˆ events å¹¶é€šè¿‡ Action() æ–¹æ³•å‘é€ events åˆ° Broadcaster çš„ channel é˜Ÿåˆ—ä¸­ã€‚ 3ã€Broadcaster çš„ä½œç”¨å°±æ˜¯æŽ¥æ”¶æ‰€æœ‰çš„ events å¹¶è¿›è¡Œå¹¿æ’­ï¼ŒBroadcaster åˆå§‹åŒ–åŽä¼šåœ¨åŽå°å¯åŠ¨ä¸€ä¸ª goroutineï¼Œç„¶åŽæŽ¥æ”¶æ‰€æœ‰ä»Ž EventRecorder å‘æ¥çš„ eventsã€‚ 4ã€EventBroadcaster å¯¹ events æœ‰ä¸‰ä¸ªå¤„ç†æ–¹æ³•ï¼šStartEventWatcher()ã€StartRecordingToSink()ã€StartLogging()ï¼ŒStartEventWatcher() æ˜¯å…¶ä¸­çš„æ ¸å¿ƒæ–¹æ³•ï¼Œä¼šåˆå§‹åŒ–ä¸€ä¸ª watcher æ³¨å†Œåˆ° Broadcasterï¼Œå…¶ä½™ä¸¤ä¸ªå¤„ç†å‡½æ•°å¯¹ StartEventWatcher() è¿›è¡Œäº†å°è£…ï¼Œå¹¶å®žçŽ°äº†è‡ªå·±çš„å¤„ç†å‡½æ•°ã€‚ 5ã€ Broadcaster ä¸­æœ‰ä¸€ä¸ª map ä¼šä¿å­˜æ¯ä¸€ä¸ªæ³¨å†Œçš„ watcherï¼Œå…¶ä¼šå°†æ‰€æœ‰çš„ events å¹¿æ’­ç»™æ¯ä¸€ä¸ª watcherï¼Œæ¯ä¸ª watcher é€šè¿‡å®ƒçš„ ResultChan() æ–¹æ³•ä»Ž channel æŽ¥æ”¶ eventsã€‚ 6ã€kubelet ä¼šä½¿ç”¨ StartRecordingToSink() å’Œ StartLogging() å¯¹ events è¿›è¡Œå¤„ç†ï¼ŒStartRecordingToSink() å¤„ç†å‡½æ•°æ”¶åˆ° events åŽä¼šè¿›è¡Œç¼“å­˜ã€è¿‡æ»¤ã€èšåˆè€ŒåŽå‘é€åˆ° apiserverï¼Œapiserver ä¼šå°† events ä¿å­˜åˆ° etcd ä¸­ï¼Œä½¿ç”¨ kubectl æˆ–å…¶ä»–å®¢æˆ·ç«¯å¯ä»¥æŸ¥çœ‹ã€‚StartLogging() ä»…å°† events ä¿å­˜åˆ° kubelet çš„æ—¥å¿—ä¸­ã€‚ ç›¸å…³æŽ¨èkubelet çŠ¶æ€ä¸ŠæŠ¥çš„æ–¹å¼kubelet åˆ›å»º pod çš„æµç¨‹kubelet å¯åŠ¨æµç¨‹åˆ†æž]]></content>
      <tags>
        <tag>events</tag>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s ä¸­å®šæ—¶ä»»åŠ¡çš„å®žçŽ°]]></title>
    <url>%2F2019%2F02%2F16%2Fk8s-crontab%2F</url>
    <content type="text"><![CDATA[k8s ä¸­æœ‰è®¸å¤šä¼˜ç§€çš„åŒ…éƒ½å¯ä»¥åœ¨å¹³æ—¶çš„å¼€å‘ä¸­å€Ÿé‰´ä¸Žä½¿ç”¨ï¼Œæ¯”å¦‚ï¼Œä»»åŠ¡çš„å®šæ—¶è½®è¯¢ã€é«˜å¯ç”¨çš„å®žçŽ°ã€æ—¥å¿—å¤„ç†ã€ç¼“å­˜ä½¿ç”¨ç­‰éƒ½æ˜¯ç‹¬ç«‹çš„åŒ…ï¼Œå¯ä»¥ç›´æŽ¥å¼•ç”¨ã€‚æœ¬ç¯‡æ–‡ç« ä¼šä»‹ç» k8s ä¸­å®šæ—¶ä»»åŠ¡çš„å®žçŽ°ï¼Œk8s ä¸­å®šæ—¶ä»»åŠ¡éƒ½æ˜¯é€šè¿‡ wait åŒ…å®žçŽ°çš„ï¼Œwait åŒ…åœ¨ k8s çš„å¤šä¸ªç»„ä»¶ä¸­éƒ½æœ‰ç”¨åˆ°ï¼Œä»¥ä¸‹æ˜¯ wait åŒ…åœ¨ kubelet ä¸­çš„å‡ å¤„ä½¿ç”¨ï¼š 123456789101112131415161718192021222324func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123; ... // kubelet æ¯5åˆ†é’Ÿä¸€æ¬¡ä»Ž apiserver èŽ·å–è¯ä¹¦ closeAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute) if err != nil &#123; return err &#125; closeAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute) if err != nil &#123; return err &#125; ...&#125;...func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) &#123; // æŒç»­ç›‘å¬ pod çš„å˜åŒ– go wait.Until(func() &#123; k.Run(podCfg.Updates()) &#125;, 0, wait.NeverStop) ...&#125; golang ä¸­å¯ä»¥é€šè¿‡ time.Ticker å®žçŽ°å®šæ—¶ä»»åŠ¡çš„æ‰§è¡Œï¼Œä½†åœ¨ k8s ä¸­ç”¨äº†æ›´åŽŸç”Ÿçš„æ–¹å¼ï¼Œä½¿ç”¨ time.Timer å®žçŽ°çš„ã€‚time.Ticker å’Œ time.Timer çš„ä½¿ç”¨åŒºåˆ«å¦‚ä¸‹ï¼š ticker åªè¦å®šä¹‰å®Œæˆï¼Œä»Žæ­¤åˆ»å¼€å§‹è®¡æ—¶ï¼Œä¸éœ€è¦ä»»ä½•å…¶ä»–çš„æ“ä½œï¼Œæ¯éš”å›ºå®šæ—¶é—´éƒ½ä¼šè‡ªåŠ¨è§¦å‘ã€‚ timer å®šæ—¶å™¨æ˜¯åˆ°äº†å›ºå®šæ—¶é—´åŽä¼šæ‰§è¡Œä¸€æ¬¡ï¼Œä»…æ‰§è¡Œä¸€æ¬¡ å¦‚æžœ timer å®šæ—¶å™¨è¦æ¯éš”é—´éš”çš„æ—¶é—´æ‰§è¡Œï¼Œå®žçŽ° ticker çš„æ•ˆæžœï¼Œä½¿ç”¨ func (t *Timer) Reset(d Duration) bool ä¸€ä¸ªç¤ºä¾‹ï¼š 1234567891011121314151617181920212223242526272829303132333435package mainimport ( &quot;fmt&quot; &quot;sync&quot; &quot;time&quot;)func main() &#123; var wg sync.WaitGroup timer1 := time.NewTimer(2 * time.Second) ticker1 := time.NewTicker(2 * time.Second) wg.Add(1) go func(t *time.Ticker) &#123; defer wg.Done() for &#123; &lt;-t.C fmt.Println(&quot;exec ticker&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;)) &#125; &#125;(ticker1) wg.Add(1) go func(t *time.Timer) &#123; defer wg.Done() for &#123; &lt;-t.C fmt.Println(&quot;exec timer&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;)) t.Reset(2 * time.Second) &#125; &#125;(timer1) wg.Wait()&#125; ä¸€ã€wait åŒ…ä¸­çš„æ ¸å¿ƒä»£ç æ ¸å¿ƒä»£ç ï¼ˆk8s.io/apimachinery/pkg/util/wait/wait.goï¼‰ï¼š 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh &lt;-chan struct&#123;&#125;) &#123; var t *time.Timer var sawTimeout bool for &#123; select &#123; case &lt;-stopCh: return default: &#125; jitteredPeriod := period if jitterFactor &gt; 0.0 &#123; jitteredPeriod = Jitter(period, jitterFactor) &#125; if !sliding &#123; t = resetOrReuseTimer(t, jitteredPeriod, sawTimeout) &#125; func() &#123; defer runtime.HandleCrash() f() &#125;() if sliding &#123; t = resetOrReuseTimer(t, jitteredPeriod, sawTimeout) &#125; select &#123; case &lt;-stopCh: return case &lt;-t.C: sawTimeout = true &#125; &#125;&#125;...func resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer &#123; if t == nil &#123; return time.NewTimer(d) &#125; if !t.Stop() &amp;&amp; !sawTimeout &#123; &lt;-t.C &#125; t.Reset(d) return t&#125; å‡ ä¸ªå…³é”®ç‚¹çš„è¯´æ˜Žï¼š 1ã€å¦‚æžœ sliding ä¸º trueï¼Œåˆ™åœ¨ f() è¿è¡Œä¹‹åŽè®¡ç®—å‘¨æœŸã€‚å¦‚æžœä¸º falseï¼Œé‚£ä¹ˆ period åŒ…å« f() çš„æ‰§è¡Œæ—¶é—´ã€‚ 2ã€åœ¨ golang ä¸­ select æ²¡æœ‰ä¼˜å…ˆçº§é€‰æ‹©ï¼Œä¸ºäº†é¿å…é¢å¤–æ‰§è¡Œ f(),åœ¨æ¯æ¬¡å¾ªçŽ¯å¼€å§‹åŽä¼šå…ˆåˆ¤æ–­ stopCh chanã€‚ k8s ä¸­ wait åŒ…å…¶å®žæ˜¯å¯¹ time.Timer åšäº†ä¸€å±‚å°è£…å®žçŽ°ã€‚ äºŒã€wait åŒ…å¸¸ç”¨çš„æ–¹æ³•1ã€å®šæœŸæ‰§è¡Œä¸€ä¸ªå‡½æ•°ï¼Œæ°¸ä¸åœæ­¢ï¼Œå¯ä»¥ä½¿ç”¨ Forever æ–¹æ³•ï¼šfunc Forever(f func(), period time.Duration) 2ã€åœ¨éœ€è¦çš„æ—¶å€™åœæ­¢å¾ªçŽ¯ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„æ–¹æ³•ï¼Œå¢žåŠ ä¸€ä¸ªç”¨äºŽåœæ­¢çš„ chan å³å¯ï¼Œæ–¹æ³•å®šä¹‰å¦‚ä¸‹ï¼šfunc Until(f func(), period time.Duration, stopCh &lt;-chan struct{}) ä¸Šé¢çš„ç¬¬ä¸‰ä¸ªå‚æ•° stopCh å°±æ˜¯ç”¨äºŽé€€å‡ºæ— é™å¾ªçŽ¯çš„æ ‡å¿—ï¼Œåœæ­¢çš„æ—¶å€™æˆ‘ä»¬ close æŽ‰è¿™ä¸ª chan å°±å¯ä»¥äº†ã€‚ 3ã€æœ‰æ—¶å€™ï¼Œæˆ‘ä»¬è¿˜ä¼šéœ€è¦åœ¨è¿è¡Œå‰åŽ»æ£€æŸ¥å…ˆå†³æ¡ä»¶ï¼Œåœ¨æ¡ä»¶æ»¡è¶³çš„æ—¶å€™æ‰åŽ»è¿è¡ŒæŸä¸€ä»»åŠ¡ï¼Œè¿™æ—¶å€™å¯ä»¥ä½¿ç”¨ Poll æ–¹æ³•ï¼šfunc Poll(interval, timeout time.Duration, condition ConditionFunc) è¿™ä¸ªå‡½æ•°ä¼šä»¥ interval ä¸ºé—´éš”ï¼Œä¸æ–­åŽ»æ£€æŸ¥ condition æ¡ä»¶æ˜¯å¦ä¸ºçœŸï¼Œå¦‚æžœä¸ºçœŸåˆ™å¯ä»¥ç»§ç»­åŽç»­å¤„ç†ï¼›å¦‚æžœæŒ‡å®šäº† timeout å‚æ•°ï¼Œåˆ™è¯¥å‡½æ•°ä¹Ÿå¯ä»¥åªå¸¸è¯†æŒ‡å®šçš„æ—¶é—´ã€‚ 4ã€PollUntil æ–¹æ³•å’Œä¸Šé¢çš„ç±»ä¼¼ï¼Œä½†æ˜¯æ²¡æœ‰ timeout å‚æ•°ï¼Œå¤šäº†ä¸€ä¸ª stopCh å‚æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼šPollUntil(interval time.Duration, condition ConditionFunc, stopCh &lt;-chan struct{}) error æ­¤å¤–è¿˜æœ‰ PollImmediate ã€ PollInfinite å’Œ PollImmediateInfinite æ–¹æ³•ã€‚ ä¸‰ã€æ€»ç»“æœ¬ç¯‡æ–‡ç« ä¸»è¦è®²äº† k8s ä¸­å®šæ—¶ä»»åŠ¡çš„å®žçŽ°ä¸Žå¯¹åº”åŒ…ï¼ˆwaitï¼‰ä¸­æ–¹æ³•çš„ä½¿ç”¨ã€‚é€šè¿‡é˜…è¯» k8s çš„æºä»£ç ï¼Œå¯ä»¥å‘çŽ° k8s ä¸­è®¸å¤šåŠŸèƒ½çš„å®žçŽ°ä¹Ÿéƒ½æ˜¯æˆ‘ä»¬éœ€è¦åœ¨å¹³æ—¶å·¥ä½œä¸­ç”¨çš„ï¼Œå…¶å¤§éƒ¨åˆ†åŒ…çš„æ€§èƒ½éƒ½æ˜¯ç»è¿‡å¤§è§„æ¨¡è€ƒéªŒçš„ï¼Œé€šè¿‡ä½¿ç”¨å…¶ç›¸å…³çš„å·¥å…·åŒ…ä¸ä»…èƒ½å­¦åˆ°å¤§é‡çš„ç¼–ç¨‹æŠ€å·§ä¹Ÿèƒ½é¿å…è‡ªå·±é€ è½®å­ã€‚ ç›¸å…³æŽ¨èLinux å®šæ—¶ä»»åŠ¡ä¸Ž crontab ç®€ä»‹]]></content>
      <tags>
        <tag>crontab</tag>
        <tag>wait</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes å®¡è®¡æ—¥å¿—åŠŸèƒ½]]></title>
    <url>%2F2019%2F01%2F30%2Fk8s-audit-webhook%2F</url>
    <content type="text"><![CDATA[å®¡è®¡æ—¥å¿—å¯ä»¥è®°å½•æ‰€æœ‰å¯¹ apiserver æŽ¥å£çš„è°ƒç”¨ï¼Œè®©æˆ‘ä»¬èƒ½å¤Ÿéžå¸¸æ¸…æ™°çš„çŸ¥é“é›†ç¾¤åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆäº‹æƒ…ï¼Œé€šè¿‡è®°å½•çš„æ—¥å¿—å¯ä»¥æŸ¥åˆ°æ‰€å‘ç”Ÿçš„äº‹ä»¶ã€æ“ä½œçš„ç”¨æˆ·å’Œæ—¶é—´ã€‚kubernetes åœ¨ v1.7 ä¸­æ”¯æŒäº†æ—¥å¿—å®¡è®¡åŠŸèƒ½ï¼ˆAlphaï¼‰ï¼Œåœ¨ v1.8 ä¸­ä¸º Beta ç‰ˆæœ¬ï¼Œv1.12 ä¸º GA ç‰ˆæœ¬ã€‚ kubernetes feature-gates ä¸­çš„åŠŸèƒ½ Alpha ç‰ˆæœ¬é»˜è®¤ä¸º falseï¼Œåˆ° Beta ç‰ˆæœ¬æ—¶é»˜è®¤ä¸º trueï¼Œæ‰€ä»¥ v1.8 ä¼šé»˜è®¤å¯ç”¨å®¡è®¡æ—¥å¿—çš„åŠŸèƒ½ã€‚ ä¸€ã€å®¡è®¡æ—¥å¿—çš„ç­–ç•¥1ã€æ—¥å¿—è®°å½•é˜¶æ®µkube-apiserver æ˜¯è´Ÿè´£æŽ¥æ”¶åŠç›¸åº”ç”¨æˆ·è¯·æ±‚çš„ä¸€ä¸ªç»„ä»¶ï¼Œæ¯ä¸€ä¸ªè¯·æ±‚éƒ½ä¼šæœ‰å‡ ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½æœ‰å¯¹åº”çš„æ—¥å¿—ï¼Œå½“å‰æ”¯æŒçš„é˜¶æ®µæœ‰ï¼š RequestReceived - apiserver åœ¨æŽ¥æ”¶åˆ°è¯·æ±‚åŽä¸”åœ¨å°†è¯¥è¯·æ±‚ä¸‹å‘ä¹‹å‰ä¼šç”Ÿæˆå¯¹åº”çš„å®¡è®¡æ—¥å¿—ã€‚ ResponseStarted - åœ¨å“åº” header å‘é€åŽå¹¶åœ¨å“åº” body å‘é€å‰ç”Ÿæˆæ—¥å¿—ã€‚è¿™ä¸ªé˜¶æ®µä»…ä¸ºé•¿æ—¶é—´è¿è¡Œçš„è¯·æ±‚ç”Ÿæˆï¼ˆä¾‹å¦‚ watchï¼‰ã€‚ ResponseComplete - å½“å“åº” body å‘é€å®Œå¹¶ä¸”ä¸å†å‘é€æ•°æ®ã€‚ Panic - å½“æœ‰ panic å‘ç”Ÿæ—¶ç”Ÿæˆã€‚ ä¹Ÿå°±æ˜¯è¯´å¯¹ apiserver çš„æ¯ä¸€ä¸ªè¯·æ±‚ç†è®ºä¸Šä¼šæœ‰ä¸‰ä¸ªé˜¶æ®µçš„å®¡è®¡æ—¥å¿—ç”Ÿæˆã€‚ 2ã€æ—¥å¿—è®°å½•çº§åˆ«å½“å‰æ”¯æŒçš„æ—¥å¿—è®°å½•çº§åˆ«æœ‰ï¼š None - ä¸è®°å½•æ—¥å¿—ã€‚ Metadata - åªè®°å½• Request çš„ä¸€äº› metadata (ä¾‹å¦‚ user, timestamp, resource, verb ç­‰)ï¼Œä½†ä¸è®°å½• Request æˆ– Response çš„bodyã€‚ Request - è®°å½• Request çš„ metadata å’Œ bodyã€‚ RequestResponse - æœ€å…¨è®°å½•æ–¹å¼ï¼Œä¼šè®°å½•æ‰€æœ‰çš„ metadataã€Request å’Œ Response çš„ bodyã€‚ 3ã€æ—¥å¿—è®°å½•ç­–ç•¥åœ¨è®°å½•æ—¥å¿—çš„æ—¶å€™å°½é‡åªè®°å½•æ‰€éœ€è¦çš„ä¿¡æ¯ï¼Œä¸éœ€è¦çš„æ—¥å¿—å°½å¯èƒ½ä¸è®°å½•ï¼Œé¿å…é€ æˆç³»ç»Ÿèµ„æºçš„æµªè´¹ã€‚ ä¸€ä¸ªè¯·æ±‚ä¸è¦é‡å¤è®°å½•ï¼Œæ¯ä¸ªè¯·æ±‚æœ‰ä¸‰ä¸ªé˜¶æ®µï¼Œåªè®°å½•å…¶ä¸­éœ€è¦çš„é˜¶æ®µ ä¸è¦è®°å½•æ‰€æœ‰çš„èµ„æºï¼Œä¸è¦è®°å½•ä¸€ä¸ªèµ„æºçš„æ‰€æœ‰å­èµ„æº ç³»ç»Ÿçš„è¯·æ±‚ä¸éœ€è¦è®°å½•ï¼Œkubeletã€kube-proxyã€kube-schedulerã€kube-controller-manager ç­‰å¯¹ kube-apiserver çš„è¯·æ±‚ä¸éœ€è¦è®°å½• å¯¹ä¸€äº›è®¤è¯ä¿¡æ¯ï¼ˆsecertsã€configmapsã€token ç­‰ï¼‰çš„ body ä¸è®°å½• k8s å®¡è®¡æ—¥å¿—çš„ä¸€ä¸ªç¤ºä¾‹ï¼š 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;kind&quot;: &quot;EventList&quot;, &quot;apiVersion&quot;: &quot;audit.k8s.io/v1beta1&quot;, &quot;Items&quot;: [ &#123; &quot;Level&quot;: &quot;Request&quot;, &quot;AuditID&quot;: &quot;793e7ae2-5ca7-4ad3-a632-19708d2f8265&quot;, &quot;Stage&quot;: &quot;RequestReceived&quot;, &quot;RequestURI&quot;: &quot;/api/v1/namespaces/default/pods/test-pre-sf-de7cc-0&quot;, &quot;Verb&quot;: &quot;get&quot;, &quot;User&quot;: &#123; &quot;Username&quot;: &quot;system:unsecured&quot;, &quot;UID&quot;: &quot;&quot;, &quot;Groups&quot;: [ &quot;system:masters&quot;, &quot;system:authenticated&quot; ], &quot;Extra&quot;: null &#125;, &quot;ImpersonatedUser&quot;: null, &quot;SourceIPs&quot;: [ &quot;192.168.1.11&quot; ], &quot;UserAgent&quot;: &quot;kube-scheduler/v1.12.2 (linux/amd64) kubernetes/73f3294/scheduler&quot;, &quot;ObjectRef&quot;: &#123; &quot;Resource&quot;: &quot;pods&quot;, &quot;Namespace&quot;: &quot;default&quot;, &quot;Name&quot;: &quot;test-pre-sf-de7cc-0&quot;, &quot;UID&quot;: &quot;&quot;, &quot;APIGroup&quot;: &quot;&quot;, &quot;APIVersion&quot;: &quot;v1&quot;, &quot;ResourceVersion&quot;: &quot;&quot;, &quot;Subresource&quot;: &quot;&quot; &#125;, &quot;ResponseStatus&quot;: null, &quot;RequestObject&quot;: null, &quot;ResponseObject&quot;: null, &quot;RequestReceivedTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;, &quot;StageTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;, &quot;Annotations&quot;: null &#125; ]&#125; äºŒã€å¯ç”¨å®¡è®¡æ—¥å¿—å½“å‰çš„å®¡è®¡æ—¥å¿—æ”¯æŒä¸¤ç§æ”¶é›†æ–¹å¼ï¼šä¿å­˜ä¸ºæ—¥å¿—æ–‡ä»¶å’Œè°ƒç”¨è‡ªå®šä¹‰çš„ webhookï¼Œåœ¨ v1.13 ä¸­è¿˜æ”¯æŒåŠ¨æ€çš„ webhookã€‚ 1ã€å°†å®¡è®¡æ—¥å¿—ä»¥ json æ ¼å¼ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶apiserver é…ç½®æ–‡ä»¶çš„ KUBE_API_ARGS ä¸­éœ€è¦æ·»åŠ å¦‚ä¸‹å‚æ•°ï¼š1--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json æ—¥å¿—ä¿å­˜åˆ°æœ¬åœ°åŽå†é€šè¿‡ fluentd ç­‰å…¶ä»–ç»„ä»¶è¿›è¡Œæ”¶é›†ã€‚è¿˜æœ‰å…¶ä»–å‡ ä¸ªé€‰é¡¹å¯ä»¥æŒ‡å®šä¿ç•™å®¡è®¡æ—¥å¿—æ–‡ä»¶çš„æœ€å¤§å¤©æ•°ã€æ–‡ä»¶çš„æœ€å¤§æ•°é‡ã€æ–‡ä»¶çš„å¤§å°ç­‰ã€‚ 2ã€å°†å®¡è®¡æ—¥å¿—æ‰“åˆ°åŽç«¯æŒ‡å®šçš„ webhook1--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig webhook é…ç½®æ–‡ä»¶å®žé™…ä¸Šæ˜¯ä¸€ä¸ª kubeconfigï¼Œapiserver ä¼šå°†å®¡è®¡æ—¥å¿—å‘é€ åˆ°æŒ‡å®šçš„ webhook åŽï¼Œwebhook æŽ¥æ”¶åˆ°æ—¥å¿—åŽå¯ä»¥å†åˆ†å‘åˆ° kafka æˆ–å…¶ä»–ç»„ä»¶è¿›è¡Œæ”¶é›†ã€‚ audit-webhook-kubeconfig ç¤ºä¾‹ï¼š1234567891011121314apiVersion: v1clusters:- cluster: server: http://127.0.0.1:8081/audit/webhook name: metriccontexts:- context: cluster: metric user: &quot;&quot; name: default-contextcurrent-context: default-contextkind: Configpreferences: &#123;&#125;users: [] å‰é¢æåˆ°è¿‡ï¼Œapiserver çš„æ¯ä¸€ä¸ªè¯·æ±‚ä¼šè®°å½•ä¸‰ä¸ªé˜¶æ®µçš„å®¡è®¡æ—¥å¿—ï¼Œä½†æ˜¯åœ¨å®žé™…ä¸­å¹¶ä¸æ˜¯éœ€è¦æ‰€æœ‰çš„å®¡è®¡æ—¥å¿—ï¼Œå®˜æ–¹ä¹Ÿè¯´æ˜Žäº†å¯ç”¨å®¡è®¡æ—¥å¿—ä¼šå¢žåŠ  apiserver å¯¹å†…å­˜çš„ä½¿ç”¨é‡ã€‚ Note: The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration. audit-policy.yaml é…ç½®ç¤ºä¾‹ï¼š 123456789101112131415161718192021222324apiVersion: audit.k8s.io/v1kind: Policy# ResponseStarted é˜¶æ®µä¸è®°å½•omitStages: - &quot;ResponseStarted&quot;rules: # è®°å½•ç”¨æˆ·å¯¹ pod å’Œ statefulset çš„æ“ä½œ - level: RequestResponse resources: - group: &quot;&quot; resources: [&quot;pods&quot;,&quot;pods/status&quot;] - group: &quot;apps&quot; resources: [&quot;statefulsets&quot;,&quot;statefulsets/scale&quot;] # kube-controller-managerã€kube-scheduler ç­‰å·²ç»è®¤è¯è¿‡èº«ä»½çš„è¯·æ±‚ä¸éœ€è¦è®°å½• - level: None userGroups: [&quot;system:authenticated&quot;] nonResourceURLs: - &quot;/api*&quot; - &quot;/version&quot; # å¯¹ configã€secretã€token ç­‰è®¤è¯ä¿¡æ¯ä¸è®°å½•è¯·æ±‚ä½“å’Œè¿”å›žä½“ - level: Metadata resources: - group: &quot;&quot; # core API group resources: [&quot;secrets&quot;, &quot;configmaps&quot;] å®˜æ–¹æä¾›ä¸¤ä¸ªå‚è€ƒç¤ºä¾‹ï¼š Use fluentd to collect and distribute audit events from log file Use logstash to collect and distribute audit events from webhook backend 3ã€subresource è¯´æ˜Žkubernetes æ¯ä¸ªèµ„æºå¯¹è±¡éƒ½æœ‰ subresource,é€šè¿‡è°ƒç”¨ master çš„ api å¯ä»¥èŽ·å– kubernetes ä¸­æ‰€æœ‰çš„ resource ä»¥åŠå¯¹åº”çš„ subresource,æ¯”å¦‚ pod æœ‰ logsã€exec ç­‰ subresourceã€‚ 12èŽ·å–æ‰€æœ‰ resourceï¼ˆ 1.10 ä¹‹åŽä½¿ç”¨ï¼‰ï¼š$ curl 127.0.0.1:8080/openapi/v2 å‚è€ƒï¼šhttps://kubernetes.io/docs/concepts/overview/kubernetes-api/ ä¸‰ã€webhook çš„ä¸€ä¸ªç®€å•ç¤ºä¾‹123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package mainimport ( &quot;encoding/json&quot; &quot;io/ioutil&quot; &quot;log&quot; &quot;net/http&quot; &quot;github.com/emicklei/go-restful&quot; &quot;github.com/gosoon/glog&quot; &quot;k8s.io/apiserver/pkg/apis/audit&quot;)func main() &#123; // NewContainer creates a new Container using a new ServeMux and default router (CurlyRouter) container := restful.NewContainer() ws := new(restful.WebService) ws.Path(&quot;/audit&quot;). Consumes(restful.MIME_JSON). Produces(restful.MIME_JSON) ws.Route(ws.POST(&quot;/webhook&quot;).To(AuditWebhook)) //WebService ws2è¢«æ·»åŠ åˆ°container2ä¸­ container.Add(ws) server := &amp;http.Server&#123; Addr: &quot;:8081&quot;, Handler: container, &#125; //go consumer() log.Fatal(server.ListenAndServe())&#125;func AuditWebhook(req *restful.Request, resp *restful.Response) &#123; body, err := ioutil.ReadAll(req.Request.Body) if err != nil &#123; glog.Errorf(&quot;read body err is: %v&quot;, err) &#125; var eventList audit.EventList err = json.Unmarshal(body, &amp;eventList) if err != nil &#123; glog.Errorf(&quot;unmarshal failed with:%v,body is :\n&quot;, err, string(body)) return &#125; for _, event := range eventList.Items &#123; jsonBytes, err := json.Marshal(event) if err != nil &#123; glog.Infof(&quot;marshal failed with:%v,event is \n %+v&quot;, err, event) &#125; // æ¶ˆè´¹æ—¥å¿— asyncProducer(string(jsonBytes)) &#125; resp.AddHeader(&quot;Content-Type&quot;, &quot;application/json&quot;) resp.WriteEntity(&quot;success&quot;)&#125; å®Œæ•´ä»£ç è¯·å‚è€ƒï¼šhttps://github.com/gosoon/k8s-audit-webhook å››ã€æ€»ç»“æœ¬æ–‡ä¸»è¦ä»‹ç»äº† kubernetes çš„æ—¥å¿—å®¡è®¡åŠŸèƒ½ï¼Œkubernetes æœ€è¿‘ä¹Ÿè¢«çˆ†å‡ºå¤šä¸ªå®‰å…¨æ¼æ´žï¼Œå®‰å…¨é—®é¢˜æ˜¯æ¯ä¸ªå›¢é˜Ÿä¸å¯å¿½è§†çš„ï¼Œkubernetes è™½ç„¶è¢«å¤šæ•°å…¬å¸ç”¨ä½œç§æœ‰äº‘ï¼Œä½†æ—¥å¿—å®¡è®¡ä¹Ÿæ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚ å‚è€ƒï¼šhttps://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/ttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/é˜¿é‡Œäº‘ Kubernetes å®¡è®¡æ—¥å¿—æ–¹æ¡ˆ]]></content>
      <tags>
        <tag>audit</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubeadm å®‰è£… kubernetes]]></title>
    <url>%2F2019%2F01%2F17%2Fkubeadm%2F</url>
    <content type="text"><![CDATA[kubeadm æ˜¯ Kubernetes ä¸»æŽ¨çš„éƒ¨ç½²å·¥å…·ä¹‹ä¸€ï¼Œæ­£åœ¨å¿«é€Ÿè¿­ä»£å¼€å‘ä¸­ï¼Œå½“å‰ç‰ˆæœ¬ä¸º GAï¼Œæš‚ä¸å»ºè®®ç”¨äºŽéƒ¨ç½²ç”Ÿäº§çŽ¯å¢ƒï¼Œå…¶å…ˆè¿›çš„è®¾è®¡ç†å¿µå¯ä»¥å€Ÿé‰´ã€‚ ä¸€ã€kubeadm åŽŸç†ä»‹ç»kubeadm ä¼šåœ¨åˆå§‹åŒ–çš„æœºå™¨ä¸Šé¦–å…ˆéƒ¨ç½² kubelet æœåŠ¡ï¼Œkubelet åˆ›å»º pod çš„æ–¹å¼æœ‰ä¸‰ç§ï¼Œå…¶ä¸­ä¸€ç§å°±æ˜¯ç›‘æŽ§æŒ‡å®šç›®ä¸‹ï¼ˆ/etc/kubernetes/manifestsï¼‰å®¹å™¨çŠ¶æ€çš„å˜åŒ–ç„¶åŽè¿›è¡Œç›¸åº”çš„æ“ä½œã€‚kubeadm å¯åŠ¨ kubelet åŽä¼šåœ¨ /etc/kubernetes/manifests ç›®å½•ä¸‹åˆ›å»ºå‡º etcdã€kube-apiserverã€kube-controller-managerã€kube-scheduler å››ä¸ªç»„ä»¶ static pod çš„ yaml æ–‡ä»¶ï¼Œæ­¤æ—¶ kubelet ç›‘æµ‹åˆ°è¯¥ç›®å½•ä¸‹æœ‰ yaml æ–‡ä»¶ä¾¿ä¼šå°†å…¶åˆ›å»ºä¸ºå¯¹åº”çš„ podï¼Œæœ€ç»ˆ kube-apiserverã€kube-controller-managerã€kube-scheduler ä»¥åŠ etcd ä¼šä»¥ static pod çš„æ–¹å¼è¿è¡Œã€‚ æœ¬æ¬¡å®‰è£… kubernetes ç‰ˆæœ¬ï¼šv1.12.0 å½“å‰å®¿ä¸»æœºç³»ç»Ÿä¸Žå†…æ ¸ç‰ˆæœ¬ï¼š12345$ uname -r3.10.0-514.16.1.el7.x86_64$ cat /etc/redhat-releaseCentOS Linux release 7.2.1511 (Core) äºŒã€å®‰è£…å‰çš„å‡†å¤‡å·¥ä½œ1234567891011121314151617# å…³é—­swap$ sudo swapoff -a# å…³é—­selinux$ sed -i &apos;s/SELINUX=permissive/SELINUX=disabled/&apos; /etc/sysconfig/selinux $ setenforce 0# å…³é—­é˜²ç«å¢™$ systemctl disable firewalld.service &amp;&amp; systemctl stop firewalld.service# é…ç½®è½¬å‘ç›¸å…³å‚æ•°$ cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOF $ sysctl -p ä¸‰ã€å®‰è£… Docker CE æœ¬æ¬¡å®‰è£…çš„ docker ç‰ˆæœ¬ï¼šdocker-ce-18.06.1.ce 123456789101112131415161718192021222324252627282930313233343536# Install Docker CE## Set up the repository### Install required packages.yum install yum-utils device-mapper-persistent-data lvm2### Add docker repository.yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo## Install docker ce.yum update &amp;&amp; yum install docker-ce-18.06.1.ce## Create /etc/docker directory.mkdir /etc/docker# Setup daemon.cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;100m&quot; &#125;, &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; ]&#125;EOFmkdir -p /etc/systemd/system/docker.service.d# Restart docker.systemctl daemon-reloadsystemctl restart docker å‚è€ƒï¼šhttps://kubernetes.io/docs/setup/cri/ å››ã€å®‰è£… kubernetes master ç»„ä»¶ä½¿ç”¨ kubeadm åˆå§‹åŒ–é›†ç¾¤ï¼š12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758$ kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16[init] using Kubernetes version: v1.12.0[preflight] running pre-flight checks[preflight/images] Pulling images required for setting up a Kubernetes cluster[preflight/images] This might take a minute or two, depending on the speed of your internet connection[preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[preflight] Activating the kubelet service[certificates] Using the existing front-proxy-client certificate and key.[certificates] Using the existing etcd/server certificate and key.[certificates] Using the existing etcd/peer certificate and key.[certificates] Using the existing etcd/healthcheck-client certificate and key.[certificates] Using the existing apiserver-etcd-client certificate and key.[certificates] Using the existing apiserver certificate and key.[certificates] Using the existing apiserver-kubelet-client certificate and key.[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;[certificates] Using the existing sa key.[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/admin.conf&quot;[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/kubelet.conf&quot;[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/controller-manager.conf&quot;[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/scheduler.conf&quot;[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;[init] this might take a minute or longer if the control plane images have to be pulled[apiclient] All control plane components are healthy after 14.002350 seconds[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster[markmaster] Marking the node 192.168.1.110 as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;[markmaster] Marking the node 192.168.1.110 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule][patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;192.168.1.110&quot; as an annotation[bootstraptoken] using token: wu5hfy.lkuz9fih6hlqe1jt[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash sha256:e8d2649fceae9d7f6de94af0b7e294680b87f7d1e207c75c3cb496841b12ec23 è¿™ä¸ªå‘½ä»¤ä¼šè‡ªåŠ¨æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š ç³»ç»ŸçŠ¶æ€æ£€æŸ¥ ç”Ÿæˆ token ç”Ÿæˆè‡ªç­¾å CA å’Œ client ç«¯è¯ä¹¦ ç”Ÿæˆ kubeconfig ç”¨äºŽ kubelet è¿žæŽ¥ API server ä¸º Master ç»„ä»¶ç”Ÿæˆ Static Pod manifestsï¼Œå¹¶æ”¾åˆ° /etc/kubernetes/manifests ç›®å½•ä¸­ é…ç½® RBAC å¹¶è®¾ç½® Master node åªè¿è¡ŒæŽ§åˆ¶å¹³é¢ç»„ä»¶ åˆ›å»ºé™„åŠ æœåŠ¡ï¼Œæ¯”å¦‚ kube-proxy å’Œ CoreDNS é…ç½® kubetl è®¤è¯ä¿¡æ¯ï¼š123$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config å°†æœ¬æœºä½œä¸º node åŠ å…¥åˆ° master ä¸­ï¼š1$ kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash kubeadm é»˜è®¤ master èŠ‚ç‚¹ä¸ä½œä¸º node èŠ‚ç‚¹ä½¿ç”¨ï¼Œåˆå§‹åŒ–å®ŒæˆåŽä¼šç»™ master èŠ‚ç‚¹æ‰“ä¸Š taint æ ‡ç­¾ï¼Œè‹¥å•æœºéƒ¨ç½²ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åŽ»æŽ‰ taint æ ‡ç­¾ï¼š1$ kubectl taint nodes --all node-role.kubernetes.io/master- æŸ¥çœ‹å„ç»„ä»¶æ˜¯å¦æ­£å¸¸è¿è¡Œï¼š 123456789$ kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-99b9bb8bd-pgh5t 1/1 Running 0 48metcd 1/1 Running 2 48mkube-apiserver 1/1 Running 1 48mkube-controller-manager 1/1 Running 0 49mkube-flannel-ds-amd64-b5rjg 1/1 Running 0 31mkube-proxy-c8ktg 1/1 Running 0 48mkube-scheduler 1/1 Running 2 48m äº”ã€å®‰è£… kubernetes ç½‘ç»œkubernetes æœ¬èº«æ˜¯ä¸æä¾›ç½‘ç»œæ–¹æ¡ˆçš„ï¼Œä½†æ˜¯æœ‰å¾ˆå¤šå¼€æºç»„ä»¶å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ‰“é€šå®¹å™¨å’Œå®¹å™¨ä¹‹é—´çš„ç½‘ç»œï¼Œå®žçŽ° Kubernetes è¦æ±‚çš„ç½‘ç»œæ¨¡åž‹ã€‚ä»Žå®žçŽ°åŽŸç†ä¸Šæ¥è¯´å¤§è‡´åˆ†ä¸ºä»¥ä¸‹ä¸¤ç§ï¼š overlay ç½‘ç»œï¼Œé€šè¿‡å°åŒ…è§£åŒ…çš„æ–¹å¼æž„é€ ä¸€ä¸ªéš§é“ï¼Œä»£è¡¨çš„æ–¹æ¡ˆæœ‰ flannel(udp/vxlanï¼‰ã€weaveã€calico(ipip)ï¼Œopenvswitch ç­‰ é€šè¿‡è·¯ç”±æ¥å®žçŽ°(æ›´æ”¹ iptables ç­‰æ‰‹æ®µ)ï¼Œflannel(host-gw)ï¼Œcalico(bgp)ï¼Œmacvlan ç­‰ å½“ç„¶æ¯ç§æ–¹æ¡ˆéƒ½æœ‰è‡ªå·±é€‚åˆçš„åœºæ™¯ï¼Œflannel å’Œ calico æ˜¯ä¸¤ç§æœ€å¸¸è§çš„ç½‘ç»œæ–¹æ¡ˆï¼Œæˆ‘ä»¬è¦æ ¹æ®è‡ªå·±çš„å®žé™…éœ€è¦è¿›è¡Œé€‰æ‹©ã€‚æ­¤æ¬¡å®‰è£…é€‰æ‹© flannel ç½‘ç»œï¼š æ­¤æ“ä½œä¹Ÿä¼šä¸º flannel åˆ›å»ºå¯¹åº”çš„ RBAC è§„åˆ™ï¼Œflannel ä¼šä»¥ daemonset çš„æ–¹å¼åˆ›å»ºå‡ºæ¥ï¼š1$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml åˆ›å»ºä¸€ä¸ª pod éªŒè¯é›†ç¾¤æ˜¯å¦æ­£å¸¸ï¼š 123456789101112apiVersion: v1kind: Podmetadata: name: nginx labels: name: nginxspec: containers: - name: nginx image: nginx ports: - containerPort: 80 å…­ã€kubeadm å…¶ä»–ç›¸å…³çš„æ“ä½œ1ã€åˆ é™¤å®‰è£…:1$ kubeadm reset 2ã€ç‰ˆæœ¬å‡çº§12345# æŸ¥çœ‹å¯å‡çº§çš„ç‰ˆæœ¬$ kubeadm upgrade plan# å‡çº§è‡³æŒ‡å®šç‰ˆæœ¬$ kubeadm upgrade apply [version] è¦æ‰§è¡Œå‡çº§ï¼Œéœ€è¦å…ˆå°† kubeadm å‡çº§åˆ°å¯¹åº”çš„ç‰ˆæœ¬ï¼› kubeadm å¹¶ä¸è´Ÿè´£ kubelet çš„å‡çº§ï¼Œéœ€è¦åœ¨å‡çº§å®Œ master ç»„ä»¶åŽï¼Œæ‰‹å·¥å¯¹ kubelet è¿›è¡Œå‡çº§ã€‚ ä¸ƒã€åˆ›å»ºè¿‡ç¨‹ä¸­çš„ä¸€äº› case è®°å½•1ã€flannel å®¹å™¨å¯åŠ¨æŠ¥é”™ï¼špod cidr not assgnedéœ€è¦åœ¨ /etc/kubernetes/manifests/kube-controller-manager.yaml æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹é…ç½®ï¼š â€“allocate-node-cidrs=trueâ€“cluster-cidr=10.244.0.0/16 å‚è€ƒï¼šhttps://github.com/coreos/flannel/issues/728 2ã€coredns å®¹å™¨å¯åŠ¨å¤±è´¥æŠ¥é”™ï¼š/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory123$ vim /etc/default/grub and change the value of kernel parameter ipv6.disable from 1 to 0 in line$ grub2-mkconfig -o /boot/grub2/grub.cfg$ shutdown -r now å‚è€ƒï¼šhttps://github.com/containernetworking/cni/issues/569 3ã€kubeadm è¯ä¹¦æœ‰æ•ˆæœŸé—®é¢˜é»˜è®¤æƒ…å†µä¸‹ï¼Œkubeadm ä¼šç”Ÿæˆé›†ç¾¤è¿è¡Œæ‰€éœ€çš„æ‰€æœ‰è¯ä¹¦ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡æä¾›è‡ªå·±çš„è¯ä¹¦æ¥è¦†ç›–æ­¤è¡Œä¸ºã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œå¿…é¡»æŠŠå®ƒä»¬æ”¾åœ¨ â€“cert-dir å‚æ•°æˆ–è€…é…ç½®æ–‡ä»¶ä¸­çš„ CertificatesDir æŒ‡å®šçš„ç›®å½•ï¼ˆé»˜è®¤ç›®å½•ä¸º /etc/kubernetes/pkiï¼‰ï¼Œå¦‚æžœå­˜åœ¨ä¸€ä¸ªç»™å®šçš„è¯ä¹¦å’Œå¯†é’¥å¯¹ï¼Œkubeadm å°†ä¼šè·³è¿‡ç”Ÿæˆæ­¥éª¤å¹¶ä¸”ä½¿ç”¨å·²å­˜åœ¨çš„æ–‡ä»¶ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥æ‹·è´ä¸€ä¸ªå·²æœ‰çš„ CA åˆ° /etc/kubernetes/pki/ca.crt å’Œ /etc/kubernetes/pki/ca.keyï¼Œkubeadm å°†ä¼šä½¿ç”¨è¿™ä¸ª CA æ¥ç­¾ç½²å…¶ä½™çš„è¯ä¹¦ã€‚æ‰€ä»¥åªè¦æˆ‘ä»¬è‡ªå·±æä¾›ä¸€ä¸ªæœ‰æ•ˆæœŸå¾ˆé•¿çš„è¯ä¹¦åŽ»è¦†ç›–æŽ‰é»˜è®¤çš„è¯ä¹¦å°±å¯ä»¥æ¥é¿å…è¿™ä¸ªçš„é—®é¢˜ã€‚ 4ã€kubeadm join æ—¶ token æ— æ³•ç”Ÿæ•ˆtoken çš„å¤±æ•ˆä¸º24å°æ—¶ï¼Œè‹¥å¿˜è®°æˆ–è€… token è¿‡æœŸå¯ä»¥ä½¿ç”¨ kubeadm token create é‡æ–°ç”Ÿæˆ tokenã€‚ å…«ã€æ€»ç»“æœ¬ç¯‡æ–‡ç« è®²è¿°äº†ä½¿ç”¨ kubeadm æ¥æ­å»ºä¸€ä¸ª kubernetes é›†ç¾¤ï¼Œkubeadm æš‚æ—¶è¿˜ä¸å»ºè®®ç”¨äºŽç”Ÿäº§çŽ¯å¢ƒï¼Œè‹¥éƒ¨ç½²ç”Ÿäº§çŽ¯å¢ƒè¯·ä½¿ç”¨äºŒè¿›åˆ¶æ–‡ä»¶ã€‚kubeadm æ­å»ºå‡ºçš„é›†ç¾¤è¿˜æ˜¯æœ‰å¾ˆå¤šä¸å®Œå–„çš„åœ°æ–¹ï¼Œæ¯”å¦‚ï¼Œé›†ç¾¤ master ç»„ä»¶çš„å‚æ•°é…ç½®é—®é¢˜ï¼Œå®˜æ–¹é»˜è®¤çš„å¹¶ä¸ä¼šæ»¡è¶³éœ€æ±‚ï¼Œæœ‰è®¸å¤šå‚æ•°éœ€è¦æ ¹æ®å®žé™…æƒ…å†µè¿›è¡Œä¿®æ”¹ã€‚ å‚è€ƒï¼šCreating a single master cluster with kubeadmkubeadm å·¥ä½œåŽŸç†DockOneå¾®ä¿¡åˆ†äº«ï¼ˆä¸€å…­ä¸‰ï¼‰ï¼šKuberneteså®˜æ–¹é›†ç¾¤éƒ¨ç½²å·¥å…·kubeadmåŽŸç†è§£æžcentos7.2 å®‰è£…k8s v1.11.0]]></content>
      <tags>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes ä¸­ kubeconfig çš„ç”¨æ³•]]></title>
    <url>%2F2019%2F01%2F09%2Fkubeconfig%2F</url>
    <content type="text"><![CDATA[ç”¨äºŽé…ç½®é›†ç¾¤è®¿é—®ä¿¡æ¯çš„æ–‡ä»¶å«ä½œ kubeconfig æ–‡ä»¶ï¼Œåœ¨å¼€å¯äº† TLS çš„é›†ç¾¤ä¸­ï¼Œæ¯æ¬¡ä¸Žé›†ç¾¤äº¤äº’æ—¶éƒ½éœ€è¦èº«ä»½è®¤è¯ï¼Œç”Ÿäº§çŽ¯å¢ƒä¸€èˆ¬ä½¿ç”¨è¯ä¹¦è¿›è¡Œè®¤è¯ï¼Œå…¶è®¤è¯æ‰€éœ€è¦çš„ä¿¡æ¯ä¼šæ”¾åœ¨ kubeconfig æ–‡ä»¶ä¸­ã€‚æ­¤å¤–ï¼Œk8s çš„ç»„ä»¶éƒ½å¯ä»¥ä½¿ç”¨ kubeconfig è¿žæŽ¥ apiserverï¼Œclient-go ã€operatorã€helm ç­‰å…¶ä»–ç»„ä»¶ä¹Ÿä½¿ç”¨ kubeconfig è®¿é—® apiserverã€‚ ä¸€ã€kubeconfig é…ç½®æ–‡ä»¶çš„ç”Ÿæˆkubeconfig çš„ä¸€ä¸ªç¤ºä¾‹ï¼š123456789101112131415161718192021222324252627apiVersion: v1clusters:- cluster: certificate-authority-data: xxx server: https://xxx:6443 name: cluster1- cluster: certificate-authority-data: xxx server: https://xxx:6443 name: cluster2contexts:- context: cluster: cluster1 user: kubelet name: cluster1-context- context: cluster: cluster2 user: kubelet name: cluster2-contextcurrent-context: cluster1-contextkind: Configpreferences: &#123;&#125;users:- name: kubelet user: client-certificate-data: xxx client-key-data: xxx apiVersion å’Œ kind æ ‡è¯†å®¢æˆ·ç«¯è§£æžå™¨çš„ç‰ˆæœ¬å’Œæ¨¡å¼ï¼Œä¸åº”æ‰‹åŠ¨ç¼–è¾‘ã€‚ preferences æŒ‡å®šå¯é€‰ï¼ˆå’Œå½“å‰æœªä½¿ç”¨ï¼‰çš„ kubectl é¦–é€‰é¡¹ã€‚ 1ã€clustersæ¨¡å—clusterä¸­åŒ…å« kubernetes é›†ç¾¤çš„ç«¯ç‚¹æ•°æ®ï¼ŒåŒ…æ‹¬ kubernetes apiserver çš„å®Œæ•´ url ä»¥åŠé›†ç¾¤çš„è¯ä¹¦é¢å‘æœºæž„ã€‚ å¯ä»¥ä½¿ç”¨ kubectl config set-cluster æ·»åŠ æˆ–ä¿®æ”¹ cluster æ¡ç›®ã€‚ 2ã€users æ¨¡å—user å®šä¹‰ç”¨äºŽå‘ kubernetes é›†ç¾¤è¿›è¡Œèº«ä»½éªŒè¯çš„å®¢æˆ·ç«¯å‡­æ®ã€‚ å¯ç”¨å‡­è¯æœ‰ client-certificateã€client-keyã€token å’Œ username/passwordã€‚username/password å’Œ token æ˜¯äºŒè€…åªèƒ½é€‰æ‹©ä¸€ä¸ªï¼Œä½† client-certificate å’Œ client-key å¯ä»¥åˆ†åˆ«ä¸Žå®ƒä»¬ç»„åˆã€‚ å¯ä»¥ä½¿ç”¨ kubectl config set-credentials æ·»åŠ æˆ–è€…ä¿®æ”¹ user æ¡ç›®ã€‚ 3ã€contexts æ¨¡å—context å®šä¹‰äº†ä¸€ä¸ªå‘½åçš„clusterã€userã€namespaceå…ƒç»„ï¼Œç”¨äºŽä½¿ç”¨æä¾›çš„è®¤è¯ä¿¡æ¯å’Œå‘½åç©ºé—´å°†è¯·æ±‚å‘é€åˆ°æŒ‡å®šçš„é›†ç¾¤ã€‚ ä¸‰ä¸ªéƒ½æ˜¯å¯é€‰çš„ï¼›ä»…ä½¿ç”¨ clusterã€userã€namespace ä¹‹ä¸€æŒ‡å®šä¸Šä¸‹æ–‡ï¼Œæˆ–æŒ‡å®šnoneã€‚ æœªæŒ‡å®šçš„å€¼æˆ–åœ¨åŠ è½½çš„ kubeconfig ä¸­æ²¡æœ‰ç›¸åº”æ¡ç›®çš„å‘½åå€¼å°†è¢«æ›¿æ¢ä¸ºé»˜è®¤å€¼ã€‚åŠ è½½å’Œåˆå¹¶ kubeconfig æ–‡ä»¶çš„è§„åˆ™å¾ˆç®€å•ï¼Œä½†æœ‰å¾ˆå¤šï¼Œå…·ä½“å¯ä»¥æŸ¥çœ‹åŠ è½½å’Œåˆå¹¶kubeconfigè§„åˆ™ã€‚ å¯ä»¥ä½¿ç”¨kubectl config set-contextæ·»åŠ æˆ–ä¿®æ”¹ä¸Šä¸‹æ–‡æ¡ç›®ã€‚ 4ã€current-context æ¨¡å—current-context æ˜¯ä½œä¸ºclusterã€userã€namespaceå…ƒç»„çš„ keyï¼Œå½“ kubectl ä»Žè¯¥æ–‡ä»¶ä¸­åŠ è½½é…ç½®çš„æ—¶å€™ä¼šè¢«é»˜è®¤ä½¿ç”¨ã€‚ å¯ä»¥åœ¨ kubectl å‘½ä»¤è¡Œé‡Œè¦†ç›–è¿™äº›å€¼ï¼Œé€šè¿‡åˆ†åˆ«ä¼ å…¥--context=CONTEXTã€--cluster=CLUSTERã€--user=USER å’Œ --namespace=NAMESPACEã€‚ä»¥ä¸Šç¤ºä¾‹ä¸­è‹¥ä¸æŒ‡å®š context åˆ™é»˜è®¤ä½¿ç”¨ cluster1-contextã€‚1kubectl get node --kubeconfig=./kubeconfig --context=cluster2-context å¯ä»¥ä½¿ç”¨ kubectl config use-context æ›´æ”¹ current-contextã€‚ 5ã€kubectl ç”Ÿæˆ kubeconfig çš„ç¤ºä¾‹kubectl å¯ä»¥å¿«é€Ÿç”Ÿæˆ kubeconfigï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š123456$ kubectl config set-credentials myself --username=admin --password=secret$ kubectl config set-cluster local-server --server=http://localhost:8080$ kubectl config set-context default-context --cluster=local-server --user=myself$ kubectl config use-context cluster-context$ kubectl config set contexts.default-context.namespace the-right-prefix$ kubectl config view è‹¥ä½¿ç”¨æ‰‹å†™ kubeconfig çš„æ–¹å¼ï¼ŒæŽ¨èä¸€ä¸ªå·¥å…· kubevalï¼Œå¯ä»¥æ ¡éªŒ kubernetes yaml æˆ– json æ ¼å¼çš„é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡®ã€‚ äºŒã€ä½¿ç”¨ kubeconfig æ–‡ä»¶é…ç½® kuebctl è·¨é›†ç¾¤è®¤è¯kubectl ä½œä¸ºæ“ä½œ k8s çš„ä¸€ä¸ªå®¢æˆ·ç«¯å·¥å…·ï¼Œåªè¦ä¸º kubectl æä¾›è¿žæŽ¥ apiserver çš„é…ç½®(kubeconfig)ï¼Œkubectl å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹æ“ä½œè¯¥é›†ç¾¤ï¼Œå½“ç„¶ï¼Œè‹¥ kubeconfig æ–‡ä»¶ä¸­é…ç½®å¤šä¸ªé›†ç¾¤ï¼Œkubectl ä¹Ÿå¯ä»¥è½»æ¾åœ°åœ¨å¤šä¸ªé›†ç¾¤ä¹‹é—´åˆ‡æ¢ã€‚ kubectl åŠ è½½é…ç½®æ–‡ä»¶çš„é¡ºåºï¼š1ã€kubectl é»˜è®¤è¿žæŽ¥æœ¬æœºçš„ 8080 ç«¯å£2ã€ä»Ž $HOME/.kube ç›®å½•ä¸‹æŸ¥æ‰¾æ–‡ä»¶åä¸º config çš„æ–‡ä»¶3ã€é€šè¿‡è®¾ç½®çŽ¯å¢ƒå˜é‡ KUBECONFIG æˆ–è€…é€šè¿‡è®¾ç½®åŽ»æŒ‡å®šå…¶å®ƒ kubeconfig æ–‡ä»¶123456# è®¾ç½® KUBECONFIG çš„çŽ¯å¢ƒå˜é‡export KUBECONFIG=/etc/kubernetes/kubeconfig/kubelet.kubeconfig# æŒ‡å®š kubeconfig æ–‡ä»¶kubectl get node --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig# ä½¿ç”¨ä¸åŒçš„ context åœ¨å¤šä¸ªé›†ç¾¤ä¹‹é—´åˆ‡æ¢kubectl get node --kubeconfig=./kubeconfig --context=cluster1-context å¼€ç¯‡çš„ç¤ºä¾‹å°±æ˜¯å¤šé›†ç¾¤è®¤è¯æ–¹å¼é…ç½®çš„ä¸€ç§ã€‚ å‚è€ƒï¼šhttps://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/]]></content>
      <tags>
        <tag>kubeconfig</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet åˆ›å»º pod çš„æµç¨‹]]></title>
    <url>%2F2019%2F01%2F03%2Fkubelet_create_pod%2F</url>
    <content type="text"><![CDATA[ä¸Šç¯‡æ–‡ç« ä»‹ç»äº† kubelet çš„å¯åŠ¨æµç¨‹ï¼Œæœ¬ç¯‡æ–‡ç« ä¸»è¦ä»‹ç» kubelet åˆ›å»º pod çš„æµç¨‹ã€‚ kubernetes ç‰ˆæœ¬ï¼š v1.12 kubelet çš„å·¥ä½œæ ¸å¿ƒå°±æ˜¯åœ¨å›´ç»•ç€ä¸åŒçš„ç”Ÿäº§è€…ç”Ÿäº§å‡ºæ¥çš„ä¸åŒçš„æœ‰å…³ pod çš„æ¶ˆæ¯æ¥è°ƒç”¨ç›¸åº”çš„æ¶ˆè´¹è€…ï¼ˆä¸åŒçš„å­æ¨¡å—ï¼‰å®Œæˆä¸åŒçš„è¡Œä¸º(åˆ›å»ºå’Œåˆ é™¤ pod ç­‰)ï¼Œå³å›¾ä¸­çš„æŽ§åˆ¶å¾ªçŽ¯ï¼ˆSyncLoopï¼‰ï¼Œé€šè¿‡ä¸åŒçš„äº‹ä»¶é©±åŠ¨è¿™ä¸ªæŽ§åˆ¶å¾ªçŽ¯è¿è¡Œã€‚ æœ¬æ–‡ä»…åˆ†æžæ–°å»º pod çš„æµç¨‹ï¼Œå½“ä¸€ä¸ª pod å®Œæˆè°ƒåº¦ï¼Œä¸Žä¸€ä¸ª node ç»‘å®šèµ·æ¥ä¹‹åŽï¼Œè¿™ä¸ª pod å°±ä¼šè§¦å‘ kubelet åœ¨å¾ªçŽ¯æŽ§åˆ¶é‡Œæ³¨å†Œçš„ handlerï¼Œä¸Šå›¾ä¸­çš„ HandlePods éƒ¨åˆ†ã€‚æ­¤æ—¶ï¼Œé€šè¿‡æ£€æŸ¥ pod åœ¨ kubelet å†…å­˜ä¸­çš„çŠ¶æ€ï¼Œkubelet å°±èƒ½åˆ¤æ–­å‡ºè¿™æ˜¯ä¸€ä¸ªæ–°è°ƒåº¦è¿‡æ¥çš„ podï¼Œä»Žè€Œè§¦å‘ Handler é‡Œçš„ ADD äº‹ä»¶å¯¹åº”çš„é€»è¾‘å¤„ç†ã€‚ç„¶åŽ kubelet ä¼šä¸ºè¿™ä¸ª pod ç”Ÿæˆå¯¹åº”çš„ podStatusï¼ŒæŽ¥ç€æ£€æŸ¥ pod æ‰€å£°æ˜Žçš„ volume æ˜¯ä¸æ˜¯å‡†å¤‡å¥½äº†ï¼Œç„¶åŽè°ƒç”¨ä¸‹å±‚çš„å®¹å™¨è¿è¡Œæ—¶ã€‚å¦‚æžœæ˜¯ update äº‹ä»¶çš„è¯ï¼Œkubelet å°±ä¼šæ ¹æ® pod å¯¹è±¡å…·ä½“çš„å˜æ›´æƒ…å†µï¼Œè°ƒç”¨ä¸‹å±‚çš„å®¹å™¨è¿è¡Œæ—¶è¿›è¡Œå®¹å™¨çš„é‡å»ºã€‚ kubelet åˆ›å»º pod çš„æµç¨‹ 1ã€kubelet çš„æŽ§åˆ¶å¾ªçŽ¯ï¼ˆsyncLoopï¼‰syncLoop ä¸­é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ª syncTicker å’Œ housekeepingTickerï¼Œå³ä½¿æ²¡æœ‰éœ€è¦æ›´æ–°çš„ pod é…ç½®ï¼Œkubelet ä¹Ÿä¼šå®šæ—¶åŽ»åšåŒæ­¥å’Œæ¸…ç† pod çš„å·¥ä½œã€‚ç„¶åŽåœ¨ for å¾ªçŽ¯ä¸­ä¸€ç›´è°ƒç”¨ syncLoopIterationï¼Œå¦‚æžœåœ¨æ¯æ¬¡å¾ªçŽ¯è¿‡ç¨‹ä¸­å‡ºçŽ°æ¯”è¾ƒä¸¥é‡çš„é”™è¯¯ï¼Œkubelet ä¼šè®°å½•åˆ° runtimeState ä¸­ï¼Œé‡åˆ°é”™è¯¯å°±ç­‰å¾… 5 ç§’ä¸­ç»§ç»­å¾ªçŽ¯ã€‚ 12345678910111213141516171819202122232425262728293031323334func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123; glog.Info(&quot;Starting kubelet main sync loop.&quot;) // syncTicker æ¯ç§’æ£€æµ‹ä¸€æ¬¡æ˜¯å¦æœ‰éœ€è¦åŒæ­¥çš„ pod workers syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() // æ¯ä¸¤ç§’æ£€æµ‹ä¸€æ¬¡æ˜¯å¦æœ‰éœ€è¦æ¸…ç†çš„ pod housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() // pod çš„ç”Ÿå‘½å‘¨æœŸå˜åŒ– plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for &#123; if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123; time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue &#125; ... kl.syncLoopMonitor.Store(kl.clock.Now()) // ç¬¬äºŒä¸ªå‚æ•°ä¸º SyncHandler ç±»åž‹ï¼ŒSyncHandler æ˜¯ä¸€ä¸ª interfaceï¼Œ // åœ¨è¯¥æ–‡ä»¶å¼€å¤´å¤„å®šä¹‰ if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123; break &#125; kl.syncLoopMonitor.Store(kl.clock.Now()) &#125;&#125; 2ã€ç›‘å¬ pod å˜åŒ–ï¼ˆsyncLoopIterationï¼‰syncLoopIteration è¿™ä¸ªæ–¹æ³•å°±ä¼šå¯¹å¤šä¸ªç®¡é“è¿›è¡ŒéåŽ†ï¼Œå‘çŽ°ä»»ä½•ä¸€ä¸ªç®¡é“æœ‰æ¶ˆæ¯å°±äº¤ç»™ handler åŽ»å¤„ç†ã€‚å®ƒä¼šä»Žä»¥ä¸‹ç®¡é“ä¸­èŽ·å–æ¶ˆæ¯ï¼š configChï¼šè¯¥ä¿¡æ¯æºç”± kubeDeps å¯¹è±¡ä¸­çš„ PodConfig å­æ¨¡å—æä¾›ï¼Œè¯¥æ¨¡å—å°†åŒæ—¶ watch 3 ä¸ªä¸åŒæ¥æºçš„ pod ä¿¡æ¯çš„å˜åŒ–ï¼ˆfileï¼Œhttpï¼Œapiserverï¼‰ï¼Œä¸€æ—¦æŸä¸ªæ¥æºçš„ pod ä¿¡æ¯å‘ç”Ÿäº†æ›´æ–°ï¼ˆåˆ›å»º/æ›´æ–°/åˆ é™¤ï¼‰ï¼Œè¿™ä¸ª channel ä¸­å°±ä¼šå‡ºçŽ°è¢«æ›´æ–°çš„ pod ä¿¡æ¯å’Œæ›´æ–°çš„å…·ä½“æ“ä½œã€‚ syncChï¼šå®šæ—¶å™¨ç®¡é“ï¼Œæ¯éš”ä¸€ç§’åŽ»åŒæ­¥æœ€æ–°ä¿å­˜çš„ pod çŠ¶æ€ houseKeepingChï¼šhousekeeping äº‹ä»¶çš„ç®¡é“ï¼Œåš pod æ¸…ç†å·¥ä½œ plegChï¼šè¯¥ä¿¡æ¯æºç”± kubelet å¯¹è±¡ä¸­çš„ pleg å­æ¨¡å—æä¾›ï¼Œè¯¥æ¨¡å—ä¸»è¦ç”¨äºŽå‘¨æœŸæ€§åœ°å‘ container runtime æŸ¥è¯¢å½“å‰æ‰€æœ‰å®¹å™¨çš„çŠ¶æ€ï¼Œå¦‚æžœçŠ¶æ€å‘ç”Ÿå˜åŒ–ï¼Œåˆ™è¿™ä¸ª channel äº§ç”Ÿäº‹ä»¶ã€‚ livenessManager.Updates()ï¼šå¥åº·æ£€æŸ¥å‘çŽ°æŸä¸ª pod ä¸å¯ç”¨ï¼Œkubelet å°†æ ¹æ® Pod çš„restartPolicy è‡ªåŠ¨æ‰§è¡Œæ­£ç¡®çš„æ“ä½œ 12345678910111213141516171819202122232425262728293031323334353637func (kl *Kubelet) syncLoopIteration(configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool &#123; select &#123; case u, open := &lt;-configCh: if !open &#123; glog.Errorf(&quot;Update channel is closed. Exiting the sync loop.&quot;) return false &#125; switch u.Op &#123; case kubetypes.ADD: ... case kubetypes.UPDATE: ... case kubetypes.REMOVE: ... case kubetypes.RECONCILE: ... case kubetypes.DELETE: ... case kubetypes.RESTORE: ... case kubetypes.SET: ... &#125; ... case e := &lt;-plegCh: ... case &lt;-syncCh: ... case update := &lt;-kl.livenessManager.Updates(): ... case &lt;-housekeepingCh: ... &#125; return true&#125; 3ã€å¤„ç†æ–°å¢ž podï¼ˆHandlePodAddtionsï¼‰å¯¹äºŽäº‹ä»¶ä¸­çš„æ¯ä¸ª podï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š 1ã€æŠŠæ‰€æœ‰çš„ pod æŒ‰ç…§åˆ›å»ºæ—¥æœŸè¿›è¡ŒæŽ’åºï¼Œä¿è¯æœ€å…ˆåˆ›å»ºçš„ pod ä¼šæœ€å…ˆè¢«å¤„ç† 2ã€æŠŠå®ƒåŠ å…¥åˆ° podManager ä¸­ï¼ŒpodManager å­æ¨¡å—è´Ÿè´£ç®¡ç†è¿™å°æœºå™¨ä¸Šçš„ pod çš„ä¿¡æ¯ï¼Œpod å’Œ mirrorPod ä¹‹é—´çš„å¯¹åº”å…³ç³»ç­‰ç­‰ã€‚æ‰€æœ‰è¢«ç®¡ç†çš„ pod éƒ½è¦å‡ºçŽ°åœ¨é‡Œé¢ï¼Œå¦‚æžœ podManager ä¸­æ‰¾ä¸åˆ°æŸä¸ª podï¼Œå°±è®¤ä¸ºè¿™ä¸ª pod è¢«åˆ é™¤äº† 3ã€å¦‚æžœæ˜¯ mirror pod è°ƒç”¨å…¶å•ç‹¬çš„æ–¹æ³• 4ã€éªŒè¯ pod æ˜¯å¦èƒ½åœ¨è¯¥èŠ‚ç‚¹è¿è¡Œï¼Œå¦‚æžœä¸å¯ä»¥ç›´æŽ¥æ‹’ç» 5ã€é€šè¿‡ dispatchWork æŠŠåˆ›å»º pod çš„å·¥ä½œä¸‹å‘ç»™ podWorkers å­æ¨¡å—åšå¼‚æ­¥å¤„ç† 6ã€åœ¨ probeManager ä¸­æ·»åŠ  podï¼Œå¦‚æžœ pod ä¸­å®šä¹‰äº† readiness å’Œ liveness å¥åº·æ£€æŸ¥ï¼Œå¯åŠ¨ goroutine å®šæœŸè¿›è¡Œæ£€æµ‹ 1234567891011121314151617181920212223242526272829303132333435func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) &#123; start := kl.clock.Now() // å¯¹æ‰€æœ‰ pod æŒ‰ç…§æ—¥æœŸæŽ’åºï¼Œä¿è¯æœ€å…ˆåˆ›å»ºçš„ pod ä¼˜å…ˆè¢«å¤„ç† sort.Sort(sliceutils.PodsByCreationTime(pods)) for _, pod := range pods &#123; if kl.dnsConfigurer != nil &amp;&amp; kl.dnsConfigurer.ResolverConfig != &quot;&quot; &#123; kl.dnsConfigurer.CheckLimitsForResolvConf() &#125; existingPods := kl.podManager.GetPods() // æŠŠ pod åŠ å…¥åˆ° podManager ä¸­ kl.podManager.AddPod(pod) // åˆ¤æ–­æ˜¯å¦æ˜¯ mirror podï¼ˆå³ static podï¼‰ if kubepod.IsMirrorPod(pod) &#123; kl.handleMirrorPod(pod, start) continue &#125; if !kl.podIsTerminated(pod) &#123; activePods := kl.filterOutTerminatedPods(existingPods) // é€šè¿‡ canAdmitPod æ–¹æ³•æ ¡éªŒPodèƒ½å¦åœ¨è¯¥è®¡ç®—èŠ‚ç‚¹åˆ›å»º(å¦‚:ç£ç›˜ç©ºé—´) // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok &#123; kl.rejectPod(pod, reason, message) continue &#125; &#125; mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) // é€šè¿‡ dispatchWork åˆ†å‘ pod åšå¼‚æ­¥å¤„ç†ï¼ŒdispatchWork ä¸»è¦å·¥ä½œå°±æ˜¯æŠŠæŽ¥æ”¶åˆ°çš„å‚æ•°å°è£…æˆ UpdatePodOptionsï¼Œè°ƒç”¨ UpdatePod æ–¹æ³•. kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start) // åœ¨ probeManager ä¸­æ·»åŠ  podï¼Œå¦‚æžœ pod ä¸­å®šä¹‰äº† readiness å’Œ liveness å¥åº·æ£€æŸ¥ï¼Œå¯åŠ¨ goroutine å®šæœŸè¿›è¡Œæ£€æµ‹ kl.probeManager.AddPod(pod) &#125;&#125; static pod æ˜¯ç”± kubelet ç›´æŽ¥ç®¡ç†çš„ï¼Œk8s apiserver å¹¶ä¸ä¼šæ„ŸçŸ¥åˆ° static pod çš„å­˜åœ¨ï¼Œå½“ç„¶ä¹Ÿä¸ä¼šå’Œä»»ä½•ä¸€ä¸ª rs å…³è”ä¸Šï¼Œå®Œå…¨æ˜¯ç”± kubelet è¿›ç¨‹æ¥ç›‘ç®¡ï¼Œå¹¶åœ¨å®ƒå¼‚å¸¸æ—¶è´Ÿè´£é‡å¯ã€‚Kubelet ä¼šé€šè¿‡ apiserver ä¸ºæ¯ä¸€ä¸ª static pod åˆ›å»ºä¸€ä¸ªå¯¹åº”çš„ mirror podï¼Œå¦‚æ­¤ä»¥æ¥å°±å¯ä»¥å¯ä»¥é€šè¿‡ kubectl å‘½ä»¤æŸ¥çœ‹å¯¹åº”çš„ pod,å¹¶ä¸”å¯ä»¥é€šè¿‡ kubectl logs å‘½ä»¤ç›´æŽ¥æŸ¥çœ‹åˆ°static pod çš„æ—¥å¿—ä¿¡æ¯ã€‚ 4ã€ä¸‹å‘ä»»åŠ¡ï¼ˆdispatchWorkï¼‰dispatchWorker çš„ä¸»è¦ä½œç”¨æ˜¯æŠŠæŸä¸ªå¯¹ Pod çš„æ“ä½œï¼ˆåˆ›å»º/æ›´æ–°/åˆ é™¤ï¼‰ä¸‹å‘ç»™ podWorkersã€‚ 12345678910111213141516171819202122func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) &#123; if kl.podIsTerminated(pod) &#123; if pod.DeletionTimestamp != nil &#123; kl.statusManager.TerminatePod(pod) &#125; return &#125; // è½å®žåœ¨ podWorkers ä¸­ kl.podWorkers.UpdatePod(&amp;UpdatePodOptions&#123; Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, OnCompleteFunc: func(err error) &#123; if err != nil &#123; metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start)) &#125; &#125;, &#125;) if syncType == kubetypes.SyncPodCreate &#123; metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) &#125;&#125; 5ã€æ›´æ–°äº‹ä»¶çš„ channelï¼ˆUpdatePodï¼‰podWorkers å­æ¨¡å—ä¸»è¦çš„ä½œç”¨å°±æ˜¯å¤„ç†é’ˆå¯¹æ¯ä¸€ä¸ªçš„ Pod çš„æ›´æ–°äº‹ä»¶ï¼Œæ¯”å¦‚ Pod çš„åˆ›å»ºï¼Œåˆ é™¤ï¼Œæ›´æ–°ã€‚è€Œ podWorkers é‡‡å–çš„åŸºæœ¬æ€è·¯æ˜¯ï¼šä¸ºæ¯ä¸€ä¸ª Pod éƒ½å•ç‹¬åˆ›å»ºä¸€ä¸ª goroutine å’Œæ›´æ–°äº‹ä»¶çš„ channelï¼Œgoroutine ä¼šé˜»å¡žå¼çš„ç­‰å¾… channel ä¸­çš„äº‹ä»¶ï¼Œå¹¶ä¸”å¯¹èŽ·å–çš„äº‹ä»¶è¿›è¡Œå¤„ç†ã€‚è€Œ podWorkers å¯¹è±¡è‡ªèº«åˆ™ä¸»è¦è´Ÿè´£å¯¹æ›´æ–°äº‹ä»¶è¿›è¡Œä¸‹å‘ã€‚ 1234567891011121314151617181920212223242526272829303132func (p *podWorkers) UpdatePod(options *UpdatePodOptions) &#123; pod := options.Pod uid := pod.UID var podUpdates chan UpdatePodOptions var exists bool p.podLock.Lock() defer p.podLock.Unlock() // å¦‚æžœå½“å‰ pod è¿˜æ²¡æœ‰å¯åŠ¨è¿‡ goroutine ï¼Œåˆ™å¯åŠ¨ goroutineï¼Œå¹¶ä¸”åˆ›å»º channel if podUpdates, exists = p.podUpdates[uid]; !exists &#123; // åˆ›å»º channel podUpdates = make(chan UpdatePodOptions, 1) p.podUpdates[uid] = podUpdates // å¯åŠ¨ goroutine go func() &#123; defer runtime.HandleCrash() p.managePodLoop(podUpdates) &#125;() &#125; // ä¸‹å‘æ›´æ–°äº‹ä»¶ if !p.isWorking[pod.UID] &#123; p.isWorking[pod.UID] = true podUpdates &lt;- *options &#125; else &#123; update, found := p.lastUndeliveredWorkUpdate[pod.UID] if !found || update.UpdateType != kubetypes.SyncPodKill &#123; p.lastUndeliveredWorkUpdate[pod.UID] = *options &#125; &#125;&#125; 6ã€è°ƒç”¨ syncPodFn æ–¹æ³•åŒæ­¥ podï¼ˆmanagePodLoopï¼‰managePodLoop è°ƒç”¨ syncPodFn æ–¹æ³•åŽ»åŒæ­¥ podï¼ŒsyncPodFn å®žé™…ä¸Šå°±æ˜¯kubelet.SyncPodã€‚åœ¨å®Œæˆè¿™æ¬¡ sync åŠ¨ä½œä¹‹åŽï¼Œä¼šè°ƒç”¨ wrapUp å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°å°†ä¼šåšå‡ ä»¶äº‹æƒ…: å°†è¿™ä¸ª pod ä¿¡æ¯æ’å…¥ kubelet çš„ workQueue é˜Ÿåˆ—ä¸­ï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡å‘¨æœŸæ€§çš„å¯¹è¿™ä¸ª pod çš„çŠ¶æ€è¿›è¡Œ sync å°†åœ¨è¿™æ¬¡ sync æœŸé—´å †ç§¯çš„æ²¡æœ‰èƒ½å¤Ÿæ¥å¾—åŠå¤„ç†çš„æœ€è¿‘ä¸€æ¬¡ update æ“ä½œåŠ å…¥ goroutine çš„äº‹ä»¶ channel ä¸­ï¼Œç«‹å³å¤„ç†ã€‚ 12345678910111213141516171819202122232425262728func (p *podWorkers) managePodLoop(podUpdates &lt;-chan UpdatePodOptions) &#123; var lastSyncTime time.Time for update := range podUpdates &#123; err := func() error &#123; podUID := update.Pod.UID status, err := p.podCache.GetNewerThan(podUID, lastSyncTime) if err != nil &#123; ... &#125; err = p.syncPodFn(syncPodOptions&#123; mirrorPod: update.MirrorPod, pod: update.Pod, podStatus: status, killPodOptions: update.KillPodOptions, updateType: update.UpdateType, &#125;) lastSyncTime = time.Now() return err &#125;() if update.OnCompleteFunc != nil &#123; update.OnCompleteFunc(err) &#125; if err != nil &#123; ... &#125; p.wrapUp(update.Pod.UID, err) &#125;&#125; 7ã€å®Œæˆåˆ›å»ºå®¹å™¨å‰çš„å‡†å¤‡å·¥ä½œï¼ˆSyncPodï¼‰åœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œä¸»è¦å®Œæˆä»¥ä¸‹å‡ ä»¶äº‹æƒ…ï¼š å¦‚æžœæ˜¯åˆ é™¤ podï¼Œç«‹å³æ‰§è¡Œå¹¶è¿”å›ž åŒæ­¥ podStatus åˆ° kubelet.statusManager æ£€æŸ¥ pod æ˜¯å¦èƒ½è¿è¡Œåœ¨æœ¬èŠ‚ç‚¹ï¼Œä¸»è¦æ˜¯æƒé™æ£€æŸ¥ï¼ˆæ˜¯å¦èƒ½ä½¿ç”¨ä¸»æœºç½‘ç»œæ¨¡å¼ï¼Œæ˜¯å¦å¯ä»¥ä»¥ privileged æƒé™è¿è¡Œç­‰ï¼‰ã€‚å¦‚æžœæ²¡æœ‰æƒé™ï¼Œå°±åˆ é™¤æœ¬åœ°æ—§çš„ pod å¹¶è¿”å›žé”™è¯¯ä¿¡æ¯ åˆ›å»º containerManagar å¯¹è±¡ï¼Œå¹¶ä¸”åˆ›å»º pod level cgroupï¼Œæ›´æ–° Qos level cgroup å¦‚æžœæ˜¯ static Podï¼Œå°±åˆ›å»ºæˆ–è€…æ›´æ–°å¯¹åº”çš„ mirrorPod åˆ›å»º pod çš„æ•°æ®ç›®å½•ï¼Œå­˜æ”¾ volume å’Œ plugin ä¿¡æ¯,å¦‚æžœå®šä¹‰äº† pvï¼Œç­‰å¾…æ‰€æœ‰çš„ volume mount å®Œæˆï¼ˆvolumeManager ä¼šåœ¨åŽå°åšè¿™äº›äº‹æƒ…ï¼‰,å¦‚æžœæœ‰ image secretsï¼ŒåŽ» apiserver èŽ·å–å¯¹åº”çš„ secrets æ•°æ® ç„¶åŽè°ƒç”¨ kubelet.volumeManager ç»„ä»¶ï¼Œç­‰å¾…å®ƒå°† pod æ‰€éœ€è¦çš„æ‰€æœ‰å¤–æŒ‚çš„ volume éƒ½å‡†å¤‡å¥½ã€‚ è°ƒç”¨ container runtime çš„ SyncPod æ–¹æ³•ï¼ŒåŽ»å®žçŽ°çœŸæ­£çš„å®¹å™¨åˆ›å»ºé€»è¾‘ è¿™é‡Œæ‰€æœ‰çš„äº‹æƒ…éƒ½å’Œå…·ä½“çš„å®¹å™¨æ²¡æœ‰å…³ç³»ï¼Œå¯ä»¥çœ‹åˆ°è¯¥æ–¹æ³•æ˜¯åˆ›å»º pod å®žä½“ï¼ˆå³å®¹å™¨ï¼‰ä¹‹å‰éœ€è¦å®Œæˆçš„å‡†å¤‡å·¥ä½œã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071func (kl *Kubelet) syncPod(o syncPodOptions) error &#123; // pull out the required options pod := o.pod mirrorPod := o.mirrorPod podStatus := o.podStatus updateType := o.updateType // æ˜¯å¦ä¸º åˆ é™¤ pod if updateType == kubetypes.SyncPodKill &#123; ... &#125; ... // æ£€æŸ¥ pod æ˜¯å¦èƒ½è¿è¡Œåœ¨æœ¬èŠ‚ç‚¹ runnable := kl.canRunPod(pod) if !runnable.Admit &#123; ... &#125; // æ›´æ–° pod çŠ¶æ€ kl.statusManager.SetPodStatus(pod, apiPodStatus) // å¦‚æžœ pod éž running çŠ¶æ€åˆ™ç›´æŽ¥ kill æŽ‰ if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed &#123; ... &#125; // åŠ è½½ç½‘ç»œæ’ä»¶ if rs := kl.runtimeState.networkErrors(); len(rs) != 0 &amp;&amp; !kubecontainer.IsHostNetworkPod(pod) &#123; ... &#125; pcm := kl.containerManager.NewPodContainerManager() if !kl.podIsTerminated(pod) &#123; ... // åˆ›å»ºå¹¶æ›´æ–° pod çš„ cgroups if !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) &#123; if !pcm.Exists(pod) &#123; ... &#125; &#125; &#125; // ä¸º static pod åˆ›å»ºå¯¹åº”çš„ mirror pod if kubepod.IsStaticPod(pod) &#123; ... &#125; // åˆ›å»ºæ•°æ®ç›®å½• if err := kl.makePodDataDirs(pod); err != nil &#123; ... &#125; // æŒ‚è½½ volume if !kl.podIsTerminated(pod) &#123; if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil &#123; ... &#125; &#125; // èŽ·å– secret ä¿¡æ¯ pullSecrets := kl.getPullSecretsForPod(pod) // è°ƒç”¨ containerRuntime çš„ SyncPod æ–¹æ³•å¼€å§‹åˆ›å»ºå®¹å™¨ result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil &#123; ... &#125; return nil&#125; 8ã€åˆ›å»ºå®¹å™¨containerRuntimeï¼ˆpkg/kubelet/kuberuntimeï¼‰å­æ¨¡å—çš„ SyncPod å‡½æ•°æ‰æ˜¯çœŸæ­£å®Œæˆ pod å†…å®¹å™¨å®žä½“çš„åˆ›å»ºã€‚syncPod ä¸»è¦æ‰§è¡Œä»¥ä¸‹å‡ ä¸ªæ“ä½œï¼š 1ã€è®¡ç®— sandbox å’Œ container æ˜¯å¦å‘ç”Ÿå˜åŒ– 2ã€åˆ›å»º sandbox å®¹å™¨ 3ã€å¯åŠ¨ init å®¹å™¨ 4ã€å¯åŠ¨ä¸šåŠ¡å®¹å™¨ initContainers å¯ä»¥æœ‰å¤šä¸ªï¼Œå¤šä¸ª container ä¸¥æ ¼æŒ‰ç…§é¡ºåºå¯åŠ¨ï¼Œåªæœ‰å½“å‰ä¸€ä¸ª container é€€å‡ºäº†ä»¥åŽï¼Œæ‰å¼€å§‹å¯åŠ¨ä¸‹ä¸€ä¸ª containerã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) &#123; // 1ã€è®¡ç®— sandbox å’Œ container æ˜¯å¦å‘ç”Ÿå˜åŒ– podContainerChanges := m.computePodActions(pod, podStatus) if podContainerChanges.CreateSandbox &#123; ref, err := ref.GetReference(legacyscheme.Scheme, pod) if err != nil &#123; glog.Errorf(&quot;Couldn&apos;t make a ref to pod %q: &apos;%v&apos;&quot;, format.Pod(pod), err) &#125; ... &#125; // 2ã€kill æŽ‰ sandbox å·²ç»æ”¹å˜çš„ pod if podContainerChanges.KillPod &#123; ... &#125; else &#123; // 3ã€kill æŽ‰éž running çŠ¶æ€çš„ containers ... for containerID, containerInfo := range podContainerChanges.ContainersToKill &#123; ... if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil &#123; ... &#125; &#125; &#125; m.pruneInitContainersBeforeStart(pod, podStatus) podIP := &quot;&quot; if podStatus != nil &#123; podIP = podStatus.IP &#125; // 4ã€åˆ›å»º sandbox podSandboxID := podContainerChanges.SandboxID if podContainerChanges.CreateSandbox &#123; podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) if err != nil &#123; ... &#125; ... podSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID) if err != nil &#123; ... &#125; // å¦‚æžœ pod ç½‘ç»œæ˜¯ host æ¨¡å¼ï¼Œå®¹å™¨ä¹Ÿç›¸åŒï¼›å…¶ä»–æƒ…å†µä¸‹ï¼Œå®¹å™¨ä¼šä½¿ç”¨ None ç½‘ç»œæ¨¡å¼ï¼Œè®© kubelet çš„ç½‘ç»œæ’ä»¶è‡ªå·±è¿›è¡Œç½‘ç»œé…ç½® if !kubecontainer.IsHostNetworkPod(pod) &#123; podIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus) glog.V(4).Infof(&quot;Determined the ip %q for pod %q after sandbox changed&quot;, podIP, format.Pod(pod)) &#125; &#125; configPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID) result.AddSyncResult(configPodSandboxResult) // èŽ·å– PodSandbox çš„é…ç½®(å¦‚:metadata,clusterDNS,å®¹å™¨çš„ç«¯å£æ˜ å°„ç­‰) podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt) ... // 5ã€å¯åŠ¨ init container if container := podContainerChanges.NextInitContainerToStart; container != nil &#123; ... if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil &#123; ... &#125; &#125; // 6ã€å¯åŠ¨ä¸šåŠ¡å®¹å™¨ for _, idx := range podContainerChanges.ContainersToStart &#123; ... if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil &#123; ... &#125; &#125; return&#125; 9ã€å¯åŠ¨å®¹å™¨æœ€ç»ˆç”± startContainer å®Œæˆå®¹å™¨çš„å¯åŠ¨ï¼Œå…¶ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š 1ã€æ‹‰å–é•œåƒ 2ã€ç”Ÿæˆä¸šåŠ¡å®¹å™¨çš„é…ç½®ä¿¡æ¯ 3ã€è°ƒç”¨ docker api åˆ›å»ºå®¹å™¨ 4ã€å¯åŠ¨å®¹å™¨ 5ã€æ‰§è¡Œ post start hook 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) &#123; // 1ã€æ£€æŸ¥ä¸šåŠ¡é•œåƒæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ° Docker Registry æˆ–æ˜¯ Private Registry æ‹‰å–é•œåƒã€‚ imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets) if err != nil &#123; ... &#125; ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil &#123; ... &#125; // è®¾ç½® RestartCount restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil &#123; restartCount = containerStatus.RestartCount + 1 &#125; // 2ã€ç”Ÿæˆä¸šåŠ¡å®¹å™¨çš„é…ç½®ä¿¡æ¯ containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType) if cleanupAction != nil &#123; defer cleanupAction() &#125; ... // 3ã€é€šè¿‡ client.CreateContainer è°ƒç”¨ docker api åˆ›å»ºä¸šåŠ¡å®¹å™¨ containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil &#123; ... &#125; err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil &#123; ... &#125; ... // 3ã€å¯åŠ¨ä¸šåŠ¡å®¹å™¨ err = m.runtimeService.StartContainer(containerID) if err != nil &#123; ... &#125; containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) &#123; if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil &#123; glog.Errorf(&quot;Failed to create legacy symbolic link %q to container %q log %q: %v&quot;, legacySymlink, containerID, containerLog, err) &#125; &#125; // 4ã€æ‰§è¡Œ post start hook if container.Lifecycle != nil &amp;&amp; container.Lifecycle.PostStart != nil &#123; kubeContainerID := kubecontainer.ContainerID&#123; Type: m.runtimeName, ID: containerID, &#125; // runner.Run è¿™ä¸ªæ–¹æ³•çš„ä¸»è¦ä½œç”¨å°±æ˜¯åœ¨ä¸šåŠ¡å®¹å™¨èµ·æ¥çš„æ—¶å€™ï¼Œ // é¦–å…ˆä¼šæ‰§è¡Œä¸€ä¸ª container hook(PostStart å’Œ PreStop),åšä¸€äº›é¢„å¤„ç†å·¥ä½œã€‚ // åªæœ‰ container hook æ‰§è¡ŒæˆåŠŸæ‰ä¼šè¿è¡Œå…·ä½“çš„ä¸šåŠ¡æœåŠ¡ï¼Œå¦åˆ™å®¹å™¨å¼‚å¸¸ã€‚ msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil &#123; ... &#125; &#125; return &quot;&quot;, nil&#125; æ€»ç»“æœ¬æ–‡ä¸»è¦è®²è¿°äº† kubelet ä»Žç›‘å¬åˆ°å®¹å™¨è°ƒåº¦è‡³æœ¬èŠ‚ç‚¹å†åˆ°åˆ›å»ºå®¹å™¨çš„ä¸€ä¸ªè¿‡ç¨‹ï¼Œkubelet æœ€ç»ˆè°ƒç”¨ docker api æ¥åˆ›å»ºå®¹å™¨çš„ã€‚ç»“åˆä¸Šç¯‡æ–‡ç« ï¼Œå¯ä»¥çœ‹å‡º kubelet ä»Žå¯åŠ¨åˆ°åˆ›å»º pod çš„ä¸€ä¸ªæ¸…æ™°è¿‡ç¨‹ã€‚ å‚è€ƒï¼šk8sæºç åˆ†æž-kubeletKubeletæºç åˆ†æž(ä¸€):å¯åŠ¨æµç¨‹åˆ†æžkubelet æºç åˆ†æžï¼špod æ–°å»ºæµç¨‹kubeletåˆ›å»ºPodæµç¨‹è§£æžKubelet: Pod Lifecycle Event Generator (PLEG) Design- proposals ç›¸å…³æŽ¨èkubelet çŠ¶æ€ä¸ŠæŠ¥çš„æ–¹å¼kubernets ä¸­äº‹ä»¶å¤„ç†æœºåˆ¶kubelet å¯åŠ¨æµç¨‹åˆ†æž]]></content>
      <tags>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet å¯åŠ¨æµç¨‹åˆ†æž]]></title>
    <url>%2F2018%2F12%2F23%2Fkubelet_init%2F</url>
    <content type="text"><![CDATA[ä¸Šç¯‡æ–‡ç« ï¼ˆkubelet æž¶æž„æµ…æž ï¼‰å·²ç»ä»‹ç»è¿‡ kubelet åœ¨æ•´ä¸ªé›†ç¾¤æž¶æž„ä¸­çš„åŠŸèƒ½ä»¥åŠè‡ªèº«å„æ¨¡å—çš„ç”¨é€”ï¼Œæœ¬ç¯‡æ–‡ç« ä¸»è¦ä»‹ç» kubelet çš„å¯åŠ¨æµç¨‹ã€‚ kubernetes ç‰ˆæœ¬ï¼š v1.12 kubelet å¯åŠ¨æµç¨‹kubelet ä»£ç ç»“æž„: 12345678910111213141516171819202122232425262728âžœ kubernetes git:(release-1.12) âœ— tree cmd/kubeletcmd/kubeletâ”œâ”€â”€ BUILDâ”œâ”€â”€ OWNERSâ”œâ”€â”€ appâ”‚ â”œâ”€â”€ BUILDâ”‚ â”œâ”€â”€ OWNERSâ”‚ â”œâ”€â”€ auth.goâ”‚ â”œâ”€â”€ init_others.goâ”‚ â”œâ”€â”€ init_windows.goâ”‚ â”œâ”€â”€ optionsâ”‚ â”‚ â”œâ”€â”€ BUILDâ”‚ â”‚ â”œâ”€â”€ container_runtime.goâ”‚ â”‚ â”œâ”€â”€ globalflags.goâ”‚ â”‚ â”œâ”€â”€ globalflags_linux.goâ”‚ â”‚ â”œâ”€â”€ globalflags_other.goâ”‚ â”‚ â”œâ”€â”€ options.goâ”‚ â”‚ â”œâ”€â”€ options_test.goâ”‚ â”‚ â”œâ”€â”€ osflags_others.goâ”‚ â”‚ â””â”€â”€ osflags_windows.goâ”‚ â”œâ”€â”€ plugins.goâ”‚ â”œâ”€â”€ server.goâ”‚ â”œâ”€â”€ server_linux.goâ”‚ â”œâ”€â”€ server_test.goâ”‚ â””â”€â”€ server_unsupported.goâ””â”€â”€ kubelet.go2 directories, 22 files 1ã€kubelet å…¥å£å‡½æ•° mainï¼ˆcmd/kubelet/kubelet.goï¼‰123456789101112func main() &#123; rand.Seed(time.Now().UTC().UnixNano()) command := app.NewKubeletCommand(server.SetupSignalHandler()) logs.InitLogs() defer logs.FlushLogs() if err := command.Execute(); err != nil &#123; fmt.Fprintf(os.Stderr, &quot;%v\n&quot;, err) os.Exit(1) &#125;&#125; 2ã€åˆå§‹åŒ– kubelet é…ç½®ï¼ˆcmd/kubelet/app/server.goï¼‰NewKubeletCommand() å‡½æ•°ä¸»è¦è´Ÿè´£èŽ·å–é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°ï¼Œæ ¡éªŒå‚æ•°ä»¥åŠä¸ºå‚æ•°è®¾ç½®é»˜è®¤å€¼ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041// NewKubeletCommand creates a *cobra.Command object with default parametersfunc NewKubeletCommand(stopCh &lt;-chan struct&#123;&#125;) *cobra.Command &#123; cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(flag.WordSepNormalizeFunc) // Kubeleté…ç½®åˆ†ä¸¤éƒ¨åˆ†: // KubeletFlag: æŒ‡é‚£äº›ä¸å…è®¸åœ¨ kubelet è¿è¡Œæ—¶è¿›è¡Œä¿®æ”¹çš„é…ç½®é›†ï¼Œæˆ–è€…ä¸èƒ½åœ¨é›†ç¾¤ä¸­å„ä¸ª Nodes ä¹‹é—´å…±äº«çš„é…ç½®é›†ã€‚ // KubeletConfiguration: æŒ‡å¯ä»¥åœ¨é›†ç¾¤ä¸­å„ä¸ªNodesä¹‹é—´å…±äº«çš„é…ç½®é›†ï¼Œå¯ä»¥è¿›è¡ŒåŠ¨æ€é…ç½®ã€‚ kubeletFlags := options.NewKubeletFlags() kubeletConfig, err := options.NewKubeletConfiguration() ... cmd := &amp;cobra.Command&#123; ... Run: func(cmd *cobra.Command, args []string) &#123; // è¯»å– kubelet é…ç½®æ–‡ä»¶ if configFile := kubeletFlags.KubeletConfigFile; len(configFile) &gt; 0 &#123; kubeletConfig, err = loadConfigFile(configFile) if err != nil &#123; glog.Fatal(err) &#125; ... &#125; // æ ¡éªŒ kubelet å‚æ•° if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil &#123; glog.Fatal(err) &#125; ... // æ­¤å¤„åˆå§‹åŒ–äº† kubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer) if err != nil &#123; glog.Fatal(err) &#125; ... // å¯åŠ¨ç¨‹åº if err := Run(kubeletServer, kubeletDeps, stopCh); err != nil &#123; glog.Fatal(err) &#125; &#125;, &#125; ... return cmd&#125; kubeletDeps åŒ…å« kubelet è¿è¡Œæ‰€å¿…é¡»çš„é…ç½®ï¼Œæ˜¯ä¸ºäº†å®žçŽ° dependency injectionï¼Œå…¶ç›®çš„æ˜¯ä¸ºäº†æŠŠ kubelet ä¾èµ–çš„ç»„ä»¶å¯¹è±¡ä½œä¸ºå‚æ•°ä¼ è¿›æ¥ï¼Œè¿™æ ·å¯ä»¥æŽ§åˆ¶ kubelet çš„è¡Œä¸ºã€‚ä¸»è¦åŒ…æ‹¬ç›‘æŽ§åŠŸèƒ½ï¼ˆcadvisorï¼‰ï¼Œcgroup ç®¡ç†åŠŸèƒ½ï¼ˆcontainerManagerï¼‰ç­‰ã€‚ NewKubeletCommand() ä¼šè°ƒç”¨ Run() å‡½æ•°ï¼ŒRun() ä¸­ä¸»è¦è°ƒç”¨ run() å‡½æ•°è¿›è¡Œä¸€äº›å‡†å¤‡äº‹é¡¹ã€‚ 3ã€åˆ›å»ºå’Œ apiserver é€šä¿¡çš„å¯¹è±¡ï¼ˆcmd/kubelet/app/server.goï¼‰run() å‡½æ•°çš„ä¸»è¦åŠŸèƒ½ï¼š 1ã€åˆ›å»º kubeClientï¼ŒevnetClient ç”¨æ¥å’Œ apiserver é€šä¿¡ã€‚åˆ›å»º heartbeatClient å‘ apiserver ä¸ŠæŠ¥å¿ƒè·³çŠ¶æ€ã€‚ 2ã€ä¸º kubeDeps è®¾å®šä¸€äº›é»˜è®¤å€¼ã€‚ 3ã€å¯åŠ¨ç›‘å¬ Healthz ç«¯å£çš„ http serverï¼Œé»˜è®¤ç«¯å£æ˜¯ 10248ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123; ... // åˆ¤æ–­ kubelet çš„å¯åŠ¨æ¨¡å¼ if standaloneMode &#123; ... &#125; else if kubeDeps.KubeClient == nil || kubeDeps.EventClient == nil || kubeDeps.HeartbeatClient == nil || kubeDeps.DynamicKubeClient == nil &#123; ... // åˆ›å»ºå¯¹è±¡ kubeClient kubeClient, err = clientset.NewForConfig(clientConfig) ... // åˆ›å»ºå¯¹è±¡ evnetClient eventClient, err = v1core.NewForConfig(&amp;eventClientConfig) ... // heartbeatClient ä¸ŠæŠ¥çŠ¶æ€ heartbeatClient, err = clientset.NewForConfig(&amp;heartbeatClientConfig) ... &#125; // ä¸º kubeDeps è®¾å®šä¸€äº›é»˜è®¤å€¼ if kubeDeps.Auth == nil &#123; auth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration) if err != nil &#123; return err &#125; kubeDeps.Auth = auth &#125; if kubeDeps.CAdvisorInterface == nil &#123; imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint) kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cadvisor.UsingLegacyCadvisorStats(s.ContainerRuntime, s.RemoteRuntimeEndpoint)) if err != nil &#123; return err &#125; &#125; &#125; // if err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil &#123; return err &#125; ... // å¯åŠ¨ç›‘å¬ Healthz ç«¯å£çš„ http server if s.HealthzPort &gt; 0 &#123; healthz.DefaultHealthz() go wait.Until(func() &#123; err := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), nil) if err != nil &#123; glog.Errorf(&quot;Starting health server failed: %v&quot;, err) &#125; &#125;, 5*time.Second, wait.NeverStop) &#125; ...&#125; kubelet å¯¹ pod èµ„æºçš„èŽ·å–æ–¹å¼æœ‰ä¸‰ç§ï¼šç¬¬ä¸€ç§æ˜¯é€šè¿‡æ–‡ä»¶èŽ·å¾—ï¼Œæ–‡ä»¶ä¸€èˆ¬æ”¾åœ¨ /etc/kubernetes/manifests ç›®å½•ä¸‹é¢ï¼›ç¬¬äºŒç§ä¹Ÿæ˜¯é€šè¿‡æ–‡ä»¶è¿‡å¾—ï¼Œåªä¸è¿‡æ–‡ä»¶æ˜¯é€šè¿‡ URL èŽ·å–çš„ï¼›ç¬¬ä¸‰ç§æ˜¯é€šè¿‡ watch kube-apiserver èŽ·å–ã€‚å…¶ä¸­å‰ä¸¤ç§æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬ç§° kubelet è¿è¡Œåœ¨ standalone æ¨¡å¼ä¸‹ï¼Œè¿è¡Œåœ¨ standalone æ¨¡å¼ä¸‹çš„ kubelet ä¸€èˆ¬ç”¨äºŽè°ƒè¯•æŸäº›åŠŸèƒ½ã€‚ run() ä¸­è°ƒç”¨ RunKubelet() å‡½æ•°è¿›è¡ŒåŽç»­æ“ä½œã€‚ 4ã€åˆå§‹åŒ– kubelet ç»„ä»¶å†…éƒ¨çš„æ¨¡å—ï¼ˆcmd/kubelet/app/server.goï¼‰RunKubelet() ä¸»è¦åŠŸèƒ½ï¼š 1ã€åˆå§‹åŒ– kubelet ç»„ä»¶ä¸­çš„å„ä¸ªæ¨¡å—ï¼Œåˆ›å»ºå‡º kubelet å¯¹è±¡ã€‚ 2ã€å¯åŠ¨åžƒåœ¾å›žæ”¶æœåŠ¡ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123; ... // åˆå§‹åŒ– kubelet å†…éƒ¨æ¨¡å— k, err := CreateAndInitKubelet(&amp;kubeServer.KubeletConfiguration, kubeDeps, &amp;kubeServer.ContainerRuntimeOptions, kubeServer.ContainerRuntime, kubeServer.RuntimeCgroups, kubeServer.HostnameOverride, kubeServer.NodeIP, kubeServer.ProviderID, kubeServer.CloudProvider, kubeServer.CertDirectory, kubeServer.RootDirectory, kubeServer.RegisterNode, kubeServer.RegisterWithTaints, kubeServer.AllowedUnsafeSysctls, kubeServer.RemoteRuntimeEndpoint, kubeServer.RemoteImageEndpoint, kubeServer.ExperimentalMounterPath, kubeServer.ExperimentalKernelMemcgNotification, kubeServer.ExperimentalCheckNodeCapabilitiesBeforeMount, kubeServer.ExperimentalNodeAllocatableIgnoreEvictionThreshold, kubeServer.MinimumGCAge, kubeServer.MaxPerPodContainerCount, kubeServer.MaxContainerCount, kubeServer.MasterServiceNamespace, kubeServer.RegisterSchedulable, kubeServer.NonMasqueradeCIDR, kubeServer.KeepTerminatedPodVolumes, kubeServer.NodeLabels, kubeServer.SeccompProfileRoot, kubeServer.BootstrapCheckpointPath, kubeServer.NodeStatusMaxImages) if err != nil &#123; return fmt.Errorf(&quot;failed to create kubelet: %v&quot;, err) &#125; ... if runOnce &#123; if _, err := k.RunOnce(podCfg.Updates()); err != nil &#123; return fmt.Errorf(&quot;runonce failed: %v&quot;, err) &#125; glog.Infof(&quot;Started kubelet as runonce&quot;) &#125; else &#123; // startKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableServer) glog.Infof(&quot;Started kubelet&quot;) &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445func CreateAndInitKubelet(...)&#123; // NewMainKubelet å®žä¾‹åŒ–ä¸€ä¸ª kubelet å¯¹è±¡ï¼Œå¹¶å¯¹ kubelet å†…éƒ¨å„ä¸ªæ¨¡å—è¿›è¡Œåˆå§‹åŒ– k, err = kubelet.NewMainKubelet(kubeCfg, kubeDeps, crOptions, containerRuntime, runtimeCgroups, hostnameOverride, nodeIP, providerID, cloudProvider, certDirectory, rootDirectory, registerNode, registerWithTaints, allowedUnsafeSysctls, remoteRuntimeEndpoint, remoteImageEndpoint, experimentalMounterPath, experimentalKernelMemcgNotification, experimentalCheckNodeCapabilitiesBeforeMount, experimentalNodeAllocatableIgnoreEvictionThreshold, minimumGCAge, maxPerPodContainerCount, maxContainerCount, masterServiceNamespace, registerSchedulable, nonMasqueradeCIDR, keepTerminatedPodVolumes, nodeLabels, seccompProfileRoot, bootstrapCheckpointPath, nodeStatusMaxImages) if err != nil &#123; return nil, err &#125; // é€šçŸ¥ apiserver kubelet å¯åŠ¨äº† k.BirthCry() // å¯åŠ¨åžƒåœ¾å›žæ”¶æœåŠ¡ k.StartGarbageCollection() return k, nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,...)&#123; ... if kubeDeps.PodConfig == nil &#123; var err error // åˆå§‹åŒ– makePodSourceConfigï¼Œç›‘å¬ pod å…ƒæ•°æ®çš„æ¥æº(FILE, URL, api-server)ï¼Œå°†ä¸åŒ source çš„ pod configuration åˆå¹¶åˆ°ä¸€ä¸ªç»“æž„ä¸­ kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath) if err != nil &#123; return nil, err &#125; &#125; // kubelet æœåŠ¡ç«¯å£ï¼Œé»˜è®¤ 10250 daemonEndpoints := &amp;v1.NodeDaemonEndpoints&#123; KubeletEndpoint: v1.DaemonEndpoint&#123;Port: kubeCfg.Port&#125;, &#125; // ä½¿ç”¨ reflector æŠŠ ListWatch å¾—åˆ°çš„æœåŠ¡ä¿¡æ¯å®žæ—¶åŒæ­¥åˆ° serviceStore å¯¹è±¡ä¸­ serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;cache.NamespaceIndex: cache.MetaNamespaceIndexFunc&#125;) if kubeDeps.KubeClient != nil &#123; serviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;services&quot;, metav1.NamespaceAll, fields.Everything()) r := cache.NewReflector(serviceLW, &amp;v1.Service&#123;&#125;, serviceIndexer, 0) go r.Run(wait.NeverStop) &#125; serviceLister := corelisters.NewServiceLister(serviceIndexer) // ä½¿ç”¨ reflector æŠŠ ListWatch å¾—åˆ°çš„èŠ‚ç‚¹ä¿¡æ¯å®žæ—¶åŒæ­¥åˆ° nodeStore å¯¹è±¡ä¸­ nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;&#125;) if kubeDeps.KubeClient != nil &#123; fieldSelector := fields.Set&#123;api.ObjectNameField: string(nodeName)&#125;.AsSelector() nodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;nodes&quot;, metav1.NamespaceAll, fieldSelector) r := cache.NewReflector(nodeLW, &amp;v1.Node&#123;&#125;, nodeIndexer, 0) go r.Run(wait.NeverStop) &#125; nodeInfo := &amp;predicates.CachedNodeInfo&#123;NodeLister: corelisters.NewNodeLister(nodeIndexer)&#125; ... // node èµ„æºä¸è¶³æ—¶çš„é©±é€ç­–ç•¥çš„è®¾å®š thresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg.EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim) if err != nil &#123; return nil, err &#125; evictionConfig := eviction.Config&#123; PressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration, MaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod), Thresholds: thresholds, KernelMemcgNotification: experimentalKernelMemcgNotification, PodCgroupRoot: kubeDeps.ContainerManager.GetPodCgroupRoot(), &#125; ... // å®¹å™¨å¼•ç”¨çš„ç®¡ç† containerRefManager := kubecontainer.NewRefManager() // oom ç›‘æŽ§ oomWatcher := NewOOMWatcher(kubeDeps.CAdvisorInterface, kubeDeps.Recorder) // æ ¹æ®é…ç½®ä¿¡æ¯å’Œå„ç§å¯¹è±¡åˆ›å»º Kubelet å®žä¾‹ klet := &amp;Kubelet&#123; hostname: hostname, hostnameOverridden: len(hostnameOverride) &gt; 0, nodeName: nodeName, ... &#125; // ä»Ž cAdvisor èŽ·å–å½“å‰æœºå™¨çš„ä¿¡æ¯ machineInfo, err := klet.cadvisor.MachineInfo() // å¯¹ pod çš„ç®¡ç†ï¼ˆå¦‚: å¢žåˆ æ”¹ç­‰ï¼‰ klet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager) // å®¹å™¨è¿è¡Œæ—¶ç®¡ç† runtime, err := kuberuntime.NewKubeGenericRuntimeManager(...) // pleg klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock&#123;&#125;) // åˆ›å»º containerGC å¯¹è±¡ï¼Œè¿›è¡Œå‘¨æœŸæ€§çš„å®¹å™¨æ¸…ç†å·¥ä½œ containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady) // åˆ›å»º imageManager ç®¡ç†é•œåƒ imageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage) // statusManager å®žæ—¶æ£€æµ‹èŠ‚ç‚¹ä¸Š pod çš„çŠ¶æ€ï¼Œå¹¶æ›´æ–°åˆ° apiserver å¯¹åº”çš„ pod klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet) // æŽ¢é’ˆç®¡ç† klet.probeManager = prober.NewManager(...) // token ç®¡ç† tokenManager := token.NewManager(kubeDeps.KubeClient) // ç£ç›˜ç®¡ç† klet.volumeManager = volumemanager.NewVolumeManager() // å°† syncPod() æ³¨å…¥åˆ° podWorkers ä¸­ klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache) // å®¹å™¨é©±é€ç­–ç•¥ç®¡ç† evictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock) ...&#125; RunKubelet æœ€åŽä¼šè°ƒç”¨ startKubelet() è¿›è¡ŒåŽç»­çš„æ“ä½œã€‚ 5ã€å¯åŠ¨ kubelet å†…éƒ¨çš„æ¨¡å—åŠæœåŠ¡ï¼ˆcmd/kubelet/app/server.goï¼‰startKubelet() çš„ä¸»è¦åŠŸèƒ½ï¼š 1ã€ä»¥ goroutine æ–¹å¼å¯åŠ¨ kubelet ä¸­çš„å„ä¸ªæ¨¡å—ã€‚ 2ã€å¯åŠ¨ kubelet http serverã€‚ 123456789101112131415func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) &#123; go wait.Until(func() &#123; // ä»¥ goroutine æ–¹å¼å¯åŠ¨ kubelet ä¸­çš„å„ä¸ªæ¨¡å— k.Run(podCfg.Updates()) &#125;, 0, wait.NeverStop) // å¯åŠ¨ kubelet http server if enableServer &#123; go k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, kubeCfg.EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling) &#125; if kubeCfg.ReadOnlyPort &gt; 0 &#123; go k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort)) &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// Run starts the kubelet reacting to config updatesfunc (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123; if kl.logServer == nil &#123; kl.logServer = http.StripPrefix(&quot;/logs/&quot;, http.FileServer(http.Dir(&quot;/var/log/&quot;))) &#125; if kl.kubeClient == nil &#123; glog.Warning(&quot;No api server defined - no node status update will be sent.&quot;) &#125; // Start the cloud provider sync manager if kl.cloudResourceSyncManager != nil &#123; go kl.cloudResourceSyncManager.Run(wait.NeverStop) &#125; if err := kl.initializeModules(); err != nil &#123; kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) glog.Fatal(err) &#125; // Start volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) if kl.kubeClient != nil &#123; // Start syncing node status immediately, this may set up things the runtime needs to run. go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop) go kl.fastStatusUpdateOnce() // start syncing lease if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123; go kl.nodeLeaseController.Run(wait.NeverStop) &#125; &#125; go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // Start loop to sync iptables util rules if kl.makeIPTablesUtilChains &#123; go wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop) &#125; // Start a goroutine responsible for killing pods (that are not properly // handled by pod workers). go wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop) // Start component sync loops. kl.statusManager.Start() kl.probeManager.Start() // Start syncing RuntimeClasses if enabled. if kl.runtimeClassManager != nil &#123; go kl.runtimeClassManager.Run(wait.NeverStop) &#125; // Start the pod lifecycle event generator. kl.pleg.Start() kl.syncLoop(updates, kl)&#125; syncLoop æ˜¯ kubelet çš„ä¸»å¾ªçŽ¯æ–¹æ³•ï¼Œå®ƒä»Žä¸åŒçš„ç®¡é“(FILE,URL, API-SERVER)ç›‘å¬ pod çš„å˜åŒ–ï¼Œå¹¶æŠŠå®ƒä»¬æ±‡èšèµ·æ¥ã€‚å½“æœ‰æ–°çš„å˜åŒ–å‘ç”Ÿæ—¶ï¼Œå®ƒä¼šè°ƒç”¨å¯¹åº”çš„å‡½æ•°ï¼Œä¿è¯ Pod å¤„äºŽæœŸæœ›çš„çŠ¶æ€ã€‚ 12345678910111213141516171819202122232425262728293031323334func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123; glog.Info(&quot;Starting kubelet main sync loop.&quot;) // syncTicker æ¯ç§’æ£€æµ‹ä¸€æ¬¡æ˜¯å¦æœ‰éœ€è¦åŒæ­¥çš„ pod workers syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for &#123; if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123; glog.Infof(&quot;skipping pod synchronization - %v&quot;, rs) // exponential backoff time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue &#125; // reset backoff if we have a success duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) // if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123; break &#125; kl.syncLoopMonitor.Store(kl.clock.Now()) &#125;&#125; syncLoopIteration() æ–¹æ³•å¯¹å¤šä¸ªç®¡é“è¿›è¡ŒéåŽ†ï¼Œå¦‚æžœ pod å‘ç”Ÿå˜åŒ–ï¼Œåˆ™ä¼šè°ƒç”¨ç›¸åº”çš„ Handlerï¼Œåœ¨ Handler ä¸­é€šè¿‡è°ƒç”¨ dispatchWork åˆ†å‘ä»»åŠ¡ã€‚ æ€»ç»“æœ¬ç¯‡æ–‡ç« ä¸»è¦è®²è¿°äº† kubelet ç»„ä»¶ä»ŽåŠ è½½é…ç½®åˆ°åˆå§‹åŒ–å†…éƒ¨çš„å„ä¸ªæ¨¡å—å†åˆ°å¯åŠ¨ kubelet æœåŠ¡çš„æ•´ä¸ªæµç¨‹ï¼Œä¸Šé¢çš„æ—¶åºå›¾èƒ½æ¸…æ¥šçš„çœ‹åˆ°å‡½æ•°ä¹‹é—´çš„è°ƒç”¨å…³ç³»ï¼Œä½†æ˜¯å…¶ä¸­æ¯ä¸ªç»„ä»¶å…·ä½“çš„å·¥ä½œæ–¹å¼ä»¥åŠç»„ä»¶ä¹‹é—´çš„äº¤äº’æ–¹å¼è¿˜ä¸å¾—è€ŒçŸ¥ï¼ŒåŽé¢ä¼šä¸€æŽ¢ç©¶ç«Ÿã€‚ å‚è€ƒï¼škubernetes node components â€“ kubeletKubelet æºç åˆ†æž(ä¸€):å¯åŠ¨æµç¨‹åˆ†æžkubelet æºç åˆ†æžï¼šå¯åŠ¨æµç¨‹kubernetes çš„ kubelet çš„å·¥ä½œè¿‡ç¨‹kubelet å†…éƒ¨å®žçŽ°è§£æž ç›¸å…³æŽ¨èkubelet çŠ¶æ€ä¸ŠæŠ¥çš„æ–¹å¼kubernets ä¸­äº‹ä»¶å¤„ç†æœºåˆ¶kubelet åˆ›å»º pod çš„æµç¨‹]]></content>
      <tags>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet æž¶æž„æµ…æž]]></title>
    <url>%2F2018%2F12%2F16%2Fkubelet-modules%2F</url>
    <content type="text"><![CDATA[ä¸€ã€æ¦‚è¦kubelet æ˜¯è¿è¡Œåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ä¸»è¦çš„â€œèŠ‚ç‚¹ä»£ç†â€ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½ä¼šå¯åŠ¨ kubeletè¿›ç¨‹ï¼Œç”¨æ¥å¤„ç† Master èŠ‚ç‚¹ä¸‹å‘åˆ°æœ¬èŠ‚ç‚¹çš„ä»»åŠ¡ï¼ŒæŒ‰ç…§ PodSpec æè¿°æ¥ç®¡ç†Pod å’Œå…¶ä¸­çš„å®¹å™¨ï¼ˆPodSpec æ˜¯ç”¨æ¥æè¿°ä¸€ä¸ª pod çš„ YAML æˆ–è€… JSON å¯¹è±¡ï¼‰ã€‚ kubelet é€šè¿‡å„ç§æœºåˆ¶ï¼ˆä¸»è¦é€šè¿‡ apiserver ï¼‰èŽ·å–ä¸€ç»„ PodSpec å¹¶ä¿è¯åœ¨è¿™äº› PodSpec ä¸­æè¿°çš„å®¹å™¨å¥åº·è¿è¡Œã€‚ äºŒã€kubelet çš„ä¸»è¦åŠŸèƒ½1ã€kubelet é»˜è®¤ç›‘å¬å››ä¸ªç«¯å£ï¼Œåˆ†åˆ«ä¸º 10250 ã€10255ã€10248ã€4194ã€‚ 1234LISTEN 0 128 *:10250 *:* users:((&quot;kubelet&quot;,pid=48500,fd=28))LISTEN 0 128 *:10255 *:* users:((&quot;kubelet&quot;,pid=48500,fd=26))LISTEN 0 128 *:4194 *:* users:((&quot;kubelet&quot;,pid=48500,fd=13))LISTEN 0 128 127.0.0.1:10248 *:* users:((&quot;kubelet&quot;,pid=48500,fd=23)) 10250ï¼ˆkubelet APIï¼‰ï¼škubelet server ä¸Ž apiserver é€šä¿¡çš„ç«¯å£ï¼Œå®šæœŸè¯·æ±‚ apiserver èŽ·å–è‡ªå·±æ‰€åº”å½“å¤„ç†çš„ä»»åŠ¡ï¼Œé€šè¿‡è¯¥ç«¯å£å¯ä»¥è®¿é—®èŽ·å– node èµ„æºä»¥åŠçŠ¶æ€ã€‚ 10248ï¼ˆå¥åº·æ£€æŸ¥ç«¯å£ï¼‰ï¼šé€šè¿‡è®¿é—®è¯¥ç«¯å£å¯ä»¥åˆ¤æ–­ kubelet æ˜¯å¦æ­£å¸¸å·¥ä½œ, é€šè¿‡ kubelet çš„å¯åŠ¨å‚æ•° --healthz-port å’Œ --healthz-bind-address æ¥æŒ‡å®šç›‘å¬çš„åœ°å€å’Œç«¯å£ã€‚ 12$ curl http://127.0.0.1:10248/healthzok 4194ï¼ˆcAdvisor ç›‘å¬ï¼‰ï¼škublet é€šè¿‡è¯¥ç«¯å£å¯ä»¥èŽ·å–åˆ°è¯¥èŠ‚ç‚¹çš„çŽ¯å¢ƒä¿¡æ¯ä»¥åŠ node ä¸Šè¿è¡Œçš„å®¹å™¨çŠ¶æ€ç­‰å†…å®¹ï¼Œè®¿é—® http://localhost:4194 å¯ä»¥çœ‹åˆ° cAdvisor çš„ç®¡ç†ç•Œé¢,é€šè¿‡ kubelet çš„å¯åŠ¨å‚æ•° --cadvisor-port å¯ä»¥æŒ‡å®šå¯åŠ¨çš„ç«¯å£ã€‚ 1$ curl http://127.0.0.1:4194/metrics 10255 ï¼ˆreadonly APIï¼‰ï¼šæä¾›äº† pod å’Œ node çš„ä¿¡æ¯ï¼ŒæŽ¥å£ä»¥åªè¯»å½¢å¼æš´éœ²å‡ºåŽ»ï¼Œè®¿é—®è¯¥ç«¯å£ä¸éœ€è¦è®¤è¯å’Œé‰´æƒã€‚123456// èŽ·å– pod çš„æŽ¥å£ï¼Œä¸Ž apiserver çš„ // http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName= æŽ¥å£ç±»ä¼¼$ curl http://127.0.0.1:10255/pods// èŠ‚ç‚¹ä¿¡æ¯æŽ¥å£,æä¾›ç£ç›˜ã€ç½‘ç»œã€CPUã€å†…å­˜ç­‰ä¿¡æ¯$ curl http://127.0.0.1:10255/spec/ 2ã€kubelet ä¸»è¦åŠŸèƒ½ï¼š pod ç®¡ç†ï¼škubelet å®šæœŸä»Žæ‰€ç›‘å¬çš„æ•°æ®æºèŽ·å–èŠ‚ç‚¹ä¸Š pod/container çš„æœŸæœ›çŠ¶æ€ï¼ˆè¿è¡Œä»€ä¹ˆå®¹å™¨ã€è¿è¡Œçš„å‰¯æœ¬æ•°é‡ã€ç½‘ç»œæˆ–è€…å­˜å‚¨å¦‚ä½•é…ç½®ç­‰ç­‰ï¼‰ï¼Œå¹¶è°ƒç”¨å¯¹åº”çš„å®¹å™¨å¹³å°æŽ¥å£è¾¾åˆ°è¿™ä¸ªçŠ¶æ€ã€‚ å®¹å™¨å¥åº·æ£€æŸ¥ï¼škubelet åˆ›å»ºäº†å®¹å™¨ä¹‹åŽè¿˜è¦æŸ¥çœ‹å®¹å™¨æ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œå¦‚æžœå®¹å™¨è¿è¡Œå‡ºé”™ï¼Œå°±è¦æ ¹æ® pod è®¾ç½®çš„é‡å¯ç­–ç•¥è¿›è¡Œå¤„ç†ã€‚ å®¹å™¨ç›‘æŽ§ï¼škubelet ä¼šç›‘æŽ§æ‰€åœ¨èŠ‚ç‚¹çš„èµ„æºä½¿ç”¨æƒ…å†µï¼Œå¹¶å®šæ—¶å‘ master æŠ¥å‘Šï¼Œèµ„æºä½¿ç”¨æ•°æ®éƒ½æ˜¯é€šè¿‡ cAdvisor èŽ·å–çš„ã€‚çŸ¥é“æ•´ä¸ªé›†ç¾¤æ‰€æœ‰èŠ‚ç‚¹çš„èµ„æºæƒ…å†µï¼Œå¯¹äºŽ pod çš„è°ƒåº¦å’Œæ­£å¸¸è¿è¡Œè‡³å…³é‡è¦ã€‚ ä¸‰ã€kubelet ç»„ä»¶ä¸­çš„æ¨¡å— ä¸Šå›¾å±•ç¤ºäº† kubelet ç»„ä»¶ä¸­çš„æ¨¡å—ä»¥åŠæ¨¡å—é—´çš„åˆ’åˆ†ã€‚ 1ã€PLEG(Pod Lifecycle Event Generatorï¼‰PLEG æ˜¯ kubelet çš„æ ¸å¿ƒæ¨¡å—,PLEG ä¼šä¸€ç›´è°ƒç”¨ container runtime èŽ·å–æœ¬èŠ‚ç‚¹ containers/sandboxes çš„ä¿¡æ¯ï¼Œå¹¶ä¸Žè‡ªèº«ç»´æŠ¤çš„ pods cache ä¿¡æ¯è¿›è¡Œå¯¹æ¯”ï¼Œç”Ÿæˆå¯¹åº”çš„ PodLifecycleEventï¼Œç„¶åŽè¾“å‡ºåˆ° eventChannel ä¸­ï¼Œé€šè¿‡ eventChannel å‘é€åˆ° kubelet syncLoop è¿›è¡Œæ¶ˆè´¹ï¼Œç„¶åŽç”± kubelet syncPod æ¥è§¦å‘ pod åŒæ­¥å¤„ç†è¿‡ç¨‹ï¼Œæœ€ç»ˆè¾¾åˆ°ç”¨æˆ·çš„æœŸæœ›çŠ¶æ€ã€‚ 2ã€cAdvisorcAdvisorï¼ˆhttps://github.com/google/cadvisorï¼‰æ˜¯ google å¼€å‘çš„å®¹å™¨ç›‘æŽ§å·¥å…·ï¼Œé›†æˆåœ¨ kubelet ä¸­ï¼Œèµ·åˆ°æ”¶é›†æœ¬èŠ‚ç‚¹å’Œå®¹å™¨çš„ç›‘æŽ§ä¿¡æ¯ï¼Œå¤§éƒ¨åˆ†å…¬å¸å¯¹å®¹å™¨çš„ç›‘æŽ§æ•°æ®éƒ½æ˜¯ä»Ž cAdvisor ä¸­èŽ·å–çš„ ï¼ŒcAvisor æ¨¡å—å¯¹å¤–æä¾›äº† interface æŽ¥å£ï¼Œè¯¥æŽ¥å£ä¹Ÿè¢« imageManagerï¼ŒOOMWatcherï¼ŒcontainerManager ç­‰æ‰€ä½¿ç”¨ã€‚ 3ã€OOMWatcherç³»ç»Ÿ OOM çš„ç›‘å¬å™¨ï¼Œä¼šä¸Ž cadvisor æ¨¡å—ä¹‹é—´å»ºç«‹ SystemOOM,é€šè¿‡ Watchæ–¹å¼ä»Ž cadvisor é‚£é‡Œæ”¶åˆ°çš„ OOM ä¿¡å·ï¼Œå¹¶äº§ç”Ÿç›¸å…³äº‹ä»¶ã€‚ 4ã€probeManagerprobeManager ä¾èµ–äºŽ statusManager,livenessManager,containerRefManagerï¼Œä¼šå®šæ—¶åŽ»ç›‘æŽ§ pod ä¸­å®¹å™¨çš„å¥åº·çŠ¶å†µï¼Œå½“å‰æ”¯æŒä¸¤ç§ç±»åž‹çš„æŽ¢é’ˆï¼šlivenessProbe å’ŒreadinessProbeã€‚livenessProbeï¼šç”¨äºŽåˆ¤æ–­å®¹å™¨æ˜¯å¦å­˜æ´»ï¼Œå¦‚æžœæŽ¢æµ‹å¤±è´¥ï¼Œkubelet ä¼š kill æŽ‰è¯¥å®¹å™¨ï¼Œå¹¶æ ¹æ®å®¹å™¨çš„é‡å¯ç­–ç•¥åšç›¸åº”çš„å¤„ç†ã€‚readinessProbeï¼šç”¨äºŽåˆ¤æ–­å®¹å™¨æ˜¯å¦å¯åŠ¨å®Œæˆï¼Œå°†æŽ¢æµ‹æˆåŠŸçš„å®¹å™¨åŠ å…¥åˆ°è¯¥ pod æ‰€åœ¨ service çš„ endpoints ä¸­ï¼Œåä¹‹åˆ™ç§»é™¤ã€‚readinessProbe å’Œ livenessProbe æœ‰ä¸‰ç§å®žçŽ°æ–¹å¼ï¼šhttpã€tcp ä»¥åŠ cmdã€‚ 5ã€statusManagerstatusManager è´Ÿè´£ç»´æŠ¤çŠ¶æ€ä¿¡æ¯ï¼Œå¹¶æŠŠ pod çŠ¶æ€æ›´æ–°åˆ° apiserverï¼Œä½†æ˜¯å®ƒå¹¶ä¸è´Ÿè´£ç›‘æŽ§ pod çŠ¶æ€çš„å˜åŒ–ï¼Œè€Œæ˜¯æä¾›å¯¹åº”çš„æŽ¥å£ä¾›å…¶ä»–ç»„ä»¶è°ƒç”¨ï¼Œæ¯”å¦‚ probeManagerã€‚ 6ã€containerRefManagerå®¹å™¨å¼•ç”¨çš„ç®¡ç†ï¼Œç›¸å¯¹ç®€å•çš„Managerï¼Œç”¨æ¥æŠ¥å‘Šå®¹å™¨çš„åˆ›å»ºï¼Œå¤±è´¥ç­‰äº‹ä»¶ï¼Œé€šè¿‡å®šä¹‰ map æ¥å®žçŽ°äº† containerID ä¸Ž v1.ObjectReferece å®¹å™¨å¼•ç”¨çš„æ˜ å°„ã€‚ 7ã€evictionManagerå½“èŠ‚ç‚¹çš„å†…å­˜ã€ç£ç›˜æˆ– inode ç­‰èµ„æºä¸è¶³æ—¶ï¼Œè¾¾åˆ°äº†é…ç½®çš„ evict ç­–ç•¥ï¼Œ node ä¼šå˜ä¸º pressure çŠ¶æ€ï¼Œæ­¤æ—¶ kubelet ä¼šæŒ‰ç…§ qosClass é¡ºåºæ¥é©±èµ¶ podï¼Œä»¥æ­¤æ¥ä¿è¯èŠ‚ç‚¹çš„ç¨³å®šæ€§ã€‚å¯ä»¥é€šè¿‡é…ç½® kubelet å¯åŠ¨å‚æ•° --eviction-hard= æ¥å†³å®š evict çš„ç­–ç•¥å€¼ã€‚ 8ã€imageGCimageGC è´Ÿè´£ node èŠ‚ç‚¹çš„é•œåƒå›žæ”¶ï¼Œå½“æœ¬åœ°çš„å­˜æ”¾é•œåƒçš„æœ¬åœ°ç£ç›˜ç©ºé—´è¾¾åˆ°æŸé˜ˆå€¼çš„æ—¶å€™ï¼Œä¼šè§¦å‘é•œåƒçš„å›žæ”¶ï¼Œåˆ é™¤æŽ‰ä¸è¢« pod æ‰€ä½¿ç”¨çš„é•œåƒï¼Œå›žæ”¶é•œåƒçš„é˜ˆå€¼å¯ä»¥é€šè¿‡ kubelet çš„å¯åŠ¨å‚æ•° --image-gc-high-threshold å’Œ --image-gc-low-threshold æ¥è®¾ç½®ã€‚ 9ã€containerGCcontainerGC è´Ÿè´£æ¸…ç† node èŠ‚ç‚¹ä¸Šå·²æ¶ˆäº¡çš„ containerï¼Œå…·ä½“çš„ GC æ“ä½œç”±runtime æ¥å®žçŽ°ã€‚ 10ã€imageManagerè°ƒç”¨ kubecontainer æä¾›çš„PullImage/GetImageRef/ListImages/RemoveImage/ImageStates æ–¹æ³•æ¥ä¿è¯pod è¿è¡Œæ‰€éœ€è¦çš„é•œåƒã€‚ 11ã€volumeManagerè´Ÿè´£ node èŠ‚ç‚¹ä¸Š pod æ‰€ä½¿ç”¨ volume çš„ç®¡ç†ï¼Œvolume ä¸Ž pod çš„ç”Ÿå‘½å‘¨æœŸå…³è”ï¼Œè´Ÿè´£ pod åˆ›å»ºåˆ é™¤è¿‡ç¨‹ä¸­ volume çš„ mount/umount/attach/detach æµç¨‹ï¼Œkubernetes é‡‡ç”¨ volume Plugins çš„æ–¹å¼ï¼Œå®žçŽ°å­˜å‚¨å·çš„æŒ‚è½½ç­‰æ“ä½œï¼Œå†…ç½®å‡ åç§å­˜å‚¨æ’ä»¶ã€‚ 12ã€containerManagerè´Ÿè´£ node èŠ‚ç‚¹ä¸Šè¿è¡Œçš„å®¹å™¨çš„ cgroup é…ç½®ä¿¡æ¯ï¼Œkubelet å¯åŠ¨å‚æ•°å¦‚æžœæŒ‡å®š --cgroups-per-qos çš„æ—¶å€™ï¼Œkubelet ä¼šå¯åŠ¨ goroutine æ¥å‘¨æœŸæ€§çš„æ›´æ–° pod çš„ cgroup ä¿¡æ¯ï¼Œç»´æŠ¤å…¶æ­£ç¡®æ€§ï¼Œè¯¥å‚æ•°é»˜è®¤ä¸º trueï¼Œå®žçŽ°äº† pod çš„Guaranteed/BestEffort/Burstable ä¸‰ç§çº§åˆ«çš„ Qosã€‚ 13ã€runtimeManagercontainerRuntime è´Ÿè´£ kubelet ä¸Žä¸åŒçš„ runtime å®žçŽ°è¿›è¡Œå¯¹æŽ¥ï¼Œå®žçŽ°å¯¹äºŽåº•å±‚ container çš„æ“ä½œï¼Œåˆå§‹åŒ–ä¹‹åŽå¾—åˆ°çš„ runtime å®žä¾‹å°†ä¼šè¢«ä¹‹å‰æè¿°çš„ç»„ä»¶æ‰€ä½¿ç”¨ã€‚å¯ä»¥é€šè¿‡ kubelet çš„å¯åŠ¨å‚æ•° --container-runtime æ¥å®šä¹‰æ˜¯ä½¿ç”¨docker è¿˜æ˜¯ rktï¼Œé»˜è®¤æ˜¯ dockerã€‚ 14ã€podManagerpodManager æä¾›äº†æŽ¥å£æ¥å­˜å‚¨å’Œè®¿é—® pod çš„ä¿¡æ¯ï¼Œç»´æŒ static pod å’Œ mirror pods çš„å…³ç³»ï¼ŒpodManager ä¼šè¢«statusManager/volumeManager/runtimeManager æ‰€è°ƒç”¨ï¼ŒpodManager çš„æŽ¥å£å¤„ç†æµç¨‹é‡Œé¢ä¼šè°ƒç”¨ secretManager ä»¥åŠ configMapManagerã€‚ åœ¨ v1.12 ä¸­ï¼Œkubelet ç»„ä»¶æœ‰18ä¸ª managerï¼š 123456789101112131415161718certificateManagercgroupManagercontainerManagercpuManagernodeContainerManagerconfigmapManagercontainerReferenceManagerevictionManagernvidiaGpuManagerimageGCManagerkuberuntimeManagerhostportManagerpodManagerproberManagersecretManagerstatusManagervolumeManager tokenManager å…¶ä¸­æ¯”è¾ƒé‡è¦çš„æ¨¡å—åŽé¢ä¼šè¿›è¡Œä¸€ä¸€åˆ†æžã€‚ å‚è€ƒï¼šå¾®è½¯èµ„æ·±å·¥ç¨‹å¸ˆè¯¦è§£ K8S å®¹å™¨è¿è¡Œæ—¶kubernetes ç®€ä»‹ï¼š kubelet å’Œ podKubelet ç»„ä»¶è§£æž ç›¸å…³æŽ¨èkubelet çŠ¶æ€ä¸ŠæŠ¥çš„æ–¹å¼kubernets ä¸­äº‹ä»¶å¤„ç†æœºåˆ¶kubelet åˆ›å»º pod çš„æµç¨‹]]></content>
      <tags>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker æž¶æž„ä¸­çš„å‡ ä¸ªæ ¸å¿ƒæ¦‚å¿µ]]></title>
    <url>%2F2018%2F12%2F05%2Fdocker-introduces%2F</url>
    <content type="text"><![CDATA[ä¸€ã€Docker å¼€æºä¹‹è·¯2015 å¹´ 6 æœˆ ï¼Œdocker å…¬å¸å°† libcontainer æå‡ºå¹¶æ”¹åä¸º runC é¡¹ç›®ï¼Œäº¤ç”±ä¸€ä¸ªå®Œå…¨ä¸­ç«‹çš„åŸºé‡‘ä¼šç®¡ç†ï¼Œç„¶åŽä»¥ runC ä¸ºä¾æ®ï¼Œå¤§å®¶å…±åŒåˆ¶å®šä¸€å¥—å®¹å™¨å’Œé•œåƒçš„æ ‡å‡†å’Œè§„èŒƒ OCIã€‚ 2016 å¹´ 4 æœˆï¼Œdocker 1.11 ç‰ˆæœ¬ä¹‹åŽå¼€å§‹å¼•å…¥äº† containerd å’Œ runCï¼ŒDocker å¼€å§‹ä¾èµ–äºŽ containerd å’Œ runC æ¥ç®¡ç†å®¹å™¨ï¼Œcontainerd ä¹Ÿå¯ä»¥æ“ä½œæ»¡è¶³ OCI æ ‡å‡†è§„èŒƒçš„å…¶ä»–å®¹å™¨å·¥å…·ï¼Œä¹‹åŽåªè¦æ˜¯æŒ‰ç…§ OCI æ ‡å‡†è§„èŒƒå¼€å‘çš„å®¹å™¨å·¥å…·ï¼Œéƒ½å¯ä»¥è¢« containerd ä½¿ç”¨èµ·æ¥ã€‚ ä»Ž 2017 å¹´å¼€å§‹ï¼ŒDocker å…¬å¸å…ˆæ˜¯å°† Dockeré¡¹ç›®çš„å®¹å™¨è¿è¡Œæ—¶éƒ¨åˆ† Containerd æèµ ç»™CNCF ç¤¾åŒºï¼Œç´§æŽ¥ç€ï¼ŒDocker å…¬å¸å®£å¸ƒå°† Docker é¡¹ç›®æ”¹åä¸º Mobyã€‚ äºŒã€Docker æž¶æž„ ä¸‰ã€æ ¸å¿ƒæ¦‚å¿µdocker 1.13 ç‰ˆæœ¬ä¸­åŒ…å«ä»¥ä¸‹å‡ ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶ã€‚123456$ docker --versionDocker version 1.13.1, build 092cba3$ dockerdocker docker-containerd-ctr dockerd docker-proxydocker-containerd docker-containerd-shim docker-init docker-runc 1ã€dockerdocker çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œæ˜¯ç»™ç”¨æˆ·å’Œ docker daemon å»ºç«‹é€šä¿¡çš„å®¢æˆ·ç«¯ã€‚ 2ã€dockerddockerd æ˜¯ docker æž¶æž„ä¸­ä¸€ä¸ªå¸¸é©»åœ¨åŽå°çš„ç³»ç»Ÿè¿›ç¨‹ï¼Œç§°ä¸º docker daemonï¼Œdockerd å®žé™…è°ƒç”¨çš„è¿˜æ˜¯ containerd çš„ api æŽ¥å£ï¼ˆrpc æ–¹å¼å®žçŽ°ï¼‰,docker daemon çš„ä½œç”¨ä¸»è¦æœ‰ä»¥ä¸‹ä¸¤æ–¹é¢ï¼š æŽ¥æ”¶å¹¶å¤„ç† docker client å‘é€çš„è¯·æ±‚ ç®¡ç†æ‰€æœ‰çš„ docker å®¹å™¨ æœ‰äº† containerd ä¹‹åŽï¼Œdockerd å¯ä»¥ç‹¬ç«‹å‡çº§ï¼Œä»¥æ­¤é¿å…ä¹‹å‰ dockerd å‡çº§ä¼šå¯¼è‡´æ‰€æœ‰å®¹å™¨ä¸å¯ç”¨çš„é—®é¢˜ã€‚ 3ã€containerdcontainerd æ˜¯ dockerd å’Œ runc ä¹‹é—´çš„ä¸€ä¸ªä¸­é—´äº¤æµç»„ä»¶ï¼Œdocker å¯¹å®¹å™¨çš„ç®¡ç†å’Œæ“ä½œåŸºæœ¬éƒ½æ˜¯é€šè¿‡ containerd å®Œæˆçš„ã€‚containerd çš„ä¸»è¦åŠŸèƒ½æœ‰ï¼š å®¹å™¨ç”Ÿå‘½å‘¨æœŸç®¡ç† æ—¥å¿—ç®¡ç† é•œåƒç®¡ç† å­˜å‚¨ç®¡ç† å®¹å™¨ç½‘ç»œæŽ¥å£åŠç½‘ç»œç®¡ç† 4ã€containerd-shimcontainerd-shim æ˜¯ä¸€ä¸ªçœŸå®žè¿è¡Œå®¹å™¨çš„è½½ä½“ï¼Œæ¯å¯åŠ¨ä¸€ä¸ªå®¹å™¨éƒ½ä¼šèµ·ä¸€ä¸ªæ–°çš„containerd-shimçš„ä¸€ä¸ªè¿›ç¨‹ï¼Œ å®ƒç›´æŽ¥é€šè¿‡æŒ‡å®šçš„ä¸‰ä¸ªå‚æ•°ï¼šå®¹å™¨idï¼Œboundleç›®å½•ï¼ˆcontainerd å¯¹åº”æŸä¸ªå®¹å™¨ç”Ÿæˆçš„ç›®å½•ï¼Œä¸€èˆ¬ä½äºŽï¼š/var/run/docker/libcontainerd/containerIDï¼Œå…¶ä¸­åŒ…æ‹¬äº†å®¹å™¨é…ç½®å’Œæ ‡å‡†è¾“å…¥ã€æ ‡å‡†è¾“å‡ºã€æ ‡å‡†é”™è¯¯ä¸‰ä¸ªç®¡é“æ–‡ä»¶ï¼‰ï¼Œè¿è¡Œæ—¶äºŒè¿›åˆ¶ï¼ˆé»˜è®¤ä¸ºrunCï¼‰æ¥è°ƒç”¨ runc çš„ api åˆ›å»ºä¸€ä¸ªå®¹å™¨ï¼Œä¸Šé¢çš„ docker è¿›ç¨‹å›¾ä¸­å¯ä»¥ç›´è§‚çš„æ˜¾ç¤ºã€‚å…¶ä¸»è¦ä½œç”¨æ˜¯ï¼š å®ƒå…è®¸å®¹å™¨è¿è¡Œæ—¶(å³ runC)åœ¨å¯åŠ¨å®¹å™¨ä¹‹åŽé€€å‡ºï¼Œç®€å•è¯´å°±æ˜¯ä¸å¿…ä¸ºæ¯ä¸ªå®¹å™¨ä¸€ç›´è¿è¡Œä¸€ä¸ªå®¹å™¨è¿è¡Œæ—¶(runC) å³ä½¿åœ¨ containerd å’Œ dockerd éƒ½æŒ‚æŽ‰çš„æƒ…å†µä¸‹ï¼Œå®¹å™¨çš„æ ‡å‡† IO å’Œå…¶å®ƒçš„æ–‡ä»¶æè¿°ç¬¦ä¹Ÿéƒ½æ˜¯å¯ç”¨çš„ å‘ containerd æŠ¥å‘Šå®¹å™¨çš„é€€å‡ºçŠ¶æ€ æœ‰äº†å®ƒå°±å¯ä»¥åœ¨ä¸ä¸­æ–­å®¹å™¨è¿è¡Œçš„æƒ…å†µä¸‹å‡çº§æˆ–é‡å¯ dockerdï¼Œå¯¹äºŽç”Ÿäº§çŽ¯å¢ƒæ¥è¯´æ„ä¹‰é‡å¤§ã€‚ 5ã€runCrunC æ˜¯ Docker å…¬å¸æŒ‰ç…§ OCI æ ‡å‡†è§„èŒƒç¼–å†™çš„ä¸€ä¸ªæ“ä½œå®¹å™¨çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œå…¶å‰èº«æ˜¯ libcontainer é¡¹ç›®æ¼”åŒ–è€Œæ¥ï¼ŒrunC å®žé™…ä¸Šå°±æ˜¯ libcontainer é…ä¸Šäº†ä¸€ä¸ªè½»åž‹çš„å®¢æˆ·ç«¯ï¼Œæ˜¯ä¸€ä¸ªå‘½ä»¤è¡Œå·¥å…·ç«¯ï¼Œæ ¹æ® OCIï¼ˆå¼€æ”¾å®¹å™¨ç»„ç»‡ï¼‰çš„æ ‡å‡†æ¥åˆ›å»ºå’Œè¿è¡Œå®¹å™¨ï¼Œå®žçŽ°äº†å®¹å™¨å¯åœã€èµ„æºéš”ç¦»ç­‰åŠŸèƒ½ã€‚ ä¸€ä¸ªä¾‹å­ï¼Œä½¿ç”¨ runC è¿è¡Œ busybox å®¹å™¨:12345678910111213141516171819# mkdir /container# cd /container/# mkdir rootfså‡†å¤‡å®¹å™¨é•œåƒçš„æ–‡ä»¶ç³»ç»Ÿ,ä»Ž busybox é•œåƒä¸­æå–# docker export $(docker create busybox) | tar -C rootfs -xvf - # ls rootfs/bin dev etc home proc root sys tmp usr varæœ‰äº†rootfsä¹‹åŽï¼Œæˆ‘ä»¬è¿˜è¦æŒ‰ç…§ OCI æ ‡å‡†æœ‰ä¸€ä¸ªé…ç½®æ–‡ä»¶ config.json è¯´æ˜Žå¦‚ä½•è¿è¡Œå®¹å™¨ï¼ŒåŒ…æ‹¬è¦è¿è¡Œçš„å‘½ä»¤ã€æƒé™ã€çŽ¯å¢ƒå˜é‡ç­‰ç­‰å†…å®¹ï¼Œrunc æä¾›äº†ä¸€ä¸ªå‘½ä»¤å¯ä»¥è‡ªåŠ¨å¸®æˆ‘ä»¬ç”Ÿæˆ# docker-runc spec# lsconfig.json rootfs# docker-runc run simplebusybox #å¯åŠ¨å®¹å™¨/ # lsbin dev etc home proc root sys tmp usr var/ # hostnamerunc å‚è€ƒï¼šUse of containerd-shim in docker-architectureä»Ž docker åˆ° runCOCI å’Œ runcï¼šå®¹å™¨æ ‡å‡†åŒ–å’Œ dockerOpen Container Initiative]]></content>
  </entry>
  <entry>
    <title><![CDATA[kubernetes å¸¸ç”¨ API]]></title>
    <url>%2F2018%2F09%2F02%2Fkubernetes-api%2F</url>
    <content type="text"><![CDATA[kubectl çš„æ‰€æœ‰æ“ä½œéƒ½æ˜¯è°ƒç”¨ kube-apisever çš„ API å®žçŽ°çš„ï¼Œæ‰€ä»¥å…¶å­å‘½ä»¤éƒ½æœ‰ç›¸åº”çš„ APIï¼Œæ¯æ¬¡åœ¨è°ƒç”¨ kubectl æ—¶ä½¿ç”¨å‚æ•° -v=9 å¯ä»¥çœ‹è°ƒç”¨çš„ç›¸å…³ APIï¼Œä¾‹ï¼š $ kubectl get node -v=9 ä»¥ä¸‹ä¸º kubernetes å¼€å‘ä¸­å¸¸ç”¨çš„ APIï¼š Markdown è¡¨æ ¼æ˜¾ç¤ºè¿‡å¤§ï¼Œæ­¤ä»…ä»¥å›¾ç‰‡æ ¼å¼å±•ç¤ºã€‚]]></content>
  </entry>
  <entry>
    <title><![CDATA[etcd å¯ç”¨ https]]></title>
    <url>%2F2017%2F03%2F15%2Fetcd-enable-https%2F</url>
    <content type="text"><![CDATA[1ï¼Œ ç”Ÿæˆ TLS ç§˜é’¥å¯¹ 2ï¼Œæ‹·è´å¯†é’¥å¯¹åˆ°æ‰€æœ‰èŠ‚ç‚¹ 3ï¼Œé…ç½® etcd ä½¿ç”¨è¯ä¹¦ 4ï¼Œæµ‹è¯• etcd æ˜¯å¦æ­£å¸¸ 5ï¼Œé…ç½® kube-apiserver ä½¿ç”¨ CA è¿žæŽ¥ etcd 6ï¼Œæµ‹è¯• kube-apiserver 7ï¼Œæœªè§£å†³çš„é—®é¢˜ SSL/TSL è®¤è¯åˆ†å•å‘è®¤è¯å’ŒåŒå‘è®¤è¯ä¸¤ç§æ–¹å¼ã€‚ç®€å•è¯´å°±æ˜¯å•å‘è®¤è¯åªæ˜¯å®¢æˆ·ç«¯å¯¹æœåŠ¡ç«¯çš„èº«ä»½è¿›è¡ŒéªŒè¯ï¼ŒåŒå‘è®¤è¯æ˜¯å®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯äº’ç›¸è¿›è¡Œèº«ä»½è®¤è¯ã€‚å°±æ¯”å¦‚ï¼Œæˆ‘ä»¬ç™»å½•æ·˜å®ä¹°ä¸œè¥¿ï¼Œä¸ºäº†é˜²æ­¢æˆ‘ä»¬ç™»å½•çš„æ˜¯å‡æ·˜å®ç½‘ç«™ï¼Œæ­¤æ—¶æˆ‘ä»¬é€šè¿‡æµè§ˆå™¨æ‰“å¼€æ·˜å®ä¹°ä¸œè¥¿æ—¶ï¼Œæµè§ˆå™¨ä¼šéªŒè¯æˆ‘ä»¬ç™»å½•çš„ç½‘ç«™æ˜¯å¦æ˜¯çœŸçš„æ·˜å®çš„ç½‘ç«™ï¼Œè€Œæ·˜å®ç½‘ç«™ä¸å…³å¿ƒæˆ‘ä»¬æ˜¯å¦â€œåˆæ³•â€ï¼Œè¿™å°±æ˜¯å•å‘è®¤è¯ã€‚è€ŒåŒå‘è®¤è¯æ˜¯æœåŠ¡ç«¯ä¹Ÿéœ€è¦å¯¹å®¢æˆ·ç«¯åšå‡ºè®¤è¯ã€‚ å› ä¸ºå¤§éƒ¨åˆ† kubernetes åŸºäºŽå†…ç½‘éƒ¨ç½²ï¼Œè€Œå†…ç½‘åº”è¯¥éƒ½ä¼šé‡‡ç”¨ç§æœ‰ IP åœ°å€é€šè®¯ï¼Œæƒå¨ CA å¥½åƒåªèƒ½ç­¾ç½²åŸŸåè¯ä¹¦ï¼Œå¯¹äºŽç­¾ç½²åˆ° IP å¯èƒ½æ— æ³•å®žçŽ°ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦é¢„å…ˆè‡ªå»º CA ç­¾å‘è¯ä¹¦ã€‚ Generate self-signed certificates å®˜æ–¹å‚è€ƒæ–‡æ¡£ å®˜æ–¹æŽ¨èä½¿ç”¨ cfssl æ¥è‡ªå»º CA ç­¾å‘è¯ä¹¦ï¼Œå½“ç„¶ä½ ä¹Ÿå¯ä»¥ç”¨ä¼—äººç†ŸçŸ¥çš„ OpenSSL æˆ–è€… easy-rsaã€‚ä»¥ä¸‹æ­¥éª¤éµå¾ªå®˜æ–¹æ–‡æ¡£ï¼š 1ï¼Œ ç”Ÿæˆ TLS ç§˜é’¥å¯¹ç”Ÿæˆæ­¥éª¤ï¼š 1ï¼Œä¸‹è½½ cfssl 2ï¼Œåˆå§‹åŒ–è¯ä¹¦é¢å‘æœºæž„ 3ï¼Œé…ç½® CA é€‰é¡¹ 4ï¼Œç”ŸæˆæœåŠ¡å™¨ç«¯è¯ä¹¦ 5ï¼Œç”Ÿæˆå¯¹ç­‰è¯ä¹¦ 6ï¼Œç”Ÿæˆå®¢æˆ·ç«¯è¯ä¹¦ æƒ³æ·±å…¥äº†è§£ HTTPS çš„çœ‹è¿™é‡Œï¼š èŠèŠHTTPSå’ŒSSL/TLSåè®® æ•°å­—è¯ä¹¦CAåŠæ‰«ç›² äº’è”ç½‘åŠ å¯†åŠOpenSSLä»‹ç»å’Œç®€å•ä½¿ç”¨ SSLåŒå‘è®¤è¯å’Œå•å‘è®¤è¯çš„åŒºåˆ« 1ï¼Œä¸‹è½½ cfsslmkdir ~/bin curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x ~/bin/{cfssl,cfssljson} export PATH=$PATH:~/bin 2ï¼Œåˆå§‹åŒ–è¯ä¹¦é¢å‘æœºæž„1234mkdir ~/cfsslcd ~/cfsslcfssl print-defaults config &gt; ca-config.jsoncfssl print-defaults csr &gt; ca-csr.json è¯ä¹¦ç±»åž‹ä»‹ç»ï¼š client certificate ç”¨äºŽé€šè¿‡æœåŠ¡å™¨éªŒè¯å®¢æˆ·ç«¯ã€‚ä¾‹å¦‚etcdctlï¼Œetcd proxyï¼Œfleetctlæˆ–dockerå®¢æˆ·ç«¯ã€‚ server certificate ç”±æœåŠ¡å™¨ä½¿ç”¨ï¼Œå¹¶ç”±å®¢æˆ·ç«¯éªŒè¯æœåŠ¡å™¨èº«ä»½ã€‚ä¾‹å¦‚dockeræœåŠ¡å™¨æˆ–kube-apiserverã€‚ peer certificate ç”± etcd é›†ç¾¤æˆå‘˜ä½¿ç”¨ï¼Œä¾›å®ƒä»¬å½¼æ­¤ä¹‹é—´é€šä¿¡ä½¿ç”¨ã€‚ 3ï¼Œé…ç½® CA é€‰é¡¹123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566$ cat &lt;&lt; EOF &gt; ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;server&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot; ] &#125;, &quot;client&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;client auth&quot; ] &#125;, &quot;peer&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;$ cat &lt;&lt; EOF &gt; ca-csr.json&#123; &quot;CN&quot;: &quot;My own CA&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;US&quot;, &quot;L&quot;: &quot;CA&quot;, &quot;O&quot;: &quot;My Company Name&quot;, &quot;ST&quot;: &quot;San Francisco&quot;, &quot;OU&quot;: &quot;Org Unit 1&quot;, &quot;OU&quot;: &quot;Org Unit 2&quot; &#125; ]&#125;ç”Ÿæˆ CA è¯ä¹¦ï¼š$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -å°†ä¼šç”Ÿæˆä»¥ä¸‹å‡ ä¸ªæ–‡ä»¶ï¼šca-key.pemca.csrca.pem è¯·åŠ¡å¿…ä¿è¯ ca-key.pem æ–‡ä»¶çš„å®‰å…¨ï¼Œ*.csr æ–‡ä»¶åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ä¸ä¼šä½¿ç”¨ã€‚ 4ï¼Œç”ŸæˆæœåŠ¡å™¨ç«¯è¯ä¹¦12345678$ echo &apos;&#123;&quot;CN&quot;:&quot;coreos1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&quot;10.93.81.17,127.0.0.1,server&quot; - | cfssljson -bare serverhosts å­—æ®µéœ€è¦è‡ªå®šä¹‰ã€‚ç„¶åŽå°†å¾—åˆ°ä»¥ä¸‹å‡ ä¸ªæ–‡ä»¶ï¼šserver-key.pemserver.csrserver.pem 5ï¼Œç”Ÿæˆå¯¹ç­‰è¯ä¹¦1234567891011$ echo &apos;&#123;&quot;CN&quot;:&quot;member1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=&quot;10.93.81.17,127.0.0.1,server,member1&quot; - | cfssljson -bare member1hosts å­—æ®µéœ€è¦è‡ªå®šä¹‰ã€‚ç„¶åŽå°†å¾—åˆ°ä»¥ä¸‹å‡ ä¸ªæ–‡ä»¶ï¼šmember1-key.pemmember1.csrmember1.pemå¦‚æžœæœ‰å¤šä¸ª etcd æˆå‘˜ï¼Œé‡å¤æ­¤æ­¥ä¸ºæ¯ä¸ªæˆå‘˜ç”Ÿæˆå¯¹ç­‰è¯ä¹¦ã€‚ 6ï¼Œç”Ÿæˆå®¢æˆ·ç«¯è¯ä¹¦123456789$ echo &apos;&#123;&quot;CN&quot;:&quot;client&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare clienthosts å­—æ®µéœ€è¦è‡ªå®šä¹‰ã€‚ç„¶åŽå°†å¾—åˆ°ä»¥ä¸‹å‡ ä¸ªæ–‡ä»¶ï¼šclient-key.pemclient.csrclient.pem è‡³æ­¤ï¼Œæ‰€æœ‰è¯ä¹¦éƒ½å·²ç”Ÿæˆå®Œæ¯•ã€‚ 2ï¼Œæ‹·è´å¯†é’¥å¯¹åˆ°æ‰€æœ‰èŠ‚ç‚¹ 1ï¼Œæ‹·è´å¯†é’¥å¯¹åˆ°æ‰€æœ‰èŠ‚ç‚¹ 2ï¼Œæ›´æ–°ç³»ç»Ÿè¯ä¹¦åº“ 1ï¼Œæ‹·è´å¯†é’¥å¯¹åˆ°æ‰€æœ‰èŠ‚ç‚¹12345$ mkdir -pv /etc/ssl/etcd/$ cp ~/cfssl/* /etc/ssl/etcd/$ chown -R etcd:etcd /etc/ssl/etcd$ chmod 600 /etc/ssl/etcd/*-key.pem$ cp ~/cfssl/ca.pem /etc/ssl/certs/ 2ï¼Œæ›´æ–°ç³»ç»Ÿè¯ä¹¦åº“123$ yum install ca-certificates -y $ update-ca-trust 3ï¼Œé…ç½® etcd ä½¿ç”¨è¯ä¹¦12345678910111213141516171819202122232425262728293031323334353637$ etcdctl versionetcdctl version: 3.1.3API version: 3.1$ cat /etc/etcd/etcd.confETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;#ç›‘å¬URLï¼Œç”¨äºŽä¸Žå…¶ä»–èŠ‚ç‚¹é€šè®¯ETCD_LISTEN_PEER_URLS=&quot;https://10.93.81.17:2380&quot;#å‘ŠçŸ¥å®¢æˆ·ç«¯çš„URL, ä¹Ÿå°±æ˜¯æœåŠ¡çš„URLETCD_LISTEN_CLIENT_URLS=&quot;https://10.93.81.17:2379,https://10.93.81.17:4001&quot;#è¡¨ç¤ºç›‘å¬å…¶ä»–èŠ‚ç‚¹åŒæ­¥ä¿¡å·çš„åœ°å€ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.93.81.17:2380&quot;#â€“advertise-client-urls å‘ŠçŸ¥å®¢æˆ·ç«¯çš„URL, ä¹Ÿå°±æ˜¯æœåŠ¡çš„URLï¼Œtcp2379ç«¯å£ç”¨äºŽç›‘å¬å®¢æˆ·ç«¯è¯·æ±‚ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.93.81.17:2379&quot;#å¯åŠ¨å‚æ•°é…ç½®ETCD_NAME=&quot;node1&quot;ETCD_INITIAL_CLUSTER=&quot;node1=https://10.93.81.17:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#[security]ETCD_CERT_FILE=&quot;/etc/ssl/etcd/server.pem&quot;ETCD_KEY_FILE=&quot;/etc/ssl/etcd/server-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/etc/ssl/etcd/member1.pem&quot;ETCD_PEER_KEY_FILE=&quot;/etc/ssl/etcd/member1-key.pem&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;#[logging]ETCD_DEBUG=&quot;true&quot;ETCD_LOG_PACKAGE_LEVELS=&quot;etcdserver=WARNING,security=DEBUG&quot; 4ï¼Œæµ‹è¯• etcd æ˜¯å¦æ­£å¸¸123456789101112$ systemctl restart etcdå¦‚æžœæŠ¥é”™ï¼Œä½¿ç”¨ journalctl -f -t etcd å’Œ journalctl -u etcd æ¥å®šä½é—®é¢˜ã€‚$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health&#123;&quot;health&quot;: &quot;true&quot;&#125;$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list $ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar &quot;hello world&quot; $ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar 5ï¼Œé…ç½® kube-apiserver ä½¿ç”¨ CA è¿žæŽ¥ etcd1234567$ cp /etc/ssl/etcd/* /var/run/kubernetes/ $ chown -R kube.kube /var/run/kubernetes/åœ¨ /etc/kubernetes/apiserver ä¸­ KUBE_API_ARGS æ–°åŠ ä¸€ä¸‹å‡ ä¸ªå‚æ•°ï¼š--cert-dir=&apos;/var/run/kubernetes/&apos; --etcd-cafile=&apos;/var/run/kubernetes/ca.pem&apos; --etcd-certfile=&apos;/var/run/kubernetes/client.pem&apos; --etcd-keyfile=&apos;/var/run/kubernetes/client-key.pem&apos; 6ï¼Œæµ‹è¯• kube-apiserver12345678910111213141516$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy$ kubectl get node$ kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate$ ./version.shetcdctl version: 3.1.3API version: 3.1Kubernetes v1.6.0-beta.1 7ï¼Œæœªè§£å†³çš„é—®é¢˜1ï¼Œä½¿ç”¨ kubectl get cs æŸ¥çœ‹ä¼šå‡ºçŽ°å¦‚ä¸Šé¢æ‰€ç¤ºçš„æŠ¥é”™ï¼š1etcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate æ­¤é—®é¢˜æœ‰äººæäº¤ pr ä½†å°šæœªè¢« mergeï¼Œetcd component status check should include credentials 2ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹åˆ°çš„ 2380 ç«¯å£æ˜¯æœªåŠ å¯†çš„1234$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list 2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379 å‚è€ƒæ–‡æ¡£ï¼š kubernetes + etcd ssl æ”¯æŒ Security model Enabling HTTPS in an existing etcd cluster]]></content>
  </entry>
  <entry>
    <title><![CDATA[etcd å¤‡ä»½ä¸Žæ¢å¤]]></title>
    <url>%2F2017%2F03%2F02%2Fetcd-backup%2F</url>
    <content type="text"><![CDATA[etcd æ˜¯ä¸€æ¬¾å¼€æºçš„åˆ†å¸ƒå¼ä¸€è‡´æ€§é”®å€¼å­˜å‚¨,ç”± CoreOS å…¬å¸è¿›è¡Œç»´æŠ¤ï¼Œè¯¦ç»†çš„ä»‹ç»è¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ã€‚ etcd ç›®å‰æœ€æ–°çš„ç‰ˆæœ¬çš„ v3.1.1ï¼Œä½†å®ƒçš„ API åˆæœ‰ v3 å’Œ v2 ä¹‹åˆ†ï¼Œç¤¾åŒºé€šå¸¸æ‰€è¯´çš„ v3 ä¸Ž v2 éƒ½æ˜¯æŒ‡ API çš„ç‰ˆæœ¬å·ã€‚ä»Ž etcd 2.3 ç‰ˆæœ¬å¼€å§‹æŽ¨å‡ºäº†ä¸€ä¸ªå®žéªŒæ€§çš„å…¨æ–° v3 ç‰ˆæœ¬ API çš„å®žçŽ°ï¼Œv2 ä¸Ž v3 API ä½¿ç”¨äº†ä¸åŒçš„å­˜å‚¨å¼•æ“Žï¼Œæ‰€ä»¥å®¢æˆ·ç«¯å‘½ä»¤ä¹Ÿå®Œå…¨ä¸åŒã€‚ # etcdctl --version etcdctl version: 3.0.4 API version: 2 å®˜æ–¹æŒ‡å‡º etcd v2 å’Œ v3 çš„æ•°æ®ä¸èƒ½æ··åˆå­˜æ”¾ï¼Œsupport backup of v2 and v3 stores ã€‚ ç‰¹åˆ«æé†’ï¼šè‹¥ä½¿ç”¨ v3 å¤‡ä»½æ•°æ®æ—¶å­˜åœ¨ v2 çš„æ•°æ®åˆ™ä¸å½±å“æ¢å¤è‹¥ä½¿ç”¨ v2 å¤‡ä»½æ•°æ®æ—¶å­˜åœ¨ v3 çš„æ•°æ®åˆ™æ¢å¤å¤±è´¥ å¯¹äºŽ API 2 å¤‡ä»½ä¸Žæ¢å¤æ–¹æ³•å®˜æ–¹ v2 admin guide etcdçš„æ•°æ®é»˜è®¤ä¼šå­˜æ”¾åœ¨æˆ‘ä»¬çš„å‘½ä»¤å·¥ä½œç›®å½•ä¸­ï¼Œæˆ‘ä»¬å‘çŽ°æ•°æ®æ‰€åœ¨çš„ç›®å½•ï¼Œä¼šè¢«åˆ†ä¸ºä¸¤ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼š snap: å­˜æ”¾å¿«ç…§æ•°æ®,etcdé˜²æ­¢WALæ–‡ä»¶è¿‡å¤šè€Œè®¾ç½®çš„å¿«ç…§ï¼Œå­˜å‚¨etcdæ•°æ®çŠ¶æ€ã€‚ wal: å­˜æ”¾é¢„å†™å¼æ—¥å¿—,æœ€å¤§çš„ä½œç”¨æ˜¯è®°å½•äº†æ•´ä¸ªæ•°æ®å˜åŒ–çš„å…¨éƒ¨åŽ†ç¨‹ã€‚åœ¨etcdä¸­ï¼Œæ‰€æœ‰æ•°æ®çš„ä¿®æ”¹åœ¨æäº¤å‰ï¼Œéƒ½è¦å…ˆå†™å…¥åˆ°WALä¸­ã€‚ # etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup # etcd -data-dir=/home/etcd_backup/ -force-new-cluster æ¢å¤æ—¶ä¼šè¦†ç›– snapshot çš„å…ƒæ•°æ®(member ID å’Œ cluster ID)ï¼Œæ‰€ä»¥éœ€è¦å¯åŠ¨ä¸€ä¸ªæ–°çš„é›†ç¾¤ã€‚ å¯¹äºŽ API 3 å¤‡ä»½ä¸Žæ¢å¤æ–¹æ³•å®˜æ–¹ v3 admin guide åœ¨ä½¿ç”¨ API 3 æ—¶éœ€è¦ä½¿ç”¨çŽ¯å¢ƒå˜é‡ ETCDCTL_API æ˜Žç¡®æŒ‡å®šã€‚ åœ¨å‘½ä»¤è¡Œè®¾ç½®ï¼š # export ETCDCTL_API=3 å¤‡ä»½æ•°æ®ï¼š # etcdctl --endpoints localhost:2379 snapshot save snapshot.db æ¢å¤ï¼š # etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data æ¢å¤åŽçš„æ–‡ä»¶éœ€è¦ä¿®æ”¹æƒé™ä¸º etcd:etcdâ€“name:é‡æ–°æŒ‡å®šä¸€ä¸ªæ•°æ®ç›®å½•ï¼Œå¯ä»¥ä¸æŒ‡å®šï¼Œé»˜è®¤ä¸º default.etcdâ€“data-dirï¼šæŒ‡å®šæ•°æ®ç›®å½•å»ºè®®ä½¿ç”¨æ—¶ä¸æŒ‡å®š name ä½†æŒ‡å®š data-dirï¼Œå¹¶å°† data-dir å¯¹åº”äºŽ etcd æœåŠ¡ä¸­é…ç½®çš„ data-dir etcd é›†ç¾¤éƒ½æ˜¯è‡³å°‘ 3 å°æœºå™¨ï¼Œå®˜æ–¹ä¹Ÿè¯´æ˜Žäº†é›†ç¾¤å®¹é”™ä¸º (N-1)/2ï¼Œæ‰€ä»¥å¤‡ä»½æ•°æ®ä¸€èˆ¬éƒ½æ˜¯ç”¨ä¸åˆ°ï¼Œä½†æ˜¯é‰´ä¸Šæ¬¡ gitlab å‡ºçŽ°çš„é—®é¢˜ï¼Œå¯¹äºŽå¤‡ä»½æ•°æ®ä¹Ÿè¦éžå¸¸é‡è§†ã€‚ å®˜æ–¹æ–‡æ¡£ç¿»è¯‘]]></content>
  </entry>
  <entry>
    <title><![CDATA[kubernetes å­¦ä¹ ç¬”è®°]]></title>
    <url>%2F2017%2F02%2F12%2Fkubernetes-learn%2F</url>
    <content type="text"><![CDATA[1 æœˆåˆåŠžç†äº†å…¥èŒæ‰‹ç»­ï¼Œæ‰€åœ¨çš„å›¢é˜Ÿæ˜¯æžç§æœ‰äº‘çš„ï¼Œç›®å‰åªæœ‰å°è§„æ¨¡çš„åº”ç”¨ï¼Œæ‰€é‡‡ç”¨ kubernetes + docker æŠ€æœ¯æ ˆï¼Œå¹´å‰æ‰€åšçš„äº‹æƒ…ä¹Ÿä¸ç®—å¤šï¼Œç†Ÿæ‚‰äº† kubernetes çš„æž¶æž„ï¼Œè‡ªå·±æ­å»ºå•æœºç‰ˆçš„ kubernetesï¼Œä»¥åŠåœ¨ç¨‹åºä¸­è°ƒç”¨ kubernetes çš„ API è¿›è¡ŒæŸäº›æ“ä½œã€‚ 1ï¼Œkubernetes æ­å»ºkubernetes æ˜¯ google çš„ä¸€ä¸ªå¼€æºè½¯ä»¶ï¼Œå…¶ç¤¾åŒºæ´»è·ƒé‡è¿œè¶… Mesosï¼ŒCoreos çš„ï¼Œè‹¥æƒ³æ·±å…¥å­¦ä¹ å»ºè®®å‚è€ƒã€Škubernetes æƒå¨æŒ‡å—ã€‹ï¼Œæˆ‘ä»¬å›¢é˜Ÿçš„äººéƒ½æ˜¯ä»Žè¿™æœ¬ä¹¦å­¦èµ·çš„ï¼Œä½œä¸ºä¸€ä¸ªæ–°æŠ€æœ¯ï¼Œä¼šè¸©åˆ°çš„å‘éžå¸¸å¤šï¼Œä»¥ä¸‹æåŠçš„æ˜¯æˆ‘å­¦ä¹ è¿‡ç¨‹ä¸­æ•´ç†çš„éƒ¨åˆ†èµ„æ–™ã€‚ kubernetes æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæ‰€ä»¥å®ƒæœ‰å¤šä¸ªç»„ä»¶ï¼Œå¹¶ä¸”éœ€è¦å®‰è£…åœ¨å¤šä¸ªèŠ‚ç‚¹ï¼Œä¸€èˆ¬æ¥è¯´æœ‰ä¸‰ä¸ªèŠ‚ç‚¹ï¼Œetcdï¼Œmaster å’Œ minionï¼Œä½†æ˜¯æ¯ä¸ªèŠ‚ç‚¹å´åˆæœ‰å¤šå°æœºå™¨ï¼Œetcd ä½œä¸ºé«˜æ€§èƒ½å­˜å‚¨æœåŠ¡ï¼Œä¸€èˆ¬ç‹¬ç«‹ä¸ºä¸€ä¸ªèŠ‚ç‚¹ï¼Œå½“ç„¶å®¹é”™æ˜¯å¿…ä¸å¯å°‘çš„ï¼Œå®˜æ–¹å»ºè®®é›†ç¾¤ä½¿ç”¨å¥‡æ•°ä¸ªèŠ‚ç‚¹ï¼Œæˆ‘ä»¬çš„çº¿ä¸‹é›†ç¾¤ä½¿ç”¨ 3 ä¸ªèŠ‚ç‚¹ã€‚etcd çš„å­¦ä¹ å¯ä»¥å‚è€ƒ gitbook ä¸Šé¢æŸå¤§ç¥žçš„ä¸€æœ¬ä¹¦ ä¸€ etcd3å­¦ä¹ ç¬”è®°ã€‚master ç«¯éœ€è¦å®‰è£… kube-apiserverã€kube-controller-managerå’Œkube-scheduler ç»„ä»¶ï¼Œminion èŠ‚ç‚¹éœ€è¦éƒ¨ç½² kubeletã€kube-proxyã€docker ç»„ä»¶ã€‚ æ³¨æ„ï¼šå†…æ ¸ç‰ˆæœ¬ &gt; 3.10 çš„ç³»ç»Ÿæ‰æ”¯æŒ kubernetesï¼Œæ‰€ä»¥ä¸€èˆ¬å®‰è£…åœ¨centos 7 ä¸Šã€‚ etcd èŠ‚ç‚¹ï¼š # yum install -y etcd # systemctl start etcd master èŠ‚ç‚¹ï¼š # yum install -y kubernetes-master # systemctl start kube-apiserver # systemctl start kube-controller-manager # systemctl start kube-scheduler minion èŠ‚ç‚¹ï¼š # yum install -y kubernetes docker # systemctl start kubelet # systemctl start kube-proxy # systemctl start docker 2ï¼Œkubernetes ç‰ˆæœ¬å‡çº§ä»¥å‰ä¸€ç›´ä»¥ä¸ºå…¬å¸ä¼šè¿½æ±‚ç¨³å®šæ€§ï¼Œåœ¨è½¯ä»¶å’Œç³»ç»Ÿçš„é€‰å–æ–¹ä¾¿ä¼šä¼˜å…ˆè€ƒè™‘ç¨³å®šçš„ç‰ˆæœ¬ã€‚ä½†æ˜¯æ¥äº†å…¬å¸æ‰å‘çŽ°ï¼ŒæŸäº›è½¯ä»¶å‡ºäº†æ–°ç‰ˆæœ¬åŽï¼Œè‹¥æœ‰æœŸå¾…çš„åŠŸèƒ½å¹¶ä¸”åœ¨æŽŒæŽ§èŒƒå›´å†…éƒ½ä¼šåŠæ—¶æ›´æ–°ï¼Œæ‰€ä»¥ä¹ŸååŠ©è¿‡å¯¼å¸ˆæ›´æ–°äº†çº¿ä¸‹é›†ç¾¤çš„ minion èŠ‚ç‚¹ã€‚ ä¸‹é¢æ˜¯ minion èŠ‚ç‚¹çš„å‡çº§æ“ä½œï¼Œmaster èŠ‚ç‚¹çš„æ“ä½œç±»ä¼¼ã€‚é¦–å…ˆéœ€è¦ä¸‹è½½ kubernetes-server-linux-amd64.tar.gz è¿™ä¸ªåŒ…ï¼Œä¸‹è½½ä½ æ‰€è¦æ›´æ–°åˆ°çš„ç‰ˆæœ¬ã€‚ å‡çº§æ­¥éª¤ï¼š 1ï¼Œå…ˆå…³æŽ‰ docker æœåŠ¡ã€‚docker å…³é—­åŽï¼Œå½“å‰èŠ‚ç‚¹çš„ pod éšä¹‹ä¼šè¢«è°ƒåº¦åˆ°å…¶ä»–èŠ‚ç‚¹ä¸Š 2ï¼Œå¤‡ä»½äºŒè¿›åˆ¶ç¨‹åºï¼ˆkubectl,kube-proxyï¼‰ 3ï¼Œå°†è§£åŽ‹åŽçš„äºŒè¿›åˆ¶ç¨‹åºè¦†ç›–ä»¥å‰çš„ç‰ˆæœ¬ 4ï¼Œæœ€åŽé‡æ–°å¯åŠ¨æœåŠ¡ # systemctl stop docker # which kubectl kube-proxy /usr/bin/kubectl /usr/bin/kube-proxy # cp /usr/bin/{kubectl,kube-proxy} /tmp/ # yes | cp bin/{kubectl,kube-proxy} /usr/bin/ # systemctl status {kubectl,kube-proxy} # systemctl start docker 3ï¼Œkubeconfig ä½¿ç”¨è‹¥ä½ ä½¿ç”¨çš„ kubelet ç‰ˆæœ¬ä¸º 1.4ï¼Œä½¿ç”¨ systemctl status kubelet ä¼šçœ‹åˆ°è¿™æ ·ä¸€å¥è¯ï¼š --api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml ä½¿ç”¨ kuconfig æ˜¯ä¸ºäº†å°†æ‰€æœ‰çš„å‘½ä»¤è¡Œå¯åŠ¨é€‰é¡¹æ”¾åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­æ–¹ä¾¿ä½¿ç”¨ã€‚ç”±äºŽæˆ‘ä»¬å·²ç»å‡çº§åˆ°äº† 1.5ï¼Œæ‰€ä»¥ä¹Ÿå¾—å‡çº§æ­¤åŠŸèƒ½ï¼Œé¦–å…ˆéœ€è¦å†™ä¸€ä¸ª kubeconfig çš„ yaml æ–‡ä»¶ï¼Œå…¶ å®˜æ–¹æ–‡æ¡£ æœ‰æ ¼å¼è¯´æ˜Žï¼Œ æœ¬äººå·²å°†å…¶ç¿»è¯‘ï¼Œç¿»è¯‘æ–‡æ¡£è§ä¸‹æ–‡ã€‚ kubeconfig æ–‡ä»¶ç¤ºä¾‹ï¼š apiVersion: v1 clusters: - cluster: server: http://localhost:8080 name: local-server contexts: - context: cluster: local-server namespace: the-right-prefix user: myself name: default-context current-context: default-context kind: Config preferences: {} users: - name: myself user: password: secret username: admin # kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true kubeconfig å‚æ•°ï¼šè®¾ç½® kubelet é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œè¿™ä¸ªé…ç½®æ–‡ä»¶ç”¨æ¥å‘Šè¯‰ kubelet ç»„ä»¶ api-server ç»„ä»¶çš„ä½ç½®ï¼Œé»˜è®¤è·¯å¾„æ˜¯ã€‚ require-kubeconfig å‚æ•°ï¼šè¿™æ˜¯ä¸€ä¸ªå¸ƒå°”ç±»åž‹å‚æ•°ï¼Œå¯ä»¥è®¾ç½®æˆtrue æˆ–è€… falseï¼Œå¦‚æžœè®¾ç½®æˆ trueï¼Œé‚£ä¹ˆè¡¨ç¤ºå¯ç”¨ kubeconfig å‚æ•°ï¼Œä»Ž kubeconfig å‚æ•°è®¾ç½®çš„é…ç½®æ–‡ä»¶ä¸­æŸ¥æ‰¾ api-server ç»„ä»¶ï¼Œå¦‚æžœè®¾ç½®æˆ falseï¼Œé‚£ä¹ˆè¡¨ç¤ºä½¿ç”¨ kubelet å¦å¤–ä¸€ä¸ªå‚æ•° â€œapi-serversâ€ æ¥æŸ¥æ‰¾ api-server ç»„ä»¶ä½ç½®ã€‚ å…³äºŽ kubeconfig çš„ä¸€ä¸ª issueï¼ŒKubelet wonâ€™t read apiserver from kubeconfigã€‚ å‡çº§æ­¥éª¤ï¼Œå½“ç„¶å‰ææ˜¯ä½ çš„ kubelet ç‰ˆæœ¬å·²ç»åˆ°äº† 1.5ï¼š 1ï¼Œå…³é—­ kubeletã€kube-proxy æœåŠ¡ï¼› 2ï¼Œæ³¨é‡ŠæŽ‰ /etc/kubernetes/kubelet æ–‡ä»¶ä¸­ä¸‹é¢è¿™ä¸€è¡Œ: KUBELET_API_SERVER=&quot;--api-servers=http://127.0.0.1:8080&quot; ç„¶åŽåœ¨ KUBELET_ARGS ä¸­æ·»åŠ ï¼š --kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true è¿™é‡Œçš„è·¯å¾„æ˜¯ä½  yaml æ–‡ä»¶æ”¾ç½®çš„è·¯å¾„ã€‚ 3ï¼Œé‡æ–°å¯åŠ¨åˆšå…³æŽ‰çš„ä¸¤ä¸ªæœåŠ¡ 4ï¼Œä»¥ä¸‹ä¸º kubeconfig é…ç½®å®˜æ–¹æ–‡æ¡£çš„ç¿»è¯‘kubernetes ä¸­çš„éªŒè¯å¯¹äºŽä¸åŒçš„ç¾¤ä½“å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ–¹æ³•. è¿è¡Œ kubelet å¯èƒ½æœ‰çš„ä¸€ç§è®¤è¯æ–¹å¼ï¼ˆå³è¯ä¹¦ï¼‰ã€‚ ç”¨æˆ·å¯èƒ½æœ‰ä¸åŒçš„è®¤è¯æ–¹å¼ï¼ˆå³ tokenï¼‰ã€‚ ç®¡ç†å‘˜å¯ä»¥ä¸ºæ¯ä¸ªç”¨æˆ·æä¾›ä¸€ä¸ªè¯ä¹¦åˆ—è¡¨ã€‚ å¯èƒ½ä¼šæœ‰å¤šä¸ªé›†ç¾¤ï¼Œä½†æˆ‘ä»¬æƒ³åœ¨ä¸€ä¸ªåœ°æ–¹å®šä¹‰å®ƒä»¬ - ä½¿ç”¨æˆ·èƒ½å¤Ÿç”¨è‡ªå·±çš„è¯ä¹¦å¹¶é‡ç”¨ç›¸åŒçš„å…¨å±€é…ç½®ã€‚ å› æ­¤ä¸ºäº†åœ¨å¤šä¸ªé›†ç¾¤ä¹‹é—´è½»æ¾åˆ‡æ¢ï¼Œå¯¹äºŽå¤šä¸ªç”¨æˆ·ï¼Œå®šä¹‰äº†ä¸€ä¸ª kubeconfig æ–‡ä»¶ã€‚ æ­¤æ–‡ä»¶åŒ…å«ä¸€ç³»åˆ—è®¤è¯æœºåˆ¶å’Œä¸Ž nicknames æœ‰å…³çš„ç¾¤é›†è¿žæŽ¥ä¿¡æ¯ã€‚å®ƒè¿˜å¼•å…¥äº†è®¤è¯ä¿¡æ¯å…ƒç»„ï¼ˆç”¨æˆ·ï¼‰å’Œé›†ç¾¤è¿žæŽ¥ä¿¡æ¯çš„æ¦‚å¿µï¼Œè¢«ç§°ä¸ºä¸Šä¸‹æ–‡ä¹Ÿä¸Ž nickname ç›¸å…³è”ã€‚ å¦‚æžœæ˜Žç¡®æŒ‡å®šï¼Œä¹Ÿå¯ä»¥å…è®¸ä½¿ç”¨å¤šä¸ª kubeconfig æ–‡ä»¶ã€‚åœ¨è¿è¡Œæ—¶ï¼Œå®ƒä»¬è¢«åˆå¹¶åŠ è½½å¹¶è¦†ç›–ä»Žå‘½ä»¤è¡ŒæŒ‡å®šçš„é€‰é¡¹ï¼ˆå‚è§ä¸‹é¢çš„è§„åˆ™ï¼‰ã€‚ ç›¸å…³è®¨è®ºhttp://issue.k8s.io/1755 kubeconfig æ–‡ä»¶çš„ç»„ä»¶kubeconfig æ–‡ä»¶ç¤ºä¾‹ï¼š current-context: federal-context apiVersion: v1 clusters: - cluster: api-version: v1 server: http://cow.org:8080 name: cow-cluster - cluster: certificate-authority: path/to/my/cafile server: https://horse.org:4443 name: horse-cluster - cluster: insecure-skip-tls-verify: true server: https://pig.org:443 name: pig-cluster contexts: - context: cluster: horse-cluster namespace: chisel-ns user: green-user name: federal-context - context: cluster: pig-cluster namespace: saw-ns user: black-user name: queen-anne-context kind: Config preferences: colors: true users: - name: blue-user user: token: blue-token - name: green-user user: client-certificate: path/to/my/client/cert client-key: path/to/my/client/key ç»„ä»¶çš„è§£é‡Šclusterclusters: - cluster: certificate-authority: path/to/my/cafile server: https://horse.org:4443 name: horse-cluster - cluster: insecure-skip-tls-verify: true server: https://pig.org:443 name: pig-cluster cluster åŒ…å« kubernetes é›†ç¾¤çš„ endpoint æ•°æ®ã€‚å®ƒåŒ…æ‹¬ kubernetes apiserver å®Œå…¨é™å®šçš„ URLï¼Œä»¥åŠé›†ç¾¤çš„è¯ä¹¦é¢å‘æœºæž„æˆ– insecure-skip-tls-verifyï¼štrueï¼Œå¦‚æžœé›†ç¾¤çš„æœåŠ¡è¯ä¹¦æœªç”±ç³»ç»Ÿä¿¡ä»»çš„è¯ä¹¦é¢å‘æœºæž„ç­¾åã€‚é›†ç¾¤æœ‰ä¸€ä¸ªåç§°ï¼ˆnicknameï¼‰ï¼Œè¯¥åç§°ç”¨ä½œæ­¤ kubeconfig æ–‡ä»¶ä¸­çš„å­—å…¸é”®ã€‚ä½ å¯ä»¥ä½¿ç”¨ kubectl config set-cluster æ·»åŠ æˆ–ä¿®æ”¹é›†ç¾¤æ¡ç›®ã€‚ userusers: - name: blue-user user: token: blue-token - name: green-user user: client-certificate: path/to/my/client/cert client-key: path/to/my/client/key ç”¨æˆ·å®šä¹‰ç”¨äºŽå‘ Kubernetes é›†ç¾¤è¿›è¡Œèº«ä»½éªŒè¯çš„å®¢æˆ·ç«¯å‡­è¯ã€‚åœ¨ kubeconfig è¢«åŠ è½½/åˆå¹¶ä¹‹åŽï¼Œç”¨æˆ·å…·æœ‰åœ¨ç”¨æˆ·æ¡ç›®åˆ—è¡¨ä¸­å……å½“å…¶é”®çš„åç§°ï¼ˆnicknameï¼‰ã€‚å¯ç”¨çš„å‡­è¯æ˜¯å®¢æˆ·ç«¯è¯ä¹¦ï¼Œå®¢æˆ·ç«¯å¯†é’¥ï¼Œä»¤ç‰Œå’Œç”¨æˆ·å/å¯†ç ã€‚ç”¨æˆ·å/å¯†ç å’Œä»¤ç‰Œæ˜¯äº’æ–¥çš„ï¼Œä½†å®¢æˆ·ç«¯è¯ä¹¦å’Œå¯†é’¥å¯ä»¥ä¸Žå®ƒä»¬ç»„åˆã€‚ä½ å¯ä»¥ä½¿ç”¨ kubectl config set-credentials æ·»åŠ æˆ–ä¿®æ”¹ç”¨æˆ·æ¡ç›®ã€‚ contextcontexts: - context: cluster: horse-cluster namespace: chisel-ns user: green-user name: federal-context context å®šä¹‰ cluster,user,namespace å…ƒç»„çš„åç§°ï¼Œç”¨æ¥å‘æŒ‡å®šçš„é›†ç¾¤ä½¿ç”¨æä¾›çš„è®¤è¯ä¿¡æ¯å’Œå‘½åç©ºé—´å‘æŒ‡å®šçš„é›†ç¾¤å‘é€è¯·æ±‚ã€‚ä¸‰ä¸ªéƒ½æ˜¯å¯é€‰çš„ï¼Œä»…æŒ‡å®š clusterï¼Œuserï¼Œnamespace ä¸­çš„ä¸€ä¸ªä¹Ÿæ˜¯å¯ç”¨çš„ï¼Œæˆ–è€…æŒ‡å®šä¸º noneã€‚æœªæŒ‡å®šçš„å€¼æˆ–å‘½åå€¼ï¼Œåœ¨åŠ è½½çš„ kubeconfig ä¸­æ²¡æœ‰å¯¹åº”çš„æ¡ç›®ï¼ˆä¾‹å¦‚ï¼Œå¦‚æžœcontext åœ¨ä¸Šé¢çš„ kubeconfig æ–‡ä»¶æŒ‡å®šä¸º pink-user ï¼‰å°†è¢«æ›¿æ¢ä¸ºé»˜è®¤å€¼ã€‚æœ‰å…³è¦†ç›–/åˆå¹¶è¡Œä¸ºï¼Œè¯·å‚é˜…ä¸‹é¢çš„åŠ è½½/åˆå¹¶è§„åˆ™ã€‚ä½ å¯ä»¥ä½¿ç”¨ kubectl config set-context æ·»åŠ æˆ–ä¿®æ”¹ä¸Šä¸‹æ–‡æ¡ç›®ã€‚ current-contextcurrent-context: federal-context current-context æ˜¯ cluster,user,namespace ä¸­çš„ nickname æˆ–è€… â€˜keyâ€™ï¼Œkubectl åœ¨ä»Žæ­¤æ–‡ä»¶åŠ è½½é…ç½®æ—¶å°†ä½¿ç”¨é»˜è®¤å€¼ã€‚é€šè¿‡ç»™ kubelett ä¼ é€’ â€“context=CONTEXT, â€“cluster=CLUSTER, â€“user=USER, and/or â€“namespace=NAMESPACE å¯ä»¥ä»Žå‘½ä»¤è¡Œè¦†ç›–ä»»ä½•å€¼ã€‚ä½ å¯ä»¥ä½¿ç”¨ kubectl config use-context æ›´æ”¹å½“å‰ä¸Šä¸‹æ–‡ã€‚ æ‚é¡¹apiVersion: v1 kind: Config preferences: colors: true apiVersion å’Œ kind æ ‡è¯†å®¢æˆ·ç«¯è¦è§£æžçš„ç‰ˆæœ¬å’Œæ¨¡å¼ï¼Œä¸åº”æ‰‹åŠ¨ç¼–è¾‘ã€‚preferences æŒ‡å®šé€‰é¡¹(å’Œå½“å‰æœªä½¿ç”¨çš„) kubectl preferences. æŸ¥çœ‹ kubeconfig æ–‡ä»¶kubectl config view ä¼šæ˜¾ç¤ºå½“å‰çš„ kubeconfig é…ç½®ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä¼šæ˜¾ç¤ºæ‰€æœ‰åŠ è½½çš„ kubeconfig é…ç½®ï¼Œ ä½ å¯ä»¥é€šè¿‡ â€“minify é€‰é¡¹æ¥è¿‡æ»¤ä¸Ž current-context ç›¸å…³çš„è®¾ç½®ã€‚è¯·å‚è§ kubectl config view çš„å…¶ä»–é€‰é¡¹ã€‚ åˆ›å»ºä½ çš„ kubeconfig æ–‡ä»¶æ³¨æ„ï¼Œå¦‚æžœä½ é€šè¿‡ kube-up.sh éƒ¨ç½² k8sï¼Œåˆ™ä¸éœ€è¦åˆ›å»º kubeconfig æ–‡ä»¶ï¼Œè„šæœ¬å°†ä¸ºä½ åˆ›å»ºã€‚ åœ¨ä»»ä½•æƒ…å†µä¸‹ï¼Œå¯ä»¥è½»æ¾åœ°ä½¿ç”¨æ­¤æ–‡ä»¶ä½œä¸ºæ¨¡æ¿æ¥åˆ›å»ºè‡ªå·±çš„ kubeconfig æ–‡ä»¶ã€‚ å› æ­¤ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿæµè§ˆä¸Šè¿°æ–‡ä»¶çš„åŸºç¡€çŸ¥è¯†ï¼Œä»¥ä¾¿å¯ä»¥æ ¹æ®éœ€è¦è½»æ¾ä¿®æ”¹â€¦ ä»¥ä¸Šæ–‡ä»¶å¯èƒ½å¯¹åº”äºŽä½¿ç”¨â€“token-auth-file = tokens.csv é€‰é¡¹å¯åŠ¨çš„ api æœåŠ¡å™¨ï¼Œå…¶ä¸­ tokens.csvæ–‡ä»¶çœ‹èµ·æ¥åƒè¿™æ ·ï¼š blue-user,blue-user,1 mister-red,mister-red,2 æ­¤å¤–ï¼Œç”±äºŽä¸åŒç”¨æˆ·ä½¿ç”¨ä¸åŒçš„éªŒè¯æœºåˆ¶ï¼Œapi-server å¯èƒ½å·²ç»å¯åŠ¨å…¶ä»–çš„èº«ä»½éªŒè¯é€‰é¡¹ï¼ˆæœ‰è®¸å¤šè¿™æ ·çš„é€‰é¡¹ï¼Œåœ¨åˆ¶ä½œ kubeconfig æ–‡ä»¶ä¹‹å‰ç¡®ä¿ä½ ç†è§£æ‰€å…³å¿ƒçš„ï¼Œå› ä¸ºæ²¡æœ‰äººéœ€è¦å®žçŽ°æ‰€æœ‰å¯èƒ½çš„è®¤è¯æ–¹æ¡ˆï¼‰ã€‚ ç”±äºŽ current-context çš„ç”¨æˆ·æ˜¯ â€œgreen-userâ€ï¼Œå› æ­¤ä»»ä½•ä½¿ç”¨æ­¤ kubeconfig æ–‡ä»¶çš„å®¢æˆ·ç«¯è‡ªç„¶éƒ½èƒ½å¤ŸæˆåŠŸç™»å½• api-serverï¼Œå› ä¸ºæˆ‘ä»¬æä¾›äº† â€œgreen-userâ€ çš„å®¢æˆ·ç«¯å‡­æ®ã€‚ ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€‰æ‹©æ”¹å˜ current-context çš„å€¼ä¸º â€œblue-userâ€ã€‚ åœ¨ä¸Šè¿°æƒ…å†µä¸‹ï¼Œâ€œgreen-userâ€ å°†å¿…é¡»é€šè¿‡æä¾›è¯ä¹¦ç™»å½•ï¼Œè€Œ â€œblue-userâ€ åªéœ€æä¾› tokenã€‚æ‰€æœ‰çš„è¿™äº›ä¿¡æ¯å°†ç”±æˆ‘ä»¬å¤„ç†é€šè¿‡ åŠ è½½å’Œåˆå¹¶è§„åˆ™åŠ è½½å’Œåˆå¹¶ kubeconfig æ–‡ä»¶çš„è§„åˆ™å¾ˆç®€å•ï¼Œä½†æœ‰å¾ˆå¤šã€‚æœ€ç»ˆé…ç½®æŒ‰ç…§ä»¥ä¸‹é¡ºåºæž„å»ºï¼š 1ï¼Œä»Žç£ç›˜èŽ·å– kubeconfigã€‚é€šè¿‡ä»¥ä¸‹å±‚æ¬¡ç»“æž„å’Œåˆå¹¶è§„åˆ™å®Œæˆï¼šå¦‚æžœè®¾ç½®äº† CommandLineLocationï¼ˆkubeconfig å‘½ä»¤è¡Œé€‰é¡¹çš„å€¼ï¼‰ï¼Œåˆ™ä»…ä½¿ç”¨æ­¤æ–‡ä»¶ï¼Œä¸åˆå¹¶ã€‚åªå…è®¸æ­¤æ ‡å¿—çš„ä¸€ä¸ªå®žä¾‹ã€‚ å¦åˆ™ï¼Œå¦‚æžœ EnvVarLocationï¼ˆ$KUBECONFIG çš„å€¼ï¼‰å¯ç”¨ï¼Œå°†å…¶ç”¨ä½œåº”åˆå¹¶çš„æ–‡ä»¶åˆ—è¡¨ã€‚æ ¹æ®ä»¥ä¸‹è§„åˆ™å°†æ–‡ä»¶åˆå¹¶åœ¨ä¸€èµ·ã€‚å°†å¿½ç•¥ç©ºæ–‡ä»¶åã€‚æ–‡ä»¶å†…å®¹ä¸èƒ½ååºåˆ—åŒ–åˆ™äº§ç”Ÿé”™è¯¯ã€‚è®¾ç½®ç‰¹å®šå€¼æˆ–æ˜ å°„å¯†é’¥çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶å°†è¢«ä½¿ç”¨ï¼Œå¹¶ä¸”å€¼æˆ–æ˜ å°„å¯†é’¥æ°¸è¿œä¸ä¼šæ›´æ”¹ã€‚è¿™æ„å‘³ç€è®¾ç½®CurrentContext çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶å°†ä¿ç•™å…¶ contextã€‚ä¹Ÿæ„å‘³ç€å¦‚æžœä¸¤ä¸ªæ–‡ä»¶æŒ‡å®š â€œred-userâ€,ï¼Œåˆ™ä»…ä½¿ç”¨æ¥è‡ªç¬¬ä¸€ä¸ªæ–‡ä»¶çš„ â€œred-userâ€ çš„å€¼ã€‚æ¥è‡ªç¬¬äºŒä¸ª â€œred-userâ€ æ–‡ä»¶çš„éžå†²çªæ¡ç›®ä¹Ÿå°†è¢«ä¸¢å¼ƒã€‚ å¯¹äºŽå…¶ä»–çš„ï¼Œä½¿ç”¨ HomeDirectoryLocationï¼ˆ~/.kube/configï¼‰ä¹Ÿä¸ä¼šè¢«åˆå¹¶ã€‚ 2ï¼Œæ­¤é“¾ä¸­ç¬¬ä¸€ä¸ªè¢«åŒ¹é…çš„ context å°†è¢«ä½¿ç”¨ï¼š 1ï¼Œå‘½ä»¤è¡Œå‚æ•° - å‘½ä»¤è¡Œé€‰é¡¹ä¸­ context çš„å€¼ 2ï¼Œåˆå¹¶æ–‡ä»¶ä¸­çš„ current-context 3ï¼Œæ­¤æ®µå…è®¸ä¸ºç©º 3ï¼Œç¡®å®šè¦ä½¿ç”¨çš„é›†ç¾¤ä¿¡æ¯å’Œç”¨æˆ·ã€‚åœ¨æ­¤å¤„ï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰ contextã€‚è¿™ä¸ªé“¾ä¸­ç¬¬ä¸€æ¬¡ä½¿ç”¨çš„ä¼šè¢«æž„å»ºã€‚ï¼ˆè¿è¡Œä¸¤æ¬¡ï¼Œä¸€æ¬¡ä¸ºç”¨æˆ·ï¼Œä¸€æ¬¡ä¸ºé›†ç¾¤ï¼‰ï¼š 1ï¼Œå‘½ä»¤è¡Œå‚æ•° - user æ˜¯ç”¨æˆ·åï¼Œcluster æ˜¯é›†ç¾¤å 2ï¼Œå¦‚æžœå­˜åœ¨ context åˆ™ä½¿ç”¨ 3ï¼Œå…è®¸ä¸ºç©º 4ï¼Œç¡®å®šè¦ä½¿ç”¨çš„å®žé™…é›†ç¾¤ä¿¡æ¯ã€‚åœ¨æ­¤å¤„ï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰é›†ç¾¤ä¿¡æ¯ã€‚åŸºäºŽé“¾æž„å»ºæ¯ä¸ªé›†ç¾¤ä¿¡æ¯ï¼ˆé¦–æ¬¡ä½¿ç”¨çš„ï¼‰ï¼š 1ï¼Œå‘½ä»¤è¡Œå‚æ•° - serverï¼Œapi-versionï¼Œcertificate-authority å’Œ insecure-skip-tls-verify 2ï¼Œå¦‚æžœå­˜åœ¨é›†ç¾¤ä¿¡æ¯å¹¶ä¸”è¯¥å±žæ€§çš„å€¼å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨å®ƒã€‚ 3ï¼Œå¦‚æžœæ²¡æœ‰ server ä½ç½®åˆ™å‡ºé”™ã€‚ 5ï¼Œç¡®å®šè¦ä½¿ç”¨çš„å®žé™…ç”¨æˆ·ä¿¡æ¯ã€‚ç”¨æˆ·æž„å»ºä½¿ç”¨ä¸Žé›†ç¾¤ä¿¡æ¯ç›¸åŒçš„è§„åˆ™ï¼Œä½†æ¯ä¸ªç”¨æˆ·åªèƒ½å…·æœ‰ä¸€ç§è®¤è¯æ–¹æ³•ï¼š 1ï¼ŒåŠ è½½ä¼˜å…ˆçº§ä¸º 1ï¼‰å‘½ä»¤è¡Œå‚æ•°ï¼Œ2ï¼‰ kubeconfig çš„ç”¨æˆ·å­—æ®µ 2ï¼Œå‘½ä»¤è¡Œå‚æ•°ï¼šå®¢æˆ·ç«¯è¯ä¹¦ï¼Œå®¢æˆ·ç«¯å¯†é’¥ï¼Œç”¨æˆ·åï¼Œå¯†ç å’Œ tokenã€‚ 3ï¼Œå¦‚æžœä¸¤è€…æœ‰å†²çªåˆ™å¤±è´¥ 6ï¼Œå¯¹äºŽä»ç„¶ç¼ºå¤±çš„ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼å¹¶å°½å¯èƒ½æç¤ºè¾“å…¥èº«ä»½éªŒè¯ä¿¡æ¯ã€‚ 7ï¼Œkubeconfig æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ–‡ä»¶å¼•ç”¨éƒ½æ˜¯ç›¸å¯¹äºŽ kubeconfig æ–‡ä»¶æœ¬èº«çš„ä½ç½®è§£æžçš„ã€‚å½“æ–‡ä»¶å¼•ç”¨æ˜¾ç¤ºåœ¨å‘½ä»¤è¡Œä¸Šæ—¶ï¼Œå®ƒä»¬è¢«è§†ä¸ºç›¸å¯¹äºŽå½“å‰å·¥ä½œç›®å½•ã€‚å½“è·¯å¾„ä¿å­˜åœ¨ ~/.kube/config ä¸­æ—¶ï¼Œç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„è¢«åˆ†åˆ«å­˜å‚¨ã€‚ kubeconfig æ–‡ä»¶ä¸­çš„ä»»ä½•è·¯å¾„éƒ½æ˜¯ç›¸å¯¹äºŽ kubeconfig æ–‡ä»¶æœ¬èº«çš„ä½ç½®è§£æžçš„ã€‚ é€šè¿‡ kubectl config æ“ä½œ kubeconfigä¸ºäº†æ›´å®¹æ˜“åœ°æ“ä½œ kubeconfig æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨ kubectl config çš„å­å‘½ä»¤ã€‚è¯·å‚è§ kubectl/kubectl_config.md èŽ·å–å¸®åŠ©ã€‚ ä¾‹å¦‚ï¼š $ kubectl config set-credentials myself --username=admin --password=secret $ kubectl config set-cluster local-server --server=http://localhost:8080 $ kubectl config set-context default-context --cluster=local-server --user=myself $ kubectl config use-context default-context $ kubectl config set contexts.default-context.namespace the-right-prefix $ kubectl config view è¾“å‡ºï¼š apiVersion: v1 clusters: - cluster: server: http://localhost:8080 name: local-server contexts: - context: cluster: local-server namespace: the-right-prefix user: myself name: default-context current-context: default-context kind: Config preferences: {} users: - name: myself user: password: secret username: admin ä¸€ä¸ª kubeconfig æ–‡ä»¶ç±»ä¼¼è¿™æ ·ï¼š apiVersion: v1 clusters: - cluster: server: http://localhost:8080 name: local-server contexts: - context: cluster: local-server namespace: the-right-prefix user: myself name: default-context current-context: default-context kind: Config preferences: {} users: - name: myself user: password: secret username: admin ç¤ºä¾‹æ–‡ä»¶çš„å‘½ä»¤æ“ä½œï¼š $ kubectl config set preferences.colors true $ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1 $ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile $ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true $ kubectl config set-credentials blue-user --token=blue-token $ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key $ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns $ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns $ kubectl config use-context federal-context æœ€åŽçš„æ€»ç»“ï¼š æ‰€ä»¥ï¼Œçœ‹å®Œè¿™äº›ï¼Œä½ å°±å¯ä»¥å¿«é€Ÿå¼€å§‹åˆ›å»ºè‡ªå·±çš„ kubeconfig æ–‡ä»¶äº†ï¼š ä»”ç»†æŸ¥çœ‹å¹¶äº†è§£ api-server å¦‚ä½•å¯åŠ¨ï¼šäº†è§£ä½ çš„å®‰å…¨ç­–ç•¥åŽï¼Œç„¶åŽæ‰èƒ½è®¾è®¡ kubeconfig æ–‡ä»¶ä»¥ä¾¿äºŽèº«ä»½éªŒè¯ å°†ä¸Šé¢çš„ä»£ç æ®µæ›¿æ¢ä¸ºä½ é›†ç¾¤çš„ api-server endpoint çš„ä¿¡æ¯ã€‚ ç¡®ä¿ api-server å·²å¯åŠ¨ï¼Œä»¥è‡³å°‘å‘å…¶æä¾›ä¸€ä¸ªç”¨æˆ·ï¼ˆä¾‹å¦‚ï¼šgreen-userï¼‰å‡­è¯ã€‚å½“ç„¶ï¼Œä½ å¿…é¡»æŸ¥çœ‹ api-server æ–‡æ¡£ï¼Œä»¥ç¡®å®šä»¥ç›®å‰æœ€å¥½çš„æŠ€æœ¯æä¾›è¯¦ç»†çš„èº«ä»½éªŒè¯ä¿¡æ¯ã€‚]]></content>
  </entry>
</search>
