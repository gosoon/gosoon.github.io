<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pid cgroup]]></title>
    <url>%2F2021%2F02%2F03%2Fpid_cgroup%2F</url>
    <content type="text"><![CDATA[1、为了避免系统资源被耗光，需要对进程的最大进程数进行限制，通过向对应进程所在 cgroup 的 pid.max 文件中写入具体的数字来限制其进程数。默认值为 ‘max’ 也就是不限制，和 cgroup 最上层中的限制数保持一致。pids.current 表示 cgroup 该层路径下已经使用 pid 数量。如果 pid 已经达到上限，再创建进程会出现 Resource temporary unavailable 报错； 2、pid 被大量使用的原因：每一个进程都需要一个 pid，也会占用一定的资源，如果不限制进程数，可能会出现类似 fork bomb 耗光系统资源的问题。通常来说容器中可能由于 init 进程没有回收子进程而出现大量僵尸进程导致 pid 被耗光，当子进程退出时父进程没有回收子进程时，子进程就会成为僵尸进程； 3、系统 pid 最大值设置：pid 最大值可以在系统文件 /proc/sys/kernel/pid_max 中看到，系统在初始化时默认会设置最大值，一般小于等于 32 核的机器，pid_max 会被默认设置为 32768，大于32核的默认被设置为 核数*1024； 参考：https://www.kernel.org/doc/Documentation/cgroup-v1/pids.txt]]></content>
      <tags>
        <tag>cgroup v1</tag>
        <tag>pid cgroup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[blkio cgroup]]></title>
    <url>%2F2021%2F01%2F01%2Fblkio_cgroup%2F</url>
    <content type="text"><![CDATA[blkio cgroup 基本功能blkio 是 cgroup v1 中的一个子系统，使用 cgroup v1 blkio 子系统主要是为了减少进程之间共同读写同一块磁盘时相互干扰的问题。 cgroup v1 blkio 控制子系统可以限制进程读写的 IOPS 和吞吐量，但它只能对 Direct I/O 的文件读写进行限速，对 Buffered I/O 的文件读写无法限制。 Buffered I/O 指会经过 PageCache 然后再写入到存储设备中。这里面的 Buffered 的含义跟内存中 buffer cache 不同，这里的 Buffered 含义相当于内存中的buffer cache+page cache。 在 blkio cgroup 中，主要有以下四个参数来限制磁盘 I/O： 1234blkio.throttle.read_bps_deviceblkio.throttle.read_iops_deviceblkio.throttle.write_bps_deviceblkio.throttle.write_iops_device 如果要限制某个控制组对磁盘的写入吞吐量不超过 10M/s，我们可以对blkio.throttle.write_bps_device参数进行配置： 1echo &quot;8:0 10485760&quot; &gt; /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device 在 Linux 中，文件默认读写方式为 Buffered I/O，应用程序一般将文件写入到 PageCache 后就直接返回了，然后内核线程会异步将数据从内存同步到磁盘中。而 Direct I/O 不会和内存打交道，而是直接写入到存储设备中。 要了解 blkio cgroup 的限速逻辑，需要先了解下 Linux 的写文件流程。 Linux 写文件流程 上图是 Linux 写文件的一个流程图，图中主要包含三块，用户层、内核层、硬件层，Linux 在写文件时要经过系统调用、VFS、PageCache、文件系统、通用块管理层、IO调度层等多个流程后最终会将文件写入到磁盘中。而 blkio cgroup 作用在通用块管理层。Buffered I/O 是先写入到 PageCache 再走后面的流程将数据写入磁盘，而 Direct I/O 会绕过 PageCache 直接走后面的流程。 Linux 中应用程序对文件读写时默认是以 Buffered I/O 的形式写入的，此时并不需要经过通用块管理层，只需写入到 PageCache 即可，所以无法被限速，但 PageCache 中的数据总是要经过通用块管理层写入到磁盘的，原则上说也有影响，但是对于应用程序来说感受可能不一样，这与 PageCache 写入磁盘的机制也有关系。 在一般 I/O 的情况下，应用程序很可能很快的就写完了数据（在数据量小于缓存空间的情况下），然后去做其他事情了。这时应用程序感受不到自己被限速了，而内核在将数据从 PageCache 同步到磁盘阶段，由于 PageCache 中没有具体 cgroup 关联信息，所以所有 PageCache 的回写只能放到 cgroup 的 root 组中进行限制，而不能在其他cgroup 中进行限制，root cgroup 一般也是不做限制的。而在Direct IO的情况下，由于应用程序写的数据是不经过缓存层的，所以能直接感受到速度被限制，一定要等到整个数据按限制好的速度写完或者读完，才能返回。这就是当前 cgroup 的 blkio 限制所能起作用的环境限制。 PageCache 写入磁盘的机制： （1）脏页太多，Page Cache 中的脏页比例达到一定阈值时回写，主要有下面两个参数来控制脏页比例： dirty_background_ratio 表示当脏页占总内存的的百分比超过这个值时，后台线程开始刷新脏页。这个值如果设置得太小，可能不能很好地利用内存加速文件操作。如果设置得太大，则会周期性地出现一个写 I/O 的峰值，默认为 10； dirty_background_bytes：和 dirty_background_ratio 实现相同的功能，该参数依据脏页字节数来判断，但两个参数只会有其中一个生效，默认为 0； dirty_ratio 当脏页占用的内存百分比超过此值时，内核会阻塞掉写操作，并开始刷新脏页，默认为 20； dirty_bytes：和参数 dirty_ratio 实现相同功能，该参数依据脏页字节数来判断，但两个参数只会有其中一个生效，默认为 0； （2）脏页存在太久，内核线程会周期性回写，脏页存在时间主要由以下几个参数控制： dirty_writeback_centisecs 表示多久唤醒一次刷新脏页的后台线程，这个参数会和参数 dirty_background_ratio 一起来作用，一个表示大小比例，一个表示时间；即满足其中任何一个的条件都达到刷盘的条件，默认为 500； dirty_expire_centisecs 表示脏页超过多长时间就会被内核线程认为需要写回到磁盘，默认为 3000； 为什么 cgroup v1 不支持非 Buffer IO 的限制cgroup v1 通常是每个层级对应一个子系统，子系统需要挂载使用，而每个子系统之间都是独立的，很难协同工作，比如 memory cgroup 和 blkio cgroup 能分别控制某个进程的资源使用量，但是blkio cgroup 对进程资源限制的时候无法感知 memory cgroup 中进程资源的使用量，导致对 Buffered I/O 的限制一直没有实现。 cgroup v1 结构如下所示： cgroup v1 因为有很多缺陷也导致了 linux 的开发者重新设计了 cgroup，也就有了 cgroup v2，在 cgroup v2 中就可以解决 Buffered I/O 限制的问题。cgroup v2 使用了统一层级（unified hierarchy)，各个子系统都可以挂载在统一层级下，一个进程属于一个控制组，每个控制组里可以定义自己需要的多个子系统。cgroup v2 中 io 子系统等同于 v1 中的 blkio 子系统。 cgroup v2 结构如下所示： 参考： https://www.kernel.org/doc/Documentation/cgroup-v1/blkio-controller.txt http://kernel.pursuitofcloud.org/1780636]]></content>
      <tags>
        <tag>blkio cgroup</tag>
        <tag>cgroup v1</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[knative serving 组件分析]]></title>
    <url>%2F2020%2F10%2F08%2Fknative_serving%2F</url>
    <content type="text"><![CDATA[knative 部署完成后可以在 knative-serving namespace 下看到创建出的组件： 12345678910$ kubectl get pod -n knative-servingNAME READY STATUS RESTARTS AGEactivator-7fff689bcb-zt9pm 2/2 Running 2 28dautoscaler-5bcff95856-pr6nk 2/2 Running 3 28dautoscaler-hpa-75584dd678-fpk7w 2/2 Running 1 28dcontroller-bbdd78bc4-6cqm4 2/2 Running 1 28distio-webhook-5f5794dcc4-sgzlj 2/2 Running 1 28dnetworking-istio-7d875675c7-gc55v 1/1 Running 0 28dstorage-version-migration-f46wc 1/2 Running 2 28dwebhook-68bb66b676-9xk4s 2/2 Running 11 28d 创建 knative serving首先创建一个 knative service 进行测试，yaml 文件如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// 创建 helloworld-go 示例：$ cat &lt;&lt;-EOF | kubectl apply -f -apiVersion: serving.knative.dev/v1kind: Servicemetadata: name: helloworld-go namespace: examplespec: template: spec: containers: - image: registry.cn-hangzhou.aliyuncs.com/knative-sample/helloworld-go:160e4dc8 env: - name: TARGET value: &quot;Go Sample v1&quot;EOF// 查看 helloworld-go serving 所关联的资源$ kubectl get all -n exampleNAME READY STATUS RESTARTS AGEpod/helloworld-go-wkrdr-deployment-57b86596d7-7qvcf 2/2 Running 0 11sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/helloworld-go ExternalName &lt;none&gt; cluster-local-gateway.istio-system.svc.cluster.local &lt;none&gt; 8sservice/helloworld-go-wkrdr ClusterIP 10.96.64.61 &lt;none&gt; 80/TCP 11sservice/helloworld-go-wkrdr-private ClusterIP 10.96.79.32 &lt;none&gt; 80/TCP,9090/TCP,9091/TCP,8022/TCP 11sNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/helloworld-go-wkrdr-deployment 1/1 1 1 11sNAME DESIRED CURRENT READY AGEreplicaset.apps/helloworld-go-wkrdr-deployment-57b86596d7 1 1 1 11sNAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASONrevision.serving.knative.dev/helloworld-go-wkrdr helloworld-go helloworld-go-wkrdr 1 TrueNAME LATESTCREATED LATESTREADY READY REASONconfiguration.serving.knative.dev/helloworld-go helloworld-go-wkrdr helloworld-go-wkrdr TrueNAME URL LATESTCREATED LATESTREADY READY REASONservice.serving.knative.dev/helloworld-go http://helloworld-go.example.example.com helloworld-go-wkrdr helloworld-go-wkrdr TrueNAME URL READY REASONroute.serving.knative.dev/helloworld-go http://helloworld-go.example.example.com True$ kubectl get serverlessservice -n exampleNAME MODE ACTIVATORS SERVICENAME PRIVATESERVICENAME READY REASONhelloworld-go-wkrdr Proxy 2 helloworld-go-wkrdr helloworld-go-wkrdr-private Unknown NoHealthyBackends$ kubectl get svc istio-ingressgateway --namespace istio-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEistio-ingressgateway LoadBalancer 10.96.189.189 &lt;pending&gt; 15021:30883/TCP,80:31046/TCP,443:32728/TCP,15443:30769/TCP 21d 创建完成后验证一下 knative 的 kpa 功能，创建完 helloworld-go serving 后会存在一个 pod 实例，如果该 pod 长时间没有被访问则会被销毁。首先验证下首次请求 helloworld-go 的场景，当服务的实例被完全销毁后，请求 helloworld-go 的 URL 此时会先启动一个实例，如下所示： 123$ time curl -H &quot;Host: helloworld-go.example.example.com&quot; http://192.168.99.130:31046Hello Go Sample v1!curl -H &quot;Host: helloworld-go.example.example.com&quot; http://192.168.99.130:31046 0.00s user 0.01s system 0% cpu 2.189 total 使用 minkube 搭建的 k8s 集群由于没有 loadBalancer 此处使用 nodeport 来访问，nodeport 的 31046 端口会转发到后端实例的 8080 端口。当流量到达 helloworld-go 服务时，此时该服务还没有实例，activator 感知到请求后会对 helloworld-go 的 deployment 进行一次扩容，将 deployment 的 replicas 指定为 1，此时 helloworld-go deployment 会拉起一个 pod 实例，由于 pod 首次启动会比较慢。 在实际的环境中，当流量达到后再启动 pod 是不可接受的，一般会保留一个相同实例的 pod 或者一个配置较低的实例，避免冷启动时相应慢或者丢失流量等情况。 存在一个实例后，再次访问时可以看到整个请求流程非常快了，如下所示： 123$ time curl -H &quot;Host: helloworld-go.example.example.com&quot; http://192.168.99.130:31046Hello Go Sample v1!$ curl -H &quot;Host: helloworld-go.example.example.com&quot; http://192.168.99.130:31046 0.01s user 0.01s system 45% cpu 0.026 total knative serving 组件serving 共有 6 个主要的组件，其中 5 个在 knative-serving 这个 namespace 下面，分别为 controller 、webhook 、autoscaler、autoscaler-hpa、activator 这五个组件；还有一个 queue，运行在每个应用的 pod 里，作为 pod 的 sidecar 存在。 1、Controller：负载 Service 整个生命周期的管理，涉及、Configuration、Route、Revision 等的 CURD。是一个控制器，根据用户输入更新集群的状态； 2、Webhook：主要负责创建和更新的参数校验； 3、Activator：在应用缩容到 0 后，拦截用户的请求，通知 autoscaler 启动相应应用实例，等待启动后将请求转发。负责将服务缩容到 0 以及转发请求； 4、Autoscaler：根据应用的请求并发量对应用扩缩容； 5、Queue：负载拦截转发给 Pod 的请求，用于统计 Pod 的请求并发量等，autoscaler 会访问 queue 获取相应数据对应用扩缩容； 6、Autoscaler-hpa：负责 autoscaler 应用的扩缩容； Knative 把应用里的所有能力全都放到统一的 CRD 资源中管理—Service。这里的 Service 与 K8s 原生用户访问的 Service 不同，这是 Knative 的自定义资源，管理 Knative 应用的整个生命周期。 Service：service.serving.knative.dev 资源管理着工作负载的整个生命周期。它控制其他对象（Route、Configration、Revison）的创建，并确保每次对 Service 的更新都作用到其他对象。 Route: route.serving.knative.dev 资源将网络端点映射到一个或多个 Revision。可以通过配置 Route 实现多种流量管理方式，包括部分流量和命名路由。 Configuration：configuration.serving.knative.dev 资源保持部署所需的状态。它提供了代码和配置之间的清晰分离，并遵循十二要素应用程序方法。修改 Configuration 将创建新的 Revision。 Revision：revision.serving.knative.dev 资源是对工作负荷所做的每个修改的代码和配置的时间点快照。修订是不变的对象，只要有用就可以保留。Revision 可以根据进入的流量自动扩缩容。 Serving 关联的所有资源如下图所示： 1、revision 会创建 imageCache、deployment、kpa 以及 sks 几个组件，deployment 是所运行的服务，kpa 会根据并发数进行伸缩对应的 deployment，ServerlessService 会为 kpa 服务在 cluster 内部和外部都创建一个可以访问的 service。image cache 主要是为了解决冷启动时拉取镜像慢的问题； 2、route 会创建 svc、kingress、virtualService 几个组件，供 service 之间以及从外部访问； 自动扩缩容1-&gt;n: 任何访问应用的请求在进入 Pod 后都会被 Queue 拦截，统计当前 Pod 的请求并发数，同时 Queue 会开放一个 metric 接口，autoscalor 通过访问该端口去获取 Pod 的请求并发量并计算是否需要扩缩容。当需要扩缩容时，autoscalor 会通过修改 Revision 下的 deployment 的实例个数达到扩缩容的效果。 0-&gt;1: 在应用长时间无请求访问时，实例会缩减到 0。这个时候，访问应用的请求会被转发到 activator，并在请求在转发到 activator 之前会被标记请求访问的 Revision 信息（由 controller 修改 VirtualService 实现）。activator 接收到请求后，会将 Revision 的并发量加 1，并将 metric 推送给 autoscalor，启动 Pod。同时，activator 监控 Revision 的启动状态，Revision 正常启动后，将请求转发给相应的 Pod。 当然，在 Revision 正常启动后，应用的请求将不会再发送到 activator，而且直接发送至应用的 Pod（由 controller 修改 VirtualService 实现）。 在 mac 下使用 hey 进行压测，当请求数增加时对应服务的实例数同样会增加： 1$ hey -z 30s -c 50 -host &quot;helloworld-go.example.example.com&quot; http://192.168.99.130:31046 knative 网络模式knative 目前默认使用 Istio 作为网络的基础，但 knative 不强依赖 istio，除 istio 之外，还可以选择 ambassador，contour，gloo，kourier 等。网络模式分两个部分，一个为 service 之间的访问，一个为外部访问。 service 之间的访问： istio 会解析 knative service 的 virtualService 下发给各个 pod 的 envoy，当应用通过域名相互访问时，envoy 会拦截请求直接转发给相应的 pod。 外部访问： 如果是在集群外访问，默认的请求入口为 ingressgateway，ingressgateway 将请求根据访问域名转发到应用，如上面示例所示，在本地搭建的 k8s 集群上访问 ingressgateway 的 nodeport，ingressgateway 会将请求转发到后端的 service； 如果是在集群节点上访问，每个 knative service 都对应一个 k8s service, 这个 service 的后端都为 ingressgateway，ingressgateway 会根据访问域名转发到应用； knative 会给每一个 revision 都生成一个域名用于外部访问使用，service 默认的主域名是 example.com，所有 knative service 生成的独立域名都是这个主域名的子域名，可以通过修改 config 来指定默认域名： 1$ kubectl edit cm config-domain --namespace knative-serving 参考： https://knative.dev/docs/serving/ https://github.com/knative/docs/tree/master/docs/serving/samples/hello-world/helloworld-go]]></content>
      <tags>
        <tag>knative</tag>
        <tag>serving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 minikube 上部署 knative]]></title>
    <url>%2F2020%2F09%2F06%2Fdeploy_with_minkube%2F</url>
    <content type="text"><![CDATA[安装 minkube1234567891011121314151617181920212223242526272829// 部署 minkube$ minikube start --image-mirror-country cn \ --iso-url=https://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/iso/minikube-v1.5.0.iso \ --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \ --container-runtime=containerd \ --vm-driver=virtualbox \ --kubernetes-version=&apos;v1.17.0&apos; \ --network-plugin=cni \ --memory=5120 \ --cpus=4 \ --alsologtostderr -v=8// 安装 cilium 网络查件$ kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.8/install/kubernetes/quick-install.yaml// 检查 pod 是否正常运行$ kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcilium-crs5s 1/1 Running 0 3h40mcilium-operator-69c684d865-77c9w 1/1 Running 0 3h40mcoredns-7f9c544f75-47x56 1/1 Running 0 3h41mcoredns-7f9c544f75-g95kk 1/1 Running 0 3h41metcd-minikube 1/1 Running 0 3h41mkube-addon-manager-minikube 1/1 Running 0 3h41mkube-apiserver-minikube 1/1 Running 0 3h41mkube-controller-manager-minikube 1/1 Running 0 3h41mkube-proxy-59zdn 1/1 Running 0 3h41mkube-scheduler-minikube 1/1 Running 0 3h41mstorage-provisioner 1/1 Running 0 3h41m 安装 istio1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 下载 istioctl bin 文件$ curl -L https://istio.io/downloadIstio | sh -// 使用 istioctl 安装 istio 组件$ cp istio-1.5.0/bin/istioctl /usr/local/bin/$ cat &lt;&lt; EOF &gt; ./istio-minimal-operator.yamlapiVersion: install.istio.io/v1alpha1kind: IstioOperatorspec: values: global: proxy: autoInject: enabled // 自动注入 istio sidecar useMCP: false # The third-party-jwt is not enabled on all k8s. # See: https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens jwtPolicy: first-party-jwt addonComponents: pilot: enabled: true prometheus: enabled: false components: ingressGateways: - name: istio-ingressgateway enabled: true - name: cluster-local-gateway enabled: true label: istio: cluster-local-gateway app: cluster-local-gateway k8s: service: type: ClusterIP ports: - port: 15020 name: status-port - port: 80 name: http2 - port: 443 name: httpsEOF$ istioctl manifest apply -f istio-minimal-operator.yaml 安装 knative123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// 安装 knative operator$ kubectl apply -f https://github.com/knative/operator/releases/download/v0.15.0/operator.yaml// 创建 Knative Serving CR，创建完成后，knative operator 会自动安装 knative 所需要的组件$ cat &lt;&lt;-EOF | kubectl apply -f -apiVersion: v1kind: Namespacemetadata: name: knative-serving---apiVersion: operator.knative.dev/v1alpha1kind: KnativeServingmetadata: name: knative-serving namespace: knative-servingEOF// 检查 knative-serving 组件是否正常运行$ kubectl get deployment -n knative-servingNAME READY UP-TO-DATE AVAILABLE AGEactivator 1/1 1 1 3h25mautoscaler 1/1 1 1 3h25mautoscaler-hpa 1/1 1 1 3h25mcontroller 1/1 1 1 3h25mistio-webhook 1/1 1 1 3h25mnetworking-istio 1/1 1 1 3h25mwebhook 1/1 1 1 3h25m// 创建 knative-eventing cr$ cat &lt;&lt;-EOF | kubectl apply -f -apiVersion: v1kind: Namespacemetadata: name: knative-eventing---apiVersion: operator.knative.dev/v1alpha1kind: KnativeEventingmetadata: name: knative-eventing namespace: knative-eventingEOF// 检查 knative-eventing 组件是否正常运行$ kubectl get deployment -n knative-eventingNAME READY UP-TO-DATE AVAILABLE AGEbroker-controller 1/1 1 1 3h25mbroker-filter 1/1 1 1 3h25mbroker-ingress 1/1 1 1 3h25meventing-controller 1/1 1 1 3h25meventing-webhook 1/1 1 1 3h25mimc-controller 1/1 1 1 3h25mimc-dispatcher 1/1 1 1 3h25mmt-broker-controller 1/1 1 1 3h25mpingsource-mt-adapter 1/1 1 1 109m// 为 knative-serving namespace 启用 istio sidecar 自动注入功能$ kubectl label namespace knative-serving istio-injection=enabled// 启用 Istio mTLS 功能$ cat &lt;&lt;EOF | kubectl apply -f -apiVersion: &quot;security.istio.io/v1beta1&quot;kind: &quot;PeerAuthentication&quot;metadata: name: &quot;default&quot; namespace: &quot;knative-serving&quot;spec: mtls: mode: PERMISSIVEEOF// 安装 knative 客户端工具 kn，类似于 kubectl、istioctl 可以对 kantive 中的资源进行操作$ curl https://storage.googleapis.com/knative-nightly/client/latest/kn-darwin-amd64$ cp kn-darwin-amd64 /usr/local/bin/kn 检查 knative 服务状态knative 服务所启动的 pod 分别在 knative-serving 和 knative-eventing 两个 namespace 下，查看 pod 是否都已启动成功： 123456789101112131415161718192021222324$ kubectl get pod -n knative-servingNAME READY STATUS RESTARTS AGEactivator-7fff689bcb-zt9pm 2/2 Running 2 22dautoscaler-5bcff95856-pr6nk 2/2 Running 1 22dautoscaler-hpa-75584dd678-fpk7w 2/2 Running 1 22dcontroller-bbdd78bc4-6cqm4 2/2 Running 1 22distio-webhook-5f5794dcc4-sgzlj 2/2 Running 1 22dnetworking-istio-7d875675c7-gc55v 1/1 Running 0 22dstorage-version-migration-f46wc 1/2 Running 2 22dwebhook-68bb66b676-9xk4s 2/2 Running 7 22d$ kubectl get pod -n knative-eventingNAME READY STATUS RESTARTS AGEbroker-controller-5587985664-rrhsj 1/1 Running 0 22dbroker-filter-854c4dbd4d-ng75g 1/1 Running 2 22dbroker-ingress-bd8dd9fb8-6zqv2 1/1 Running 2 22deventing-controller-58db89b996-kbrsp 1/1 Running 0 22deventing-webhook-7dc8cc7798-sspfm 1/1 Running 5 22dimc-controller-d88855db5-zj6h6 1/1 Running 0 22dimc-dispatcher-549db764cc-8lbcj 1/1 Running 0 22dmt-broker-controller-55b9857d8d-7hf6n 1/1 Running 0 22dpingsource-mt-adapter-6b465cffdc-gqgn5 1/1 Running 0 22dstorage-version-migration-6mm74 0/1 Completed 0 22dv0.15.0-upgrade-5vr8z 0/1 Completed 0 22d 参考： istio 安装：https://knative.dev/development/install/installing-istio/#installing-istio-with-sidecar-injection knative 安装：https://knative.dev/v0.15-docs/install/knative-with-operators/]]></content>
      <tags>
        <tag>knative</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 中的增强特性(Kubernetes Enhancement Proposal)]]></title>
    <url>%2F2020%2F04%2F13%2Fkubernetes_keps%2F</url>
    <content type="text"><![CDATA[kubernetes 增强特性(kep)是为了解决社区中的疑难问题而创建的一个项目，每一个增强特性都对 kubernetes 的部分功能有较大的影响，需要 kubernetes 项目下的多个组(SIG)协作开发，对应的特性通常要经过 alpha、beta以及 GA 三个版本，所以每个方案的开发周期比较长，大多需要经过 9~10 个月才能完成，某些特性甚至已经讨论多年至今仍未开发完成，像 crd、dry-run、kubectl diff、pid limit 等已经开发完成的功能都是在 kep 中提出来的。本文会介绍几个比较重要的已经在 kep 中孵化的特性。 1、client-go 中对 resource 的操作支持传递 context 参数该特性的目标： （1）支持请求超时以及取消请求的调用； （2）支持分布式追踪； 以下是新旧版本中用 client-go list deployment 方式的一个对比： 12345// 老版本中的使用方式deploymentList, _ := clientset.AppsV1().Deployments(apiv1.NamespaceDefault).List( metav1.ListOptions&#123;&#125;)// 新版本中的使用方式deploymentList, err := clientset.AppsV1().Deployments(apiv1.NamespaceDefault).List(context.TODO(), metav1.ListOptions&#123;&#125;) 可以看到在新版本中 client-go 对于 resource 的操作(verbs)首个参数需要传入 context，当然，社区考虑到用户升级 client-go 代码库时需要对应大量的代码进行改动，kubernetes 社区会对 client-go 的老版本进行一个快照，快照将存在以下几个包中： 1234k8s.io/apiextensions-apiserver/pkg/client/&#123;clientset =&gt; deprecated&#125;k8s.io/client-go/&#123;kubernetes =&gt; deprecated&#125;k8s.io/kube-aggregator/pkg/client/clientset_generated/&#123;clientset =&gt; deprecated&#125;k8s.io/metrics/pkg/client/&#123;clientset =&gt; deprecated&#125; 此次升级无论对于用户还是 kubernetes 社区中的项目无疑都需要非常大的变动，使用 client-go 新版本的用户可以使用 sed 等工具修改代码中的相关用法。对于 kubernetes 社区内部项目代码，所有调用中会使用 context.TODO() 作为初始值添加到对 resource 操作的首个参数中。 参考：20200123-client-go-ctx.md 2、从 apiserver 的 watch cache 中进行一致性读取该特性的目标： 1、解决过期数据问题(https://github.com/kubernetes/kubernetes/issues/59848)；2、当 watch cache 启用后，提高对 resource get 和 list 操作的可扩展性以及性能问题； 从以上 issue 中可以看到其问题出现的场景为： 1、集群中存在多个 master 实例，node-1 与 node-2 首先都连接至 apiserver-1； 2、由 controller 管理的 pod-0 最初在 node-1 节点上运行，T2 时刻 pod-0 被删除后调度至 node-2 节点，然后 node-2 节点启动了 pod-0； 3、pod-0 在 node-2 上启动的同时 node-1 节点因异常导致 kubelet 重新启动，此时 node-1 上的 kubelet 连接到了 apiserver-2 上，但 apiserver-2 此时的 watch cache 正好延迟于 T2 时刻(因 apiserver-2 网络或者性能问题导致数据延迟)，apiserver 会将自己的 delay cache 中的 pod list 发送给 node-1，此时 node-1 也会启动一个 pod-0，而 node-1 上面的 pod-0 已经处于运行状态； kubelet 通过 apiserver list 数据时默认将 resourceVersion 设置为 0，此时返回的数据是 apiserver watch cache 中的，并非直接读取 etcd 而来，而因网络或其他原因此时 etcd 与 apiserver watch cache 中的数据可能不同。也就是说，在使用 list/get 时设置 resourceVersion 为 0 可能会获取到过期的数据，当然以上问题会出现在所有的 controller 中。众所周知，resourceVersion 有三种设置方法，第一种当不设置时会从 etcd 中基于 quorum-read 方式获取，此时数据是最新的，第二是设置为 0 从 apiserver cache 中获取，第三种则是设置为指定的 resourceVersion。 那难道在 kubelet list/get pod 时不设置 resourceVersion 解决不了吗？社区给了一个场景，试想在一个超大集群中，有 5K node 且每个 node 有 30 个 pods，此时集群中有 15 万 pods，在此集群中某个 node 使用 list 请求 apiserver 时，其仅仅需要本机的 30 个 pods，而 apiserver 需要从 etcd 中获取 15 万个 pods 对象并过滤出该 node 所需要的 30 个 pods，这种操作对集群的影响是不可预知的，集群性能骤降或者集群宕机都有可能出现。 解决办法：通过以上描述可知，根本问题是在 apiserver 与 etcd 之间的数据传输时有一定延迟导致的。而在 etcd 3.4+ 版本中支持了在客户端 watch 时启用 WithProgressNotify 参数，当 WithProgressNotify 参数启用后，etcd 会自动发送 progress events，此时客户端缓存中的数据与 etcd 中的数据是一致的，但 etcd 默认每 10 分钟发送一次，社区计划设置 progress events 的时延为 250ms 进行测试，根据社区的讨论，其会在数据准确性、性能以及可扩展性等方面进一步测试以及讨论该决策是否满足需求。 该功能会在 kubernetes 新版本中以 WatchCacheConsistentReads feature gate 的方式开放用户使用。 参考文档：20191210-consistent-reads-from-cache.md 3、支持使用 cgroup v2该特性的目标： 在 kubernetes 中支持使用 cgroup v2； Linux 内核已经支持 cgroup v2 特性两年多，cgroup v2 一个大的特性就是可以用非 root 用户操作资源限制（例如：可以使用非 root 权限模式运行 kubernetes 组件），该特性在内核中也已经处于稳定版本，某些发现版(例如 Fedora)中已经默认使用 cgroup v2，所以社区计划在 kubernetes 中支持使用 cgroup v2。这是一个庞大的计划，需要分为多步进行，社区首先会在 kubelet 中支持使用 cgroup v2（该特性已经在进行中 #85218），并保证 cgroup v1 的配置在 cgroup v2 上依然可以使用，然后会对 runtime 进行改造以及进行适配，目前 docker，containerd，runc，cAdvisor 等都已经相继增加了对 cgroupv2 的支持。 而从 cgroup v1 转换到 cgroup v2 也有一些风险存在： 1、cgroups v1 中部分特性无法在 cgroup v2 中使用，如 cpuacct.usage_percpu 和 cgroup 中的 network stats； 2、cgroups v1 中的一些 controller 在 v2 中也不可用 ，如 device 和 net_cls, net_prio 等，对于这部分不可用的 controller 社区将会使用 eBPF 替换他们； 参考文档：20191118-cgroups-v2.md 4、volume 被挂载时支持禁止更改 volume 的所有者以及权限该特性的目标： volume 在 mount 时允许跳过更改其所有者以及权限； 目前，在 pod 中使用 volume 时，将 volume 挂载到容器之前时该 volume 中文件的权限以及所有者将被递归地更改为所提供的 fsGroup 的值，这种更改权限的操作可能需要很长时间才能完成，尤其是在非常大的 volume 中(&gt;=1TB)。更改权限是为了保证所提供的 fsGroup 可以对此 volume 进行读写，但此时 pod 可能会启动超时，部分文件权限更改也可能会导致 pod 中某些应用无法启动。为了解决这一问题，社区将会在 pod 中添加一个名为 .Spec.SecurityContext.FSGroupChangePolicy 的字段，允许用户指定希望 pod 使用的 volume 权限和所有者如何更改。 参考文档：20200120-skip-permission-change.md 5. 支持禁用 ConfigMap/Secret 的自动更新机制该特性的目标： 1、引入一种保护机制来禁止 ConfigMap/Secret 的自动更新； 2、提高 kube-apiserver 的性能； 社区为 ConfigMap 和 Secret 增加了一个 Immutable 字段来禁止其自动更新： 1Immutable *bool 建议使用 Immutable 的 ConfigMap/Secret 主要有两个原因： 一是 pod 使用 ConfigMap/Secret 的模式一般是通过 Volume Mounts 的方式，而 kubelet 会通过 Watch/Poll 的方式去获取 ConfigMap/Secret 更新，同时将最近文件同步到 pod 中，这种方式下 pod 能够快速、无感地获取到 ConfigMap/Secret 更新。但这种更新是一把双刃剑，一次错误的更新可能会导致 pod 内进程异常甚至 pod 不可用，而大多数人都不希望使用这种功能，更多的是使用 Rolling Update 的方式，创建一个新的 ConfigMap/Secret 同时创建新的 pod 去引用新的 ConfigMap/Secret； 二个是在大规模集群内，kubelet 过多的 Watch/Poll 大量的 ConfigMap/Secret 会给 kube-apiserver 造成巨大的压力（尽管我们在这个 PR 中为每个 Watch 请求降低了一个 Goruntine 的消耗）。而使用了 Immutable 的 ConfigMap/Secret，kubelet 也就不会为其建立 Watch/Poll 请求； 官方文档：20191117-immutable-secrets-configmaps.md]]></content>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-apiserver 中 apiserver service 的实现]]></title>
    <url>%2F2020%2F03%2F04%2Fapiserver_bootstrap_controller%2F</url>
    <content type="text"><![CDATA[在 kubernetes，可以从集群外部和内部两种方式访问 kubernetes API，在集群外直接访问 apiserver 提供的 API，在集群内即 pod 中可以通过访问 service 为 kubernetes 的 ClusterIP。kubernetes 集群在初始化完成后就会创建一个 kubernetes service，该 service 是 kube-apiserver 创建并进行维护的，如下所示： 1234567$ kubectl get serviceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 4d22h$ kubectl get endpoints kubernetesNAME ENDPOINTS AGEkubernetes 192.168.99.113:6443 4d22h 内置的 kubernetes service 无法删除，其 ClusterIP 为通过 --service-cluster-ip-range 参数指定的 ip 段中的首个 ip，kubernetes endpoints 中的 ip 以及 port 可以通过 --advertise-address 和 --secure-port 启动参数来指定。 kubernetes service 是由 kube-apiserver 中的 bootstrap controller 进行控制的，其主要以下几个功能： 创建 kubernetes service； 创建 default、kube-system 和 kube-public 命名空间，如果启用了 NodeLease 特性还会创建 kube-node-lease 命名空间； 提供基于 Service ClusterIP 的修复及检查功能； 提供基于 Service NodePort 的修复及检查功能； kubernetes service 默认使用 ClusterIP 对外暴露服务，若要使用 nodePort 的方式可在 kube-apiserver 启动时通过 --kubernetes-service-node-port 参数指定对应的端口。 bootstrap controller 源码分析 kubernetes 版本：v1.16 bootstrap controller 的初始化以及启动是在 CreateKubeAPIServer 调用链的 InstallLegacyAPI 方法中完成的，bootstrap controller 的启停是由 apiserver 的 PostStartHook 和 ShutdownHook 进行控制的。 k8s.io/kubernetes/pkg/master/master.go:406 123456789101112131415161718func (m *Master) InstallLegacyAPI(......) error &#123; legacyRESTStorage, apiGroupInfo, err := legacyRESTStorageProvider.NewLegacyRESTStorage(restOptionsGetter) if err != nil &#123; return fmt.Errorf(&quot;Error building core storage: %v&quot;, err) &#125; // 初始化 bootstrap-controller controllerName := &quot;bootstrap-controller&quot; coreClient := corev1client.NewForConfigOrDie(c.GenericConfig.LoopbackClientConfig) bootstrapController := c.NewBootstrapController(......) m.GenericAPIServer.AddPostStartHookOrDie(controllerName, bootstrapController.PostStartHook) m.GenericAPIServer.AddPreShutdownHookOrDie(controllerName, bootstrapController.PreShutdownHook) if err := m.GenericAPIServer.InstallLegacyAPIGroup(genericapiserver.DefaultLegacyAPIPrefix, &amp;apiGroupInfo); err != nil &#123; return fmt.Errorf(&quot;Error in registering group versions: %v&quot;, err) &#125; return nil&#125; postStartHooks 会在 kube-apiserver 的启动方法 prepared.Run 中调用 RunPostStartHooks 启动所有 Hook。 NewBootstrapControllerbootstrap controller 在初始化时需要设定多个参数，主要有 PublicIP、ServiceCIDR、PublicServicePort 等。PublicIP 是通过命令行参数 --advertise-address 指定的，如果没有指定，系统会自动选出一个 global IP。PublicServicePort 通过 --secure-port 启动参数来指定（默认为 6443），ServiceCIDR 通过 --service-cluster-ip-range 参数指定（默认为 10.0.0.0/24）。 k8s.io/kubernetes/pkg/master/controller.go:89 1234567891011121314151617181920212223242526272829303132333435363738394041424344func (c *completedConfig) NewBootstrapController(......) *Controller &#123; // 1、获取 PublicServicePort _, publicServicePort, err := c.GenericConfig.SecureServing.HostPort() if err != nil &#123; klog.Fatalf(&quot;failed to get listener address: %v&quot;, err) &#125; // 2、指定需要创建的 kube-system 和 kube-public systemNamespaces := []string&#123;metav1.NamespaceSystem, metav1.NamespacePublic&#125; if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123; systemNamespaces = append(systemNamespaces, corev1.NamespaceNodeLease) &#125; return &amp;Controller&#123; ...... // ServiceClusterIPRegistry 是在 CreateKubeAPIServer 初始化 RESTStorage 时初始化的，是一个 etcd 实例 ServiceClusterIPRegistry: legacyRESTStorage.ServiceClusterIPAllocator, ServiceClusterIPRange: c.ExtraConfig.ServiceIPRange, SecondaryServiceClusterIPRegistry: legacyRESTStorage.SecondaryServiceClusterIPAllocator, // SecondaryServiceClusterIPRange 需要在启用 IPv6DualStack 后才能使用 SecondaryServiceClusterIPRange: c.ExtraConfig.SecondaryServiceIPRange, ServiceClusterIPInterval: 3 * time.Minute, ServiceNodePortRegistry: legacyRESTStorage.ServiceNodePortAllocator, ServiceNodePortRange: c.ExtraConfig.ServiceNodePortRange, ServiceNodePortInterval: 3 * time.Minute, // API Server 绑定的IP，这个IP会作为kubernetes service的Endpoint的IP PublicIP: c.GenericConfig.PublicAddress, // 取 clusterIP range 中的第一个 IP ServiceIP: c.ExtraConfig.APIServerServiceIP, // 默认为 6443 ServicePort: c.ExtraConfig.APIServerServicePort, ExtraServicePorts: c.ExtraConfig.ExtraServicePorts, ExtraEndpointPorts: c.ExtraConfig.ExtraEndpointPorts, // 这里为 6443 PublicServicePort: publicServicePort, // 缺省是基于 ClusterIP 启动模式，这里为0 KubernetesServiceNodePort: c.ExtraConfig.KubernetesServiceNodePort, &#125;&#125; 自动选出 global IP 的代码如下所示： k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/net/interface.go:323 1234567891011func ChooseHostInterface() (net.IP, error) &#123; var nw networkInterfacer = networkInterface&#123;&#125; if _, err := os.Stat(ipv4RouteFile); os.IsNotExist(err) &#123; return chooseIPFromHostInterfaces(nw) &#125; routes, err := getAllDefaultRoutes() if err != nil &#123; return nil, err &#125; return chooseHostInterfaceFromRoute(routes, nw)&#125; bootstrapController.Start上文已经提到了 bootstrap controller 主要的四个功能：修复 ClusterIP、修复 NodePort、更新 kubernetes service、创建系统所需要的名字空间（default、kube-system、kube-public）。bootstrap controller 在启动后首先会完成一次 ClusterIP、NodePort 和 Kubernets 服务的处理，然后异步循环运行上面的4个工作。以下是其 start 方法： k8s.io/kubernetes/pkg/master/controller.go:146 123456789101112131415161718192021222324252627func (c *Controller) Start() &#123; if c.runner != nil &#123; return &#125; // 1、首次启动时首先从 kubernetes endpoints 中移除自身的配置， // 此时 kube-apiserver 可能处于非 ready 状态 endpointPorts := createEndpointPortSpec(c.PublicServicePort, &quot;https&quot;, c.ExtraEndpointPorts) if err := c.EndpointReconciler.RemoveEndpoints(kubernetesServiceName, c.PublicIP, endpointPorts); err != nil &#123; klog.Errorf(&quot;Unable to remove old endpoints from kubernetes service: %v&quot;, err) &#125; // 2、初始化 repairClusterIPs 和 repairNodePorts 对象 repairClusterIPs := servicecontroller.NewRepair(......) repairNodePorts := portallocatorcontroller.NewRepair(......) // 3、首先运行一次 epairClusterIPs 和 repairNodePorts，即进行初始化 if err := repairClusterIPs.RunOnce(); err != nil &#123; klog.Fatalf(&quot;Unable to perform initial IP allocation check: %v&quot;, err) &#125; if err := repairNodePorts.RunOnce(); err != nil &#123; klog.Fatalf(&quot;Unable to perform initial service nodePort check: %v&quot;, err) &#125; // 4、定期执行 bootstrap controller 主要的四个功能 c.runner = async.NewRunner(c.RunKubernetesNamespaces, c.RunKubernetesService, repairClusterIPs.RunUntil, repairNodePorts.RunUntil) c.runner.Start()&#125; c.RunKubernetesNamespacesc.RunKubernetesNamespaces 主要功能是创建 kube-system 和 kube-public 命名空间，如果启用了 NodeLease 特性功能还会创建 kube-node-lease 命名空间，之后每隔一分钟检查一次。 k8s.io/kubernetes/pkg/master/controller.go:199 123456789func (c *Controller) RunKubernetesNamespaces(ch chan struct&#123;&#125;) &#123; wait.Until(func() &#123; for _, ns := range c.SystemNamespaces &#123; if err := createNamespaceIfNeeded(c.NamespaceClient, ns); err != nil &#123; runtime.HandleError(fmt.Errorf(&quot;unable to create required kubernetes system namespace %s: %v&quot;, ns, err)) &#125; &#125; &#125;, c.SystemNamespacesInterval, ch)&#125; c.RunKubernetesServicec.RunKubernetesService 主要是检查 kubernetes service 是否处于正常状态，并定期执行同步操作。首先调用 /healthz 接口检查 apiserver 当前是否处于 ready 状态，若处于 ready 状态然后调用 c.UpdateKubernetesService 服务更新 kubernetes service 状态。 k8s.io/kubernetes/pkg/master/controller.go:210 12345678910111213func (c *Controller) RunKubernetesService(ch chan struct&#123;&#125;) &#123; wait.PollImmediateUntil(100*time.Millisecond, func() (bool, error) &#123; var code int c.healthClient.Get().AbsPath(&quot;/healthz&quot;).Do().StatusCode(&amp;code) return code == http.StatusOK, nil &#125;, ch) wait.NonSlidingUntil(func() &#123; if err := c.UpdateKubernetesService(false); err != nil &#123; runtime.HandleError(fmt.Errorf(&quot;unable to sync kubernetes service: %v&quot;, err)) &#125; &#125;, c.EndpointInterval, ch)&#125; c.UpdateKubernetesServicec.UpdateKubernetesService 的主要逻辑为： 1、调用 createNamespaceIfNeeded 创建 default namespace； 2、调用 c.CreateOrUpdateMasterServiceIfNeeded 为 master 创建 kubernetes service； 3、调用 c.EndpointReconciler.ReconcileEndpoints 更新 master 的 endpoint； k8s.io/kubernetes/pkg/master/controller.go:230 123456789101112131415func (c *Controller) UpdateKubernetesService(reconcile bool) error &#123; if err := createNamespaceIfNeeded(c.NamespaceClient, metav1.NamespaceDefault); err != nil &#123; return err &#125; servicePorts, serviceType := createPortAndServiceSpec(c.ServicePort, c.PublicServicePort, c.KubernetesServiceNodePort, &quot;https&quot;, c.ExtraServicePorts) if err := c.CreateOrUpdateMasterServiceIfNeeded(kubernetesServiceName, c.ServiceIP, servicePorts, serviceType, reconcile); err != nil &#123; return err &#125; endpointPorts := createEndpointPortSpec(c.PublicServicePort, &quot;https&quot;, c.ExtraEndpointPorts) if err := c.EndpointReconciler.ReconcileEndpoints(kubernetesServiceName, c.PublicIP, endpointPorts, reconcile); err != nil &#123; return err &#125; return nil&#125; c.EndpointReconciler.ReconcileEndpointsEndpointReconciler 的具体实现由 EndpointReconcilerType 决定，EndpointReconcilerType 是 --endpoint-reconciler-type 参数指定的，可选的参数有 master-count, lease, none，每种类型对应不同的 EndpointReconciler 实例，在 v1.16 中默认为 lease，此处仅分析 lease 对应的 EndpointReconciler 的实现。 一个集群中可能会有多个 apiserver 实例，因此需要统一管理 apiserver service 的 endpoints，c.EndpointReconciler.ReconcileEndpoints 就是用来管理 apiserver endpoints 的。一个集群中 apiserver 的所有实例会在 etcd 中的对应目录下创建 key，并定期更新这个 key 来上报自己的心跳信息，ReconcileEndpoints 会从 etcd 中获取 apiserver 的实例信息并更新 endpoint。 k8s.io/kubernetes/pkg/master/reconcilers/lease.go:144 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677func (r *leaseEndpointReconciler) ReconcileEndpoints(......) error &#123; r.reconcilingLock.Lock() defer r.reconcilingLock.Unlock() if r.stopReconcilingCalled &#123; return nil &#125; // 更新 lease 信息 if err := r.masterLeases.UpdateLease(ip.String()); err != nil &#123; return err &#125; return r.doReconcile(serviceName, endpointPorts, reconcilePorts)&#125;func (r *leaseEndpointReconciler) doReconcile(......) error &#123; // 1、获取 master 的 endpoint e, err := r.epAdapter.Get(corev1.NamespaceDefault, serviceName, metav1.GetOptions&#123;&#125;) shouldCreate := false if err != nil &#123; if !errors.IsNotFound(err) &#123; return err &#125; shouldCreate = true e = &amp;corev1.Endpoints&#123; ObjectMeta: metav1.ObjectMeta&#123; Name: serviceName, Namespace: corev1.NamespaceDefault, &#125;, &#125; &#125; // 2、从 etcd 中获取所有的 master masterIPs, err := r.masterLeases.ListLeases() if err != nil &#123; return err &#125; if len(masterIPs) == 0 &#123; return fmt.Errorf(&quot;no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service&quot;) &#125; // 3、检查 endpoint 中 master 信息，如果与 etcd 中的不一致则进行更新 formatCorrect, ipCorrect, portsCorrect := checkEndpointSubsetFormatWithLease(e, masterIPs, endpointPorts, reconcilePorts) if formatCorrect &amp;&amp; ipCorrect &amp;&amp; portsCorrect &#123; return nil &#125; if !formatCorrect &#123; e.Subsets = []corev1.EndpointSubset&#123;&#123; Addresses: []corev1.EndpointAddress&#123;&#125;, Ports: endpointPorts, &#125;&#125; &#125; if !formatCorrect || !ipCorrect &#123; e.Subsets[0].Addresses = make([]corev1.EndpointAddress, len(masterIPs)) for ind, ip := range masterIPs &#123; e.Subsets[0].Addresses[ind] = corev1.EndpointAddress&#123;IP: ip&#125; &#125; e.Subsets = endpointsv1.RepackSubsets(e.Subsets) &#125; if !portsCorrect &#123; e.Subsets[0].Ports = endpointPorts &#125; if shouldCreate &#123; if _, err = r.epAdapter.Create(corev1.NamespaceDefault, e); errors.IsAlreadyExists(err) &#123; err = nil &#125; &#125; else &#123; _, err = r.epAdapter.Update(corev1.NamespaceDefault, e) &#125; return err&#125; repairClusterIPs.RunUntilrepairClusterIP 主要解决的问题有： 保证集群中所有的 ClusterIP 都是唯一分配的； 保证分配的 ClusterIP 不会超出指定范围； 确保已经分配给 service 但是因为 crash 等其他原因没有正确创建 ClusterIP； 自动将旧版本的 Kubernetes services 迁移到 ipallocator 原子性模型； repairClusterIPs.RunUntil 其实是调用 repairClusterIPs.runOnce 来处理的，其代码中的主要逻辑如下所示： k8s.io/kubernetes/pkg/registry/core/service/ipallocator/controller/repair.go:134 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103func (c *Repair) runOnce() error &#123; ...... // 1、首先从 etcd 中获取已经使用 ClusterIP 的快照 err = wait.PollImmediate(time.Second, 10*time.Second, func() (bool, error) &#123; var err error snapshot, err = c.alloc.Get() if err != nil &#123; return false, err &#125; if c.shouldWorkOnSecondary() &#123; secondarySnapshot, err = c.secondaryAlloc.Get() if err != nil &#123; return false, err &#125; &#125; return true, nil &#125;) if err != nil &#123; return fmt.Errorf(&quot;unable to refresh the service IP block: %v&quot;, err) &#125; // 2、判断 snapshot 是否已经初始化 if snapshot.Range == &quot;&quot; &#123; snapshot.Range = c.network.String() &#125; if c.shouldWorkOnSecondary() &amp;&amp; secondarySnapshot.Range == &quot;&quot; &#123; secondarySnapshot.Range = c.secondaryNetwork.String() &#125; stored, err = ipallocator.NewFromSnapshot(snapshot) if c.shouldWorkOnSecondary() &#123; secondaryStored, secondaryErr = ipallocator.NewFromSnapshot(secondarySnapshot) &#125; if err != nil || secondaryErr != nil &#123; return fmt.Errorf(&quot;unable to rebuild allocator from snapshots: %v&quot;, err) &#125; // 3、获取 service list list, err := c.serviceClient.Services(metav1.NamespaceAll).List(metav1.ListOptions&#123;&#125;) if err != nil &#123; return fmt.Errorf(&quot;unable to refresh the service IP block: %v&quot;, err) &#125; // 4、将 CIDR 转换为对应的 IP range 格式 var rebuilt, secondaryRebuilt *ipallocator.Range rebuilt, err = ipallocator.NewCIDRRange(c.network) ...... // 5、检查每个 Service 的 ClusterIP，保证其处于正常状态 for _, svc := range list.Items &#123; if !helper.IsServiceIPSet(&amp;svc) &#123; continue &#125; ip := net.ParseIP(svc.Spec.ClusterIP) ...... actualAlloc := c.selectAllocForIP(ip, rebuilt, secondaryRebuilt) switch err := actualAlloc.Allocate(ip); err &#123; // 6、检查 ip 是否泄漏 case nil: actualStored := c.selectAllocForIP(ip, stored, secondaryStored) if actualStored.Has(ip) &#123; actualStored.Release(ip) &#125; else &#123; ...... &#125; delete(c.leaks, ip.String()) // 7、ip 重复分配 case ipallocator.ErrAllocated: ...... // 8、ip 超出范围 case err.(*ipallocator.ErrNotInRange): ...... // 9、ip 已经分配完 case ipallocator.ErrFull: ...... default: ...... &#125; &#125; // 10、对比是否有泄漏 ip c.checkLeaked(stored, rebuilt) if c.shouldWorkOnSecondary() &#123; c.checkLeaked(secondaryStored, secondaryRebuilt) &#125; // 11、更新快照 err = c.saveSnapShot(rebuilt, c.alloc, snapshot) if err != nil &#123; return err &#125; if c.shouldWorkOnSecondary() &#123; err := c.saveSnapShot(secondaryRebuilt, c.secondaryAlloc, secondarySnapshot) if err != nil &#123; return nil &#125; &#125; return nil&#125; repairNodePorts.RunUntirepairNodePorts 主要是用来纠正 service 中 nodePort 的信息，保证所有的 ports 都基于 cluster 创建的，当没有与 cluster 同步时会触发告警，其最终是调用 repairNodePorts.runOnce 进行处理的，主要逻辑与 ClusterIP 的处理逻辑类似。 k8s.io/kubernetes/pkg/registry/core/service/portallocator/controller/repair.go:84 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586func (c *Repair) runOnce() error &#123; // 1、首先从 etcd 中获取已使用 nodeport 的快照 err := wait.PollImmediate(time.Second, 10*time.Second, func() (bool, error) &#123; var err error snapshot, err = c.alloc.Get() return err == nil, err &#125;) if err != nil &#123; return fmt.Errorf(&quot;unable to refresh the port allocations: %v&quot;, err) &#125; // 2、检查 snapshot 是否初始化 if snapshot.Range == &quot;&quot; &#123; snapshot.Range = c.portRange.String() &#125; // 3、获取已分配 nodePort 信息 stored, err := portallocator.NewFromSnapshot(snapshot) if err != nil &#123; return fmt.Errorf(&quot;unable to rebuild allocator from snapshot: %v&quot;, err) &#125; // 4、获取 service list list, err := c.serviceClient.Services(metav1.NamespaceAll).List(metav1.ListOptions&#123;&#125;) if err != nil &#123; return fmt.Errorf(&quot;unable to refresh the port block: %v&quot;, err) &#125; rebuilt, err := portallocator.NewPortAllocator(c.portRange) if err != nil &#123; return fmt.Errorf(&quot;unable to create port allocator: %v&quot;, err) &#125; // 5、检查每个 Service ClusterIP 的 port，保证其处于正常状态 for i := range list.Items &#123; svc := &amp;list.Items[i] ports := collectServiceNodePorts(svc) if len(ports) == 0 &#123; continue &#125; for _, port := range ports &#123; switch err := rebuilt.Allocate(port); err &#123; // 6、检查 port 是否泄漏 case nil: if stored.Has(port) &#123; stored.Release(port) &#125; else &#123; ...... &#125; delete(c.leaks, port) // 7、port 重复分配 case portallocator.ErrAllocated: ...... // 8、port 超出分配范围 case err.(*portallocator.ErrNotInRange): ...... // 9、port 已经分配完 case portallocator.ErrFull: ...... default: ...... &#125; &#125; &#125; // 10、检查 port 是否泄漏 stored.ForEach(func(port int) &#123; count, found := c.leaks[port] switch &#123; case !found: ...... count = numRepairsBeforeLeakCleanup - 1 fallthrough case count &gt; 0: c.leaks[port] = count - 1 if err := rebuilt.Allocate(port); err != nil &#123; runtime.HandleError(fmt.Errorf(&quot;the node port %d may have leaked, but can not be allocated: %v&quot;, port, err)) &#125; default: ...... &#125; &#125;) // 11、更新 snapshot if err := rebuilt.Snapshot(snapshot); err != nil &#123; return fmt.Errorf(&quot;unable to snapshot the updated port allocations: %v&quot;, err) &#125; ...... return nil&#125; 以上就是 bootstrap controller 的主要实现。 总结本文主要分析了 kube-apiserver 中 apiserver service 的实现，apiserver service 是通过 bootstrap controller 控制的，bootstrap controller 会保证 apiserver service 以及其 endpoint 处于正常状态，需要注意的是，apiserver service 的 endpoint 根据启动时指定的参数分为三种控制方式，本文仅分析了 lease 的实现方式，如果使用 master-count 方式，需要将每个 master 实例的 port、apiserver-count 等配置参数改为一致。]]></content>
      <tags>
        <tag>kube-apiserver</tag>
        <tag>bootstrapController</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-apiserver 的设计与实现]]></title>
    <url>%2F2020%2F02%2F24%2Fkube_apiserver%2F</url>
    <content type="text"><![CDATA[kube-apiserver 是 kubernetes 中与 etcd 直接交互的一个组件，其控制着 kubernetes 中核心资源的变化。它主要提供了以下几个功能： 提供 Kubernetes API，包括认证授权、数据校验以及集群状态变更等，供客户端及其他组件调用； 代理集群中的一些附加组件组件，如 Kubernetes UI、metrics-server、npd 等； 创建 kubernetes 服务，即提供 apiserver 的 Service，kubernetes Service； 资源在不同版本之间的转换； kube-apiserver 处理流程kube-apiserver 主要通过对外提供 API 的方式与其他组件进行交互，可以调用 kube-apiserver 的接口 $ curl -k https://&lt;masterIP&gt;:6443或者通过其提供的 swagger-ui 获取到，其主要有以下三种 API： core group：主要在 /api/v1 下； named groups：其 path 为 /apis/$NAME/$VERSION； 暴露系统状态的一些 API：如/metrics 、/healthz 等； API 的 URL 大致以 /apis/group/version/namespaces/my-ns/myresource 组成，其中 API 的结构大致如下图所示： 了解了 kube-apiserver 的 API 后，下面会介绍 kube-apiserver 如何处理一个 API 请求，一个请求完整的流程如下图所示： 此处以一次 POST 请求示例说明，当请求到达 kube-apiserver 时，kube-apiserver 首先会执行在 http filter chain 中注册的过滤器链，该过滤器对其执行一系列过滤操作，主要有认证、鉴权等检查操作。当 filter chain 处理完成后，请求会通过 route 进入到对应的 handler 中，handler 中的操作主要是与 etcd 的交互，在 handler 中的主要的操作如下所示： Decoder kubernetes 中的多数 resource 都会有一个 internal version，因为在整个开发过程中一个 resource 可能会对应多个 version，比如 deployment 会有 extensions/v1beta1，apps/v1。 为了避免出现问题，kube-apiserver 必须要知道如何在每一对版本之间进行转换（例如，v1⇔v1alpha1，v1⇔v1beta1，v1beta1⇔v1alpha1），因此其使用了一个特殊的internal version，internal version 作为一个通用的 version 会包含所有 version 的字段，它具有所有 version 的功能。 Decoder 会首先把 creater object 转换到 internal version，然后将其转换为 storage version，storage version 是在 etcd 中存储时的另一个 version。 在解码时，首先从 HTTP path 中获取期待的 version，然后使用 scheme 以正确的 version 创建一个与之匹配的空对象，并使用 JSON 或 protobuf 解码器进行转换，在转换的第一步中，如果用户省略了某些字段，Decoder 会把其设置为默认值。 Admission 在解码完成后，需要通过验证集群的全局约束来检查是否可以创建或更新对象，并根据集群配置设置默认值。在 k8s.io/kubernetes/plugin/pkg/admission 目录下可以看到 kube-apiserver 可以使用的所有全局约束插件，kube-apiserver 在启动时通过设置 --enable-admission-plugins 参数来开启需要使用的插件，通过 ValidatingAdmissionWebhook 或 MutatingAdmissionWebhook 添加的插件也都会在此处进行工作。 Validation 主要检查 object 中字段的合法性。 在 handler 中执行完以上操作后最后会执行与 etcd 相关的操作，POST 操作会将数据写入到 etcd 中，以上在 handler 中的主要处理流程如下所示： 12v1beta1 ⇒ internal ⇒ | ⇒ | ⇒ v1 ⇒ json/yaml ⇒ etcd admission validation kube-apiserver 中的组件kube-apiserver 共由 3 个组件构成（Aggregator、KubeAPIServer、APIExtensionServer），这些组件依次通过 Delegation 处理请求： Aggregator：暴露的功能类似于一个七层负载均衡，将来自用户的请求拦截转发给其他服务器，并且负责整个 APIServer 的 Discovery 功能； KubeAPIServer ：负责对请求的一些通用处理，认证、鉴权等，以及处理各个内建资源的 REST 服务； APIExtensionServer：主要处理 CustomResourceDefinition（CRD）和 CustomResource（CR）的 REST 请求，也是 Delegation 的最后一环，如果对应 CR 不能被处理的话则会返回 404。 Aggregator 和 APIExtensionsServer 对应两种主要扩展 APIServer 资源的方式，即分别是 AA 和 CRD。 AggregatorAggregator 通过 APIServices 对象关联到某个 Service 来进行请求的转发，其关联的 Service 类型进一步决定了请求转发形式。Aggregator 包括一个 GenericAPIServer 和维护自身状态的 Controller。其中 GenericAPIServer 主要处理 apiregistration.k8s.io 组下的 APIService 资源请求。 Aggregator 除了处理资源请求外还包含几个 controller： 1、apiserviceRegistrationController：负责 APIServices 中资源的注册与删除； 2、availableConditionController：维护 APIServices 的可用状态，包括其引用 Service 是否可用等； 3、autoRegistrationController：用于保持 API 中存在的一组特定的 APIServices； 4、crdRegistrationController：负责将 CRD GroupVersions 自动注册到 APIServices 中； 5、openAPIAggregationController：将 APIServices 资源的变化同步至提供的 OpenAPI 文档； kubernetes 中的一些附加组件，比如 metrics-server 就是通过 Aggregator 的方式进行扩展的，实际环境中可以通过使用 apiserver-builder 工具轻松以 Aggregator 的扩展方式创建自定义资源。 启用 API Aggregation在 kube-apiserver 中需要增加以下配置来开启 API Aggregation： 1234567--proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt--proxy-client-key-file=/etc/kubernetes/certs/proxy.key--requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt--requestheader-allowed-names=aggregator--requestheader-extra-headers-prefix=X-Remote-Extra---requestheader-group-headers=X-Remote-Group--requestheader-username-headers=X-Remote-User KubeAPIServerKubeAPIServer 主要是提供对 API Resource 的操作请求，为 kubernetes 中众多 API 注册路由信息，暴露 RESTful API 并且对外提供 kubernetes service，使集群中以及集群外的服务都可以通过 RESTful API 操作 kubernetes 中的资源。 APIExtensionServerAPIExtensionServer 作为 Delegation 链的最后一层，是处理所有用户通过 Custom Resource Definition 定义的资源服务器。 其中包含的 controller 以及功能如下所示： 1、openapiController：将 crd 资源的变化同步至提供的 OpenAPI 文档，可通过访问 /openapi/v2 进行查看； 2、crdController：负责将 crd 信息注册到 apiVersions 和 apiResources 中，两者的信息可通过 $ kubectl api-versions 和 $ kubectl api-resources 查看； 3、namingController：检查 crd obj 中是否有命名冲突，可在 crd .status.conditions 中查看； 4、establishingController：检查 crd 是否处于正常状态，可在 crd .status.conditions 中查看； 5、nonStructuralSchemaController：检查 crd obj 结构是否正常，可在 crd .status.conditions 中查看； 6、apiApprovalController：检查 crd 是否遵循 kubernetes API 声明策略，可在 crd .status.conditions 中查看； 7、finalizingController：类似于 finalizes 的功能，与 CRs 的删除有关； kube-apiserver 启动流程分析 kubernetes 版本：v1.16 首先分析 kube-apiserver 的启动方式，kube-apiserver 也是通过其 Run 方法启动主逻辑的，在Run 方法调用之前会进行解析命令行参数、设置默认值等。 RunRun 方法的主要逻辑为： 1、调用 CreateServerChain 构建服务调用链并判断是否启动非安全的 http server，http server 链中包含 apiserver 要启动的三个 server，以及为每个 server 注册对应资源的路由； 2、调用 server.PrepareRun 进行服务运行前的准备，该方法主要完成了健康检查、存活检查和OpenAPI路由的注册工作； 3、调用 prepared.Run 启动 https server； server 的初始化使用委托模式，通过 DelegationTarget 接口，把基本的 API Server、CustomResource、Aggregator 这三种服务采用链式结构串联起来，对外提供服务。 k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:147 12345678910111213func Run(completeOptions completedServerRunOptions, stopCh &lt;-chan struct&#123;&#125;) error &#123; server, err := CreateServerChain(completeOptions, stopCh) if err != nil &#123; return err &#125; prepared, err := server.PrepareRun() if err != nil &#123; return err &#125; return prepared.Run(stopCh)&#125; CreateServerChainCreateServerChain 是完成 server 初始化的方法，里面包含 APIExtensionsServer、KubeAPIServer、AggregatorServer 初始化的所有流程，最终返回 aggregatorapiserver.APIAggregator 实例，初始化流程主要有：http filter chain 的配置、API Group 的注册、http path 与 handler 的关联以及 handler 后端存储 etcd 的配置。其主要逻辑为： 1、调用 CreateKubeAPIServerConfig 创建 KubeAPIServer 所需要的配置，主要是创建 master.Config，其中会调用 buildGenericConfig 生成 genericConfig，genericConfig 中包含 apiserver 的核心配置； 2、判断是否启用了扩展的 API server 并调用 createAPIExtensionsConfig 为其创建配置，apiExtensions server 是一个代理服务，用于代理 kubeapiserver 中的其他 server，比如 metric-server； 3、调用 createAPIExtensionsServer 创建 apiExtensionsServer 实例； 4、调用 CreateKubeAPIServer初始化 kubeAPIServer； 5、调用 createAggregatorConfig 为 aggregatorServer 创建配置并调用 createAggregatorServer 初始化 aggregatorServer； 6、配置并判断是否启动非安全的 http server； k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:165 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051func CreateServerChain(completedOptions completedServerRunOptions, stopCh &lt;-chan struct&#123;&#125;) (*aggregatorapiserver.APIAggregator, error) &#123; nodeTunneler, proxyTransport, err := CreateNodeDialer(completedOptions) if err != nil &#123; return nil, err &#125; // 1、为 kubeAPIServer 创建配置 kubeAPIServerConfig, insecureServingInfo, serviceResolver, pluginInitializer, admissionPostStartHook, err := CreateKubeAPIServerConfig(completedOptions, nodeTunneler, proxyTransport) if err != nil &#123; return nil, err &#125; // 2、判断是否配置了 APIExtensionsServer，创建 apiExtensionsConfig apiExtensionsConfig, err := createAPIExtensionsConfig(*kubeAPIServerConfig.GenericConfig, kubeAPIServerConfig.ExtraConfig.VersionedInformers, pluginInitializer, completedOptions.ServerRunOptions, completedOptions.MasterCount, serviceResolver, webhook.NewDefaultAuthenticationInfoResolverWrapper(proxyTransport, kubeAPIServerConfig.GenericConfig.LoopbackClientConfig)) if err != nil &#123; return nil, err &#125; // 3、初始化 APIExtensionsServer apiExtensionsServer, err := createAPIExtensionsServer(apiExtensionsConfig, genericapiserver.NewEmptyDelegate()) if err != nil &#123; return nil, err &#125; // 4、初始化 KubeAPIServer kubeAPIServer, err := CreateKubeAPIServer(kubeAPIServerConfig, apiExtensionsServer.GenericAPIServer, admissionPostStartHook) if err != nil &#123; return nil, err &#125; // 5、创建 AggregatorConfig aggregatorConfig, err := createAggregatorConfig(*kubeAPIServerConfig.GenericConfig, completedOptions.ServerRunOptions, kubeAPIServerConfig. ExtraConfig.VersionedInformers, serviceResolver, proxyTransport, pluginInitializer) if err != nil &#123; return nil, err &#125; // 6、初始化 AggregatorServer aggregatorServer, err := createAggregatorServer(aggregatorConfig, kubeAPIServer.GenericAPIServer, apiExtensionsServer.Informers) if err != nil &#123; return nil, err &#125; // 7、判断是否启动非安全端口的 http server if insecureServingInfo != nil &#123; insecureHandlerChain := kubeserver.BuildInsecureHandlerChain(aggregatorServer.GenericAPIServer.UnprotectedHandler(), kubeAPIServerConfig.GenericConfig) if err := insecureServingInfo.Serve(insecureHandlerChain, kubeAPIServerConfig.GenericConfig.RequestTimeout, stopCh); err != nil &#123; return nil, err &#125; &#125; return aggregatorServer, nil&#125; CreateKubeAPIServerConfig在 CreateKubeAPIServerConfig 中主要是调用 buildGenericConfig 创建 genericConfig 以及构建 master.Config 对象。 k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:271 123456789101112131415161718192021222324252627282930313233343536373839404142434445func CreateKubeAPIServerConfig( s completedServerRunOptions, nodeTunneler tunneler.Tunneler, proxyTransport *http.Transport,) (......) &#123; // 1、构建 genericConfig genericConfig, versionedInformers, insecureServingInfo, serviceResolver, pluginInitializers, admissionPostStartHook, storageFactory, lastErr = buildGenericConfig(s.ServerRunOptions, proxyTransport) if lastErr != nil &#123; return &#125; ...... // 2、初始化所支持的 capabilities capabilities.Initialize(capabilities.Capabilities&#123; AllowPrivileged: s.AllowPrivileged, PrivilegedSources: capabilities.PrivilegedSources&#123; HostNetworkSources: []string&#123;&#125;, HostPIDSources: []string&#123;&#125;, HostIPCSources: []string&#123;&#125;, &#125;, PerConnectionBandwidthLimitBytesPerSec: s.MaxConnectionBytesPerSec, &#125;) // 3、获取 service ip range 以及 api server service IP serviceIPRange, apiServerServiceIP, lastErr := master.DefaultServiceIPRange(s.PrimaryServiceClusterIPRange) if lastErr != nil &#123; return &#125; ...... // 4、构建 master.Config 对象 config = &amp;master.Config&#123;......&#125; if nodeTunneler != nil &#123; config.ExtraConfig.KubeletClientConfig.Dial = nodeTunneler.Dial &#125; if config.GenericConfig.EgressSelector != nil &#123; config.ExtraConfig.KubeletClientConfig.Lookup = config.GenericConfig.EgressSelector.Lookup &#125; return&#125; buildGenericConfig主要逻辑为： 1、调用 genericapiserver.NewConfig 生成默认的 genericConfig，genericConfig 中主要配置了 DefaultBuildHandlerChain，DefaultBuildHandlerChain 中包含了认证、鉴权等一系列 http filter chain； 2、调用 master.DefaultAPIResourceConfigSource 加载需要启用的 API Resource，集群中所有的 API Resource 可以在代码的 k8s.io/api 目录中看到，随着版本的迭代也会不断变化； 3、为 genericConfig 中的部分字段设置默认值； 4、调用 completedStorageFactoryConfig.New 创建 storageFactory，后面会使用 storageFactory 为每种API Resource 创建对应的 RESTStorage； k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:386 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293func buildGenericConfig( s *options.ServerRunOptions, proxyTransport *http.Transport,) (......) &#123; // 1、为 genericConfig 设置默认值 genericConfig = genericapiserver.NewConfig(legacyscheme.Codecs) genericConfig.MergedResourceConfig = master.DefaultAPIResourceConfigSource() if lastErr = s.GenericServerRunOptions.ApplyTo(genericConfig); lastErr != nil &#123; return &#125; ...... genericConfig.OpenAPIConfig = genericapiserver.DefaultOpenAPIConfig(......) genericConfig.OpenAPIConfig.Info.Title = &quot;Kubernetes&quot; genericConfig.LongRunningFunc = filters.BasicLongRunningRequestCheck( sets.NewString(&quot;watch&quot;, &quot;proxy&quot;), sets.NewString(&quot;attach&quot;, &quot;exec&quot;, &quot;proxy&quot;, &quot;log&quot;, &quot;portforward&quot;), ) kubeVersion := version.Get() genericConfig.Version = &amp;kubeVersion storageFactoryConfig := kubeapiserver.NewStorageFactoryConfig() storageFactoryConfig.ApiResourceConfig = genericConfig.MergedResourceConfig completedStorageFactoryConfig, err := storageFactoryConfig.Complete(s.Etcd) if err != nil &#123; lastErr = err return &#125; // 初始化 storageFactory storageFactory, lastErr = completedStorageFactoryConfig.New() if lastErr != nil &#123; return &#125; if genericConfig.EgressSelector != nil &#123; storageFactory.StorageConfig.Transport.EgressLookup = genericConfig.EgressSelector.Lookup &#125; // 2、初始化 RESTOptionsGetter，后期根据其获取操作 Etcd 的句柄，同时添加 etcd 的健康检查方法 if lastErr = s.Etcd.ApplyWithStorageFactoryTo(storageFactory, genericConfig); lastErr != nil &#123; return &#125; // 3、设置使用 protobufs 用来内部交互，并且禁用压缩功能 genericConfig.LoopbackClientConfig.ContentConfig.ContentType = &quot;application/vnd.kubernetes.protobuf&quot; genericConfig.LoopbackClientConfig.DisableCompression = true // 4、创建 clientset kubeClientConfig := genericConfig.LoopbackClientConfig clientgoExternalClient, err := clientgoclientset.NewForConfig(kubeClientConfig) if err != nil &#123; lastErr = fmt.Errorf(&quot;failed to create real external clientset: %v&quot;, err) return &#125; versionedInformers = clientgoinformers.NewSharedInformerFactory(clientgoExternalClient, 10*time.Minute) // 5、创建认证实例，支持多种认证方式：请求 Header 认证、Auth 文件认证、CA 证书认证、Bearer token 认证、 // ServiceAccount 认证、BootstrapToken 认证、WebhookToken 认证等 genericConfig.Authentication.Authenticator, genericConfig.OpenAPIConfig.SecurityDefinitions, err = BuildAuthenticator(s, clientgoExternalClient, versionedInformers) if err != nil &#123; lastErr = fmt.Errorf(&quot;invalid authentication config: %v&quot;, err) return &#125; // 6、创建鉴权实例，包含：Node、RBAC、Webhook、ABAC、AlwaysAllow、AlwaysDeny genericConfig.Authorization.Authorizer, genericConfig.RuleResolver, err = BuildAuthorizer(s, versionedInformers) ...... serviceResolver = buildServiceResolver(s.EnableAggregatorRouting, genericConfig.LoopbackClientConfig.Host, versionedInformers) authInfoResolverWrapper := webhook.NewDefaultAuthenticationInfoResolverWrapper(proxyTransport, genericConfig.LoopbackClientConfig) // 7、审计插件的初始化 lastErr = s.Audit.ApplyTo(......) if lastErr != nil &#123; return &#125; // 8、准入插件的初始化 pluginInitializers, admissionPostStartHook, err = admissionConfig.New(proxyTransport, serviceResolver) if err != nil &#123; lastErr = fmt.Errorf(&quot;failed to create admission plugin initializer: %v&quot;, err) return &#125; err = s.Admission.ApplyTo(......) if err != nil &#123; lastErr = fmt.Errorf(&quot;failed to initialize admission: %v&quot;, err) &#125; return&#125; 以上主要分析 KubeAPIServerConfig 的初始化，其他两个 server config 的初始化暂且不详细分析，下面接着继续分析 server 的初始化。 createAPIExtensionsServerAPIExtensionsServer 是最先被初始化的，在 createAPIExtensionsServer 中调用 apiextensionsConfig.Complete().New 来完成 server 的初始化，其主要逻辑为： 1、首先调用 c.GenericConfig.New 按照go-restful的模式初始化 Container，在 c.GenericConfig.New 中会调用 NewAPIServerHandler 初始化 handler，APIServerHandler 包含了 API Server 使用的多种http.Handler 类型，包括 go-restful 以及 non-go-restful，以及在以上两者之间选择的 Director 对象，go-restful 用于处理已经注册的 handler，non-go-restful 用来处理不存在的 handler，API URI 处理的选择过程为：FullHandlerChain-&gt; Director -&gt;{GoRestfulContainer， NonGoRestfulMux}。在 c.GenericConfig.New 中还会调用 installAPI来添加包括 /、/debug/*、/metrics、/version 等路由信息。三种 server 在初始化时首先都会调用 c.GenericConfig.New 来初始化一个 genericServer，然后进行 API 的注册； 2、调用 s.GenericAPIServer.InstallAPIGroup 在路由中注册 API Resources，此方法的调用链非常深，主要是为了将需要暴露的 API Resource 注册到 server 中，以便能通过 http 接口进行 resource 的 REST 操作，其他几种 server 在初始化时也都会执行对应的 InstallAPI； 3、初始化 server 中需要使用的 controller，主要有 openapiController、crdController、namingController、establishingController、nonStructuralSchemaController、apiApprovalController、finalizingController； 4、将需要启动的 controller 以及 informer 添加到 PostStartHook 中； k8s.io/kubernetes/cmd/kube-apiserver/app/apiextensions.go:94 123func createAPIExtensionsServer(apiextensionsConfig *apiextensionsapiserver.Config, delegateAPIServer genericapiserver.DelegationTarget) (* apiextensionsapiserver.CustomResourceDefinitions, error) &#123; return apiextensionsConfig.Complete().New(delegateAPIServer)&#125; k8s.io/kubernetes/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/apiserver.go:132 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485func (c completedConfig) New(delegationTarget genericapiserver.DelegationTarget) (*CustomResourceDefinitions, error) &#123; // 1、初始化 genericServer genericServer, err := c.GenericConfig.New(&quot;apiextensions-apiserver&quot;, delegationTarget) if err != nil &#123; return nil, err &#125; s := &amp;CustomResourceDefinitions&#123; GenericAPIServer: genericServer, &#125; // 2、初始化 APIGroup Info，APIGroup 指该 server 需要暴露的 API apiResourceConfig := c.GenericConfig.MergedResourceConfig apiGroupInfo := genericapiserver.NewDefaultAPIGroupInfo(apiextensions.GroupName, Scheme, metav1.ParameterCodec, Codecs) if apiResourceConfig.VersionEnabled(v1beta1.SchemeGroupVersion) &#123; storage := map[string]rest.Storage&#123;&#125; customResourceDefintionStorage := customresourcedefinition.NewREST(Scheme, c.GenericConfig.RESTOptionsGetter) storage[&quot;customresourcedefinitions&quot;] = customResourceDefintionStorage storage[&quot;customresourcedefinitions/status&quot;] = customresourcedefinition.NewStatusREST(Scheme, customResourceDefintionStorage) apiGroupInfo.VersionedResourcesStorageMap[v1beta1.SchemeGroupVersion.Version] = storage &#125; if apiResourceConfig.VersionEnabled(v1.SchemeGroupVersion) &#123; ...... &#125; // 3、注册 APIGroup if err := s.GenericAPIServer.InstallAPIGroup(&amp;apiGroupInfo); err != nil &#123; return nil, err &#125; // 4、初始化需要使用的 controller crdClient, err := internalclientset.NewForConfig(s.GenericAPIServer.LoopbackClientConfig) if err != nil &#123; return nil, fmt.Errorf(&quot;failed to create clientset: %v&quot;, err) &#125; s.Informers = internalinformers.NewSharedInformerFactory(crdClient, 5*time.Minute) ...... establishingController := establish.NewEstablishingController(s.Informers.Apiextensions().InternalVersion(). CustomResourceDefinitions(), crdClient.Apiextensions()) crdHandler, err := NewCustomResourceDefinitionHandler(......) if err != nil &#123; return nil, err &#125; s.GenericAPIServer.Handler.NonGoRestfulMux.Handle(&quot;/apis&quot;, crdHandler) s.GenericAPIServer.Handler.NonGoRestfulMux.HandlePrefix(&quot;/apis/&quot;, crdHandler) crdController := NewDiscoveryController(s.Informers.Apiextensions().InternalVersion().CustomResourceDefinitions(), versionDiscoveryHandler, groupDiscoveryHandler) namingController := status.NewNamingConditionController(s.Informers.Apiextensions().InternalVersion().CustomResourceDefinitions(), crdClient.Apiextensions()) nonStructuralSchemaController := nonstructuralschema.NewConditionController(s.Informers.Apiextensions().InternalVersion(). CustomResourceDefinitions(), crdClient.Apiextensions()) apiApprovalController := apiapproval.NewKubernetesAPIApprovalPolicyConformantConditionController(s.Informers.Apiextensions(). InternalVersion().CustomResourceDefinitions(), crdClient.Apiextensions()) finalizingController := finalizer.NewCRDFinalizer( s.Informers.Apiextensions().InternalVersion().CustomResourceDefinitions(), crdClient.Apiextensions(), crdHandler, ) var openapiController *openapicontroller.Controller if utilfeature.DefaultFeatureGate.Enabled(apiextensionsfeatures.CustomResourcePublishOpenAPI) &#123; openapiController = openapicontroller.NewController(s.Informers.Apiextensions().InternalVersion().CustomResourceDefinitions()) &#125; // 5、将 informer 以及 controller 添加到 PostStartHook 中 s.GenericAPIServer.AddPostStartHookOrDie(&quot;start-apiextensions-informers&quot;, func(context genericapiserver.PostStartHookContext) error &#123; s.Informers.Start(context.StopCh) return nil &#125;) s.GenericAPIServer.AddPostStartHookOrDie(&quot;start-apiextensions-controllers&quot;, func(context genericapiserver.PostStartHookContext) error &#123; ...... go crdController.Run(context.StopCh) go namingController.Run(context.StopCh) go establishingController.Run(context.StopCh) go nonStructuralSchemaController.Run(5, context.StopCh) go apiApprovalController.Run(5, context.StopCh) go finalizingController.Run(5, context.StopCh) return nil &#125;) s.GenericAPIServer.AddPostStartHookOrDie(&quot;crd-informer-synced&quot;, func(context genericapiserver.PostStartHookContext) error &#123; return wait.PollImmediateUntil(100*time.Millisecond, func() (bool, error) &#123; return s.Informers.Apiextensions().InternalVersion().CustomResourceDefinitions().Informer().HasSynced(), nil &#125;, context.StopCh) &#125;) return s, nil&#125; 以上是 APIExtensionsServer 的初始化流程，其中最核心方法是 s.GenericAPIServer.InstallAPIGroup，也就是 API 的注册过程，三种 server 中 API 的注册过程都是其核心。 CreateKubeAPIServer本节继续分析 KubeAPIServer 的初始化，在CreateKubeAPIServer 中调用了 kubeAPIServerConfig.Complete().New 来完成相关的初始化操作。 kubeAPIServerConfig.Complete().New主要逻辑为： 1、调用 c.GenericConfig.New 初始化 GenericAPIServer，其主要实现在上文已经分析过； 2、判断是否支持 logs 相关的路由，如果支持，则添加 /logs 路由； 3、调用 m.InstallLegacyAPI 将核心 API Resource 添加到路由中，对应到 apiserver 就是以 /api 开头的 resource； 4、调用 m.InstallAPIs 将扩展的 API Resource 添加到路由中，在 apiserver 中即是以 /apis 开头的 resource； k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:214 12345678910func CreateKubeAPIServer(......) (*master.Master, error) &#123; kubeAPIServer, err := kubeAPIServerConfig.Complete().New(delegateAPIServer) if err != nil &#123; return nil, err &#125; kubeAPIServer.GenericAPIServer.AddPostStartHookOrDie(&quot;start-kube-apiserver-admission-initializer&quot;, admissionPostStartHook) return kubeAPIServer, nil&#125; k8s.io/kubernetes/pkg/master/master.go:325 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (c completedConfig) New(delegationTarget genericapiserver.DelegationTarget) (*Master, error) &#123; ...... // 1、初始化 GenericAPIServer s, err := c.GenericConfig.New(&quot;kube-apiserver&quot;, delegationTarget) if err != nil &#123; return nil, err &#125; // 2、注册 logs 相关的路由 if c.ExtraConfig.EnableLogsSupport &#123; routes.Logs&#123;&#125;.Install(s.Handler.GoRestfulContainer) &#125; m := &amp;Master&#123; GenericAPIServer: s, &#125; // 3、安装 LegacyAPI if c.ExtraConfig.APIResourceConfigSource.VersionEnabled(apiv1.SchemeGroupVersion) &#123; legacyRESTStorageProvider := corerest.LegacyRESTStorageProvider&#123; StorageFactory: c.ExtraConfig.StorageFactory, ProxyTransport: c.ExtraConfig.ProxyTransport, ...... &#125; if err := m.InstallLegacyAPI(&amp;c, c.GenericConfig.RESTOptionsGetter, legacyRESTStorageProvider); err != nil &#123; return nil, err &#125; &#125; restStorageProviders := []RESTStorageProvider&#123; auditregistrationrest.RESTStorageProvider&#123;&#125;, authenticationrest.RESTStorageProvider&#123;Authenticator: c.GenericConfig.Authentication.Authenticator, APIAudiences: c.GenericConfig. Authentication.APIAudiences&#125;, ...... &#125; // 4、安装 APIs if err := m.InstallAPIs(c.ExtraConfig.APIResourceConfigSource, c.GenericConfig.RESTOptionsGetter, restStorageProviders...); err != nil &#123; return nil, err &#125; if c.ExtraConfig.Tunneler != nil &#123; m.installTunneler(c.ExtraConfig.Tunneler, corev1client.NewForConfigOrDie(c.GenericConfig.LoopbackClientConfig).Nodes()) &#125; m.GenericAPIServer.AddPostStartHookOrDie(&quot;ca-registration&quot;, c.ExtraConfig.ClientCARegistrationHook.PostStartHook) return m, nil&#125; m.InstallLegacyAPI此方法的主要功能是将 core API 注册到路由中，是 apiserver 初始化流程中最核心的方法之一，不过其调用链非常深，下面会进行深入分析。将 API 注册到路由其最终的目的就是对外提供 RESTful API 来操作对应 resource，注册 API 主要分为两步，第一步是为 API 中的每个 resource 初始化 RESTStorage 以此操作后端存储中数据的变更，第二步是为每个 resource 根据其 verbs 构建对应的路由。m.InstallLegacyAPI 的主要逻辑为： 1、调用 legacyRESTStorageProvider.NewLegacyRESTStorage 为 LegacyAPI 中各个资源创建 RESTStorage，RESTStorage 的目的是将每种资源的访问路径及其后端存储的操作对应起来； 2、初始化 bootstrap-controller，并将其加入到 PostStartHook 中，bootstrap-controller 是 apiserver 中的一个 controller，主要功能是创建系统所需要的一些 namespace 以及创建 kubernetes service 并定期触发对应的 sync 操作，apiserver 在启动后会通过调用 PostStartHook 来启动 bootstrap-controller； 3、在为资源创建完 RESTStorage 后，调用 m.GenericAPIServer.InstallLegacyAPIGroup 为 APIGroup 注册路由信息，InstallLegacyAPIGroup方法的调用链非常深，主要为InstallLegacyAPIGroup--&gt; installAPIResources --&gt; InstallREST --&gt; Install --&gt; registerResourceHandlers，最终核心的路由构造在registerResourceHandlers方法内，该方法比较复杂，其主要功能是通过上一步骤构造的 REST Storage 判断该资源可以执行哪些操作（如 create、update等），将其对应的操作存入到 action 中，每一个 action 对应一个标准的 REST 操作，如 create 对应的 action 操作为 POST、update 对应的 action 操作为PUT。最终根据 actions 数组依次遍历，对每一个操作添加一个 handler 方法，注册到 route 中去，再将 route 注册到 webservice 中去，webservice 最终会注册到 container 中，遵循 go-restful 的设计模式； 关于 legacyRESTStorageProvider.NewLegacyRESTStorage 以及 m.GenericAPIServer.InstallLegacyAPIGroup 方法的详细说明在后文中会继续进行讲解。 k8s.io/kubernetes/pkg/master/master.go:406 1234567891011121314151617func (m *Master) InstallLegacyAPI(......) error &#123; legacyRESTStorage, apiGroupInfo, err := legacyRESTStorageProvider.NewLegacyRESTStorage(restOptionsGetter) if err != nil &#123; return fmt.Errorf(&quot;Error building core storage: %v&quot;, err) &#125; controllerName := &quot;bootstrap-controller&quot; coreClient := corev1client.NewForConfigOrDie(c.GenericConfig.LoopbackClientConfig) bootstrapController := c.NewBootstrapController(legacyRESTStorage, coreClient, coreClient, coreClient, coreClient.RESTClient()) m.GenericAPIServer.AddPostStartHookOrDie(controllerName, bootstrapController.PostStartHook) m.GenericAPIServer.AddPreShutdownHookOrDie(controllerName, bootstrapController.PreShutdownHook) if err := m.GenericAPIServer.InstallLegacyAPIGroup(genericapiserver.DefaultLegacyAPIPrefix, &amp;apiGroupInfo); err != nil &#123; return fmt.Errorf(&quot;Error in registering group versions: %v&quot;, err) &#125; return nil&#125; InstallAPIs 与 InstallLegacyAPI 的主要流程是类似的，限于篇幅此处不再深入分析。 createAggregatorServerAggregatorServer 主要用于自定义的聚合控制器的，使 CRD 能够自动注册到集群中。 主要逻辑为： 1、调用 aggregatorConfig.Complete().NewWithDelegate 创建 aggregatorServer； 2、初始化 crdRegistrationController 和 autoRegistrationController，crdRegistrationController 负责注册 CRD，autoRegistrationController 负责将 CRD 对应的 APIServices 自动注册到 apiserver 中，CRD 创建后可通过 $ kubectl get apiservices 查看是否注册到 apiservices 中； 3、将 autoRegistrationController 和 crdRegistrationController 加入到 PostStartHook 中； k8s.io/kubernetes/cmd/kube-apiserver/app/aggregator.go:124 123456789101112131415161718192021222324252627282930313233343536373839404142func createAggregatorServer(......) (*aggregatorapiserver.APIAggregator, error) &#123; // 1、初始化 aggregatorServer aggregatorServer, err := aggregatorConfig.Complete().NewWithDelegate(delegateAPIServer) if err != nil &#123; return nil, err &#125; // 2、初始化 auto-registration controller apiRegistrationClient, err := apiregistrationclient.NewForConfig(aggregatorConfig.GenericConfig.LoopbackClientConfig) if err != nil &#123; return nil, err &#125; autoRegistrationController := autoregister.NewAutoRegisterController(......) apiServices := apiServicesToRegister(delegateAPIServer, autoRegistrationController) crdRegistrationController := crdregistration.NewCRDRegistrationController(......) err = aggregatorServer.GenericAPIServer.AddPostStartHook(&quot;kube-apiserver-autoregistration&quot;, func(context genericapiserver.PostStartHookContext) error &#123; go crdRegistrationController.Run(5, context.StopCh) go func() &#123; if aggregatorConfig.GenericConfig.MergedResourceConfig.AnyVersionForGroupEnabled(&quot;apiextensions.k8s.io&quot;) &#123; crdRegistrationController.WaitForInitialSync() &#125; autoRegistrationController.Run(5, context.StopCh) &#125;() return nil &#125;) if err != nil &#123; return nil, err &#125; err = aggregatorServer.GenericAPIServer.AddBootSequenceHealthChecks( makeAPIServiceAvailableHealthCheck( &quot;autoregister-completion&quot;, apiServices, aggregatorServer.APIRegistrationInformers.Apiregistration().V1().APIServices(), ), ) if err != nil &#123; return nil, err &#125; return aggregatorServer, nil&#125; aggregatorConfig.Complete().NewWithDelegateaggregatorConfig.Complete().NewWithDelegate 是初始化 aggregatorServer 的方法，主要逻辑为： 1、调用 c.GenericConfig.New 初始化 GenericAPIServer，其内部的主要功能在上文已经分析过； 2、调用 apiservicerest.NewRESTStorage 为 APIServices 资源创建 RESTStorage，RESTStorage 的目的是将每种资源的访问路径及其后端存储的操作对应起来； 3、调用 s.GenericAPIServer.InstallAPIGroup 为 APIGroup 注册路由信息； k8s.io/kubernetes/staging/src/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go:158 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061func (c completedConfig) NewWithDelegate(delegationTarget genericapiserver.DelegationTarget) (*APIAggregator, error) &#123; openAPIConfig := c.GenericConfig.OpenAPIConfig c.GenericConfig.OpenAPIConfig = nil // 1、初始化 genericServer genericServer, err := c.GenericConfig.New(&quot;kube-aggregator&quot;, delegationTarget) if err != nil &#123; return nil, err &#125; apiregistrationClient, err := clientset.NewForConfig(c.GenericConfig.LoopbackClientConfig) if err != nil &#123; return nil, err &#125; informerFactory := informers.NewSharedInformerFactory( apiregistrationClient, 5*time.Minute, ) s := &amp;APIAggregator&#123; GenericAPIServer: genericServer, delegateHandler: delegationTarget.UnprotectedHandler(), ...... &#125; // 2、为 API 注册路由 apiGroupInfo := apiservicerest.NewRESTStorage(c.GenericConfig.MergedResourceConfig, c.GenericConfig.RESTOptionsGetter) if err := s.GenericAPIServer.InstallAPIGroup(&amp;apiGroupInfo); err != nil &#123; return nil, err &#125; // 3、初始化 apiserviceRegistrationController、availableController apisHandler := &amp;apisHandler&#123; codecs: aggregatorscheme.Codecs, lister: s.lister, &#125; s.GenericAPIServer.Handler.NonGoRestfulMux.Handle(&quot;/apis&quot;, apisHandler) s.GenericAPIServer.Handler.NonGoRestfulMux.UnlistedHandle(&quot;/apis/&quot;, apisHandler) apiserviceRegistrationController := NewAPIServiceRegistrationController(informerFactory.Apiregistration().V1().APIServices(), s) availableController, err := statuscontrollers.NewAvailableConditionController( ...... ) if err != nil &#123; return nil, err &#125; // 4、添加 PostStartHook s.GenericAPIServer.AddPostStartHookOrDie(&quot;start-kube-aggregator-informers&quot;, func(context genericapiserver.PostStartHookContext) error &#123; informerFactory.Start(context.StopCh) c.GenericConfig.SharedInformerFactory.Start(context.StopCh) return nil &#125;) s.GenericAPIServer.AddPostStartHookOrDie(&quot;apiservice-registration-controller&quot;, func(context genericapiserver.PostStartHookContext) error &#123; go apiserviceRegistrationController.Run(context.StopCh) return nil &#125;) s.GenericAPIServer.AddPostStartHookOrDie(&quot;apiservice-status-available-controller&quot;, func(context genericapiserver.PostStartHookContext) error &#123; go availableController.Run(5, context.StopCh) return nil &#125;) return s, nil&#125; 以上是对 AggregatorServer 初始化流程的分析，可以看出，在创建 APIExtensionsServer、KubeAPIServer 以及 AggregatorServer 时，其模式都是类似的，首先调用 c.GenericConfig.New 按照go-restful的模式初始化 Container，然后为 server 中需要注册的资源创建 RESTStorage，最后将 resource 的 APIGroup 信息注册到路由中。 至此，CreateServerChain 中流程已经分析完，其中的调用链如下所示： 123456789101112131415161718192021222324 |--&gt; CreateNodeDialer | |--&gt; CreateKubeAPIServerConfig |CreateServerChain --|--&gt; createAPIExtensionsConfig | | |--&gt; c.GenericConfig.New |--&gt; createAPIExtensionsServer --&gt; apiextensionsConfig.Complete().New --| | |--&gt; s.GenericAPIServer.InstallAPIGroup | | |--&gt; c.GenericConfig.New --&gt; legacyRESTStorageProvider.NewLegacyRESTStorage | | |--&gt; CreateKubeAPIServer --&gt; kubeAPIServerConfig.Complete().New --|--&gt; m.InstallLegacyAPI | | | |--&gt; m.InstallAPIs | | |--&gt; createAggregatorConfig | | |--&gt; c.GenericConfig.New | | |--&gt; createAggregatorServer --&gt; aggregatorConfig.Complete().NewWithDelegate --|--&gt; apiservicerest.NewRESTStorage | |--&gt; s.GenericAPIServer.InstallAPIGroup prepared.Run在 Run 方法中首先调用 CreateServerChain 完成各 server 的初始化，然后调用 server.PrepareRun 完成服务启动前的准备工作，最后调用 prepared.Run 方法来启动安全的 http server。server.PrepareRun 主要完成了健康检查、存活检查和OpenAPI路由的注册工作，下面继续分析 prepared.Run 的流程，在 prepared.Run 中主要调用 s.NonBlockingRun 来完成启动工作。 k8s.io/kubernetes/staging/src/k8s.io/kube-aggregator/pkg/apiserver/apiserver.go:269 123func (s preparedAPIAggregator) Run(stopCh &lt;-chan struct&#123;&#125;) error &#123; return s.runnable.Run(stopCh)&#125; k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:316 123456789101112131415161718192021222324252627func (s preparedGenericAPIServer) Run(stopCh &lt;-chan struct&#123;&#125;) error &#123; delayedStopCh := make(chan struct&#123;&#125;) go func() &#123; defer close(delayedStopCh) &lt;-stopCh time.Sleep(s.ShutdownDelayDuration) &#125;() // 调用 s.NonBlockingRun 完成启动流程 err := s.NonBlockingRun(delayedStopCh) if err != nil &#123; return err &#125; // 当收到退出信号后完成一些收尾工作 &lt;-stopCh err = s.RunPreShutdownHooks() if err != nil &#123; return err &#125; &lt;-delayedStopCh s.HandlerChainWaitGroup.Wait() return nil&#125; s.NonBlockingRuns.NonBlockingRun 的主要逻辑为： 1、判断是否要启动审计日志服务； 2、调用 s.SecureServingInfo.Serve 配置并启动 https server； 3、执行 postStartHooks； 4、向 systemd 发送 ready 信号； k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/genericapiserver.go:351 1234567891011121314151617181920212223242526272829303132333435363738394041424344func (s preparedGenericAPIServer) NonBlockingRun(stopCh &lt;-chan struct&#123;&#125;) error &#123; auditStopCh := make(chan struct&#123;&#125;) // 1、判断是否要启动审计日志 if s.AuditBackend != nil &#123; if err := s.AuditBackend.Run(auditStopCh); err != nil &#123; return fmt.Errorf(&quot;failed to run the audit backend: %v&quot;, err) &#125; &#125; // 2、启动 https server internalStopCh := make(chan struct&#123;&#125;) var stoppedCh &lt;-chan struct&#123;&#125; if s.SecureServingInfo != nil &amp;&amp; s.Handler != nil &#123; var err error stoppedCh, err = s.SecureServingInfo.Serve(s.Handler, s.ShutdownTimeout, internalStopCh) if err != nil &#123; close(internalStopCh) close(auditStopCh) return err &#125; &#125; go func() &#123; &lt;-stopCh close(s.readinessStopCh) close(internalStopCh) if stoppedCh != nil &#123; &lt;-stoppedCh &#125; s.HandlerChainWaitGroup.Wait() close(auditStopCh) &#125;() // 3、执行 postStartHooks s.RunPostStartHooks(stopCh) // 4、向 systemd 发送 ready 信号 if _, err := systemd.SdNotify(true, &quot;READY=1\n&quot;); err != nil &#123; klog.Errorf(&quot;Unable to send systemd daemon successful start message: %v\n&quot;, err) &#125; return nil&#125; 以上就是 server 的初始化以及启动流程过程的分析，上文已经提到各 server 初始化过程中最重要的就是 API Resource RESTStorage 的初始化以及路由的注册，由于该过程比较复杂，下文会单独进行讲述。 storageFactory 的构建上文已经提到过，apiserver 最终实现的 handler 对应的后端数据是以 Store 的结构保存的，这里以 /api 开头的路由举例，通过NewLegacyRESTStorage方法创建各个资源的RESTStorage。RESTStorage 是一个结构体，具体的定义在k8s.io/apiserver/pkg/registry/generic/registry/store.go下，结构体内主要包含NewFunc返回特定资源信息、NewListFunc返回特定资源列表、CreateStrategy特定资源创建时的策略、UpdateStrategy更新时的策略以及DeleteStrategy删除时的策略等重要方法。在NewLegacyRESTStorage内部，可以看到创建了多种资源的 RESTStorage。 NewLegacyRESTStorage 的调用链为 CreateKubeAPIServer --&gt; kubeAPIServerConfig.Complete().New --&gt; m.InstallLegacyAPI --&gt; legacyRESTStorageProvider.NewLegacyRESTStorage。 NewLegacyRESTStorage一个 API Group 下的资源都有其 REST 实现，k8s.io/kubernetes/pkg/registry下所有的 Group 都有一个rest目录，存储的就是对应资源的 RESTStorage。在NewLegacyRESTStorage方法中，通过NewREST或者NewStorage会生成各种资源对应的 Storage，此处以 pod 为例进行说明。 k8s.io/kubernetes/pkg/registry/core/rest/storage_core.go:102 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495func (c LegacyRESTStorageProvider) NewLegacyRESTStorage(restOptionsGetter generic.RESTOptionsGetter) (LegacyRESTStorage, genericapiserver. APIGroupInfo, error) &#123; apiGroupInfo := genericapiserver.APIGroupInfo&#123; PrioritizedVersions: legacyscheme.Scheme.PrioritizedVersionsForGroup(&quot;&quot;), VersionedResourcesStorageMap: map[string]map[string]rest.Storage&#123;&#125;, Scheme: legacyscheme.Scheme, ParameterCodec: legacyscheme.ParameterCodec, NegotiatedSerializer: legacyscheme.Codecs, &#125; var podDisruptionClient policyclient.PodDisruptionBudgetsGetter if policyGroupVersion := (schema.GroupVersion&#123;Group: &quot;policy&quot;, Version: &quot;v1beta1&quot;&#125;); legacyscheme.Scheme. IsVersionRegistered(policyGroupVersion) &#123; var err error podDisruptionClient, err = policyclient.NewForConfig(c.LoopbackClientConfig) if err != nil &#123; return LegacyRESTStorage&#123;&#125;, genericapiserver.APIGroupInfo&#123;&#125;, err &#125; &#125; // 1、LegacyAPI 下的 resource RESTStorage 的初始化 restStorage := LegacyRESTStorage&#123;&#125; podTemplateStorage, err := podtemplatestore.NewREST(restOptionsGetter) if err != nil &#123; return LegacyRESTStorage&#123;&#125;, genericapiserver.APIGroupInfo&#123;&#125;, err &#125; eventStorage, err := eventstore.NewREST(restOptionsGetter, uint64(c.EventTTL.Seconds())) if err != nil &#123; return LegacyRESTStorage&#123;&#125;, genericapiserver.APIGroupInfo&#123;&#125;, err &#125; limitRangeStorage, err := limitrangestore.NewREST(restOptionsGetter) if err != nil &#123; return LegacyRESTStorage&#123;&#125;, genericapiserver.APIGroupInfo&#123;&#125;, err &#125; ...... endpointsStorage, err := endpointsstore.NewREST(restOptionsGetter) if err != nil &#123; return LegacyRESTStorage&#123;&#125;, genericapiserver.APIGroupInfo&#123;&#125;, err &#125; nodeStorage, err := nodestore.NewStorage(restOptionsGetter, c.KubeletClientConfig, c.ProxyTransport) if err != nil &#123; return LegacyRESTStorage&#123;&#125;, genericapiserver.APIGroupInfo&#123;&#125;, err &#125; // 2、pod RESTStorage 的初始化 podStorage, err := podstore.NewStorage(......) if err != nil &#123; return LegacyRESTStorage&#123;&#125;, genericapiserver.APIGroupInfo&#123;&#125;, err &#125; ...... serviceClusterIPAllocator, err := ipallocator.NewAllocatorCIDRRange(&amp;serviceClusterIPRange, func(max int, rangeSpec string) (allocator. Interface, error) &#123; ...... &#125;) if err != nil &#123; return LegacyRESTStorage&#123;&#125;, genericapiserver.APIGroupInfo&#123;&#125;, fmt.Errorf(&quot;cannot create cluster IP allocator: %v&quot;, err) &#125; restStorage.ServiceClusterIPAllocator = serviceClusterIPRegistry var secondaryServiceClusterIPAllocator ipallocator.Interface if utilfeature.DefaultFeatureGate.Enabled(features.IPv6DualStack) &amp;&amp; c.SecondaryServiceIPRange.IP != nil &#123; ...... &#125; var serviceNodePortRegistry rangeallocation.RangeRegistry serviceNodePortAllocator, err := portallocator.NewPortAllocatorCustom(c.ServiceNodePortRange, func(max int, rangeSpec string) (allocator.Interface, error) &#123; ...... &#125;) if err != nil &#123; return LegacyRESTStorage&#123;&#125;, genericapiserver.APIGroupInfo&#123;&#125;, fmt.Errorf(&quot;cannot create cluster port allocator: %v&quot;, err) &#125; restStorage.ServiceNodePortAllocator = serviceNodePortRegistry controllerStorage, err := controllerstore.NewStorage(restOptionsGetter) if err != nil &#123; return LegacyRESTStorage&#123;&#125;, genericapiserver.APIGroupInfo&#123;&#125;, err &#125; serviceRest, serviceRestProxy := servicestore.NewREST(......) // 3、restStorageMap 保存 resource http path 与 RESTStorage 对应关系 restStorageMap := map[string]rest.Storage&#123; &quot;pods&quot;: podStorage.Pod, &quot;pods/attach&quot;: podStorage.Attach, &quot;pods/status&quot;: podStorage.Status, &quot;pods/log&quot;: podStorage.Log, &quot;pods/exec&quot;: podStorage.Exec, &quot;pods/portforward&quot;: podStorage.PortForward, &quot;pods/proxy&quot;: podStorage.Proxy, ...... &quot;componentStatuses&quot;: componentstatus.NewStorage(componentStatusStorage&#123;c.StorageFactory&#125;.serversToValidate), &#125; ......&#125; podstore.NewStoragepodstore.NewStorage 是为 pod 生成 storage 的方法，该方法主要功能是为 pod 创建后端存储最终返回一个 RESTStorage 对象，其中调用 store.CompleteWithOptions 来创建后端存储的。 k8s.io/kubernetes/pkg/registry/core/pod/storage/storage.go:71 1234567891011121314151617181920212223242526272829303132333435363738func NewStorage(......) (PodStorage, error) &#123; store := &amp;genericregistry.Store&#123; NewFunc: func() runtime.Object &#123; return &amp;api.Pod&#123;&#125; &#125;, NewListFunc: func() runtime.Object &#123; return &amp;api.PodList&#123;&#125; &#125;, ...... &#125; options := &amp;generic.StoreOptions&#123; RESTOptions: optsGetter, AttrFunc: pod.GetAttrs, TriggerFunc: map[string]storage.IndexerFunc&#123;&quot;spec.nodeName&quot;: pod.NodeNameTriggerFunc&#125;, &#125; // 调用 store.CompleteWithOptions if err := store.CompleteWithOptions(options); err != nil &#123; return PodStorage&#123;&#125;, err &#125; statusStore := *store statusStore.UpdateStrategy = pod.StatusStrategy ephemeralContainersStore := *store ephemeralContainersStore.UpdateStrategy = pod.EphemeralContainersStrategy bindingREST := &amp;BindingREST&#123;store: store&#125; // PodStorage 对象 return PodStorage&#123; Pod: &amp;REST&#123;store, proxyTransport&#125;, Binding: &amp;BindingREST&#123;store: store&#125;, LegacyBinding: &amp;LegacyBindingREST&#123;bindingREST&#125;, Eviction: newEvictionStorage(store, podDisruptionBudgetClient), Status: &amp;StatusREST&#123;store: &amp;statusStore&#125;, EphemeralContainers: &amp;EphemeralContainersREST&#123;store: &amp;ephemeralContainersStore&#125;, Log: &amp;podrest.LogREST&#123;Store: store, KubeletConn: k&#125;, Proxy: &amp;podrest.ProxyREST&#123;Store: store, ProxyTransport: proxyTransport&#125;, Exec: &amp;podrest.ExecREST&#123;Store: store, KubeletConn: k&#125;, Attach: &amp;podrest.AttachREST&#123;Store: store, KubeletConn: k&#125;, PortForward: &amp;podrest.PortForwardREST&#123;Store: store, KubeletConn: k&#125;, &#125;, nil&#125; 可以看到最终返回的对象里对 pod 的不同操作都是一个 REST 对象，REST 中自动集成了 genericregistry.Store 对象，而 store.CompleteWithOptions 方法就是对 genericregistry.Store 对象中存储实例就行初始化的。 123456789type REST struct &#123; *genericregistry.Store proxyTransport http.RoundTripper&#125;type BindingREST struct &#123; store *genericregistry.Store&#125;...... store.CompleteWithOptionsstore.CompleteWithOptions 主要功能是为 store 中的配置设置一些默认的值以及根据提供的 options 更新 store，其中最主要的就是初始化 store 的后端存储实例。 在CompleteWithOptions方法内，调用了options.RESTOptions.GetRESTOptions 方法，其最终返回generic.RESTOptions 对象，generic.RESTOptions 对象中包含对 etcd 初始化的一些配置、数据序列化方法以及对 etcd 操作的 storage.Interface 对象。其会依次调用StorageWithCacher--&gt;NewRawStorage--&gt;Create方法创建最终依赖的后端存储。 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go:1192 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879func (e *Store) CompleteWithOptions(options *generic.StoreOptions) error &#123; ...... var isNamespaced bool switch &#123; case e.CreateStrategy != nil: isNamespaced = e.CreateStrategy.NamespaceScoped() case e.UpdateStrategy != nil: isNamespaced = e.UpdateStrategy.NamespaceScoped() default: return fmt.Errorf(&quot;store for %s must have CreateStrategy or UpdateStrategy set&quot;, e.DefaultQualifiedResource.String()) &#125; ...... // 1、调用 options.RESTOptions.GetRESTOptions opts, err := options.RESTOptions.GetRESTOptions(e.DefaultQualifiedResource) if err != nil &#123; return err &#125; // 2、设置 ResourcePrefix prefix := opts.ResourcePrefix if !strings.HasPrefix(prefix, &quot;/&quot;) &#123; prefix = &quot;/&quot; + prefix &#125; if prefix == &quot;/&quot; &#123; return fmt.Errorf(&quot;store for %s has an invalid prefix %q&quot;, e.DefaultQualifiedResource.String(), opts.ResourcePrefix) &#125; if e.KeyRootFunc == nil &amp;&amp; e.KeyFunc == nil &#123; ...... &#125; keyFunc := func(obj runtime.Object) (string, error) &#123; ...... &#125; // 3、以下操作主要是将 opts 对象中的值赋值到 store 对象中 if e.DeleteCollectionWorkers == 0 &#123; e.DeleteCollectionWorkers = opts.DeleteCollectionWorkers &#125; e.EnableGarbageCollection = opts.EnableGarbageCollection if e.ObjectNameFunc == nil &#123; ...... &#125; if e.Storage.Storage == nil &#123; e.Storage.Codec = opts.StorageConfig.Codec var err error e.Storage.Storage, e.DestroyFunc, err = opts.Decorator( opts.StorageConfig, prefix, keyFunc, e.NewFunc, e.NewListFunc, attrFunc, options.TriggerFunc, ) if err != nil &#123; return err &#125; e.StorageVersioner = opts.StorageConfig.EncodeVersioner if opts.CountMetricPollPeriod &gt; 0 &#123; stopFunc := e.startObservingCount(opts.CountMetricPollPeriod) previousDestroy := e.DestroyFunc e.DestroyFunc = func() &#123; stopFunc() if previousDestroy != nil &#123; previousDestroy() &#125; &#125; &#125; &#125; return nil&#125; options.RESTOptions 是一个 interface，想要找到其 GetRESTOptions 方法的实现必须知道 options.RESTOptions 初始化时对应的实例，其初始化是在 CreateKubeAPIServerConfig --&gt; buildGenericConfig --&gt; s.Etcd.ApplyWithStorageFactoryTo 方法中进行初始化的，RESTOptions 对应的实例为 StorageFactoryRestOptionsFactory，所以 PodStorage 初始时构建的 store 对象中genericserver.Config.RESTOptionsGetter 实际的对象类型为 StorageFactoryRestOptionsFactory，其 GetRESTOptions 方法如下所示： k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/options/etcd.go:253 1234567891011121314151617181920212223242526272829func (f *StorageFactoryRestOptionsFactory) GetRESTOptions(resource schema.GroupResource) (generic.RESTOptions, error) &#123; storageConfig, err := f.StorageFactory.NewConfig(resource) if err != nil &#123; return generic.RESTOptions&#123;&#125;, fmt.Errorf(&quot;unable to find storage destination for %v, due to %v&quot;, resource, err.Error()) &#125; ret := generic.RESTOptions&#123; StorageConfig: storageConfig, Decorator: generic.UndecoratedStorage, DeleteCollectionWorkers: f.Options.DeleteCollectionWorkers, EnableGarbageCollection: f.Options.EnableGarbageCollection, ResourcePrefix: f.StorageFactory.ResourcePrefix(resource), CountMetricPollPeriod: f.Options.StorageConfig.CountMetricPollPeriod, &#125; if f.Options.EnableWatchCache &#123; sizes, err := ParseWatchCacheSizes(f.Options.WatchCacheSizes) if err != nil &#123; return generic.RESTOptions&#123;&#125;, err &#125; cacheSize, ok := sizes[resource] if !ok &#123; cacheSize = f.Options.DefaultWatchCacheSize &#125; // 调用 generic.StorageDecorator ret.Decorator = genericregistry.StorageWithCacher(cacheSize) &#125; return ret, nil&#125; 在 genericregistry.StorageWithCacher 中又调用了不同的方法最终会调用 factory.Create 来初始化存储实例，其调用链为：genericregistry.StorageWithCacher --&gt; generic.NewRawStorage --&gt; factory.Create。 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/factory.go:30 1234567891011func Create(c storagebackend.Config) (storage.Interface, DestroyFunc, error) &#123; switch c.Type &#123; case &quot;etcd2&quot;: return nil, nil, fmt.Errorf(&quot;%v is no longer a supported storage backend&quot;, c.Type) // 目前 k8s 只支持使用 etcd v3 case storagebackend.StorageTypeUnset, storagebackend.StorageTypeETCD3: return newETCD3Storage(c) default: return nil, nil, fmt.Errorf(&quot;unknown storage type: %s&quot;, c.Type) &#125;&#125; newETCD3Storage在 newETCD3Storage 中，首先通过调用 newETCD3Client 创建 etcd 的 client，client 的创建最终是通过 etcd 官方提供的客户端工具 clientv3 进行创建的。 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go:209 12345678910111213141516171819202122232425func newETCD3Storage(c storagebackend.Config) (storage.Interface, DestroyFunc, error) &#123; stopCompactor, err := startCompactorOnce(c.Transport, c.CompactionInterval) if err != nil &#123; return nil, nil, err &#125; client, err := newETCD3Client(c.Transport) if err != nil &#123; stopCompactor() return nil, nil, err &#125; var once sync.Once destroyFunc := func() &#123; once.Do(func() &#123; stopCompactor() client.Close() &#125;) &#125; transformer := c.Transformer if transformer == nil &#123; transformer = value.IdentityTransformer &#125; return etcd3.New(client, c.Codec, c.Prefix, transformer, c.Paging), destroyFunc, nil&#125; 至此对于 pod resource 中 store 的构建基本分析完成，不同 resource 对应一个 REST 对象，其中又引用了 genericregistry.Store 对象，最终是对 genericregistry.Store 的初始化。在分析完 store 的初始化后还有一个重要的步骤就是路由的注册，路由注册主要的流程是为 resource 根据不同 verbs 构建 http path 以及将 path 与对应 handler 进行绑定。 路由注册上文 RESTStorage 的构建对应的是 InstallLegacyAPI 中的 legacyRESTStorageProvider.NewLegacyRESTStorage 方法，下面继续分析 InstallLegacyAPI 中的 m.GenericAPIServer.InstallLegacyAPIGroup 方法的实现。 k8s.io/kubernetes/pkg/master/master.go:406 123456789101112func (m *Master) InstallLegacyAPI(......) error &#123; legacyRESTStorage, apiGroupInfo, err := legacyRESTStorageProvider.NewLegacyRESTStorage(restOptionsGetter) if err != nil &#123; return fmt.Errorf(&quot;Error building core storage: %v&quot;, err) &#125; ...... if err := m.GenericAPIServer.InstallLegacyAPIGroup(genericapiserver.DefaultLegacyAPIPrefix, &amp;apiGroupInfo); err != nil &#123; return fmt.Errorf(&quot;Error in registering group versions: %v&quot;, err) &#125; return nil&#125; m.GenericAPIServer.InstallLegacyAPIGroup 的调用链非常深，最终是为 Group 下每一个 API resources 注册 handler 及路由信息，其调用链为：m.GenericAPIServer.InstallLegacyAPIGroup --&gt; s.installAPIResources --&gt; apiGroupVersion.InstallREST --&gt; installer.Install --&gt; a.registerResourceHandlers。其中几个方法的作用如下所示： s.installAPIResources：为每一个 API resource 调用 apiGroupVersion.InstallREST 添加路由； apiGroupVersion.InstallREST：将 restful.WebServic 对象添加到 container 中； installer.Install：返回最终的 restful.WebService 对象 a.registerResourceHandlers该方法实现了 rest.Storage 到 restful.Route 的转换，其首先会判断 API Resource 所支持的 REST 接口，然后为 REST 接口添加对应的 handler，最后将其注册到路由中。 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:181 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122func (a *APIInstaller) registerResourceHandlers(path string, storage rest.Storage, ws *restful.WebService) (*metav1.APIResource, error) &#123; admit := a.group.Admit ...... // 1、判断该 resource 实现了哪些 REST 操作接口，以此来判断其支持的 verbs 以便为其添加路由 creater, isCreater := storage.(rest.Creater) namedCreater, isNamedCreater := storage.(rest.NamedCreater) lister, isLister := storage.(rest.Lister) getter, isGetter := storage.(rest.Getter) getterWithOptions, isGetterWithOptions := storage.(rest.GetterWithOptions) gracefulDeleter, isGracefulDeleter := storage.(rest.GracefulDeleter) collectionDeleter, isCollectionDeleter := storage.(rest.CollectionDeleter) updater, isUpdater := storage.(rest.Updater) patcher, isPatcher := storage.(rest.Patcher) watcher, isWatcher := storage.(rest.Watcher) connecter, isConnecter := storage.(rest.Connecter) storageMeta, isMetadata := storage.(rest.StorageMetadata) storageVersionProvider, isStorageVersionProvider := storage.(rest.StorageVersionProvider) if !isMetadata &#123; storageMeta = defaultStorageMetadata&#123;&#125; &#125; exporter, isExporter := storage.(rest.Exporter) if !isExporter &#123; exporter = nil &#125; ...... // 2、为 resource 添加对应的 actions 并根据是否支持 namespace switch &#123; case !namespaceScoped: ...... actions = appendIf(actions, action&#123;&quot;LIST&quot;, resourcePath, resourceParams, namer, false&#125;, isLister) actions = appendIf(actions, action&#123;&quot;POST&quot;, resourcePath, resourceParams, namer, false&#125;, isCreater) actions = appendIf(actions, action&#123;&quot;DELETECOLLECTION&quot;, resourcePath, resourceParams, namer, false&#125;, isCollectionDeleter) actions = appendIf(actions, action&#123;&quot;WATCHLIST&quot;, &quot;watch/&quot; + resourcePath, resourceParams, namer, false&#125;, allowWatchList) actions = appendIf(actions, action&#123;&quot;GET&quot;, itemPath, nameParams, namer, false&#125;, isGetter) if getSubpath &#123; actions = appendIf(actions, action&#123;&quot;GET&quot;, itemPath + &quot;/&#123;path:*&#125;&quot;, proxyParams, namer, false&#125;, isGetter) &#125; actions = appendIf(actions, action&#123;&quot;PUT&quot;, itemPath, nameParams, namer, false&#125;, isUpdater) actions = appendIf(actions, action&#123;&quot;PATCH&quot;, itemPath, nameParams, namer, false&#125;, isPatcher) actions = appendIf(actions, action&#123;&quot;DELETE&quot;, itemPath, nameParams, namer, false&#125;, isGracefulDeleter) actions = appendIf(actions, action&#123;&quot;WATCH&quot;, &quot;watch/&quot; + itemPath, nameParams, namer, false&#125;, isWatcher) actions = appendIf(actions, action&#123;&quot;CONNECT&quot;, itemPath, nameParams, namer, false&#125;, isConnecter) actions = appendIf(actions, action&#123;&quot;CONNECT&quot;, itemPath + &quot;/&#123;path:*&#125;&quot;, proxyParams, namer, false&#125;, isConnecter &amp;&amp; connectSubpath) default: ...... actions = appendIf(actions, action&#123;&quot;LIST&quot;, resourcePath, resourceParams, namer, false&#125;, isLister) actions = appendIf(actions, action&#123;&quot;POST&quot;, resourcePath, resourceParams, namer, false&#125;, isCreater) actions = appendIf(actions, action&#123;&quot;DELETECOLLECTION&quot;, resourcePath, resourceParams, namer, false&#125;, isCollectionDeleter) actions = appendIf(actions, action&#123;&quot;WATCHLIST&quot;, &quot;watch/&quot; + resourcePath, resourceParams, namer, false&#125;, allowWatchList) actions = appendIf(actions, action&#123;&quot;GET&quot;, itemPath, nameParams, namer, false&#125;, isGetter) ...... &#125; // 3、根据 action 创建对应的 route kubeVerbs := map[string]struct&#123;&#125;&#123;&#125; reqScope := handlers.RequestScope&#123; Serializer: a.group.Serializer, ParameterCodec: a.group.ParameterCodec, Creater: a.group.Creater, Convertor: a.group.Convertor, ...... &#125; ...... // 4、从 rest.Storage 到 restful.Route 映射 // 为每个操作添加对应的 handler for _, action := range actions &#123; ...... verbOverrider, needOverride := storage.(StorageMetricsOverride) switch action.Verb &#123; case &quot;GET&quot;: ...... case &quot;LIST&quot;: case &quot;PUT&quot;: case &quot;PATCH&quot;: // 此处以 POST 操作进行说明 case &quot;POST&quot;: var handler restful.RouteFunction // 5、初始化 handler if isNamedCreater &#123; handler = restfulCreateNamedResource(namedCreater, reqScope, admit) &#125; else &#123; handler = restfulCreateResource(creater, reqScope, admit) &#125; handler = metrics.InstrumentRouteFunc(action.Verb, group, version, resource, subresource, requestScope, metrics.APIServerComponent, handler) article := GetArticleForNoun(kind, &quot; &quot;) doc := &quot;create&quot; + article + kind if isSubresource &#123; doc = &quot;create &quot; + subresource + &quot; of&quot; + article + kind &#125; // 6、route 与 handler 进行绑定 route := ws.POST(action.Path).To(handler). Doc(doc). Param(ws.QueryParameter(&quot;pretty&quot;, &quot;If &apos;true&apos;, then the output is pretty printed.&quot;)). Operation(&quot;create&quot;+namespaced+kind+strings.Title(subresource)+operationSuffix). Produces(append(storageMeta.ProducesMIMETypes(action.Verb), mediaTypes...)...). Returns(http.StatusOK, &quot;OK&quot;, producedObject). Returns(http.StatusCreated, &quot;Created&quot;, producedObject). Returns(http.StatusAccepted, &quot;Accepted&quot;, producedObject). Reads(defaultVersionedObject). Writes(producedObject) if err := AddObjectParams(ws, route, versionedCreateOptions); err != nil &#123; return nil, err &#125; addParams(route, action.Params) // 7、添加到路由中 routes = append(routes, route) case &quot;DELETE&quot;: case &quot;DELETECOLLECTION&quot;: case &quot;WATCH&quot;: case &quot;WATCHLIST&quot;: case &quot;CONNECT&quot;: default: &#125; ...... return &amp;apiResource, nil&#125; restfulCreateNamedResourcerestfulCreateNamedResource 是 POST 操作对应的 handler，最终会调用 createHandler 方法完成。 k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:1087 123456789func restfulCreateNamedResource(r rest.NamedCreater, scope handlers.RequestScope, admit admission.Interface) restful.RouteFunction &#123; return func(req *restful.Request, res *restful.Response) &#123; handlers.CreateNamedResource(r, &amp;scope, admit)(res.ResponseWriter, req.Request) &#125;&#125;func CreateNamedResource(r rest.NamedCreater, scope *RequestScope, admission admission.Interface) http.HandlerFunc &#123; return createHandler(r, scope, admission, true)&#125; createHandler createHandler 是将数据写入到后端存储的方法，对于资源的操作都有相关的权限控制，在 createHandler 中首先会执行 decoder 和 admission 操作，然后调用 create 方法完成 resource 的创建，在 create 方法中会进行 validate 以及最终将数据保存到后端存储中。admit 操作即执行 kube-apiserver 中的 admission-plugins，admission-plugins 在 CreateKubeAPIServerConfig 中被初始化为了 admissionChain，其初始化的调用链为 CreateKubeAPIServerConfig --&gt; buildGenericConfig --&gt; s.Admission.ApplyTo --&gt; a.GenericAdmission.ApplyTo --&gt; a.Plugins.NewFromPlugins，最终在 a.Plugins.NewFromPlugins 中将所有已启用的 plugins 封装为 admissionChain，此处要执行的 admit 操作即执行 admission-plugins 中的 admit 操作。 createHandler 中调用的 create 方法是genericregistry.Store 对象的方法，在每个 resource 初始化 RESTStorage 都会引入 genericregistry.Store 对象。 createHandler 中所有的操作就是本文开头提到的请求流程，如下所示： 12v1beta1 ⇒ internal ⇒ | ⇒ | ⇒ v1 ⇒ json/yaml ⇒ etcd admission validation k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/create.go:46 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465func createHandler(r rest.NamedCreater, scope *RequestScope, admit admission.Interface, includeName bool) http.HandlerFunc &#123; return func(w http.ResponseWriter, req *http.Request) &#123; trace := utiltrace.New(&quot;Create&quot;, utiltrace.Field&#123;&quot;url&quot;, req.URL.Path&#125;) defer trace.LogIfLong(500 * time.Millisecond) ...... gv := scope.Kind.GroupVersion() // 1、得到合适的SerializerInfo s, err := negotiation.NegotiateInputSerializer(req, false, scope.Serializer) if err != nil &#123; scope.err(err, w, req) return &#125; // 2、找到合适的 decoder decoder := scope.Serializer.DecoderToVersion(s.Serializer, scope.HubGroupVersion) body, err := limitedReadBody(req, scope.MaxRequestBodyBytes) if err != nil &#123; scope.err(err, w, req) return &#125; ...... defaultGVK := scope.Kind original := r.New() trace.Step(&quot;About to convert to expected version&quot;) // 3、decoder 解码 obj, gvk, err := decoder.Decode(body, &amp;defaultGVK, original) ...... ae := request.AuditEventFrom(ctx) admit = admission.WithAudit(admit, ae) audit.LogRequestObject(ae, obj, scope.Resource, scope.Subresource, scope.Serializer) userInfo, _ := request.UserFrom(ctx) if len(name) == 0 &#123; _, name, _ = scope.Namer.ObjectName(obj) &#125; // 4、执行 admit 操作，即执行 kube-apiserver 启动时加载的 admission-plugins， admissionAttributes := admission.NewAttributesRecord(......) if mutatingAdmission, ok := admit.(admission.MutationInterface); ok &amp;&amp; mutatingAdmission.Handles(admission.Create) &#123; err = mutatingAdmission.Admit(ctx, admissionAttributes, scope) if err != nil &#123; scope.err(err, w, req) return &#125; &#125; ...... // 5、执行 create 操作 result, err := finishRequest(timeout, func() (runtime.Object, error) &#123; return r.Create( ctx, name, obj, rest.AdmissionToValidateObjectFunc(admit, admissionAttributes, scope), options, ) &#125;) ...... &#125;&#125; 总结本文主要分析 kube-apiserver 的启动流程，kube-apiserver 中包含三个 server，分别为 KubeAPIServer、APIExtensionsServer 以及 AggregatorServer，三个 server 是通过委托模式连接在一起的，初始化过程都是类似的，首先为每个 server 创建对应的 config，然后初始化 http server，http server 的初始化过程为首先初始化 GoRestfulContainer，然后安装 server 所包含的 API，安装 API 时首先为每个 API Resource 创建对应的后端存储 RESTStorage，再为每个 API Resource 支持的 verbs 添加对应的 handler，并将 handler 注册到 route 中，最后将 route 注册到 webservice 中，启动流程中 RESTFul API 的实现流程是其核心，至于 kube-apiserver 中认证鉴权等 filter 的实现、多版本资源转换、kubernetes service 的实现等一些细节会在后面的文章中继续进行分析。 参考： https://mp.weixin.qq.com/s/hTEWatYLhTnC5X0FBM2RWQ https://bbbmj.github.io/2019/04/13/Kubernetes/code-analytics/kube-apiserver/ https://mp.weixin.qq.com/s/TQuqAAzBjeWHwKPJZ3iJhA https://blog.openshift.com/kubernetes-deep-dive-api-server-part-1/ https://www.jianshu.com/p/daa4ff387a78]]></content>
      <tags>
        <tag>kube-apiserver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet 中垃圾回收机制的设计与实现]]></title>
    <url>%2F2020%2F02%2F06%2Fkubelet_garbage_collect%2F</url>
    <content type="text"><![CDATA[kubernetes 中的垃圾回收机制主要有两部分组成： 一是由 kube-controller-manager 中的 gc controller 自动回收 kubernetes 中被删除的对象以及其依赖的对象； 二是在每个节点上需要回收已退出的容器以及当 node 上磁盘资源不足时回收已不再使用的容器镜像； 本文主要分析 kubelet 中的垃圾回收机制，垃圾回收的主要目的是为了节约宿主上的资源，gc controller 的回收机制可以参考以前的文章 garbage collector controller 源码分析。 kubelet 中与容器垃圾回收有关的主要有以下三个参数: --maximum-dead-containers-per-container: 表示一个 pod 最多可以保存多少个已经停止的容器，默认为1；（maxPerPodContainerCount） --maximum-dead-containers：一个 node 上最多可以保留多少个已经停止的容器，默认为 -1，表示没有限制； --minimum-container-ttl-duration：已经退出的容器可以存活的最小时间，默认为 0s； 与镜像回收有关的主要有以下三个参数： --image-gc-high-threshold：当 kubelet 磁盘达到多少时，kubelet 开始回收镜像，默认为 85% 开始回收，根目录以及数据盘； --image-gc-low-threshold：回收镜像时当磁盘使用率减少至多少时停止回收，默认为 80%； --minimum-image-ttl-duration：未使用的镜像在被回收前的最小存留时间，默认为 2m0s； kubelet 中容器回收过程如下:pod 中的容器退出时间超过--minimum-container-ttl-duration后会被标记为可回收，一个 pod 中最多可以保留--maximum-dead-containers-per-container个已经停止的容器，一个 node 上最多可以保留--maximum-dead-containers个已停止的容器。在回收容器时，kubelet 会按照容器的退出时间排序，最先回收退出时间最久的容器。需要注意的是，kubelet 在回收时会将 pod 中的 container 与 sandboxes 分别进行回收，且在回收容器后会将其对应的 log dir 也进行回收； kubelet 中镜像回收过程如下:当容器镜像挂载点文件系统的磁盘使用率大于--image-gc-high-threshold时（containerRuntime 为 docker 时，镜像存放目录默认为 /var/lib/docker），kubelet 开始删除节点中未使用的容器镜像，直到磁盘使用率降低至--image-gc-low-threshold 时停止镜像的垃圾回收。 kubelet GarbageCollect 源码分析 kubernetes 版本：v1.16 GarbageCollect 是在 kubelet 对象初始化完成后启动的，在 createAndInitKubelet 方法中首先调用 kubelet.NewMainKubelet 初始化了 kubelet 对象，随后调用 k.StartGarbageCollection 启动了 GarbageCollect。 k8s.io/kubernetes/cmd/kubelet/app/server.go:1089 1234567891011121314func createAndInitKubelet(......) &#123; k, err = kubelet.NewMainKubelet( ...... ) if err != nil &#123; return nil, err &#125; k.BirthCry() k.StartGarbageCollection() return k, nil&#125; k.StartGarbageCollection在 kubelet 中镜像的生命周期和容器的生命周期是通过 imageManager 和 containerGC 管理的。在 StartGarbageCollection 方法中会启动容器和镜像垃圾回收两个任务，其主要逻辑为： 1、启动 containerGC goroutine，ContainerGC 间隔时间默认为 1 分钟； 2、检查 --image-gc-high-threshold 参数的值，若为 100 则禁用 imageGC； 3、启动 imageGC goroutine，imageGC 间隔时间默认为 5 分钟； k8s.io/kubernetes/pkg/kubelet/kubelet.go:1270 1234567891011121314151617181920212223242526272829303132333435363738func (kl *Kubelet) StartGarbageCollection() &#123; loggedContainerGCFailure := false // 1、启动容器垃圾回收服务 go wait.Until(func() &#123; if err := kl.containerGC.GarbageCollect(); err != nil &#123; loggedContainerGCFailure = true &#125; else &#123; var vLevel klog.Level = 4 if loggedContainerGCFailure &#123; vLevel = 1 loggedContainerGCFailure = false &#125; klog.V(vLevel).Infof(&quot;Container garbage collection succeeded&quot;) &#125; &#125;, ContainerGCPeriod, wait.NeverStop) // 2、检查 ImageGCHighThresholdPercent 参数的值 if kl.kubeletConfiguration.ImageGCHighThresholdPercent == 100 &#123; return &#125; // 3、启动镜像垃圾回收服务 prevImageGCFailed := false go wait.Until(func() &#123; if err := kl.imageManager.GarbageCollect(); err != nil &#123; ...... prevImageGCFailed = true &#125; else &#123; var vLevel klog.Level = 4 if prevImageGCFailed &#123; vLevel = 1 prevImageGCFailed = false &#125; &#125; &#125;, ImageGCPeriod, wait.NeverStop)&#125; kl.containerGC.GarbageCollectkl.containerGC.GarbageCollect 调用的是 ContainerGC manager 中的方法，ContainerGC 是在 NewMainKubelet 中初始化的，ContainerGC 在初始化时需要指定一个 runtime，该 runtime 即 ContainerRuntime，在 kubelet 中即 kubeGenericRuntimeManager，也是在 NewMainKubelet 中初始化的。 k8s.io/kubernetes/pkg/kubelet/kubelet.go 1234567891011121314151617func NewMainKubelet()&#123; ...... // MinAge、MaxPerPodContainer、MaxContainers 分别上文章开头提到的与容器垃圾回收有关的 // 三个参数 containerGCPolicy := kubecontainer.ContainerGCPolicy&#123; MinAge: minimumGCAge.Duration, MaxPerPodContainer: int(maxPerPodContainerCount), MaxContainers: int(maxContainerCount), &#125; // 初始化 containerGC 模块 containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady) if err != nil &#123; return nil, err &#125; ......&#125; 以下是 ContainerGC 的初始化以及 GarbageCollect 的启动： k8s.io/kubernetes/pkg/kubelet/container/container_gc.go:68 123456789101112131415func NewContainerGC(runtime Runtime, policy ContainerGCPolicy, sourcesReadyProvider SourcesReadyProvider) (ContainerGC, error) &#123; if policy.MinAge &lt; 0 &#123; return nil, fmt.Errorf(&quot;invalid minimum garbage collection age: %v&quot;, policy.MinAge) &#125; return &amp;realContainerGC&#123; runtime: runtime, policy: policy, sourcesReadyProvider: sourcesReadyProvider, &#125;, nil&#125;func (cgc *realContainerGC) GarbageCollect() error &#123; return cgc.runtime.GarbageCollect(cgc.policy, cgc.sourcesReadyProvider.AllReady(), false)&#125; 可以看到，ContainerGC 中的 GarbageCollect 最终是调用 runtime 中的 GarbageCollect 方法，runtime 即 kubeGenericRuntimeManager。 cgc.runtime.GarbageCollectcgc.runtime.GarbageCollect 的实现是在 kubeGenericRuntimeManager 中，其主要逻辑为： 1、回收 pod 中的 container； 2、回收 pod 中的 sandboxes； 3、回收 pod 以及 container 的 log dir； k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_gc.go:378 123456789101112131415161718func (cgc *containerGC) GarbageCollect(gcPolicy kubecontainer.ContainerGCPolicy, allSourcesReady bool, evictTerminatedPods bool) error &#123; errors := []error&#123;&#125; // 1、回收 pod 中的 container if err := cgc.evictContainers(gcPolicy, allSourcesReady, evictTerminatedPods); err != nil &#123; errors = append(errors, err) &#125; // 2、回收 pod 中的 sandboxes if err := cgc.evictSandboxes(evictTerminatedPods); err != nil &#123; errors = append(errors, err) &#125; // 3、回收 pod 以及 container 的 log dir if err := cgc.evictPodLogsDirectories(allSourcesReady); err != nil &#123; errors = append(errors, err) &#125; return utilerrors.NewAggregate(errors)&#125; cgc.evictContainers在 cgc.evictContainers 方法中会回收所有可被回收的容器，其主要逻辑为： 1、首先调用 cgc.evictableContainers 获取可被回收的容器作为 evictUnits，可被回收的容器指非 running 状态且创建时间超过 MinAge，evictUnits 数组中包含 pod 与 container 的对应关系； 2、回收 deleted 状态以及 terminated 状态的 pod，遍历 evictUnits，若 pod 是否处于 deleted 或者 terminated 状态，则调用 cgc.removeOldestN 回收 pod 中的所有容器。deleted 状态指 pod 已经被删除或者其 status.phase 为 failed 且其 status.reason 为 evicted 或者 pod.deletionTimestamp != nil 且 pod 中所有容器的 status 为 terminated 或者 waiting 状态，terminated 状态指 pod 处于 Failed 或者 succeeded 状态； 3、对于非 deleted 或者 terminated 状态的 pod，调用 cgc.enforceMaxContainersPerEvictUnit 为其保留 MaxPerPodContainer 个已经退出的容器，按照容器退出的时间进行排序优先删除退出时间最久的，MaxPerPodContainer 在上文已经提过，表示一个 pod 最多可以保存多少个已经停止的容器，默认为1，可以使用 --maximum-dead-containers-per-container 在启动时指定； 4、若 kubelet 启动时指定了--maximum-dead-containers（默认为 -1 即不限制），即需要为 node 保留退出的容器数，若 node 上保留已经停止的容器数超过 --maximum-dead-containers，首先计算需要为每个 pod 保留多少个已退出的容器保证其总数不超过 --maximum-dead-containers 的值，若计算结果小于 1 则取 1，即至少保留一个，然后删除每个 pod 中不需要保留的容器，此时若 node 上保留已经停止的容器数依然超过需要保留的最大值，则将 evictUnits 中的容器按照退出时间进行排序删除退出时间最久的容器，使 node 上保留已经停止的容器数满足 --maximum-dead-containers 值； k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_gc.go:222 123456789101112131415161718192021222324252627282930313233343536373839404142434445func (cgc *containerGC) evictContainers(gcPolicy kubecontainer.ContainerGCPolicy, allSourcesReady bool, evictTerminatedPods bool) error &#123; // 1、获取可被回收的容器列表 evictUnits, err := cgc.evictableContainers(gcPolicy.MinAge) if err != nil &#123; return err &#125; // 2、回收 Deleted 状态以及 Terminated 状态的 pod，此处 allSourcesReady 指 kubelet // 支持的三种 podSource 是否都可用 if allSourcesReady &#123; for key, unit := range evictUnits &#123; if cgc.podStateProvider.IsPodDeleted(key.uid) || (cgc.podStateProvider.IsPodTerminated(key.uid) &amp;&amp; evictTerminatedPods) &#123; cgc.removeOldestN(unit, len(unit)) delete(evictUnits, key) &#125; &#125; &#125; // 3、为非 Deleted 状态以及 Terminated 状态的 pod 保留 MaxPerPodContainer 个已经退出的容器 if gcPolicy.MaxPerPodContainer &gt;= 0 &#123; cgc.enforceMaxContainersPerEvictUnit(evictUnits, gcPolicy.MaxPerPodContainer) &#125; // 4、若 kubelet 启动时指定了 --maximum-dead-containers（默认为 -1 即不限制）参数， // 此时需要为 node 保留退出的容器数不能超过 --maximum-dead-containers 个 if gcPolicy.MaxContainers &gt;= 0 &amp;&amp; evictUnits.NumContainers() &gt; gcPolicy.MaxContainers &#123; numContainersPerEvictUnit := gcPolicy.MaxContainers / evictUnits.NumEvictUnits() if numContainersPerEvictUnit &lt; 1 &#123; numContainersPerEvictUnit = 1 &#125; cgc.enforceMaxContainersPerEvictUnit(evictUnits, numContainersPerEvictUnit) numContainers := evictUnits.NumContainers() if numContainers &gt; gcPolicy.MaxContainers &#123; flattened := make([]containerGCInfo, 0, numContainers) for key := range evictUnits &#123; flattened = append(flattened, evictUnits[key]...) &#125; sort.Sort(byCreated(flattened)) cgc.removeOldestN(flattened, numContainers-gcPolicy.MaxContainers) &#125; &#125; return nil&#125; cgc.evictSandboxescgc.evictSandboxes 方法会回收所有可回收的 sandboxes，其主要逻辑为： 1、首先获取 node 上所有的 containers 和 sandboxes； 2、构建 sandboxes 与 pod 的对应关系并将其保存在 sandboxesByPodUID 中； 3、对 sandboxesByPodUID 列表按创建时间进行排序； 4、若 sandboxes 所在的 pod 处于 deleted 状态，则删除该 pod 中所有的 sandboxes 否则只保留退出时间最短的一个 sandboxes，deleted 状态在上文 cgc.evictContainers 方法中已经解释过； k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_gc.go:274 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354func (cgc *containerGC) evictSandboxes(evictTerminatedPods bool) error &#123; // 1、获取 node 上所有的 container containers, err := cgc.manager.getKubeletContainers(true) if err != nil &#123; return err &#125; // 2、获取 node 上所有的 sandboxes sandboxes, err := cgc.manager.getKubeletSandboxes(true) if err != nil &#123; return err &#125; // 3、收集所有 container 的 PodSandboxId sandboxIDs := sets.NewString() for _, container := range containers &#123; sandboxIDs.Insert(container.PodSandboxId) &#125; // 4、构建 sandboxes 与 pod 的对应关系并将其保存在 sandboxesByPodUID 中 sandboxesByPod := make(sandboxesByPodUID) for _, sandbox := range sandboxes &#123; podUID := types.UID(sandbox.Metadata.Uid) sandboxInfo := sandboxGCInfo&#123; id: sandbox.Id, createTime: time.Unix(0, sandbox.CreatedAt), &#125; if sandbox.State == runtimeapi.PodSandboxState_SANDBOX_READY &#123; sandboxInfo.active = true &#125; if sandboxIDs.Has(sandbox.Id) &#123; sandboxInfo.active = true &#125; sandboxesByPod[podUID] = append(sandboxesByPod[podUID], sandboxInfo) &#125; // 5、对 sandboxesByPod 进行排序 for uid := range sandboxesByPod &#123; sort.Sort(sandboxByCreated(sandboxesByPod[uid])) &#125; // 6、遍历 sandboxesByPod，若 sandboxes 所在的 pod 处于 deleted 状态， // 则删除该 pod 中所有的 sandboxes 否则只保留退出时间最短的一个 sandboxes for podUID, sandboxes := range sandboxesByPod &#123; if cgc.podStateProvider.IsPodDeleted(podUID) || (cgc.podStateProvider.IsPodTerminated(podUID) &amp;&amp; evictTerminatedPods) &#123; cgc.removeOldestNSandboxes(sandboxes, len(sandboxes)) &#125; else &#123; cgc.removeOldestNSandboxes(sandboxes, len(sandboxes)-1) &#125; &#125; return nil&#125; cgc.evictPodLogsDirectoriescgc.evictPodLogsDirectories 方法会回收所有可回收 pod 以及 container 的 log dir，其主要逻辑为： 1、首先回收 deleted 状态 pod logs dir，遍历 pod logs dir /var/log/pods，/var/log/pods 为 pod logs 的默认目录，pod logs dir 的格式为 /var/log/pods/NAMESPACE_NAME_UID，解析 pod logs dir 获取 pod uid，判断 pod 是否处于 deleted 状态，若处于 deleted 状态则删除其 logs dir； 2、回收 deleted 状态 container logs 链接目录，/var/log/containers 为 container log 的默认目录，其会软链接到 pod 的 log dir 下，例如： 1/var/log/containers/storage-provisioner_kube-system_storage-provisioner-acc8386e409dfb3cc01618cbd14c373d8ac6d7f0aaad9ced018746f31d0081e2.log -&gt; /var/log/pods/kube-system_storage-provisioner_b448e496-eb5d-4d71-b93f-ff7ff77d2348/storage-provisioner/0.log k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_gc.go:333 1234567891011121314151617181920212223242526272829303132func (cgc *containerGC) evictPodLogsDirectories(allSourcesReady bool) error &#123; osInterface := cgc.manager.osInterface // 1、回收 deleted 状态 pod logs dir if allSourcesReady &#123; dirs, err := osInterface.ReadDir(podLogsRootDirectory) if err != nil &#123; return fmt.Errorf(&quot;failed to read podLogsRootDirectory %q: %v&quot;, podLogsRootDirectory, err) &#125; for _, dir := range dirs &#123; name := dir.Name() podUID := parsePodUIDFromLogsDirectory(name) if !cgc.podStateProvider.IsPodDeleted(podUID) &#123; continue &#125; err := osInterface.RemoveAll(filepath.Join(podLogsRootDirectory, name)) if err != nil &#123; klog.Errorf(&quot;Failed to remove pod logs directory %q: %v&quot;, name, err) &#125; &#125; &#125; // 2、回收 deleted 状态 container logs 链接目录 logSymlinks, _ := osInterface.Glob(filepath.Join(legacyContainerLogsDir, fmt.Sprintf(&quot;*.%s&quot;, legacyLogSuffix))) for _, logSymlink := range logSymlinks &#123; if _, err := osInterface.Stat(logSymlink); os.IsNotExist(err) &#123; err := osInterface.Remove(logSymlink) if err != nil &#123; klog.Errorf(&quot;Failed to remove container log dead symlink %q: %v&quot;, logSymlink, err) &#125; &#125; &#125; return nil&#125; kl.imageManager.GarbageCollect上面已经分析了容器回收的主要流程，下面会继续分析镜像回收的流程，kl.imageManager.GarbageCollect 是镜像回收任务启动的方法，镜像回收流程是在 imageManager 中进行的，首先了解下 imageManager 的初始化，imageManager 也是在 NewMainKubelet 方法中进行初始化的。 k8s.io/kubernetes/pkg/kubelet/kubelet.go 12345678910111213141516func NewMainKubelet()&#123; ...... // 初始化时需要指定三个参数，三个参数已经在上文中提到过 imageGCPolicy := images.ImageGCPolicy&#123; MinAge: kubeCfg.ImageMinimumGCAge.Duration, HighThresholdPercent: int(kubeCfg.ImageGCHighThresholdPercent), LowThresholdPercent: int(kubeCfg.ImageGCLowThresholdPercent), &#125; ...... imageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions.PodSandboxImage) if err != nil &#123; return nil, fmt.Errorf(&quot;failed to initialize image manager: %v&quot;, err) &#125; klet.imageManager = imageManager ......&#125; kl.imageManager.GarbageCollect 方法的主要逻辑为： 1、首先调用 im.statsProvider.ImageFsStats 获取容器镜像存储目录挂载点文件系统的磁盘信息； 2、获取挂载点的 available 和 capacity 信息并计算其使用率； 3、若使用率大于 HighThresholdPercent，首先根据 LowThresholdPercent 值计算需要释放的磁盘量，然后调用 im.freeSpace 释放未使用的 image 直到满足磁盘空闲率； k8s.io/kubernetes/pkg/kubelet/images/image_gc_manager.go:269 123456789101112131415161718192021222324252627282930313233343536373839404142434445func (im *realImageGCManager) GarbageCollect() error &#123; // 1、获取容器镜像存储目录挂载点文件系统的磁盘信息 fsStats, err := im.statsProvider.ImageFsStats() if err != nil &#123; return err &#125; var capacity, available int64 if fsStats.CapacityBytes != nil &#123; capacity = int64(*fsStats.CapacityBytes) &#125; if fsStats.AvailableBytes != nil &#123; available = int64(*fsStats.AvailableBytes) &#125; if available &gt; capacity &#123; available = capacity &#125; if capacity == 0 &#123; err := goerrors.New(&quot;invalid capacity 0 on image filesystem&quot;) im.recorder.Eventf(im.nodeRef, v1.EventTypeWarning, events.InvalidDiskCapacity, err.Error()) return err &#125; // 2、若使用率大于 HighThresholdPercent，此时需要回收镜像 usagePercent := 100 - int(available*100/capacity) if usagePercent &gt;= im.policy.HighThresholdPercent &#123; // 3、计算需要释放的磁盘量 amountToFree := capacity*int64(100-im.policy.LowThresholdPercent)/100 - available // 4、调用 im.freeSpace 回收未使用的镜像信息 freed, err := im.freeSpace(amountToFree, time.Now()) if err != nil &#123; return err &#125; if freed &lt; amountToFree &#123; err := fmt.Errorf(&quot;failed to garbage collect required amount of images. Wanted to free %d bytes, but freed %d bytes&quot;, amountToFree, freed) im.recorder.Eventf(im.nodeRef, v1.EventTypeWarning, events.FreeDiskSpaceFailed, err.Error()) return err &#125; &#125; return nil&#125; im.freeSpaceim.freeSpace 是回收未使用镜像的方法，其主要逻辑为： 1、首先调用 im.detectImages 获取已经使用的 images 列表作为 imagesInUse； 2、遍历 im.imageRecords 根据 imagesInUse 获取所有未使用的 images 信息，im.imageRecords 记录 node 上所有 images 的信息； 3、根据使用时间对未使用的 images 列表进行排序； 4、遍历未使用的 images 列表然后调用 im.runtime.RemoveImage 删除镜像，直到回收完所有未使用 images 或者满足空闲率； k8s.io/kubernetes/pkg/kubelet/images/image_gc_manager.go:328 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354func (im *realImageGCManager) freeSpace(bytesToFree int64, freeTime time.Time) (int64, error) &#123; // 1、获取已经使用的 images 列表 imagesInUse, err := im.detectImages(freeTime) if err != nil &#123; return 0, err &#125; im.imageRecordsLock.Lock() defer im.imageRecordsLock.Unlock() // 2、获取所有未使用的 images 信息 images := make([]evictionInfo, 0, len(im.imageRecords)) for image, record := range im.imageRecords &#123; if isImageUsed(image, imagesInUse) &#123; klog.V(5).Infof(&quot;Image ID %s is being used&quot;, image) continue &#125; images = append(images, evictionInfo&#123; id: image, imageRecord: *record, &#125;) &#125; // 3、按镜像使用时间进行排序 sort.Sort(byLastUsedAndDetected(images)) // 4、回收未使用的镜像 var deletionErrors []error spaceFreed := int64(0) for _, image := range images &#123; if image.lastUsed.Equal(freeTime) || image.lastUsed.After(freeTime) &#123; continue &#125; if freeTime.Sub(image.firstDetected) &lt; im.policy.MinAge &#123; continue &#125; // 5、调用 im.runtime.RemoveImage 删除镜像 err := im.runtime.RemoveImage(container.ImageSpec&#123;Image: image.id&#125;) if err != nil &#123; deletionErrors = append(deletionErrors, err) continue &#125; delete(im.imageRecords, image.id) spaceFreed += image.size if spaceFreed &gt;= bytesToFree &#123; break &#125; &#125; if len(deletionErrors) &gt; 0 &#123; return spaceFreed, fmt.Errorf(&quot;wanted to free %d bytes, but freed %d bytes space with errors in image deletion: %v&quot;, bytesToFree, spaceFreed, errors.NewAggregate(deletionErrors)) &#125; return spaceFreed, nil&#125; 总结本文主要分析了 kubelet 中垃圾回收机制的实现，kubelet 中会定期回收 node 上已经退出的容器已经当 node 磁盘资源不足时回收不再使用的镜像来释放磁盘资源，容器以及镜像回收策略主要是通过 kubelet 中几个参数的阈值进行控制的。]]></content>
      <tags>
        <tag>kubelet</tag>
        <tag>GarbageCollect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 中 Qos 的设计与实现]]></title>
    <url>%2F2020%2F01%2F21%2Fkubelet_qos%2F</url>
    <content type="text"><![CDATA[kubernetes 中的 QosQoS(Quality of Service) 即服务质量，QoS 是一种控制机制，它提供了针对不同用户或者不同数据流采用相应不同的优先级，或者是根据应用程序的要求，保证数据流的性能达到一定的水准。kubernetes 中有三种 Qos，分别为： 1、Guaranteed：pod 的 requests 与 limits 设定的值相等； 2、Burstable：pod requests 小于 limits 的值且不为 0； 3、BestEffort：pod 的 requests 与 limits 均为 0； 三者的优先级如下所示，依次递增： 1BestEffort -&gt; Burstable -&gt; Guaranteed 不同 Qos 的本质区别三种 Qos 在调度和底层表现上都不一样： 1、在调度时调度器只会根据 request 值进行调度； 2、二是当系统 OOM上时对于处理不同 OOMScore 的进程表现不同，OOMScore 是针对 memory 的，当宿主上 memory 不足时系统会优先 kill 掉 OOMScore 值高的进程，可以使用 $ cat /proc/$PID/oom_score 查看进程的 OOMScore。OOMScore 的取值范围为 [-1000, 1000]，Guaranteed pod 的默认值为 -998，Burstable pod 的值为 2~999，BestEffort pod 的值为 1000，也就是说当系统 OOM 时，首先会 kill 掉 BestEffort pod 的进程，若系统依然处于 OOM 状态，然后才会 kill 掉 Burstable pod，最后是 Guaranteed pod； 3、三是 cgroup 的配置不同，kubelet 为会三种 Qos 分别创建对应的 QoS level cgroups，Guaranteed Pod Qos 的 cgroup level 会直接创建在 RootCgroup/kubepods 下，Burstable Pod Qos 的创建在 RootCgroup/kubepods/burstable 下，BestEffort Pod Qos 的创建在 RootCgroup/kubepods/BestEffort 下，上文已经说了 root cgroup 可以通过 $ mount | grep cgroup看到，在 cgroup 的每个子系统下都会创建 Qos level cgroups， 此外在对应的 QoS level cgroups 还会为 pod 创建 Pod level cgroups； 启用 Qos 和 Pod level cgroup在 kubernetes 中为了限制容器资源的使用，避免容器之间争抢资源或者容器影响所在的宿主机，kubelet 组件需要使用 cgroup 限制容器资源的使用量，cgroup 目前支持对进程多种资源的限制，而 kubelet 只支持限制 cpu、memory、pids、hugetlb 几种资源，与此资源有关的几个参数如下所示：--cgroups-per-qos：启用后会为每个 pod 以及 pod 对应的 Qos 创建 cgroups 层级树，默认启用；--cgroup-root：指定 root cgroup，如果不指定默认为“”，若为默认值则直接使用 root cgroup dir，在 node 上执行 $ mount | grep cgroup 可以看到 cgroup 所有子系统的挂载点，这些挂载点就是 root cgroup；--cpu-manager-policy：默认为 “none”，即默认不开启 ,支持使用 “static”，开启后可以支持对 Guaranteed Pod 进行绑核操作，绑核的主要目的是为了高效使用 cpu cache 以及内存节点；--kube-reserved：为 kubernetes 系统组件设置预留资源值，可以设置 cpu、memory、ephemeral-storage；--kube-reserved-cgroup：指定 kube-reserved 的 cgroup dir name，默认为 “/kube-reserved”；--system-reserved：为非 kubernetes 组件设置预留资源值，可以设置 cpu、memory、ephemeral-storage；--system-reserved-cgroup：设置 system-reserved 的 cgroup dir name，默认为 “/system-reserved”；--qos-reserved：Alpha feature，可以通过此参数为高优先级 pod 设置预留资源比例，目前只支持预留 memory，使用前需要开启 QOSReserved feature gate； 当启用了 --cgroups-per-qos 后，kubelet 会为不同 Qos 创建对应的 level cgroups，在 Qos level cgroups 下也会为 pod 创建对应的 pod level cgroups，在 pod level cgroups 下最终会为 container 创建对应的 level cgroups，从 Qos –&gt; pod –&gt; container，层层限制每个 level cgroups 的资源使用量。 配置 cgroup driverruntime 有两种 cgroup 驱动：一种是 systemd，另外一种是 cgroupfs： cgroupfs 比较好理解，比如说要限制内存是多少、要用 CPU share 为多少，其实直接把 pid 写入到对应cgroup task 文件中，然后把对应需要限制的资源也写入相应的 memory cgroup 文件和 CPU 的 cgroup 文件就可以了； 另外一个是 systemd 的 cgroup 驱动，这个驱动是因为 systemd 本身可以提供一个 cgroup 管理方式。所以如果用 systemd 做 cgroup 驱动的话，所有的写 cgroup 操作都必须通过 systemd 的接口来完成，不能手动更改 cgroup 的文件； kubernetes 中默认 kubelet 的 cgroup 驱动就是 cgroupfs，若要使用 systemd，则必须将 kubelet 以及 runtime 都需要配置为 systemd 驱动。 关于 cgroupfs 与 systemd driver 的区别可以参考 k8s 官方文档：container-runtimes/#cgroup-drivers，或者 runc 中的实现 github.com/opencontainers/runc/libcontainer/cgroups。 kubernetes 中的 cgroup levelkubelet 启动后会在 root cgroup 下面创建一个叫做 kubepods 子 cgroup，kubelet 会把本机的 allocatable 资源写入到 kubepods 下对应的 cgroup 文件中，比如 kubepods/cpu.share，而这个 cgroup 下面也会存放节点上面所有 pod 的 cgroup，以此来达到限制节点上所有 pod 资源的目的。在 kubepods cgroup 下面，kubernetes 会进一步再分别创建两个 QoS level cgroup，名字分别叫做 burstable 和 besteffort，这两个 QoS level 的 cgroup 是作为各自 QoS 级别的所有 Pod 的父 cgroup 来存在的，在为 pod 创建 cgroup 时，首先在对应的 Qos cgroup 下创建 pod level cgroup，然后在 pod level cgroup 继续创建对应的 container level cgroup，对于 Guaranteed Qos 对应的 pod 会直接在 kubepods 同级的 cgroup 中创建 pod cgroup。 目前 kubernetes 仅支持 cpu、memory、pids 、hugetlb 四个 cgroup 子系统。 当 kubernetes 在收到一个 pod 的资源申请信息后通过 kubelet 为 pod 分配资源，kubelet 基于 pod 申请的资源以及 pod 对应的 QoS 级别来通过 cgroup 机制最终为这个 pod 分配资源的，针对每一种资源，它会做以下几件事情： 首先判断 pod 属于哪种 Qos，在对应的 Qos level cgroup 下对 pod 中的每一个容器在 cgroup 所有子系统下都创建一个 pod level cgroup 以及 container level cgroup，并且 pod level cgroup 是 container level cgroup 的父 cgroup，Qos level cgroup 在 kubelet 初始化时已经创建完成了； 然后根据 pod 的资源信息更新 QoS level cgroup 中的值； 最后会更新 kubepods level cgroup 中的值； 对于每一个 pod 设定的 requests 和 limits，kubernetes 都会转换为 cgroup 中的计算方式，CPU 的转换方式如下所示： cpu.shares = (cpu in millicores * 1024) / 1000 cpu.cfs_period_us = 100000 (i.e. 100ms) cpu.cfs_quota_us = quota = (cpu in millicores * 100000) / 1000 memory.limit_in_bytes CPU 最终都会转换为以微秒为单位，memory 会转换为以 bytes 为单位。 以下是 kubernetes 中的 cgroup level 的一个示例，此处仅展示 cpu、memory 对应的子 cgroup： 12345678910111213141516171819202122232425262728293031323334.|-- blkio|-- cpu -&gt; cpu,cpuacct|-- cpu,cpuacct| |-- init.scope| |-- kubepods| | |-- besteffort| | |-- burstable| | `-- podd15c4b83-c250-4f1e-94ff-8a4bf31c6f25| |-- system.slice| `-- user.slice|-- cpuacct -&gt; cpu,cpuacct|-- cpuset| |-- kubepods| | |-- besteffort| | |-- burstable| | `-- podd15c4b83-c250-4f1e-94ff-8a4bf31c6f25|-- devices|-- hugetlb|-- memory| |-- init.scope| |-- kubepods| | |-- besteffort| | |-- burstable| | `-- podd15c4b83-c250-4f1e-94ff-8a4bf31c6f25| |-- system.slice| | |-- -.mount| `-- user.slice|-- net_cls -&gt; net_cls,net_prio|-- net_cls,net_prio|-- net_prio -&gt; net_cls,net_prio|-- perf_event|-- pids`-- systemd 例如，当创建资源如下所示的 pod： 123456789101112spec: containers: - name: nginx image: nginx:latest imagePullPolicy: IfNotPresent resources: requests: cpu: 250m memory: 1Gi limits: cpu: 500m memory: 2Gi 首先会根据 pod 的 Qos 该 pod 为 burstable 在其所属 Qos 下创建 ROOT/kubepods/burstable/pod&lt;UID&gt;/container&lt;UID&gt; 两个 cgroup level，然后会更新 pod 的父 cgroup 也就是 burstable/ cgroup 中的值，最后会更新 kubepods cgroup 中的值，下面会针对每个 cgroup level 一一进行解释。 Container level cgroups在 Container level cgroups 中，kubelet 会根据上述公式将 pod 中每个 container 的资源转换为 cgroup 中的值并写入到对应的文件中。 123/sys/fs/cgroup/cpu/kubepods/burstable/pod&lt;UID&gt;/container&lt;UID&gt;/cpu.shares = 256/sys/fs/cgroup/cpu/kubepods/burstable/pod&lt;UID&gt;/container&lt;UID&gt;/cpu.cfs_quota_us = 50000/sys/fs/cgroup/memory/kubepods/burstable/pod&lt;UID&gt;/container&lt;UID&gt;/memory.limit_in_bytes = 104857600 Pod level cgroups在创建完 container level 的 cgroup 之后，kubelet 会为同属于某个 pod 的 containers 创建一个 pod level cgroup。为何要引入 pod level cgroup，主要是基于以下几点原因： 方便对 pod 内的容器资源进行统一的限制； 方便对 pod 使用的资源进行统一统计； 对于不同 Pod level cgroups 的设置方法如下所示： Guaranteed Pod QoS1234pod&lt;UID&gt;/cpu.shares = sum(pod.spec.containers.resources.requests[cpu])pod&lt;UID&gt;/cpu.cfs_period_us = 100000pod&lt;UID&gt;/cpu.cfs_quota_us = sum(pod.spec.containers.resources.limits[cpu])pod&lt;UID&gt;/memory.limit_in_bytes = sum(pod.spec.containers.resources.limits[memory]) Burstable Pod QoS1234pod&lt;UID&gt;/cpu.shares = sum(pod.spec.containers.resources.requests[cpu])pod&lt;UID&gt;/cpu.cfs_period_us = 100000pod&lt;UID&gt;/cpu.cfs_quota_us = sum(pod.spec.containers.resources.limits[cpu])pod&lt;UID&gt;/memory.limit_in_bytes = sum(pod.spec.containers.resources.limits[memory]) BestEffort Pod QoS12pod&lt;UID&gt;/cpu.shares = 2pod&lt;UID&gt;/cpu.cfs_quota_us = -1 cpu.shares 指定了 cpu 可以使用的下限，cpu 的上限通过使用 cpu.cfs_period_us + cpu.cfs_quota_us 两个参数做动态绝对配额，两个参数的意义如下所示： cpu.cfs_period_us：指 cpu 使用时间的周期统计； cpu.cfs_quota_us：指周期内允许占用的 cpu 时间(指单核的时间, 多核则需要在设置时累加) ； container runtime 中 cpu.cfs_period_us 的值默认为 100000。若 kubelet 启用了 --cpu-manager-policy=static 时，对于 Guaranteed Qos，如果它的 request 是一个整数的话，cgroup 会同时设置 cpuset.cpus 和 cpuset.mems 两个参数以此来对它进行绑核。 如果 pod 指定了 requests 和 limits，kubelet 会按以上的计算方式为 pod 设置资源限制，如果没有指定 limit 的话，那么 cpu.cfs_quota_us 将会被设置为 -1，即没有限制。而如果 limit 和 request 都没有指定的话，cpu.shares 将会被指定为 2，这个是 cpu.shares 允许指定的最小数值了，可见针对这种 pod，kubernetes 只会给它分配最少的 cpu 资源。而对于内存来说，如果没有 limit 的指定的话，memory.limit_in_bytes 将会被指定为一个非常大的值，一般是 2^64 ，可见含义就是不对内存做出限制。 针对上面的例子，其 pod level cgroups 中的配置如下所示： 12pod&lt;UID&gt;/cpu.shares = 102pod&lt;UID&gt;/cpu.cfs_quota_us = 20000 QoS level cgroups上文已经提到了 kubelet 会首先创建 kubepods cgroup，然后会在 kubepods cgroup 下面再分别创建 burstable 和 besteffort 两个 QoS level cgroup，那么这两个 QoS level cgroup 存在的目的是什么？为什么不为 guaranteed Qos 创建 cgroup level？ 首先看一下三种 QoS level cgroups 的设置方法，对于 guaranteed Qos 因其直接使用 root cgroup，此处只看另外两种的计算方式： Burstable cgroup： 123ROOT/burstable/cpu.shares = max(sum(Burstable pods cpu requests）, 2)ROOT/burstable/memory.limit_in_bytes = Node.Allocatable - &#123;(summation of memory requests of `Guaranteed` pods)*(reservePercent / 100)&#125; BestEffort cgroup： 123ROOT/besteffort/cpu.shares = 2ROOT/besteffort/memory.limit_in_bytes = Node.Allocatable - &#123;(summation of memory requests of all `Guaranteed` and `Burstable` pods)*(reservePercent / 100)&#125; 首先第一个问题，所有 guaranteed 级别的 pod 的 cgroup 直接位于 kubepods 这个 cgroup 之下，和 burstable、besteffort QoS level cgroup 同级，主要原因在于 guaranteed 级别的 pod 有明确的资源申请量(request)和资源限制量(limit)，所以并不需要一个统一的 QoS level 的 cgroup 进行管理或限制。 针对 burstable 和 besteffort 这两种类型的 pod，在默认情况下，kubernetes 则是希望能尽可能地提升资源利用率，所以并不会对这两种 QoS 的 pod 的资源使用做限制。但在某些场景下我们还是希望能够尽可能保证 guaranteed level pod 这种高 QoS 级别 pod 的资源，尤其是不可压缩资源（如内存），不要被低 QoS 级别的 pod 抢占，导致高 QoS 级别的 pod 连它 request 的资源量的资源都无法得到满足，此时就可以使用 --qos-reserved 为高 Qos pod 进行预留资源，举个例子，当前机器的 allocatable 内存资源量为 8G，当为这台机器的 kubelet 开启 --qos-reserved 参数后，并且设置为 memory=100%，如果此时创建了一个内存 request 为 1G 的 guaranteed level 的 pod，那么需要预留的资源就是 1G，此时这台机器上面的 burstable QoS level cgroup 的 memory.limit_in_bytes 的值将会被设置为 7G，besteffort QoS level cgroup 的 memory.limit_in_bytes 的值也会被设置为 7G。而如果此时又创建了一个 burstable level 的 pod，它的内存申请量为 2G，那么此时需要预留的资源为 3G，而 besteffort QoS level cgroup 的 memory.limit_in_bytes 的值也会被调整为 5G。 由上面的公式也可以看到，burstable 的 cgroup 需要为比他等级高的 guaranteed 级别的 pod 的内存资源做预留，而 besteffort 需要为 burstable 和 guaranteed 都要预留内存资源。 小结kubelet 启动时首先会创建 root cgroups 以及为 Qos 创建对应的 level cgroups，然后当 pod 调度到节点上时，kubelet 也会为 pod 以及 pod 下的 container 创建对应的 level cgroups。root cgroups 限制节点上所有 pod 的资源使用量，Qos level cgroups 限制不同 Qos 下 pod 的资源使用量，Pod level cgroups 限制一个 pod 下的资源使用量，Container level cgroups 限制 pod 下 container 的资源使用量。 节点上 cgroup 层级树如下所示： 12345678910111213141516171819202122232425262728293031$ROOT | +- Pod1 | | | +- Container1 | +- Container2 | ... +- Pod2 | +- Container3 | ... +- ... | +- burstable | | | +- Pod3 | | | | | +- Container4 | | ... | +- Pod4 | | +- Container5 | | ... | +- ... | +- besteffort | | | +- Pod5 | | | | | +- Container6 | | +- Container7 | | ... | +- ... QOSContainerManager 源码分析 kubernetes 版本：v1.16 qos 的具体实现是在 kubelet 中的 QOSContainerManager，QOSContainerManager 被包含在 containerManager 模块中，kubelet 的 containerManager 模块中包含多个模块还有，cgroupManager、containerManager、nodeContainerManager、podContainerManager、topologyManager、deviceManager、cpuManager 等。 qosContainerManager 的初始化首先看 QOSContainerManager 的初始化，因为 QOSContainerManager 包含在 containerManager 中，在初始化 containerManager 时也会初始化 QOSContainerManager。 k8s.io/kubernetes/cmd/kubelet/app/server.go:471 12345func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123; ...... kubeDeps.ContainerManager, err = cm.NewContainerManager(......) ......&#125; k8s.io/kubernetes/pkg/kubelet/cm/container_manager_linux.go:200 123456789// 在 NewContainerManager 中会初始化 qosContainerManagerfunc NewContainerManager(......) (ContainerManager, error) &#123; ...... qosContainerManager, err := NewQOSContainerManager(subsystems, cgroupRoot, nodeConfig, cgroupManager) if err != nil &#123; return nil, err &#125; ......&#125; qosContainerManager 的启动在调用 kl.containerManager.Start 启动 containerManager 时也会启动 qosContainerManager，代码如下所示： k8s.io/kubernetes/pkg/kubelet/kubelet.go:1361 1234567func (kl *Kubelet) initializeRuntimeDependentModules() &#123; ...... if err := kl.containerManager.Start(node, kl.GetActivePods, kl.sourcesReady, kl.statusManager, kl.runtimeService); err != nil &#123; klog.Fatalf(&quot;Failed to start ContainerManager %v&quot;, err) &#125; ......&#125; cm.setupNodecm.setupNode 是启动 qosContainerManager 的方法，其主要逻辑为： 1、检查 kubelet 依赖的内核参数是否配置正确； 2、若 CgroupsPerQOS 为 true，首先调用 cm.createNodeAllocatableCgroups 创建 root cgroup，然后调用 cm.qosContainerManager.Start 启动 qosContainerManager； 3、调用 cm.enforceNodeAllocatableCgroups 计算 node 的 allocatable 资源并配置到 root cgroup 中，然后判断是否启用了 SystemReserved 以及 KubeReserved 并配置对应的 cgroup； 4、为系统组件配置对应的 cgroup 资源限制； 5、为系统进程配置 oom_score_adj； k8s.io/kubernetes/pkg/kubelet/cm/container_manager_linux.go:568 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103func (cm *containerManagerImpl) Start(......) &#123; ...... if err := cm.setupNode(activePods); err != nil &#123; return err &#125;&#125;// 在 setupNode 中会启动 qosContainerManagerfunc (cm *containerManagerImpl) setupNode(activePods ActivePodsFunc) error &#123; f, err := validateSystemRequirements(cm.mountUtil) if err != nil &#123; return err &#125; if !f.cpuHardcapping &#123; cm.status.SoftRequirements = fmt.Errorf(&quot;CPU hardcapping unsupported&quot;) &#125; b := KernelTunableModify if cm.GetNodeConfig().ProtectKernelDefaults &#123; b = KernelTunableError &#125; // 1、检查依赖的内核参数是否配置正确 if err := setupKernelTunables(b); err != nil &#123; return err &#125; if cm.NodeConfig.CgroupsPerQOS &#123; // 2、创建 root cgroup，即 kubepods dir if err := cm.createNodeAllocatableCgroups(); err != nil &#123; return err &#125; // 3、启动 qosContainerManager err = cm.qosContainerManager.Start(cm.getNodeAllocatableAbsolute, activePods) if err != nil &#123; return fmt.Errorf(&quot;failed to initialize top level QOS containers: %v&quot;, err) &#125; &#125; // 4、为 node 配置 cgroup 资源限制 if err := cm.enforceNodeAllocatableCgroups(); err != nil &#123; return err &#125; if cm.ContainerRuntime == &quot;docker&quot; &#123; cm.periodicTasks = append(cm.periodicTasks, func() &#123; cont, err := getContainerNameForProcess(dockerProcessName, dockerPidFile) if err != nil &#123; klog.Error(err) return &#125; cm.Lock() defer cm.Unlock() cm.RuntimeCgroupsName = cont &#125;) &#125; // 5、为系统组件配置对应的 cgroup 资源限制 if cm.SystemCgroupsName != &quot;&quot; &#123; if cm.SystemCgroupsName == &quot;/&quot; &#123; return fmt.Errorf(&quot;system container cannot be root (\&quot;/\&quot;)&quot;) &#125; cont := newSystemCgroups(cm.SystemCgroupsName) cont.ensureStateFunc = func(manager *fs.Manager) error &#123; return ensureSystemCgroups(&quot;/&quot;, manager) &#125; systemContainers = append(systemContainers, cont) &#125; // 6、为系统进程配置 oom_score_adj if cm.KubeletCgroupsName != &quot;&quot; &#123; cont := newSystemCgroups(cm.KubeletCgroupsName) allowAllDevices := true manager := fs.Manager&#123; Cgroups: &amp;configs.Cgroup&#123; Parent: &quot;/&quot;, Name: cm.KubeletCgroupsName, Resources: &amp;configs.Resources&#123; AllowAllDevices: &amp;allowAllDevices, &#125;, &#125;, &#125; cont.ensureStateFunc = func(_ *fs.Manager) error &#123; return ensureProcessInContainerWithOOMScore(os.Getpid(), qos.KubeletOOMScoreAdj, &amp;manager) &#125; systemContainers = append(systemContainers, cont) &#125; else &#123; cm.periodicTasks = append(cm.periodicTasks, func() &#123; if err := ensureProcessInContainerWithOOMScore(os.Getpid(), qos.KubeletOOMScoreAdj, nil); err != nil &#123; klog.Error(err) return &#125; cont, err := getContainer(os.Getpid()) if err != nil &#123; klog.Errorf(&quot;failed to find cgroups of kubelet - %v&quot;, err) return &#125; cm.Lock() defer cm.Unlock() cm.KubeletCgroupsName = cont &#125;) &#125; cm.systemContainers = systemContainers return nil&#125; cm.qosContainerManager.Startcm.qosContainerManager.Start 主要逻辑为： 1、检查 root cgroup 是否存在，root cgroup 会在启动 qosContainerManager 之前创建； 2、为 Burstable 和 BestEffort 创建 Qos level cgroups 并设置默认值； 3、调用 m.UpdateCgroups 每分钟定期更新 cgroup 信息； k8s.io/kubernetes/pkg/kubelet/cm/qos_container_manager_linux.go:80 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455func (m *qosContainerManagerImpl) Start(getNodeAllocatable func() v1.ResourceList, activePods ActivePodsFunc) error &#123; cm := m.cgroupManager rootContainer := m.cgroupRoot // 1、检查 root cgroup 是否存在 if !cm.Exists(rootContainer) &#123; return fmt.Errorf(&quot;root container %v doesn&apos;t exist&quot;, rootContainer) &#125; // 2、为 Qos 配置 Top level cgroups qosClasses := map[v1.PodQOSClass]CgroupName&#123; v1.PodQOSBurstable: NewCgroupName(rootContainer, strings.ToLower(string(v1.PodQOSBurstable))), v1.PodQOSBestEffort: NewCgroupName(rootContainer, strings.ToLower(string(v1.PodQOSBestEffort))), &#125; // 3、为 Qos 创建 top level cgroups for qosClass, containerName := range qosClasses &#123; resourceParameters := &amp;ResourceConfig&#123;&#125; // 4、为 BestEffort QoS cpu.shares 设置默认值，默认为 2 if qosClass == v1.PodQOSBestEffort &#123; minShares := uint64(MinShares) resourceParameters.CpuShares = &amp;minShares &#125; containerConfig := &amp;CgroupConfig&#123; Name: containerName, ResourceParameters: resourceParameters, &#125; // 5、配置 huge page size m.setHugePagesUnbounded(containerConfig) // 6、为 Qos 创建 cgroup 目录 if !cm.Exists(containerName) &#123; if err := cm.Create(containerConfig); err != nil &#123; ...... &#125; &#125; else &#123; if err := cm.Update(containerConfig); err != nil &#123; ...... &#125; &#125; &#125; ...... // 7、每分钟定期更新 cgroup 配置 go wait.Until(func() &#123; err := m.UpdateCgroups() if err != nil &#123; klog.Warningf(&quot;[ContainerManager] Failed to reserve QoS requests: %v&quot;, err) &#125; &#125;, periodicQOSCgroupUpdateInterval, wait.NeverStop) return nil&#125; m.UpdateCgroupsm.UpdateCgroups 是用来更新 Qos level cgroup 中的值，其主要逻辑为： 1、调用 m.setCPUCgroupConfig 计算 node 上的 activePods 的资源以此来更新 bestEffort 和 burstable Qos level cgroup 的 cpu.shares 值，besteffort 的 cpu.shares 值默认为 2，burstable cpu.shares 的计算方式为：max(sum(Burstable pods cpu requests）* 1024 /1000, 2)； 2、调用m.setHugePagesConfig 更新 huge pages； 3、检查是否启用了--qos-reserved 参数，若启用了则调用 m.setMemoryReserve 计算每个 Qos class 中需要设定的值然后调用 m.cgroupManager.Update 更新 cgroup 中的值； 4、最后调用 m.cgroupManager.Update 更新 cgroup 中的值； k8s.io/kubernetes/pkg/kubelet/cm/qos_container_manager_linux.go:269 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364func (m *qosContainerManagerImpl) UpdateCgroups() error &#123; m.Lock() defer m.Unlock() qosConfigs := map[v1.PodQOSClass]*CgroupConfig&#123; v1.PodQOSBurstable: &#123; Name: m.qosContainersInfo.Burstable, ResourceParameters: &amp;ResourceConfig&#123;&#125;, &#125;, v1.PodQOSBestEffort: &#123; Name: m.qosContainersInfo.BestEffort, ResourceParameters: &amp;ResourceConfig&#123;&#125;, &#125;, &#125; // 1、更新 bestEffort 和 burstable Qos level cgroup 的 cpu.shares 值 if err := m.setCPUCgroupConfig(qosConfigs); err != nil &#123; return err &#125; // 2、调用 m.setHugePagesConfig 更新 huge pages if err := m.setHugePagesConfig(qosConfigs); err != nil &#123; return err &#125; // 3、设置资源预留 if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.QOSReserved) &#123; for resource, percentReserve := range m.qosReserved &#123; switch resource &#123; case v1.ResourceMemory: m.setMemoryReserve(qosConfigs, percentReserve) &#125; &#125; updateSuccess := true for _, config := range qosConfigs &#123; err := m.cgroupManager.Update(config) if err != nil &#123; updateSuccess = false &#125; &#125; if updateSuccess &#123; klog.V(4).Infof(&quot;[ContainerManager]: Updated QoS cgroup configuration&quot;) return nil &#125; for resource, percentReserve := range m.qosReserved &#123; switch resource &#123; case v1.ResourceMemory: m.retrySetMemoryReserve(qosConfigs, percentReserve) &#125; &#125; &#125; // 4、更新 cgroup 中的值 for _, config := range qosConfigs &#123; err := m.cgroupManager.Update(config) if err != nil &#123; return err &#125; &#125; return nil&#125; m.cgroupManager.Updatem.cgroupManager.Update 方法主要是根据 cgroup 配置来更新 cgroup 中的值，其主要逻辑为： 1、调用 m.buildCgroupPaths 创建对应的 cgroup 目录，在每个 cgroup 子系统下面都有一个 kubelet 对应的 root cgroup 目录； 2、调用 setSupportedSubsystems 更新的 cgroup 子系统中的值； k8s.io/kubernetes/pkg/kubelet/cm/cgroup_manager_linux.go:409 123456789101112131415161718192021222324252627func (m *cgroupManagerImpl) Update(cgroupConfig *CgroupConfig) error &#123; ...... resourceConfig := cgroupConfig.ResourceParameters resources := m.toResources(resourceConfig) cgroupPaths := m.buildCgroupPaths(cgroupConfig.Name) libcontainerCgroupConfig := &amp;libcontainerconfigs.Cgroup&#123; Resources: resources, Paths: cgroupPaths, &#125; if m.adapter.cgroupManagerType == libcontainerSystemd &#123; updateSystemdCgroupInfo(libcontainerCgroupConfig, cgroupConfig.Name) &#125; else &#123; libcontainerCgroupConfig.Path = cgroupConfig.Name.ToCgroupfs() &#125; if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.SupportPodPidsLimit) &amp;&amp; cgroupConfig.ResourceParameters != nil &amp;&amp; cgroupConfig. ResourceParameters.PidsLimit != nil &#123; libcontainerCgroupConfig.PidsLimit = *cgroupConfig.ResourceParameters.PidsLimit &#125; if err := setSupportedSubsystems(libcontainerCgroupConfig); err != nil &#123; return fmt.Errorf(&quot;failed to set supported cgroup subsystems for cgroup %v: %v&quot;, cgroupConfig.Name, err) &#125; return nil&#125; setSupportedSubsystemsetSupportedSubsystems 首先通过 getSupportedSubsystems 获取 kubelet 支持哪些 cgroup 子系统，然后调用 sys.Set 设置对应子系统的值，sys.Set 是调用 runc/libcontainer 中的包进行设置的，其主要逻辑是在 cgroup 子系统对应的文件中写入值。 k8s.io/kubernetes/pkg/kubelet/cm/cgroup_manager_linux.go:345 123456789101112131415func setSupportedSubsystems(cgroupConfig *libcontainerconfigs.Cgroup) error &#123; for sys, required := range getSupportedSubsystems() &#123; if _, ok := cgroupConfig.Paths[sys.Name()]; !ok &#123; if required &#123; return fmt.Errorf(&quot;failed to find subsystem mount for required subsystem: %v&quot;, sys.Name()) &#125; ...... continue &#125; if err := sys.Set(cgroupConfig.Paths[sys.Name()], cgroupConfig); err != nil &#123; return fmt.Errorf(&quot;failed to set config for supported subsystems : %v&quot;, err) &#125; &#125; return nil&#125; 例如为 cgroup 中 cpu 子系统设置值的方法如下所示： 123456789101112131415161718func (s *CpuGroup) Set(path string, cgroup *configs.Cgroup) error &#123; if cgroup.Resources.CpuShares != 0 &#123; if err := writeFile(path, &quot;cpu.shares&quot;, strconv.FormatUint(cgroup.Resources.CpuShares, 10)); err != nil &#123; return err &#125; &#125; if cgroup.Resources.CpuPeriod != 0 &#123; if err := writeFile(path, &quot;cpu.cfs_period_us&quot;, strconv.FormatUint(cgroup.Resources.CpuPeriod, 10)); err != nil &#123; return err &#125; &#125; if cgroup.Resources.CpuQuota != 0 &#123; if err := writeFile(path, &quot;cpu.cfs_quota_us&quot;, strconv.FormatInt(cgroup.Resources.CpuQuota, 10)); err != nil &#123; return err &#125; &#125; return s.SetRtSched(path, cgroup)&#125; Pod Level CgroupPod Level cgroup 是 kubelet 在创建 pod 时创建的，创建 pod 是在 kubelet 的 syncPod 方法中进行的，在 syncPod 方法中首先会调用 kl.containerManager.UpdateQOSCgroups 更新 Qos Level cgroup，然后调用 pcm.EnsureExists 创建 pod level cgroup。 1234567891011121314151617func (kl *Kubelet) syncPod(o syncPodOptions) error &#123; ...... if !kl.podIsTerminated(pod) &#123; ...... if !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) &#123; if !pcm.Exists(pod) &#123; if err := kl.containerManager.UpdateQOSCgroups(); err != nil &#123; ...... &#125; if err := pcm.EnsureExists(pod); err != nil &#123; ...... &#125; &#125; &#125; &#125; ......&#125; EnsureExists 的主要逻辑是检查 pod 的 cgroup 是否存在，若不存在则调用 m.cgroupManager.Create 进行创建。 k8s.io/kubernetes/pkg/kubelet/cm/pod_container_manager_linux.go:79 12345678910111213141516171819func (m *podContainerManagerImpl) EnsureExists(pod *v1.Pod) error &#123; podContainerName, _ := m.GetPodContainerName(pod) alreadyExists := m.Exists(pod) if !alreadyExists &#123; containerConfig := &amp;CgroupConfig&#123; Name: podContainerName, ResourceParameters: ResourceConfigForPod(pod, m.enforceCPULimits, m.cpuCFSQuotaPeriod), &#125; if utilfeature.DefaultFeatureGate.Enabled(kubefeatures.SupportPodPidsLimit) &amp;&amp; m.podPidsLimit &gt; 0 &#123; containerConfig.ResourceParameters.PidsLimit = &amp;m.podPidsLimit &#125; if err := m.cgroupManager.Create(containerConfig); err != nil &#123; return fmt.Errorf(&quot;failed to create container for %v : %v&quot;, podContainerName, err) &#125; &#125; ...... return nil&#125; Container Level CgroupContainer Level Cgroup 是通过 runtime 进行创建的，若使用 runc 其会调用 runc 的 InitProcess.start 方法对 cgroup 资源组进行配置与应用。 k8s.io/kubernetes/vendor/github.com/opencontainers/runc/libcontainer/process_linux.go:282 1234567891011121314func (p *initProcess) start() error &#123; ...... // 调用 p.manager.Apply 为进程配置 cgroup if err := p.manager.Apply(p.pid()); err != nil &#123; return newSystemErrorWithCause(err, &quot;applying cgroup configuration for process&quot;) &#125; if p.intelRdtManager != nil &#123; if err := p.intelRdtManager.Apply(p.pid()); err != nil &#123; return newSystemErrorWithCause(err, &quot;applying Intel RDT configuration for process&quot;) &#125; &#125; ......&#125; 总结kubernetes 中有三种 Qos，分别为 Guaranteed、Burstable、BestEffort，三种 Qos 以 node 上 allocatable 资源量为基于为 pod 进行分配，并通过多个 level cgroup 进行层层限制，对 cgroup 的配置都是通过调用 runc/libcontainer/cgroups/fs 中的方法进行资源更新的。对于 Qos level cgroup，kubelet 会根据以下事件动态更新： 1、kubelet 服务启动时； 2、在创建 pod level cgroup 之前，即创建 pod 前； 3、在删除 pod level cgroup 后； 4、定期检测是否需要为 qos level cgroup 预留资源； 参考： https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers https://zhuanlan.zhihu.com/p/38359775 https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/pod-resource-management.md https://github.com/cri-o/cri-o/issues/842 https://yq.aliyun.com/articles/737784?spm=a2c4e.11153940.0.0.577f6149mYFkTR]]></content>
      <tags>
        <tag>kubelet</tag>
        <tag>qos</tag>
        <tag>cgroup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NodeController 源码分析]]></title>
    <url>%2F2020%2F01%2F09%2Fnodelifecycle_controller%2F</url>
    <content type="text"><![CDATA[在早期的版本中 NodeController 只有一种，v1.16 版本中 NodeController 已经分为了 NodeIpamController 与 NodeLifecycleController，本文主要介绍 NodeLifecycleController。 NodeLifecycleController 的功能NodeLifecycleController 主要功能是定期监控 node 的状态并根据 node 的 condition 添加对应的 taint 标签或者直接驱逐 node 上的 pod。 taint 的作用在介绍 NodeLifecycleController 的源码前有必要先介绍一下 taint 的作用，因为 NodeLifecycleController 功能最终的结果有很大一部分都体现在 node taint 上。 taint 使用效果(Effect): PreferNoSchedule：调度器尽量避免把 pod 调度到具有该污点的节点上，如果不能避免(如其他节点资源不足)，pod 也能调度到这个污点节点上，已存在于此节点上的 pod 不会被驱逐； NoSchedule：不容忍该污点的 pod 不会被调度到该节点上，通过 kubelet 管理的 pod(static pod)不受限制，之前没有设置污点的 pod 如果已运行在此节点(有污点的节点)上，可以继续运行； NoExecute：不容忍该污点的 pod 不会被调度到该节点上，同时会将已调度到该节点上但不容忍 node 污点的 pod 驱逐掉； NodeLifecycleController 中的 feature-gates在 NodeLifecycleController 用到了多个 feature-gates，此处先进行解释下： NodeDisruptionExclusion：该特性在 v1.16 引入，Alpha 版本，默认为 false，其功能是当 node 存在 node.kubernetes.io/exclude-disruption 标签时，当 node 网络中断时其节点上的 pod 不会被驱逐掉； LegacyNodeRoleBehavior：该特性在 v1.16 中引入，Alpha 版本且默认为 true，在创建 load balancers 以及中断处理时不会忽略具有 node-role.kubernetes.io/master label 的 node，该功能在 v1.19 中将被移除； TaintBasedEvictions：该特性从 v1.13 开始为 Beta 版本，默认为 true。其功能是当 node 处于 NodeNotReady、NodeUnreachable 状态时为 node 添加对应的 taint，TaintBasedEvictions 添加的 taint effect 为 NoExecute，即会驱逐 node 上对应的 pod； TaintNodesByCondition：该特性从 v1.12 开始为 Beta 版本，默认为 true，v1.17 为 GA 版本。其功能是基于节点状态添加 taint，当节点处于 NetworkUnavailable、MemoryPressure、PIDPressure、DiskPressure 状态时会添加对应的 taint，TaintNodesByCondition 添加的 taint effect 仅为NoSchedule，即仅仅不会让新创建的 pod 调度到该 node 上； NodeLease：该特性在 v1.12 引入，v 1.14 为 Beta 版本且默认启用，v 1.17 GA，主要功能是减少 node 的心跳请求以减轻 apiserver 的负担； NodeLifecycleController 源码分析 kubernetes 版本：v1.16 startNodeLifecycleController首先还是看 NodeLifecycleController 的启动方法 startNodeLifecycleController，在 startNodeLifecycleController 中主要调用了 lifecyclecontroller.NewNodeLifecycleController 对 lifecycleController 进行初始化，在该方法中传入了组件的多个参数以及 TaintBasedEvictions 和 TaintNodesByCondition 两个 feature-gates，然后调用了 lifecycleController.Run 启动 lifecycleController，可以看到 NodeLifecycleController 主要监听 lease、pods、nodes、daemonSets 四种对象。 其中在启动时指定的几个参数默认值分别为： NodeMonitorPeriod：通过--node-monitor-period 设置，默认为 5s，表示在 NodeController 中同步NodeStatus 的周期； NodeStartupGracePeriod：--node-startup-grace-period 默认 60s，在 node 启动完成前标记节点为unhealthy 的允许无响应时间； NodeMonitorGracePeriod：通过--node-monitor-grace-period 设置，默认 40s，表示在标记某个 node为 unhealthy 前，允许 40s 内该 node 无响应； PodEvictionTimeout：通过--pod-eviction-timeout 设置，默认 5 分钟，表示在强制删除 node 上的 pod 时，容忍 pod 时间； NodeEvictionRate：通过--node-eviction-rate设置， 默认 0.1，表示当集群下某个 zone 为 unhealthy 时，每秒应该剔除的 node 数量，默认即每 10s 剔除1个 node； SecondaryNodeEvictionRate：通过 --secondary-node-eviction-rate设置，默认为 0.01，表示如果某个 zone 下的 unhealthy 节点的百分比超过 --unhealthy-zone-threshold （默认为 0.55）时，驱逐速率将会减小，如果集群较小（小于等于 --large-cluster-size-threshold 个 节点 - 默认为 50），驱逐操作将会停止，否则驱逐速率将降为每秒 --secondary-node-eviction-rate 个（默认为 0.01）； LargeClusterSizeThreshold：通过--large-cluster-size-threshold 设置，默认为 50，当该 zone 的节点超过该阈值时，则认为该 zone 是一个大集群； UnhealthyZoneThreshold：通过--unhealthy-zone-threshold 设置，默认为 0.55，不健康 zone 阈值，会影响什么时候开启二级驱赶速率，即当该 zone 中节点宕机数目超过 55%，认为该 zone 不健康； EnableTaintManager：--enable-taint-manager 默认为 true，Beta feature，如果为 true，则表示NodeController 将会启动 TaintManager，当已经调度到该 node 上的 pod 不能容忍 node 的 taint 时，由 TaintManager 负责驱逐此类 pod，若不开启该特性则已调度到该 node 上的 pod 会继续存在； TaintBasedEvictions ：默认为 true； TaintNodesByCondition ：默认为 true； k8s.io/kubernetes/cmd/kube-controller-manager/app/core.go:163 12345678910111213141516171819202122232425func startNodeLifecycleController(ctx ControllerContext) (http.Handler, bool, error) &#123; lifecycleController, err := lifecyclecontroller.NewNodeLifecycleController( ctx.InformerFactory.Coordination().V1beta1().Leases(), ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Core().V1().Nodes(), ctx.InformerFactory.Apps().V1().DaemonSets(), ctx.ClientBuilder.ClientOrDie(&quot;node-controller&quot;), ctx.ComponentConfig.KubeCloudShared.NodeMonitorPeriod.Duration, ctx.ComponentConfig.NodeLifecycleController.NodeStartupGracePeriod.Duration, ctx.ComponentConfig.NodeLifecycleController.NodeMonitorGracePeriod.Duration, ctx.ComponentConfig.NodeLifecycleController.PodEvictionTimeout.Duration, ctx.ComponentConfig.NodeLifecycleController.NodeEvictionRate, ctx.ComponentConfig.NodeLifecycleController.SecondaryNodeEvictionRate, ctx.ComponentConfig.NodeLifecycleController.LargeClusterSizeThreshold, ctx.ComponentConfig.NodeLifecycleController.UnhealthyZoneThreshold, ctx.ComponentConfig.NodeLifecycleController.EnableTaintManager, utilfeature.DefaultFeatureGate.Enabled(features.TaintBasedEvictions), utilfeature.DefaultFeatureGate.Enabled(features.TaintNodesByCondition), ) if err != nil &#123; return nil, true, err &#125; go lifecycleController.Run(ctx.Stop) return nil, true, nil&#125; NewNodeLifecycleController首先有必要说明一下 NodeLifecycleController 对象中部分字段的意义，其结构体如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859type Controller struct &#123; taintManager *scheduler.NoExecuteTaintManager podInformerSynced cache.InformerSynced kubeClient clientset.Interface now func() metav1.Time // 计算 zone 下 node 驱逐速率 enterPartialDisruptionFunc func(nodeNum int) float32 enterFullDisruptionFunc func(nodeNum int) float32 // 计算 zone 状态 computeZoneStateFunc func(nodeConditions []*v1.NodeCondition) (int, ZoneState) // 用来记录NodeController observed节点的集合 knownNodeSet map[string]*v1.Node // 记录 node 最近一次状态的集合 nodeHealthMap map[string]*nodeHealthData evictorLock sync.Mutex // 需要驱逐节点上 pod 的 node 队列 zonePodEvictor map[string]*scheduler.RateLimitedTimedQueue // 需要打 taint 标签的 node 队列 zoneNoExecuteTainter map[string]*scheduler.RateLimitedTimedQueue // 将 node 划分为不同的 zone zoneStates map[string]ZoneState daemonSetStore appsv1listers.DaemonSetLister daemonSetInformerSynced cache.InformerSynced leaseLister coordlisters.LeaseLister leaseInformerSynced cache.InformerSynced nodeLister corelisters.NodeLister nodeInformerSynced cache.InformerSynced getPodsAssignedToNode func(nodeName string) ([]v1.Pod, error) recorder record.EventRecorder // kube-controller-manager 启动时指定的几个参数 nodeMonitorPeriod time.Duration nodeStartupGracePeriod time.Duration nodeMonitorGracePeriod time.Duration podEvictionTimeout time.Duration evictionLimiterQPS float32 secondaryEvictionLimiterQPS float32 largeClusterThreshold int32 unhealthyZoneThreshold float32 // 启动时默认开启的几个 feature-gates runTaintManager bool useTaintBasedEvictions bool taintNodeByCondition bool nodeUpdateQueue workqueue.Interface&#125; NewNodeLifecycleController 的主要逻辑为： 1、初始化 controller 对象； 2、为 podInformer 注册与 taintManager 相关的 EventHandler； 3、若启用 TaintManager 则为 nodeInformer 注册与 taintManager 相关的 EventHandler； 4、为 NodeLifecycleController 注册 nodeInformer； 5、检查是否启用了 NodeLease feature-gates； 6、daemonSet 默认不会注册对应的 EventHandler，此处仅仅是同步该对象； 由以上逻辑可以看出，taintManager 以及 NodeLifecycleController 都会 watch node 的变化并进行不同的处理。 k8s.io/kubernetes/pkg/controller/nodelifecycle/node_lifecycle_controller.go:268 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134func NewNodeLifecycleController(......) (*Controller, error) &#123; ...... // 1、初始化 controller 对象 nc := &amp;Controller&#123; ...... &#125; ...... // 2、注册计算 node 驱逐速率以及 zone 状态的方法 nc.enterPartialDisruptionFunc = nc.ReducedQPSFunc nc.enterFullDisruptionFunc = nc.HealthyQPSFunc nc.computeZoneStateFunc = nc.ComputeZoneState // 3、为 podInformer 注册 EventHandler，监听到的对象会被放到 nc.taintManager.PodUpdated 中 podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; pod := obj.(*v1.Pod) if nc.taintManager != nil &#123; nc.taintManager.PodUpdated(nil, pod) &#125; &#125;, UpdateFunc: func(prev, obj interface&#123;&#125;) &#123; prevPod := prev.(*v1.Pod) newPod := obj.(*v1.Pod) if nc.taintManager != nil &#123; nc.taintManager.PodUpdated(prevPod, newPod) &#125; &#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123; pod, isPod := obj.(*v1.Pod) if !isPod &#123; deletedState, ok := obj.(cache.DeletedFinalStateUnknown) if !ok &#123; return &#125; pod, ok = deletedState.Obj.(*v1.Pod) if !ok &#123; return &#125; &#125; if nc.taintManager != nil &#123; nc.taintManager.PodUpdated(pod, nil) &#125; &#125;, &#125;) nc.podInformerSynced = podInformer.Informer().HasSynced podInformer.Informer().AddIndexers(cache.Indexers&#123; nodeNameKeyIndex: func(obj interface&#123;&#125;) ([]string, error) &#123; pod, ok := obj.(*v1.Pod) if !ok &#123; return []string&#123;&#125;, nil &#125; if len(pod.Spec.NodeName) == 0 &#123; return []string&#123;&#125;, nil &#125; return []string&#123;pod.Spec.NodeName&#125;, nil &#125;, &#125;) podIndexer := podInformer.Informer().GetIndexer() nc.getPodsAssignedToNode = func(nodeName string) ([]v1.Pod, error) &#123; objs, err := podIndexer.ByIndex(nodeNameKeyIndex, nodeName) if err != nil &#123; return nil, err &#125; pods := make([]v1.Pod, 0, len(objs)) for _, obj := range objs &#123; pod, ok := obj.(*v1.Pod) if !ok &#123; continue &#125; pods = append(pods, *pod) &#125; return pods, nil &#125; // 4、初始化 TaintManager，为 nodeInformer 注册 EventHandler // 监听到的对象会被放到 nc.taintManager.NodeUpdated 中 if nc.runTaintManager &#123; podLister := podInformer.Lister() podGetter := func(name, namespace string) (*v1.Pod, error) &#123; return podLister.Pods(namespace).Get(name) &#125; nodeLister := nodeInformer.Lister() nodeGetter := func(name string) (*v1.Node, error) &#123; return nodeLister.Get(name) &#125; // 5、初始化 taintManager nc.taintManager = scheduler.NewNoExecuteTaintManager(kubeClient, podGetter, nodeGetter, nc.getPodsAssignedToNode) nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: nodeutil.CreateAddNodeHandler(func(node *v1.Node) error &#123; nc.taintManager.NodeUpdated(nil, node) return nil &#125;), UpdateFunc: nodeutil.CreateUpdateNodeHandler(func(oldNode, newNode *v1.Node) error &#123; nc.taintManager.NodeUpdated(oldNode, newNode) return nil &#125;), DeleteFunc: nodeutil.CreateDeleteNodeHandler(func(node *v1.Node) error &#123; nc.taintManager.NodeUpdated(node, nil) return nil &#125;), &#125;) &#125; // 6、为 NodeLifecycleController 注册 nodeInformer nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: nodeutil.CreateAddNodeHandler(func(node *v1.Node) error &#123; nc.nodeUpdateQueue.Add(node.Name) return nil &#125;), UpdateFunc: nodeutil.CreateUpdateNodeHandler(func(_, newNode *v1.Node) error &#123; nc.nodeUpdateQueue.Add(newNode.Name) return nil &#125;), &#125;) ...... // 7、检查是否启用了 NodeLease feature-gates nc.leaseLister = leaseInformer.Lister() if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123; nc.leaseInformerSynced = leaseInformer.Informer().HasSynced &#125; else &#123; nc.leaseInformerSynced = func() bool &#123; return true &#125; &#125; nc.nodeLister = nodeInformer.Lister() nc.nodeInformerSynced = nodeInformer.Informer().HasSynced nc.daemonSetStore = daemonSetInformer.Lister() nc.daemonSetInformerSynced = daemonSetInformer.Informer().HasSynced return nc, nil&#125; RunRun 方法是 NodeLifecycleController 的启动方法，其中会启动多个 goroutine 完成 controller 的功能，主要逻辑为： 1、等待四种对象 Informer 中的 cache 同步完成； 2、若指定要运行 taintManager 则调用 nc.taintManager.Run 启动 taintManager； 3、启动多个 goroutine 调用 nc.doNodeProcessingPassWorker 处理 nc.nodeUpdateQueue 队列中的 node； 4、若启用了 TaintBasedEvictions 特性则启动一个 goroutine 调用 nc.doNoExecuteTaintingPass 处理 nc.zoneNoExecuteTainter 队列中的 node，否则调用 nc.doEvictionPass 处理 nc.zonePodEvictor 队列中的 node； 5、启动一个 goroutine 调用 nc.monitorNodeHealth 定期监控 node 的状态； k8s.io/kubernetes/pkg/controller/nodelifecycle/node_lifecycle_controller.go:455 123456789101112131415161718192021222324252627282930313233343536func (nc *Controller) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() defer klog.Infof(&quot;Shutting down node controller&quot;) if !cache.WaitForNamedCacheSync(&quot;taint&quot;, stopCh, nc.leaseInformerSynced, nc.nodeInformerSynced, nc.podInformerSynced, nc.daemonSetInformerSynced) &#123; return &#125; // 1、启动 taintManager if nc.runTaintManager &#123; go nc.taintManager.Run(stopCh) &#125; defer nc.nodeUpdateQueue.ShutDown() // 2、执行 nc.doNodeProcessingPassWorker for i := 0; i &lt; scheduler.UpdateWorkerSize; i++ &#123; go wait.Until(nc.doNodeProcessingPassWorker, time.Second, stopCh) &#125; // 3、根据是否启用 TaintBasedEvictions 执行不同的处理逻辑 if nc.useTaintBasedEvictions &#123; go wait.Until(nc.doNoExecuteTaintingPass, scheduler.NodeEvictionPeriod, stopCh) &#125; else &#123; go wait.Until(nc.doEvictionPass, scheduler.NodeEvictionPeriod, stopCh) &#125; // 4、执行 nc.monitorNodeHealth go wait.Until(func() &#123; if err := nc.monitorNodeHealth(); err != nil &#123; klog.Errorf(&quot;Error monitoring node health: %v&quot;, err) &#125; &#125;, nc.nodeMonitorPeriod, stopCh) &lt;-stopCh&#125; Run 方法中主要调用了 5 个方法来完成其核心功能： nc.taintManager.Run：处理 taintManager 中 nodeUpdateQueue 和 podUpdateQueue 中的 pod 以及 node，若 pod 不能容忍 node 上的 taint 则将其加入到 taintEvictionQueue 中并最终会删除； nc.doNodeProcessingPassWorker：从 NodeLifecycleController 的 nodeUpdateQueue 取出 node，（1）若启用 taintNodeByCondition 特性时根据 node condition 以及 node 是否调度为 node 添加对应的 NoSchedule taint 标签；（2）调用 nc.reconcileNodeLabels 为 node 添加默认的 label； nc.doNoExecuteTaintingPass：处理 nc.zoneNoExecuteTainter 队列中的数据，根据 node 的 NodeReadyCondition 添加或移除对应的 taint； nc.doEvictionPass：处理 nc.zonePodEvictor 队列中的 node，将 node 上的 pod 进行驱逐； nc.monitorNodeHealth：持续监控 node 的状态，当 node 处于异常状态时更新 node 的 taint 以及 node 上 pod 的状态或者直接驱逐 node 上的 pod，此外还会为集群下的所有 node 划分 zoneStates 并为每个 zoneStates 设置对应的驱逐速率； 下文会详细分析以上 5 种方法的具体实现。 nc.taintManager.Run当组件启动时设置 --enable-taint-manager 参数为 true 时(默认为 true)，该功能会启用，其主要作用是当该 node 上的 pod 不容忍 node taint 时将 pod 进行驱逐，若不开启该功能则已调度到该 node 上的 pod 会继续存在，新创建的 pod 需要容忍 node 的 taint 才会调度至该 node 上。 主要逻辑为： 1、处理 nodeUpdateQueue 中的 node 并将其发送到 nodeUpdateChannels 中； 2、处理 podUpdateQueue 中的 pod 并将其发送到 podUpdateChannels 中； 3、调用 tc.worker 处理 nodeUpdateChannels 和 podUpdateChannels 中的数据； k8s.io/kubernetes/pkg/controller/nodelifecycle/scheduler/taint_manager.go:185 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647func (tc *NoExecuteTaintManager) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; for i := 0; i &lt; UpdateWorkerSize; i++ &#123; tc.nodeUpdateChannels = append(tc.nodeUpdateChannels, make(chan nodeUpdateItem, NodeUpdateChannelSize)) tc.podUpdateChannels = append(tc.podUpdateChannels, make(chan podUpdateItem, podUpdateChannelSize)) &#125; go func(stopCh &lt;-chan struct&#123;&#125;) &#123; for &#123; item, shutdown := tc.nodeUpdateQueue.Get() if shutdown &#123; break &#125; nodeUpdate := item.(nodeUpdateItem) hash := hash(nodeUpdate.nodeName, UpdateWorkerSize) select &#123; case &lt;-stopCh: tc.nodeUpdateQueue.Done(item) return case tc.nodeUpdateChannels[hash] &lt;- nodeUpdate: &#125; &#125; &#125;(stopCh) go func(stopCh &lt;-chan struct&#123;&#125;) &#123; for &#123; item, shutdown := tc.podUpdateQueue.Get() if shutdown &#123; break &#125; podUpdate := item.(podUpdateItem) hash := hash(podUpdate.nodeName, UpdateWorkerSize) select &#123; case &lt;-stopCh: tc.podUpdateQueue.Done(item) return case tc.podUpdateChannels[hash] &lt;- podUpdate: &#125; &#125; &#125;(stopCh) wg := sync.WaitGroup&#123;&#125; wg.Add(UpdateWorkerSize) for i := 0; i &lt; UpdateWorkerSize; i++ &#123; go tc.worker(i, wg.Done, stopCh) &#125; wg.Wait()&#125; tc.workertc.worker 主要功能是调用 tc.handleNodeUpdate 和 tc.handlePodUpdate 处理 tc.nodeUpdateChannels 和 tc.podUpdateChannels 两个 channel 中的数据，但会优先处理 nodeUpdateChannels 中的数据。 k8s.io/kubernetes/pkg/controller/nodelifecycle/scheduler/taint_manager.go:243 12345678910111213141516171819202122232425262728func (tc *NoExecuteTaintManager) worker(worker int, done func(), stopCh &lt;-chan struct&#123;&#125;) &#123; defer done() for &#123; select &#123; case &lt;-stopCh: return case nodeUpdate := &lt;-tc.nodeUpdateChannels[worker]: tc.handleNodeUpdate(nodeUpdate) tc.nodeUpdateQueue.Done(nodeUpdate) case podUpdate := &lt;-tc.podUpdateChannels[worker]: // 优先处理 nodeUpdateChannels priority: for &#123; select &#123; case nodeUpdate := &lt;-tc.nodeUpdateChannels[worker]: tc.handleNodeUpdate(nodeUpdate) tc.nodeUpdateQueue.Done(nodeUpdate) default: break priority &#125; &#125; tc.handlePodUpdate(podUpdate) tc.podUpdateQueue.Done(podUpdate) &#125; &#125;&#125; tc.handleNodeUpdatetc.handleNodeUpdate 的主要逻辑为： 1、首先通过 nodeLister 获取 node 对象； 2、获取 node 上 effect 为 NoExecute 的 taints； 3、调用 tc.getPodsAssignedToNode 获取该 node 上的所有 pods； 4、若 node 上的 taints 为空直接返回，否则遍历每一个 pod 调用 tc.processPodOnNode 检查 pod 是否要被驱逐； k8s.io/kubernetes/pkg/controller/nodelifecycle/scheduler/taint_manager.go:417 12345678910111213141516171819202122232425262728293031323334353637383940414243func (tc *NoExecuteTaintManager) handleNodeUpdate(nodeUpdate nodeUpdateItem) &#123; node, err := tc.getNode(nodeUpdate.nodeName) if err != nil &#123; ...... &#125; // 1、获取 node 的 taints taints := getNoExecuteTaints(node.Spec.Taints) func() &#123; tc.taintedNodesLock.Lock() defer tc.taintedNodesLock.Unlock() if len(taints) == 0 &#123; delete(tc.taintedNodes, node.Name) &#125; else &#123; tc.taintedNodes[node.Name] = taints &#125; &#125;() // 2、获取 node 上的所有 pod pods, err := tc.getPodsAssignedToNode(node.Name) if err != nil &#123; klog.Errorf(err.Error()) return &#125; if len(pods) == 0 &#123; return &#125; // 3、若不存在 taints，则取消所有的驱逐操作 if len(taints) == 0 &#123; for i := range pods &#123; tc.cancelWorkWithEvent(types.NamespacedName&#123;Namespace: pods[i].Namespace, Name: pods[i].Name&#125;) &#125; return &#125; now := time.Now() for i := range pods &#123; pod := &amp;pods[i] podNamespacedName := types.NamespacedName&#123;Namespace: pod.Namespace, Name: pod.Name&#125; // 4、调用 tc.processPodOnNode 进行处理 tc.processPodOnNode(podNamespacedName, node.Name, pod.Spec.Tolerations, taints, now) &#125;&#125; tc.handlePodUpdate主要逻辑为： 1、通过 podLister 获取 pod 对象； 2、获取 pod 所在 node 的 taints； 3、调用 tc.processPodOnNode 进行处理； k8s.io/kubernetes/pkg/controller/nodelifecycle/scheduler/taint_manager.go:377 1234567891011121314151617181920212223242526272829func (tc *NoExecuteTaintManager) handlePodUpdate(podUpdate podUpdateItem) &#123; pod, err := tc.getPod(podUpdate.podName, podUpdate.podNamespace) if err != nil &#123; ...... &#125; if pod.Spec.NodeName != podUpdate.nodeName &#123; return &#125; podNamespacedName := types.NamespacedName&#123;Namespace: pod.Namespace, Name: pod.Name&#125; nodeName := pod.Spec.NodeName if nodeName == &quot;&quot; &#123; return &#125; taints, ok := func() ([]v1.Taint, bool) &#123; tc.taintedNodesLock.Lock() defer tc.taintedNodesLock.Unlock() taints, ok := tc.taintedNodes[nodeName] return taints, ok &#125;() if !ok &#123; return &#125; // 调用 tc.processPodOnNode 进行处理 tc.processPodOnNode(podNamespacedName, nodeName, pod.Spec.Tolerations, taints, time.Now())&#125; tc.processPodOnNodetc.handlePodUpdate 和 tc.handleNodeUpdate 最终都是调用 tc.processPodOnNode 检查 pod 是否容忍 node 的 taints，tc.processPodOnNode 首先检查 pod 的 tolerations 是否能匹配 node 上所有的 taints，若无法完全匹配则将 pod 加入到 taintEvictionQueue 然后被删除，若能匹配首先获取 pod tolerations 中的最小容忍时间，如果 tolerations 未设置容忍时间说明会一直容忍则直接返回，否则加入到 taintEvictionQueue 的延迟队列中，当达到最小容忍时间时 pod 会被加入到 taintEvictionQueue 中并驱逐。 通常情况下，如果给一个节点添加了一个 effect 值为 NoExecute 的 taint，则任何不能忍受这个 taint 的 pod 都会马上被驱逐，任何可以忍受这个 taint 的 pod 都不会被驱逐。但是，如果 pod 存在一个 effect 值为 NoExecute 的 toleration 指定了可选属性 tolerationSeconds 的值，则表示在给节点添加了上述 taint 之后，pod 还能继续在节点上运行的时间。例如， 123456tolerations:- key: &quot;key1&quot; operator: &quot;Equal&quot; value: &quot;value1&quot; effect: &quot;NoExecute&quot; tolerationSeconds: 3600 这表示如果这个 pod 正在运行，然后一个匹配的 taint 被添加到其所在的节点，那么 pod 还将继续在节点上运行 3600 秒，然后被驱逐。如果在此之前上述 taint 被删除了，则 pod 不会被驱逐。 k8s.io/kubernetes/pkg/controller/nodelifecycle/scheduler/taint_manager.go:339 12345678910111213141516171819202122232425262728293031func (tc *NoExecuteTaintManager) processPodOnNode(......) &#123; if len(taints) == 0 &#123; tc.cancelWorkWithEvent(podNamespacedName) &#125; // 1、检查 pod 的 tolerations 是否匹配所有 taints allTolerated, usedTolerations := v1helper.GetMatchingTolerations(taints, tolerations) if !allTolerated &#123; tc.cancelWorkWithEvent(podNamespacedName) tc.taintEvictionQueue.AddWork(NewWorkArgs(podNamespacedName.Name, podNamespacedName.Namespace), time.Now(), time.Now()) return &#125; // 2、获取最小容忍时间 minTolerationTime := getMinTolerationTime(usedTolerations) if minTolerationTime &lt; 0 &#123; return &#125; // 3、若存在最小容忍时间则将其加入到延时队列中 startTime := now triggerTime := startTime.Add(minTolerationTime) scheduledEviction := tc.taintEvictionQueue.GetWorkerUnsafe(podNamespacedName.String()) if scheduledEviction != nil &#123; startTime = scheduledEviction.CreatedAt if startTime.Add(minTolerationTime).Before(triggerTime) &#123; return &#125; tc.cancelWorkWithEvent(podNamespacedName) &#125; tc.taintEvictionQueue.AddWork(NewWorkArgs(podNamespacedName.Name, podNamespacedName.Namespace), startTime, triggerTime)&#125; nc.doNodeProcessingPassWorkerNodeLifecycleController 中 nodeInformer 监听到 node 变化时会将其添加到 nodeUpdateQueue 中，nc.doNodeProcessingPassWorker 主要是处理 nodeUpdateQueue 中的 node，为其添加合适的 NoSchedule taint 以及 label，其主要逻辑为： 1、从 nc.nodeUpdateQueue 中取出 node； 2、若启用了 TaintNodeByCondition feature-gates，调用nc.doNoScheduleTaintingPass 检查该 node 是否需要添加对应的 NoSchedule taint；nc.doNoScheduleTaintingPass 中的主要逻辑为： 1、从 nodeLister 中获取该 node 对象； 2、判断该 node 是否存在以下几种 Condition：(1) False 或 Unknown 状态的 NodeReady Condition；(2) MemoryPressureCondition；(3) DiskPressureCondition；(4) NetworkUnavailableCondition；(5) PIDPressureCondition；若任一一种存在会添加对应的 NoSchedule taint； 3、判断 node 是否处于 Unschedulable 状态，若为 Unschedulable 也添加对应的 NoSchedule taint； 4、对比 node 已有的 taints 以及需要添加的 taints，以需要添加的 taints 为准，调用 nodeutil.SwapNodeControllerTaint 为 node 添加不存在的 taints 并删除不需要的 taints； 3、调用 nc.reconcileNodeLabels 检查 node 是否存在以下 label，若不存在则为其添加； 12345labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux kubernetes.io/arch: amd64 kubernetes.io/os: linux k8s.io/kubernetes/pkg/controller/nodelifecycle/node_lifecycle_controller.go:502 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071func (nc *Controller) doNodeProcessingPassWorker() &#123; for &#123; obj, shutdown := nc.nodeUpdateQueue.Get() if shutdown &#123; return &#125; nodeName := obj.(string) if nc.taintNodeByCondition &#123; if err := nc.doNoScheduleTaintingPass(nodeName); err != nil &#123; ...... &#125; &#125; if err := nc.reconcileNodeLabels(nodeName); err != nil &#123; ...... &#125; nc.nodeUpdateQueue.Done(nodeName) &#125;&#125;func (nc *Controller) doNoScheduleTaintingPass(nodeName string) error &#123; // 1、获取 node 对象 node, err := nc.nodeLister.Get(nodeName) if err != nil &#123; ...... &#125; // 2、若 node 存在对应的 condition 则为其添加对应的 taint var taints []v1.Taint for _, condition := range node.Status.Conditions &#123; if taintMap, found := nodeConditionToTaintKeyStatusMap[condition.Type]; found &#123; if taintKey, found := taintMap[condition.Status]; found &#123; taints = append(taints, v1.Taint&#123; Key: taintKey, Effect: v1.TaintEffectNoSchedule, &#125;) &#125; &#125; &#125; // 3、判断是否为 Unschedulable if node.Spec.Unschedulable &#123; taints = append(taints, v1.Taint&#123; Key: schedulerapi.TaintNodeUnschedulable, Effect: v1.TaintEffectNoSchedule, &#125;) &#125; nodeTaints := taintutils.TaintSetFilter(node.Spec.Taints, func(t *v1.Taint) bool &#123; if t.Effect != v1.TaintEffectNoSchedule &#123; return false &#125; if t.Key == schedulerapi.TaintNodeUnschedulable &#123; return true &#125; _, found := taintKeyToNodeConditionMap[t.Key] return found &#125;) // 4、对比 node 已有 taints 和需要添加的 taints 得到 taintsToAdd, taintsToDel taintsToAdd, taintsToDel := taintutils.TaintSetDiff(taints, nodeTaints) if len(taintsToAdd) == 0 &amp;&amp; len(taintsToDel) == 0 &#123; return nil &#125; // 5、更新 node 的 taints if !nodeutil.SwapNodeControllerTaint(nc.kubeClient, taintsToAdd, taintsToDel, node) &#123; return fmt.Errorf(&quot;failed to swap taints of node %+v&quot;, node) &#125; return nil&#125; nc.doNoExecuteTaintingPass当启用了 TaintBasedEvictions 特性时，通过 nc.monitorNodeHealth 检测到 node 异常时会将其加入到 nc.zoneNoExecuteTainter 队列中，nc.doNoExecuteTaintingPass 会处理 nc.zoneNoExecuteTainter 队列中的 node，并且会按一定的速率进行，此时会根据 node 实际的 NodeCondition 为 node 添加对应的 taint，当 node 存在 taint 时，taintManager 会驱逐 node 上的 pod。此过程中为 node 添加 taint 时进行了限速避免一次性驱逐过多 pod，在驱逐 node 上的 pod 时不会限速。 nc.doNoExecuteTaintingPass 的主要逻辑为： 1、遍历 zoneNoExecuteTainter 中的 node 列表，从 nodeLister 中获取 node 对象； 2、获取该 node 的 NodeReadyCondition； 3、判断 NodeReadyCondition 的状态，若为 false，则为 node 添加 node.kubernetes.io/not-ready:NoExecute 的 taint 且保证 node 不存在 node.kubernetes.io/unreachable:NoExecute 的 taint; 4、若 NodeReadyCondition 为 unknown，则为 node 添加 node.kubernetes.io/unreachable:NoExecute 的 taint 且保证 node 不存在 node.kubernetes.io/not-ready:NoExecute 的 taint；“unreachable” 和 “not ready” 两个 taint 是互斥的，只能存在一个； 5、若 NodeReadyCondition 为 true，此时说明该 node 处于正常状态直接返回； 6、调用 nodeutil.SwapNodeControllerTaint 更新 node 的 taint； 7、若整个过程中有失败的操作会进行重试； k8s.io/kubernetes/pkg/controller/nodelifecycle/node_lifecycle_controller.go:582 1234567891011121314151617181920212223242526272829303132333435363738394041func (nc *Controller) doNoExecuteTaintingPass() &#123; nc.evictorLock.Lock() defer nc.evictorLock.Unlock() for k := range nc.zoneNoExecuteTainter &#123; nc.zoneNoExecuteTainter[k].Try(func(value scheduler.TimedValue) (bool, time.Duration) &#123; // 1、获取 node 对象 node, err := nc.nodeLister.Get(value.Value) if apierrors.IsNotFound(err) &#123; return true, 0 &#125; else if err != nil &#123; return false, 50 * time.Millisecond &#125; // 2、获取 node 的 NodeReadyCondition _, condition := nodeutil.GetNodeCondition(&amp;node.Status, v1.NodeReady) taintToAdd := v1.Taint&#123;&#125; oppositeTaint := v1.Taint&#123;&#125; // 3、判断 Condition 状态，并为其添加对应的 taint switch condition.Status &#123; case v1.ConditionFalse: taintToAdd = *NotReadyTaintTemplate oppositeTaint = *UnreachableTaintTemplate case v1.ConditionUnknown: taintToAdd = *UnreachableTaintTemplate oppositeTaint = *NotReadyTaintTemplate default: return true, 0 &#125; // 4、更新 node 的 taint result := nodeutil.SwapNodeControllerTaint(nc.kubeClient, []*v1.Taint&#123;&amp;taintToAdd&#125;, []*v1.Taint&#123;&amp;oppositeTaint&#125;, node) if result &#123; zone := utilnode.GetZoneKey(node) evictionsNumber.WithLabelValues(zone).Inc() &#125; return result, 0 &#125;) &#125;&#125; nc.doEvictionPass若未启用 TaintBasedEvictions 特性，此时通过 nc.monitorNodeHealth 检测到 node 异常时会将其加入到 nc.zonePodEvictor 队列中，nc.doEvictionPass 会将 nc.zonePodEvictor 队列中 node 上的 pod 驱逐掉。 nc.doEvictionPass 的主要逻辑为： 1、遍历 zonePodEvictor 的 node 列表，从 nodeLister 中获取 node 对象； 2、调用 nodeutil.DeletePods 删除该 node 上的所有 pod，在 nodeutil.DeletePods 中首先通过从 apiserver 获取该 node 上所有的 pod，逐个删除 pod，若该 pod 为 daemonset 所管理的 pod 则忽略； 3、若整个过程中有失败的操作会进行重试； k8s.io/kubernetes/pkg/controller/nodelifecycle/node_lifecycle_controller.go:626 123456789101112131415161718192021222324func (nc *Controller) doEvictionPass() &#123; nc.evictorLock.Lock() defer nc.evictorLock.Unlock() for k := range nc.zonePodEvictor &#123; nc.zonePodEvictor[k].Try(func(value scheduler.TimedValue) (bool, time.Duration) &#123; node, err := nc.nodeLister.Get(value.Value) ...... nodeUID, _ := value.UID.(string) remaining, err := nodeutil.DeletePods(nc.kubeClient, nc.recorder, value.Value, nodeUID, nc.daemonSetStore) if err != nil &#123; utilruntime.HandleError(fmt.Errorf(&quot;unable to evict node %q: %v&quot;, value.Value, err)) return false, 0 &#125; ...... if node != nil &#123; zone := utilnode.GetZoneKey(node) evictionsNumber.WithLabelValues(zone).Inc() &#125; return true, 0 &#125;) &#125;&#125; nc.monitorNodeHealth上面已经介绍了无论是否启用了 TaintBasedEvictions 特性，需要打 taint 或者驱逐 pod 的 node 都会被放在 zoneNoExecuteTainter 或者 zonePodEvictor 队列中，而 nc.monitorNodeHealth 就是这两个队列中数据的生产者。nc.monitorNodeHealth 的主要功能是持续监控 node 的状态，当 node 处于异常状态时更新 node 的 taint 以及 node 上 pod 的状态或者直接驱逐 node 上的 pod，此外还会为集群下的所有 node 划分 zoneStates 并为每个 zoneStates 设置对应的驱逐速率。 nc.monitorNodeHealth 主要逻辑为： 1、从 nodeLister 中获取所有的 node； 2、NodeLifecycleController 根据自身 knownNodeSet 列表中的数据调用 nc.classifyNodes 将 node 分为三类：added、deleted、newZoneRepresentatives，added 表示新增的，deleted 表示被删除的，newZoneRepresentatives 代表该 node 不存在 zoneStates，NodeLifecycleController 会为每一个 node 划分一个 zoneStates，zoneStates 有 Initial、Normal、FullDisruption、PartialDisruption 四种，新增加的 node 默认的 zoneStates 为 Initial，其余的几个 zoneStates 分别对应着不同的驱逐速率； 3、对于 newZoneRepresentatives 中 node 列表，调用 nc.addPodEvictorForNewZone 将 node 添加到对应的的 zoneStates 中，然后根据是否启用了 TaintBasedEvictions 特性将 node 分别加入到 zonePodEvictor 或 zoneNoExecuteTainter 列表中，若启用了则加入到 zoneNoExecuteTainter 列表中否则加入到 zonePodEvictor 中； 4、对应 added 列表中的 node，首先将其加入到 knownNodeSet 列表中，然后调用 nc.addPodEvictorForNewZone 将该 node 添加到对应的 zoneStates 中，判断是否启用了 TaintBasedEvictions 特性，若启用了则调用 nc.markNodeAsReachable 移除该 node 上的 UnreachableTaint 和 NotReadyTaint，并从 zoneNoExecuteTainter 中移除该 node，表示为该 node 进行一次初始化，若未启用 TaintBasedEvictions 特性则调用 nc.cancelPodEviction 将该 node 从 zonePodEvictor 中删除； 5、对于 deleted 列表中的 node，将其从 knownNodeSet 列表中删除； 6、遍历所有的 nodes： 7、调用 nc.tryUpdateNodeHealth 获取该 node 的 gracePeriod、observedReadyCondition、currentReadyCondition，observedReadyCondition 可以理解为 node 上一次的状态， currentReadyCondition 为本次的状态； 8、检查 node 是否在中断检查中被排除，主要判断当启用 LegacyNodeRoleBehavior 或 NodeDisruptionExclusion 特性时，node 是否存在对应的标签，如果该 node 没有被排除，则将其对应的 zone 加入到 zoneToNodeConditions 中； 9、当该 node 的 currentReadyCondition 不为空时，检查 observedReadyCondition，即检查上一次的状态： 1、若 observedReadyCondition 为 false，此时若启用了 TaintBasedEvictions 时，为其添加 NotReadyTaint 并且确保 node 不存在 UnreachableTaint 。若未启用 TaintBasedEvictions 则判断距 node 上一次 readyTransitionTimestamp 的时间是否超过了 podEvictionTimeout（默认 5 分钟），若超过则将 node 加入到 zonePodEvictor 队列中，最终会驱逐 node 上的所有 pod； 2、若 observedReadyCondition 为 unknown，此时若启用了 TaintBasedEvictions 时，则为 node 添加 UnreachableTaint 并且确保 node 不会有 NotReadyTaint。若未启用 TaintBasedEvictions 则判断距 node 上一次 probeTimestamp 的时间是否超过了 podEvictionTimeout（默认 5 分钟），若超过则将 node 加入到 zonePodEvictor 队列中，最终会驱逐 node 上的所有 pod； 3、若 observedReadyCondition 为 true 时，此时若启用了 TaintBasedEvictions 时，调用 nc.markNodeAsReachable 移除 node 上的 NotReadyTaint 和 UnreachableTaint ，若未启用 TaintBasedEvictions 则将 node 从 zonePodEvictor 队列中移除；此处主要是判断是否启用了 TaintBasedEvictions 特性，然后根据 node 的 ReadyCondition 判断是否直接驱逐 node 上的 pod 还是为 node 打 taint 等待 taintManager 驱逐 node 上的 pod； 10、最后判断当 node ReadyCondition 由 true 变为 false 时，调用 nodeutil.MarkAllPodsNotReady 将该node 上的所有 pod 标记为 notReady； 11、调用 nc.handleDisruption 处理中断情况，为不同 zoneState 设置驱逐的速度； k8s.io/kubernetes/pkg/controller/nodelifecycle/node_lifecycle_controller.go:664 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133func (nc *Controller) monitorNodeHealth() error &#123; // 1、从 nodeLister 获取所有 node nodes, err := nc.nodeLister.List(labels.Everything()) if err != nil &#123; return err &#125; // 2、根据 controller knownNodeSet 中的记录将 node 分为三类 added, deleted, newZoneRepresentatives := nc.classifyNodes(nodes) // 3、为没有 zone 的 node 添加对应的 zone for i := range newZoneRepresentatives &#123; nc.addPodEvictorForNewZone(newZoneRepresentatives[i]) &#125; // 4、将新增加的 node 添加到 knownNodeSet 中并且对 node 进行初始化 for i := range added &#123; ...... nc.knownNodeSet[added[i].Name] = added[i] nc.addPodEvictorForNewZone(added[i]) if nc.useTaintBasedEvictions &#123; nc.markNodeAsReachable(added[i]) &#125; else &#123; nc.cancelPodEviction(added[i]) &#125; &#125; // 5、将 deleted 列表中的 node 从 knownNodeSet 中删除 for i := range deleted &#123; ...... delete(nc.knownNodeSet, deleted[i].Name) &#125; zoneToNodeConditions := map[string][]*v1.NodeCondition&#123;&#125; for i := range nodes &#123; var gracePeriod time.Duration var observedReadyCondition v1.NodeCondition var currentReadyCondition *v1.NodeCondition node := nodes[i].DeepCopy() // 6、获取 node 的 gracePeriod, observedReadyCondition, currentReadyCondition if err := wait.PollImmediate(retrySleepTime, retrySleepTime*scheduler.NodeHealthUpdateRetry, func() (bool, error) &#123; gracePeriod, observedReadyCondition, currentReadyCondition, err = nc.tryUpdateNodeHealth(node) if err == nil &#123; return true, nil &#125; name := node.Name node, err = nc.kubeClient.CoreV1().Nodes().Get(name, metav1.GetOptions&#123;&#125;) if err != nil &#123; return false, err &#125; return false, nil &#125;); err != nil &#123; ...... &#125; // 7、若 node 没有被排除则加入到 zoneToNodeConditions 列表中 if !isNodeExcludedFromDisruptionChecks(node) &#123; zoneToNodeConditions[utilnode.GetZoneKey(node)] = append(zoneToNodeConditions[utilnode.GetZoneKey(node)], currentReadyCondition) &#125; decisionTimestamp := nc.now() // 8、根据 observedReadyCondition 为 node 添加不同的 taint if currentReadyCondition != nil &#123; switch observedReadyCondition.Status &#123; case v1.ConditionFalse: // 9、false 状态添加 NotReady taint if nc.useTaintBasedEvictions &#123; if taintutils.TaintExists(node.Spec.Taints, UnreachableTaintTemplate) &#123; taintToAdd := *NotReadyTaintTemplate if !nodeutil.SwapNodeControllerTaint(nc.kubeClient, []*v1.Taint&#123;&amp;taintToAdd&#125;, []*v1.Taint&#123;UnreachableTaintTemplate&#125;, node) &#123; ...... &#125; &#125; else if nc.markNodeForTainting(node) &#123; ...... &#125; // 10、或者当超过 podEvictionTimeout 后直接驱逐 node 上的 pod &#125; else &#123; if decisionTimestamp.After(nc.nodeHealthMap[node.Name].readyTransitionTimestamp.Add(nc.podEvictionTimeout)) &#123; if nc.evictPods(node) &#123; ...... &#125; &#125; &#125; case v1.ConditionUnknown: // 11、unknown 状态时添加 UnreachableTaint if nc.useTaintBasedEvictions &#123; if taintutils.TaintExists(node.Spec.Taints, NotReadyTaintTemplate) &#123; taintToAdd := *UnreachableTaintTemplate if !nodeutil.SwapNodeControllerTaint(nc.kubeClient, []*v1.Taint&#123;&amp;taintToAdd&#125;, []*v1.Taint&#123;NotReadyTaintTemplate&#125;, node) &#123; ...... &#125; &#125; else if nc.markNodeForTainting(node) &#123; ...... &#125; &#125; else &#123; if decisionTimestamp.After(nc.nodeHealthMap[node.Name].probeTimestamp.Add(nc.podEvictionTimeout)) &#123; if nc.evictPods(node) &#123; ...... &#125; &#125; &#125; case v1.ConditionTrue: // 12、true 状态时移除所有 UnreachableTaint 和 NotReadyTaint if nc.useTaintBasedEvictions &#123; removed, err := nc.markNodeAsReachable(node) if err != nil &#123; ...... &#125; // 13、从 PodEviction 队列中移除 &#125; else &#123; if nc.cancelPodEviction(node) &#123; ...... &#125; &#125; &#125; // 14、ReadyCondition 由 true 变为 false 时标记 node 上的 pod 为 notready if currentReadyCondition.Status != v1.ConditionTrue &amp;&amp; observedReadyCondition.Status == v1.ConditionTrue &#123; nodeutil.RecordNodeStatusChange(nc.recorder, node, &quot;NodeNotReady&quot;) if err = nodeutil.MarkAllPodsNotReady(nc.kubeClient, node); err != nil &#123; utilruntime.HandleError(fmt.Errorf(&quot;Unable to mark all pods NotReady on node %v: %v&quot;, node.Name, err)) &#125; &#125; &#125; &#125; // 15、处理中断情况 nc.handleDisruption(zoneToNodeConditions, nodes) return nil&#125; nc.tryUpdateNodeHealthnc.tryUpdateNodeHealth 会根据当前获取的 node status 更新 nc.nodeHealthMap 中的数据，nc.nodeHealthMap 保存 node 最近一次的状态，并会根据 nc.nodeHealthMap 判断 node 是否已经处于 unknown 状态。 nc.tryUpdateNodeHealth 的主要逻辑为： 1、获取当前 node 的 ReadyCondition 作为 currentReadyCondition，若 ReadyCondition 为空则此 node 可能未上报 status，此时为该 node fake 一个 observedReadyCondition 且其 status 为 Unknown，将其 gracePeriod 设为 nodeStartupGracePeriod，否则 observedReadyCondition 设为 currentReadyCondition 且 gracePeriod 为 nodeMonitorGracePeriod，然后在 nc.nodeHealthMap 中更新 node 的 Status； 2、若 ReadyCondition 存在，则将 observedReadyCondition 置为当前 ReadyCondition，gracePeriod 设为 40s； 3、计算 node 当前的 nodeHealthData，nodeHealthData 中保存了 node 最近一次的状态，包含 probeTimestamp、readyTransitionTimestamp、status、lease 四个字段。从 nc.nodeHealthMap 中获取 node 的 condition 和 lease 信息，更新 savedNodeHealth 中 status、probeTimestamp、readyTransitionTimestamp，若启用了 NodeLease 特性也会更新 NodeHealth 中的 lease 以及 probeTimestamp，最后将当前计算出 savedNodeHealth 保存到 nc.nodeHealthMap 中； 4、通过获取到的 savedNodeHealth 检查 node 状态，若 NodeReady condition 或者 lease 对象更新时间超过 gracePeriod，则更新 node 的 Ready、MemoryPressure、DiskPressure、PIDPressure 为 Unknown，若当前计算出来的 node status 与上一次的 status 不一致则同步到 apiserver，并且更新 nodeHealthMap； 5、最后返回 gracePeriod、observedReadyCondition、currentReadyCondition； k8s.io/kubernetes/pkg/controller/nodelifecycle/node_lifecycle_controller.go:851 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133func (nc *Controller) tryUpdateNodeHealth(node *v1.Node) (time.Duration, v1.NodeCondition, *v1.NodeCondition, error) &#123; var gracePeriod time.Duration var observedReadyCondition v1.NodeCondition _, currentReadyCondition := nodeutil.GetNodeCondition(&amp;node.Status, v1.NodeReady) // 1、若 currentReadyCondition 为 nil 则 fake 一个 observedReadyCondition if currentReadyCondition == nil &#123; observedReadyCondition = v1.NodeCondition&#123; Type: v1.NodeReady, Status: v1.ConditionUnknown, LastHeartbeatTime: node.CreationTimestamp, LastTransitionTime: node.CreationTimestamp, &#125; gracePeriod = nc.nodeStartupGracePeriod if _, found := nc.nodeHealthMap[node.Name]; found &#123; nc.nodeHealthMap[node.Name].status = &amp;node.Status &#125; else &#123; nc.nodeHealthMap[node.Name] = &amp;nodeHealthData&#123; status: &amp;node.Status, probeTimestamp: node.CreationTimestamp, readyTransitionTimestamp: node.CreationTimestamp, &#125; &#125; &#125; else &#123; observedReadyCondition = *currentReadyCondition gracePeriod = nc.nodeMonitorGracePeriod &#125; // 2、savedNodeHealth 中保存 node 最近的一次状态 savedNodeHealth, found := nc.nodeHealthMap[node.Name] var savedCondition *v1.NodeCondition var savedLease *coordv1beta1.Lease if found &#123; _, savedCondition = nodeutil.GetNodeCondition(savedNodeHealth.status, v1.NodeReady) savedLease = savedNodeHealth.lease &#125; // 3、根据 savedCondition 以及 currentReadyCondition 更新 savedNodeHealth 中的数据 if !found &#123; savedNodeHealth = &amp;nodeHealthData&#123; status: &amp;node.Status, probeTimestamp: nc.now(), readyTransitionTimestamp: nc.now(), &#125; &#125; else if savedCondition == nil &amp;&amp; currentReadyCondition != nil &#123; savedNodeHealth = &amp;nodeHealthData&#123; status: &amp;node.Status, probeTimestamp: nc.now(), readyTransitionTimestamp: nc.now(), &#125; &#125; else if savedCondition != nil &amp;&amp; currentReadyCondition == nil &#123; savedNodeHealth = &amp;nodeHealthData&#123; status: &amp;node.Status, probeTimestamp: nc.now(), readyTransitionTimestamp: nc.now(), &#125; &#125; else if savedCondition != nil &amp;&amp; currentReadyCondition != nil &amp;&amp; savedCondition.LastHeartbeatTime != currentReadyCondition.LastHeartbeatTime &#123; var transitionTime metav1.Time if savedCondition.LastTransitionTime != currentReadyCondition.LastTransitionTime &#123; transitionTime = nc.now() &#125; else &#123; transitionTime = savedNodeHealth.readyTransitionTimestamp &#125; savedNodeHealth = &amp;nodeHealthData&#123; status: &amp;node.Status, probeTimestamp: nc.now(), readyTransitionTimestamp: transitionTime, &#125; &#125; // 4、判断是否启用了 nodeLease 功能 var observedLease *coordv1beta1.Lease if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123; observedLease, _ = nc.leaseLister.Leases(v1.NamespaceNodeLease).Get(node.Name) if observedLease != nil &amp;&amp; (savedLease == nil || savedLease.Spec.RenewTime.Before(observedLease.Spec.RenewTime)) &#123; savedNodeHealth.lease = observedLease savedNodeHealth.probeTimestamp = nc.now() &#125; &#125; nc.nodeHealthMap[node.Name] = savedNodeHealth // 5、检查 node 是否已经超过 gracePeriod 时间没有上报状态了 if nc.now().After(savedNodeHealth.probeTimestamp.Add(gracePeriod)) &#123; nodeConditionTypes := []v1.NodeConditionType&#123; v1.NodeReady, v1.NodeMemoryPressure, v1.NodeDiskPressure, v1.NodePIDPressure, &#125; nowTimestamp := nc.now() // 6、若 node 超过 gracePeriod 时间没有上报状态将其所有 Condition 设置 unknown for _, nodeConditionType := range nodeConditionTypes &#123; _, currentCondition := nodeutil.GetNodeCondition(&amp;node.Status, nodeConditionType) if currentCondition == nil &#123; node.Status.Conditions = append(node.Status.Conditions, v1.NodeCondition&#123; Type: nodeConditionType, Status: v1.ConditionUnknown, Reason: &quot;NodeStatusNeverUpdated&quot;, Message: &quot;Kubelet never posted node status.&quot;, LastHeartbeatTime: node.CreationTimestamp, LastTransitionTime: nowTimestamp, &#125;) &#125; else &#123; if currentCondition.Status != v1.ConditionUnknown &#123; currentCondition.Status = v1.ConditionUnknown currentCondition.Reason = &quot;NodeStatusUnknown&quot; currentCondition.Message = &quot;Kubelet stopped posting node status.&quot; currentCondition.LastTransitionTime = nowTimestamp &#125; &#125; &#125; // 7、更新 node 最新状态至 apiserver 并更新 nodeHealthMap 中的数据 _, currentReadyCondition = nodeutil.GetNodeCondition(&amp;node.Status, v1.NodeReady) if !apiequality.Semantic.DeepEqual(currentReadyCondition, &amp;observedReadyCondition) &#123; if _, err := nc.kubeClient.CoreV1().Nodes().UpdateStatus(node); err != nil &#123; return gracePeriod, observedReadyCondition, currentReadyCondition, err &#125; nc.nodeHealthMap[node.Name] = &amp;nodeHealthData&#123; status: &amp;node.Status, probeTimestamp: nc.nodeHealthMap[node.Name].probeTimestamp, readyTransitionTimestamp: nc.now(), lease: observedLease, &#125; return gracePeriod, observedReadyCondition, currentReadyCondition, nil &#125; &#125; return gracePeriod, observedReadyCondition, currentReadyCondition, nil&#125; nc.handleDisruptionmonitorNodeHealth 中会为每个 node 划分 zone 并设置 zoneState，nc.handleDisruption 的目的是当集群中不同 zone 下出现多个 unhealthy node 时会 zone 设置不同的驱逐速率。 nc.handleDisruption 主要逻辑为： 1、设置 allAreFullyDisrupted 默认值为 true，根据 zoneToNodeConditions 中的数据，判断当前所有 zone 是否都为 FullDisruption 状态； 2、遍历 zoneToNodeConditions 首先调用 nc.computeZoneStateFunc 计算每个 zone 的状态，分为三种 fullyDisrupted（zone 下所有 node 都处于 notReady 状态）、partiallyDisrupted（notReady node 占比 &gt;= unhealthyZoneThreshold 的值且 node 数超过三个）、normal（以上两种情况之外）。若 newState 不为 stateFullDisruption 将 allAreFullyDisrupted 设为 false，将 newState 保存在 newZoneStates 中; 3、将 allWasFullyDisrupted 默认值设置为 true，根据 zoneStates 中 nodeCondition 的数据，判断上一次观察到的所有 zone 是否都为 FullDisruption 状态； 4、如果所有 zone 都为 FullyDisrupted 直接停止所有的驱逐工作，因为此时可能处于网络中断的状态； 5、如果 allAreFullyDisrupted 为 true，allWasFullyDisrupted 为 false，说明从非 FullyDisrupted 切换到了 FullyDisrupted 模式，此时需要停止所有 node 的驱逐工作，首先去掉 node 上的 taint 并设置所有zone的对应 zoneNoExecuteTainter 或者 zonePodEvictor 的 Rate Limeter 为0，最后更新所有 zone 的状态为 FullDisruption； 6、如果 allWasFullyDisrupted 为 true，allAreFullyDisrupted 为 false，说明集群从 FullyDisrupted 变为 非 FullyDisrupted 模式，此时首先更新 nc.nodeHealthMap 中所有 node 的 probeTimestamp 和 readyTransitionTimestamp 为当前时间，然后调用 nc.setLimiterInZone 重置每个 zone 的驱逐速率； 7、如果 allWasFullyDisrupted为false 且 allAreFullyDisrupted 为false，即集群状态保持为非 FullDisruption 时，此时根据 zone 的 state 为每个 zone 设置默认的驱逐速率； k8s.io/kubernetes/pkg/controller/nodelifecycle/node_lifecycle_controller.go:1017 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293func (nc *Controller) handleDisruption(zoneToNodeConditions map[string][]*v1.NodeCondition, nodes []*v1.Node) &#123; newZoneStates := map[string]ZoneState&#123;&#125; allAreFullyDisrupted := true // 1、判断当前所有 zone 是否都为 FullDisruption 状态 for k, v := range zoneToNodeConditions &#123; zoneSize.WithLabelValues(k).Set(float64(len(v))) // 2、计算 zone state 以及 unhealthy node unhealthy, newState := nc.computeZoneStateFunc(v) zoneHealth.WithLabelValues(k).Set(float64(100*(len(v)-unhealthy)) / float64(len(v))) unhealthyNodes.WithLabelValues(k).Set(float64(unhealthy)) if newState != stateFullDisruption &#123; allAreFullyDisrupted = false &#125; newZoneStates[k] = newState if _, had := nc.zoneStates[k]; !had &#123; nc.zoneStates[k] = stateInitial &#125; &#125; // 3、判断上一次观察到的所有 zone 是否都为 FullDisruption 状态 allWasFullyDisrupted := true for k, v := range nc.zoneStates &#123; if _, have := zoneToNodeConditions[k]; !have &#123; zoneSize.WithLabelValues(k).Set(0) zoneHealth.WithLabelValues(k).Set(100) unhealthyNodes.WithLabelValues(k).Set(0) delete(nc.zoneStates, k) continue &#125; if v != stateFullDisruption &#123; allWasFullyDisrupted = false break &#125; &#125; // 4、若存在一个不为 FullyDisrupted if !allAreFullyDisrupted || !allWasFullyDisrupted &#123; // 5、如果 allAreFullyDisrupted 为 true，则 allWasFullyDisrupted 为 false // 说明从非 FullyDisrupted 切换到了 FullyDisrupted 模式 if allAreFullyDisrupted &#123; for i := range nodes &#123; if nc.useTaintBasedEvictions &#123; _, err := nc.markNodeAsReachable(nodes[i]) if err != nil &#123; klog.Errorf(&quot;Failed to remove taints from Node %v&quot;, nodes[i].Name) &#125; &#125; else &#123; nc.cancelPodEviction(nodes[i]) &#125; &#125; for k := range nc.zoneStates &#123; if nc.useTaintBasedEvictions &#123; nc.zoneNoExecuteTainter[k].SwapLimiter(0) &#125; else &#123; nc.zonePodEvictor[k].SwapLimiter(0) &#125; &#125; for k := range nc.zoneStates &#123; nc.zoneStates[k] = stateFullDisruption &#125; return &#125; // 6、如果 allWasFullyDisrupted 为 true，则 allAreFullyDisrupted 为 false // 说明 cluster 从 FullyDisrupted 切换为非 FullyDisrupted 模式 if allWasFullyDisrupted &#123; now := nc.now() for i := range nodes &#123; v := nc.nodeHealthMap[nodes[i].Name] v.probeTimestamp = now v.readyTransitionTimestamp = now nc.nodeHealthMap[nodes[i].Name] = v &#125; for k := range nc.zoneStates &#123; nc.setLimiterInZone(k, len(zoneToNodeConditions[k]), newZoneStates[k]) nc.zoneStates[k] = newZoneStates[k] &#125; return &#125; // 7、根据 zoneState 为每个 zone 设置驱逐速率 for k, v := range nc.zoneStates &#123; newState := newZoneStates[k] if v == newState &#123; continue &#125; nc.setLimiterInZone(k, len(zoneToNodeConditions[k]), newState) nc.zoneStates[k] = newState &#125; &#125;&#125; nc.computeZoneStateFuncnc.computeZoneStateFunc 是计算 zone state 的方法，该方法会计算每个 zone 下 notReady 的 node 并将 zone 分为三种： fullyDisrupted：zone 下所有 node 都处于 notReady 状态； partiallyDisrupted：notReady node 占比 &gt;= unhealthyZoneThreshold 的值(默认为0.55，通过--unhealthy-zone-threshold设置)且 notReady node 数超过2个； normal：以上两种情况之外的； k8s.io/kubernetes/pkg/controller/nodelifecycle/node_lifecycle_controller.go:1262 12345678910111213141516171819func (nc *Controller) ComputeZoneState(nodeReadyConditions []*v1.NodeCondition) (int, ZoneState) &#123; readyNodes := 0 notReadyNodes := 0 for i := range nodeReadyConditions &#123; if nodeReadyConditions[i] != nil &amp;&amp; nodeReadyConditions[i].Status == v1.ConditionTrue &#123; readyNodes++ &#125; else &#123; notReadyNodes++ &#125; &#125; switch &#123; case readyNodes == 0 &amp;&amp; notReadyNodes &gt; 0: return notReadyNodes, stateFullDisruption case notReadyNodes &gt; 2 &amp;&amp; float32(notReadyNodes)/float32(notReadyNodes+readyNodes) &gt;= nc.unhealthyZoneThreshold: return notReadyNodes, statePartialDisruption default: return notReadyNodes, stateNormal &#125;&#125; nc.setLimiterInZonenc.setLimiterInZone 方法会根据不同的 zoneState 设置对应的驱逐速率： stateNormal ：驱逐速率为 evictionLimiterQPS（默认为0.1，可以通过 --node-eviction-rate 参数指定)的值，即每隔 10s 清空一个节点； statePartialDisruption：如果当前 zone size 大于 nc.largeClusterThreshold（默认为 50，通过--large-cluster-size-threshold设置），则设置为 secondaryEvictionLimiterQPS（默认为 0.01，可以通过 --secondary-node-eviction-rate 指定），否则设置为 0； stateFullDisruption：为 evictionLimiterQPS（默认为0.1，可以通过 --node-eviction-rate 参数指定)的值； k8s.io/kubernetes/pkg/controller/nodelifecycle/node_lifecycle_controller.go:1115 1234567891011121314151617181920212223242526func (nc *Controller) setLimiterInZone(zone string, zoneSize int, state ZoneState) &#123; switch state &#123; case stateNormal: if nc.useTaintBasedEvictions &#123; nc.zoneNoExecuteTainter[zone].SwapLimiter(nc.evictionLimiterQPS) &#125; else &#123; nc.zonePodEvictor[zone].SwapLimiter(nc.evictionLimiterQPS) &#125; case statePartialDisruption: if nc.useTaintBasedEvictions &#123; nc.zoneNoExecuteTainter[zone].SwapLimiter( nc.enterPartialDisruptionFunc(zoneSize)) &#125; else &#123; nc.zonePodEvictor[zone].SwapLimiter( nc.enterPartialDisruptionFunc(zoneSize)) &#125; case stateFullDisruption: if nc.useTaintBasedEvictions &#123; nc.zoneNoExecuteTainter[zone].SwapLimiter( nc.enterFullDisruptionFunc(zoneSize)) &#125; else &#123; nc.zonePodEvictor[zone].SwapLimiter( nc.enterFullDisruptionFunc(zoneSize)) &#125; &#125;&#125; 小结monitorNodeHealth 中的主要流程如下所示： 12345678910111213141516171819202122 monitorNodeHealth | | useTaintBasedEvictions | | --------------------------------------------- yes | | no | | v v addPodEvictorForNewZone evictPods | | | | v v zoneNoExecuteTainter zonePodEvictor(RateLimitedTimedQueue) (RateLimitedTimedQueue) | | | | | | v v doNoExecuteTaintingPass doEvictionPass (consumer) (consumer) NodeLifecycleController 中三个核心组件之间的交互流程如下所示: 123456789101112131415 monitorNodeHealth | | | 为 node 添加 NoExecute taint | | v 为 node 添加 watch nodeList NoSchedule taint taintManager ------&gt; APIServer &lt;----------- nc.doNodeProcessingPassWorker | | | v驱逐 node 上不容忍node taint 的 pod 至此，NodeLifecycleController 的核心代码已经分析完。 总结本文主要分析了 NodeLifecycleController 的设计与实现，NodeLifecycleController 主要是监控 node 状态，当 node 异常时驱逐 node 上的 pod，其行为与其他组件有一定关系，node 的状态由 kubelet 上报，node 异常时为 node 添加 taint 标签后，scheduler 调度 pod 也会有对应的行为。为了保证由于网络等问题引起的 pod 驱逐行为，NodeLifecycleController 会为 node 进行分区并会为每个区设置不同的驱逐速率，即实际上会以 rate-limited 的方式添加 taint，在某些情况下可以避免 pod 被大量驱逐。 此外，NodeLifecycleController 还会对外暴露多个 metrics，包括 zoneHealth、zoneSize、unhealthyNodes、evictionsNumber 等，便于用户查看集群下 node 的状态。 参考： https://kubernetes.io/zh/docs/concepts/configuration/taint-and-toleration/ https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/]]></content>
      <tags>
        <tag>kube-controller-manager</tag>
        <tag>node controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet 启动流程分析]]></title>
    <url>%2F2020%2F01%2F03%2Fkubelet_init%2F</url>
    <content type="text"><![CDATA[本来这篇文章会继续讲述 kubelet 中的主要模块，但由于网友反馈能不能先从 kubelet 的启动流程开始，kubelet 的启动流程在很久之前基于 v1.12 写过一篇文章，对比了 v1.16 中的启动流程变化不大，但之前的文章写的比较简洁，本文会重新分析 kubelet 的启动流程。 Kubelet 启动流程 kubernetes 版本：v1.16 kubelet 的启动比较复杂，首先还是把 kubelet 的启动流程图放在此处，便于在后文中清楚各种调用的流程： NewKubeletCommand首先从 kubelet 的 main 函数开始，其中调用的 NewKubeletCommand 方法主要负责获取配置文件中的参数，校验参数以及为参数设置默认值。主要逻辑为： 1、解析命令行参数； 2、为 kubelet 初始化 feature gates 参数； 3、加载 kubelet 配置文件； 4、校验配置文件中的参数； 5、检查 kubelet 是否启用动态配置功能； 6、初始化 kubeletDeps，kubeletDeps 包含 kubelet 运行所必须的配置，是为了实现 dependency injection，其目的是为了把 kubelet 依赖的组件对象作为参数传进来，这样可以控制 kubelet 的行为； 7、调用 Run 方法； k8s.io/kubernetes/cmd/kubelet/app/server.go:111 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101func NewKubeletCommand() *cobra.Command &#123; cleanFlagSet := pflag.NewFlagSet(componentKubelet, pflag.ContinueOnError) cleanFlagSet.SetNormalizeFunc(cliflag.WordSepNormalizeFunc) // 1、kubelet配置分两部分: // KubeletFlag: 指那些不允许在 kubelet 运行时进行修改的配置集，或者不能在集群中各个 Nodes 之间共享的配置集。 // KubeletConfiguration: 指可以在集群中各个Nodes之间共享的配置集，可以进行动态配置。 kubeletFlags := options.NewKubeletFlags() kubeletConfig, err := options.NewKubeletConfiguration() if err != nil &#123; klog.Fatal(err) &#125; cmd := &amp;cobra.Command&#123; Use: componentKubelet, DisableFlagParsing: true, ...... Run: func(cmd *cobra.Command, args []string) &#123; // 2、解析命令行参数 if err := cleanFlagSet.Parse(args); err != nil &#123; cmd.Usage() klog.Fatal(err) &#125; ...... verflag.PrintAndExitIfRequested() utilflag.PrintFlags(cleanFlagSet) // 3、初始化 feature gates 配置 if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil &#123; klog.Fatal(err) &#125; if err := options.ValidateKubeletFlags(kubeletFlags); err != nil &#123; klog.Fatal(err) &#125; if kubeletFlags.ContainerRuntime == &quot;remote&quot; &amp;&amp; cleanFlagSet.Changed(&quot;pod-infra-container-image&quot;) &#123; klog.Warning(&quot;Warning: For remote container runtime, --pod-infra-container-image is ignored in kubelet, which should be set in that remote runtime instead&quot;) &#125; // 4、加载 kubelet 配置文件 if configFile := kubeletFlags.KubeletConfigFile; len(configFile) &gt; 0 &#123; kubeletConfig, err = loadConfigFile(configFile) ...... &#125; // 5、校验配置文件中的参数 if err := kubeletconfigvalidation.ValidateKubeletConfiguration(kubeletConfig); err != nil &#123; klog.Fatal(err) &#125; // 6、检查 kubelet 是否启用动态配置功能 var kubeletConfigController *dynamickubeletconfig.Controller if dynamicConfigDir := kubeletFlags.DynamicConfigDir.Value(); len(dynamicConfigDir) &gt; 0 &#123; var dynamicKubeletConfig *kubeletconfiginternal.KubeletConfiguration dynamicKubeletConfig, kubeletConfigController, err = BootstrapKubeletConfigController(dynamicConfigDir, func(kc *kubeletconfiginternal.KubeletConfiguration) error &#123; return kubeletConfigFlagPrecedence(kc, args) &#125;) if err != nil &#123; klog.Fatal(err) &#125; if dynamicKubeletConfig != nil &#123; kubeletConfig = dynamicKubeletConfig if err := utilfeature.DefaultMutableFeatureGate.SetFromMap(kubeletConfig.FeatureGates); err != nil &#123; klog.Fatal(err) &#125; &#125; &#125; kubeletServer := &amp;options.KubeletServer&#123; KubeletFlags: *kubeletFlags, KubeletConfiguration: *kubeletConfig, &#125; // 7、初始化 kubeletDeps kubeletDeps, err := UnsecuredDependencies(kubeletServer) if err != nil &#123; klog.Fatal(err) &#125; kubeletDeps.KubeletConfigController = kubeletConfigController stopCh := genericapiserver.SetupSignalHandler() if kubeletServer.KubeletFlags.ExperimentalDockershim &#123; if err := RunDockershim(&amp;kubeletServer.KubeletFlags, kubeletConfig, stopCh); err != nil &#123; klog.Fatal(err) &#125; return &#125; // 8、调用 Run 方法 if err := Run(kubeletServer, kubeletDeps, stopCh); err != nil &#123; klog.Fatal(err) &#125; &#125;, &#125; kubeletFlags.AddFlags(cleanFlagSet) options.AddKubeletConfigFlags(cleanFlagSet, kubeletConfig) options.AddGlobalFlags(cleanFlagSet) ...... return cmd&#125; Run该方法中仅仅调用 run 方法执行后面的启动逻辑。 k8s.io/kubernetes/cmd/kubelet/app/server.go:408 123456789func Run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) error &#123; if err := initForOS(s.KubeletFlags.WindowsService); err != nil &#123; return fmt.Errorf(&quot;failed OS init: %v&quot;, err) &#125; if err := run(s, kubeDeps, stopCh); err != nil &#123; return fmt.Errorf(&quot;failed to run Kubelet: %v&quot;, err) &#125; return nil&#125; runrun 方法中主要是为 kubelet 的启动做一些基本的配置及检查工作，主要逻辑为： 1、为 kubelet 设置默认的 FeatureGates，kubelet 所有的 FeatureGates 可以通过命令参数查看，k8s 中处于 Alpha 状态的 FeatureGates 在组件启动时默认关闭，处于 Beta 和 GA 状态的默认开启； 2、校验 kubelet 的参数； 3、尝试获取 kubelet 的 lock file，需要在 kubelet 启动时指定 --exit-on-lock-contention 和 --lock-file，该功能处于 Alpha 版本默认为关闭状态； 4、将当前的配置文件注册到 http server /configz URL 中； 5、检查 kubelet 启动模式是否为 standalone 模式，此模式下不会和 apiserver 交互，主要用于 kubelet 的调试； 6、初始化 kubeDeps，kubeDeps 中包含 kubelet 的一些依赖，主要有 KubeClient、EventClient、HeartbeatClient、Auth、cadvisor、ContainerManager； 7、检查是否以 root 用户启动； 8、为进程设置 oom 分数，默认为 -999，分数范围为 [-1000, 1000]，越小越不容易被 kill 掉； 9、调用 RunKubelet 方法； 10、检查 kubelet 是否启动了动态配置功能； 11、启动 Healthz http server； 12、如果使用 systemd 启动，通知 systemd kubelet 已经启动； k8s.io/kubernetes/cmd/kubelet/app/server.go:472 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123; // 1、为 kubelet 设置默认的 FeatureGates err = utilfeature.DefaultMutableFeatureGate.SetFromMap(s.KubeletConfiguration.FeatureGates) if err != nil &#123; return err &#125; // 2、校验 kubelet 的参数 if err := options.ValidateKubeletServer(s); err != nil &#123; return err &#125; // 3、尝试获取 kubelet 的 lock file if s.ExitOnLockContention &amp;&amp; s.LockFilePath == &quot;&quot; &#123; return errors.New(&quot;cannot exit on lock file contention: no lock file specified&quot;) &#125; done := make(chan struct&#123;&#125;) if s.LockFilePath != &quot;&quot; &#123; klog.Infof(&quot;acquiring file lock on %q&quot;, s.LockFilePath) if err := flock.Acquire(s.LockFilePath); err != nil &#123; return fmt.Errorf(&quot;unable to acquire file lock on %q: %v&quot;, s.LockFilePath, err) &#125; if s.ExitOnLockContention &#123; klog.Infof(&quot;watching for inotify events for: %v&quot;, s.LockFilePath) if err := watchForLockfileContention(s.LockFilePath, done); err != nil &#123; return err &#125; &#125; &#125; // 4、将当前的配置文件注册到 http server /configz URL 中； err = initConfigz(&amp;s.KubeletConfiguration) if err != nil &#123; klog.Errorf(&quot;unable to register KubeletConfiguration with configz, error: %v&quot;, err) &#125; // 5、判断是否为 standalone 模式 standaloneMode := true if len(s.KubeConfig) &gt; 0 &#123; standaloneMode = false &#125; // 6、初始化 kubeDeps if kubeDeps == nil &#123; kubeDeps, err = UnsecuredDependencies(s) if err != nil &#123; return err &#125; &#125; if kubeDeps.Cloud == nil &#123; if !cloudprovider.IsExternal(s.CloudProvider) &#123; cloud, err := cloudprovider.InitCloudProvider(s.CloudProvider, s.CloudConfigFile) if err != nil &#123; return err &#125; ...... kubeDeps.Cloud = cloud &#125; &#125; hostName, err := nodeutil.GetHostname(s.HostnameOverride) if err != nil &#123; return err &#125; nodeName, err := getNodeName(kubeDeps.Cloud, hostName) if err != nil &#123; return err &#125; // 7、如果是 standalone 模式将所有 client 设置为 nil switch &#123; case standaloneMode: kubeDeps.KubeClient = nil kubeDeps.EventClient = nil kubeDeps.HeartbeatClient = nil // 8、为 kubeDeps 初始化 KubeClient、EventClient、HeartbeatClient 模块 case kubeDeps.KubeClient == nil, kubeDeps.EventClient == nil, kubeDeps.HeartbeatClient == nil: clientConfig, closeAllConns, err := buildKubeletClientConfig(s, nodeName) if err != nil &#123; return err &#125; if closeAllConns == nil &#123; return errors.New(&quot;closeAllConns must be a valid function other than nil&quot;) &#125; kubeDeps.OnHeartbeatFailure = closeAllConns kubeDeps.KubeClient, err = clientset.NewForConfig(clientConfig) if err != nil &#123; return fmt.Errorf(&quot;failed to initialize kubelet client: %v&quot;, err) &#125; eventClientConfig := *clientConfig eventClientConfig.QPS = float32(s.EventRecordQPS) eventClientConfig.Burst = int(s.EventBurst) kubeDeps.EventClient, err = v1core.NewForConfig(&amp;eventClientConfig) if err != nil &#123; return fmt.Errorf(&quot;failed to initialize kubelet event client: %v&quot;, err) &#125; heartbeatClientConfig := *clientConfig heartbeatClientConfig.Timeout = s.KubeletConfiguration.NodeStatusUpdateFrequency.Duration if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123; leaseTimeout := time.Duration(s.KubeletConfiguration.NodeLeaseDurationSeconds) * time.Second if heartbeatClientConfig.Timeout &gt; leaseTimeout &#123; heartbeatClientConfig.Timeout = leaseTimeout &#125; &#125; heartbeatClientConfig.QPS = float32(-1) kubeDeps.HeartbeatClient, err = clientset.NewForConfig(&amp;heartbeatClientConfig) if err != nil &#123; return fmt.Errorf(&quot;failed to initialize kubelet heartbeat client: %v&quot;, err) &#125; &#125; // 9、初始化 auth 模块 if kubeDeps.Auth == nil &#123; auth, err := BuildAuth(nodeName, kubeDeps.KubeClient, s.KubeletConfiguration) if err != nil &#123; return err &#125; kubeDeps.Auth = auth &#125; var cgroupRoots []string // 10、设置 cgroupRoot cgroupRoots = append(cgroupRoots, cm.NodeAllocatableRoot(s.CgroupRoot, s.CgroupDriver)) kubeletCgroup, err := cm.GetKubeletContainer(s.KubeletCgroups) if err != nil &#123; &#125; else if kubeletCgroup != &quot;&quot; &#123; cgroupRoots = append(cgroupRoots, kubeletCgroup) &#125; runtimeCgroup, err := cm.GetRuntimeContainer(s.ContainerRuntime, s.RuntimeCgroups) if err != nil &#123; &#125; else if runtimeCgroup != &quot;&quot; &#123; cgroupRoots = append(cgroupRoots, runtimeCgroup) &#125; if s.SystemCgroups != &quot;&quot; &#123; cgroupRoots = append(cgroupRoots, s.SystemCgroups) &#125; // 11、初始化 cadvisor if kubeDeps.CAdvisorInterface == nil &#123; imageFsInfoProvider := cadvisor.NewImageFsInfoProvider(s.ContainerRuntime, s.RemoteRuntimeEndpoint) kubeDeps.CAdvisorInterface, err = cadvisor.New(imageFsInfoProvider, s.RootDirectory, cgroupRoots, cadvisor.UsingLegacyCadvisorStats(s. ContainerRuntime, s.RemoteRuntimeEndpoint)) if err != nil &#123; return err &#125; &#125; makeEventRecorder(kubeDeps, nodeName) // 12、初始化 ContainerManager if kubeDeps.ContainerManager == nil &#123; if s.CgroupsPerQOS &amp;&amp; s.CgroupRoot == &quot;&quot; &#123; s.CgroupRoot = &quot;/&quot; &#125; kubeReserved, err := parseResourceList(s.KubeReserved) if err != nil &#123; return err &#125; systemReserved, err := parseResourceList(s.SystemReserved) if err != nil &#123; return err &#125; var hardEvictionThresholds []evictionapi.Threshold if !s.ExperimentalNodeAllocatableIgnoreEvictionThreshold &#123; hardEvictionThresholds, err = eviction.ParseThresholdConfig([]string&#123;&#125;, s.EvictionHard, nil, nil, nil) if err != nil &#123; return err &#125; &#125; experimentalQOSReserved, err := cm.ParseQOSReserved(s.QOSReserved) if err != nil &#123; return err &#125; devicePluginEnabled := utilfeature.DefaultFeatureGate.Enabled(features.DevicePlugins) kubeDeps.ContainerManager, err = cm.NewContainerManager( kubeDeps.Mounter, kubeDeps.CAdvisorInterface, cm.NodeConfig&#123; ...... &#125;, s.FailSwapOn, devicePluginEnabled, kubeDeps.Recorder) if err != nil &#123; return err &#125; &#125; // 13、检查是否以 root 权限启动 if err := checkPermissions(); err != nil &#123; klog.Error(err) &#125; utilruntime.ReallyCrash = s.ReallyCrashForTesting // 14、为 kubelet 进程设置 oom 分数 oomAdjuster := kubeDeps.OOMAdjuster if err := oomAdjuster.ApplyOOMScoreAdj(0, int(s.OOMScoreAdj)); err != nil &#123; klog.Warning(err) &#125; // 15、调用 RunKubelet 方法执行后续的启动操作 if err := RunKubelet(s, kubeDeps, s.RunOnce); err != nil &#123; return err &#125; if utilfeature.DefaultFeatureGate.Enabled(features.DynamicKubeletConfig) &amp;&amp; len(s.DynamicConfigDir.Value()) &gt; 0 &amp;&amp; kubeDeps.KubeletConfigController != nil &amp;&amp; !standaloneMode &amp;&amp; !s.RunOnce &#123; if err := kubeDeps.KubeletConfigController.StartSync(kubeDeps.KubeClient, kubeDeps.EventClient, string(nodeName)); err != nil &#123; return err &#125; &#125; // 16、启动 Healthz http server if s.HealthzPort &gt; 0 &#123; mux := http.NewServeMux() healthz.InstallHandler(mux) go wait.Until(func() &#123; err := http.ListenAndServe(net.JoinHostPort(s.HealthzBindAddress, strconv.Itoa(int(s.HealthzPort))), mux) if err != nil &#123; klog.Errorf(&quot;Starting healthz server failed: %v&quot;, err) &#125; &#125;, 5*time.Second, wait.NeverStop) &#125; if s.RunOnce &#123; return nil &#125; // 17、向 systemd 发送启动信号 go daemon.SdNotify(false, &quot;READY=1&quot;) select &#123; case &lt;-done: break case &lt;-stopCh: break &#125; return nil&#125; RunKubeletRunKubelet 中主要调用了 createAndInitKubelet 方法执行 kubelet 组件的初始化，然后调用 startKubelet 启动 kubelet 中的组件。 k8s.io/kubernetes/cmd/kubelet/app/server.go:989 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123; hostname, err := nodeutil.GetHostname(kubeServer.HostnameOverride) if err != nil &#123; return err &#125; nodeName, err := getNodeName(kubeDeps.Cloud, hostname) if err != nil &#123; return err &#125; makeEventRecorder(kubeDeps, nodeName) // 1、默认启动特权模式 capabilities.Initialize(capabilities.Capabilities&#123; AllowPrivileged: true, &#125;) credentialprovider.SetPreferredDockercfgPath(kubeServer.RootDirectory) if kubeDeps.OSInterface == nil &#123; kubeDeps.OSInterface = kubecontainer.RealOS&#123;&#125; &#125; // 2、调用 createAndInitKubelet k, err := createAndInitKubelet(&amp;kubeServer.KubeletConfiguration, ...... kubeServer.NodeStatusMaxImages) if err != nil &#123; return fmt.Errorf(&quot;failed to create kubelet: %v&quot;, err) &#125; if kubeDeps.PodConfig == nil &#123; return fmt.Errorf(&quot;failed to create kubelet, pod source config was nil&quot;) &#125; podCfg := kubeDeps.PodConfig rlimit.RlimitNumFiles(uint64(kubeServer.MaxOpenFiles)) if runOnce &#123; if _, err := k.RunOnce(podCfg.Updates()); err != nil &#123; return fmt.Errorf(&quot;runonce failed: %v&quot;, err) &#125; klog.Info(&quot;Started kubelet as runonce&quot;) &#125; else &#123; // 3、调用 startKubelet startKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableCAdvisorJSONEndpoints, kubeServer.EnableServer) klog.Info(&quot;Started kubelet&quot;) &#125; return nil&#125; createAndInitKubeletcreateAndInitKubelet 中主要调用了三个方法来完成 kubelet 的初始化： kubelet.NewMainKubelet：实例化 kubelet 对象，并对 kubelet 依赖的所有模块进行初始化； k.BirthCry：向 apiserver 发送一条 kubelet 启动了的 event； k.StartGarbageCollection：启动垃圾回收服务，回收 container 和 images； k8s.io/kubernetes/cmd/kubelet/app/server.go:1089 1234567891011121314func createAndInitKubelet(......) &#123; k, err = kubelet.NewMainKubelet( ...... ) if err != nil &#123; return nil, err &#125; k.BirthCry() k.StartGarbageCollection() return k, nil&#125; kubelet.NewMainKubeletNewMainKubelet 是初始化 kubelet 的一个方法，主要逻辑为： 1、初始化 PodConfig 即监听 pod 元数据的来源(file，http，apiserver)，将不同 source 的 pod configuration 合并到一个结构中； 2、初始化 containerGCPolicy、imageGCPolicy、evictionConfig 配置； 3、启动 serviceInformer 和 nodeInformer； 4、初始化 containerRefManager、oomWatcher； 5、初始化 kubelet 对象； 6、初始化 secretManager、configMapManager； 7、初始化 livenessManager、podManager、statusManager、resourceAnalyzer； 8、调用 kuberuntime.NewKubeGenericRuntimeManager 初始化 containerRuntime； 9、初始化 pleg； 10、初始化 containerGC、containerDeletor、imageManager、containerLogManager； 11、初始化 serverCertificateManager、probeManager、tokenManager、volumePluginMgr、pluginManager、volumeManager； 12、初始化 workQueue、podWorkers、evictionManager； 13、最后注册相关模块的 handler； NewMainKubelet 中对 kubelet 依赖的所有模块进行了初始化，每个模块对应的功能在上篇文章“kubelet 架构浅析”有介绍，至于每个模块初始化的流程以及功能会在后面的文章中进行详细分析。 k8s.io/kubernetes/pkg/kubelet/kubelet.go:335 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration,) &#123; if rootDirectory == &quot;&quot; &#123; return nil, fmt.Errorf(&quot;invalid root directory %q&quot;, rootDirectory) &#125; if kubeCfg.SyncFrequency.Duration &lt;= 0 &#123; return nil, fmt.Errorf(&quot;invalid sync frequency %d&quot;, kubeCfg.SyncFrequency.Duration) &#125; if kubeCfg.MakeIPTablesUtilChains &#123; ...... &#125; hostname, err := nodeutil.GetHostname(hostnameOverride) if err != nil &#123; return nil, err &#125; nodeName := types.NodeName(hostname) if kubeDeps.Cloud != nil &#123; ...... &#125; // 1、初始化 PodConfig if kubeDeps.PodConfig == nil &#123; var err error kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName, bootstrapCheckpointPath) if err != nil &#123; return nil, err &#125; &#125; // 2、初始化 containerGCPolicy、imageGCPolicy、evictionConfig containerGCPolicy := kubecontainer.ContainerGCPolicy&#123; MinAge: minimumGCAge.Duration, MaxPerPodContainer: int(maxPerPodContainerCount), MaxContainers: int(maxContainerCount), &#125; daemonEndpoints := &amp;v1.NodeDaemonEndpoints&#123; KubeletEndpoint: v1.DaemonEndpoint&#123;Port: kubeCfg.Port&#125;, &#125; imageGCPolicy := images.ImageGCPolicy&#123; MinAge: kubeCfg.ImageMinimumGCAge.Duration, HighThresholdPercent: int(kubeCfg.ImageGCHighThresholdPercent), LowThresholdPercent: int(kubeCfg.ImageGCLowThresholdPercent), &#125; enforceNodeAllocatable := kubeCfg.EnforceNodeAllocatable if experimentalNodeAllocatableIgnoreEvictionThreshold &#123; enforceNodeAllocatable = []string&#123;&#125; &#125; thresholds, err := eviction.ParseThresholdConfig(enforceNodeAllocatable, kubeCfg.EvictionHard, kubeCfg.EvictionSoft, kubeCfg. EvictionSoftGracePeriod, kubeCfg.EvictionMinimumReclaim) if err != nil &#123; return nil, err &#125; evictionConfig := eviction.Config&#123; PressureTransitionPeriod: kubeCfg.EvictionPressureTransitionPeriod.Duration, MaxPodGracePeriodSeconds: int64(kubeCfg.EvictionMaxPodGracePeriod), Thresholds: thresholds, KernelMemcgNotification: experimentalKernelMemcgNotification, PodCgroupRoot: kubeDeps.ContainerManager.GetPodCgroupRoot(), &#125; // 3、启动 serviceInformer 和 nodeInformer serviceIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;cache.NamespaceIndex: cache.MetaNamespaceIndexFunc&#125;) if kubeDeps.KubeClient != nil &#123; serviceLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;services&quot;, metav1.NamespaceAll, fields.Everything()) r := cache.NewReflector(serviceLW, &amp;v1.Service&#123;&#125;, serviceIndexer, 0) go r.Run(wait.NeverStop) &#125; serviceLister := corelisters.NewServiceLister(serviceIndexer) nodeIndexer := cache.NewIndexer(cache.MetaNamespaceKeyFunc, cache.Indexers&#123;&#125;) if kubeDeps.KubeClient != nil &#123; fieldSelector := fields.Set&#123;api.ObjectNameField: string(nodeName)&#125;.AsSelector() nodeLW := cache.NewListWatchFromClient(kubeDeps.KubeClient.CoreV1().RESTClient(), &quot;nodes&quot;, metav1.NamespaceAll, fieldSelector) r := cache.NewReflector(nodeLW, &amp;v1.Node&#123;&#125;, nodeIndexer, 0) go r.Run(wait.NeverStop) &#125; nodeInfo := &amp;CachedNodeInfo&#123;NodeLister: corelisters.NewNodeLister(nodeIndexer)&#125; ...... // 4、初始化 containerRefManager、oomWatcher containerRefManager := kubecontainer.NewRefManager() oomWatcher := oomwatcher.NewWatcher(kubeDeps.Recorder) clusterDNS := make([]net.IP, 0, len(kubeCfg.ClusterDNS)) for _, ipEntry := range kubeCfg.ClusterDNS &#123; ip := net.ParseIP(ipEntry) if ip == nil &#123; klog.Warningf(&quot;Invalid clusterDNS ip &apos;%q&apos;&quot;, ipEntry) &#125; else &#123; clusterDNS = append(clusterDNS, ip) &#125; &#125; httpClient := &amp;http.Client&#123;&#125; parsedNodeIP := net.ParseIP(nodeIP) protocol := utilipt.ProtocolIpv4 if parsedNodeIP != nil &amp;&amp; parsedNodeIP.To4() == nil &#123; protocol = utilipt.ProtocolIpv6 &#125; // 5、初始化 kubelet 对象 klet := &amp;Kubelet&#123;......&#125; if klet.cloud != nil &#123; klet.cloudResourceSyncManager = cloudresource.NewSyncManager(klet.cloud, nodeName, klet.nodeStatusUpdateFrequency) &#125; // 6、初始化 secretManager、configMapManager var secretManager secret.Manager var configMapManager configmap.Manager switch kubeCfg.ConfigMapAndSecretChangeDetectionStrategy &#123; case kubeletconfiginternal.WatchChangeDetectionStrategy: secretManager = secret.NewWatchingSecretManager(kubeDeps.KubeClient) configMapManager = configmap.NewWatchingConfigMapManager(kubeDeps.KubeClient) case kubeletconfiginternal.TTLCacheChangeDetectionStrategy: secretManager = secret.NewCachingSecretManager( kubeDeps.KubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode)) configMapManager = configmap.NewCachingConfigMapManager( kubeDeps.KubeClient, manager.GetObjectTTLFromNodeFunc(klet.GetNode)) case kubeletconfiginternal.GetChangeDetectionStrategy: secretManager = secret.NewSimpleSecretManager(kubeDeps.KubeClient) configMapManager = configmap.NewSimpleConfigMapManager(kubeDeps.KubeClient) default: return nil, fmt.Errorf(&quot;unknown configmap and secret manager mode: %v&quot;, kubeCfg.ConfigMapAndSecretChangeDetectionStrategy) &#125; klet.secretManager = secretManager klet.configMapManager = configMapManager if klet.experimentalHostUserNamespaceDefaulting &#123; klog.Infof(&quot;Experimental host user namespace defaulting is enabled.&quot;) &#125; machineInfo, err := klet.cadvisor.MachineInfo() if err != nil &#123; return nil, err &#125; klet.machineInfo = machineInfo imageBackOff := flowcontrol.NewBackOff(backOffPeriod, MaxContainerBackOff) // 7、初始化 livenessManager、podManager、statusManager、resourceAnalyzer klet.livenessManager = proberesults.NewManager() klet.podCache = kubecontainer.NewCache() var checkpointManager checkpointmanager.CheckpointManager if bootstrapCheckpointPath != &quot;&quot; &#123; checkpointManager, err = checkpointmanager.NewCheckpointManager(bootstrapCheckpointPath) if err != nil &#123; return nil, fmt.Errorf(&quot;failed to initialize checkpoint manager: %+v&quot;, err) &#125; &#125; klet.podManager = kubepod.NewBasicPodManager(kubepod.NewBasicMirrorClient(klet.kubeClient), secretManager, configMapManager, checkpointManager) klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet) if remoteRuntimeEndpoint != &quot;&quot; &#123; if remoteImageEndpoint == &quot;&quot; &#123; remoteImageEndpoint = remoteRuntimeEndpoint &#125; &#125; pluginSettings := dockershim.NetworkPluginSettings&#123;......&#125; klet.resourceAnalyzer = serverstats.NewResourceAnalyzer(klet, kubeCfg.VolumeStatsAggPeriod.Duration) var legacyLogProvider kuberuntime.LegacyLogProvider // 8、调用 kuberuntime.NewKubeGenericRuntimeManager 初始化 containerRuntime switch containerRuntime &#123; case kubetypes.DockerContainerRuntime: streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, &amp;pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) if err != nil &#123; return nil, err &#125; if crOptions.RedirectContainerStreaming &#123; klet.criHandler = ds &#125; server := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := server.Start(); err != nil &#123; return nil, err &#125; supported, err := ds.IsCRISupportedLogDriver() if err != nil &#123; return nil, err &#125; if !supported &#123; klet.dockerLegacyService = ds legacyLogProvider = ds &#125; case kubetypes.RemoteContainerRuntime: break default: return nil, fmt.Errorf(&quot;unsupported CRI runtime: %q&quot;, containerRuntime) &#125; runtimeService, imageService, err := getRuntimeAndImageServices(remoteRuntimeEndpoint, remoteImageEndpoint, kubeCfg.RuntimeRequestTimeout) if err != nil &#123; return nil, err &#125; klet.runtimeService = runtimeService if utilfeature.DefaultFeatureGate.Enabled(features.RuntimeClass) &amp;&amp; kubeDeps.KubeClient != nil &#123; klet.runtimeClassManager = runtimeclass.NewManager(kubeDeps.KubeClient) &#125; runtime, err := kuberuntime.NewKubeGenericRuntimeManager(......) if err != nil &#123; return nil, err &#125; klet.containerRuntime = runtime klet.streamingRuntime = runtime klet.runner = runtime runtimeCache, err := kubecontainer.NewRuntimeCache(klet.containerRuntime) if err != nil &#123; return nil, err &#125; klet.runtimeCache = runtimeCache if cadvisor.UsingLegacyCadvisorStats(containerRuntime, remoteRuntimeEndpoint) &#123; klet.StatsProvider = stats.NewCadvisorStatsProvider(......) &#125; else &#123; klet.StatsProvider = stats.NewCRIStatsProvider(......) &#125; // 9、初始化 pleg klet.pleg = pleg.NewGenericPLEG(klet.containerRuntime, plegChannelCapacity, plegRelistPeriod, klet.podCache, clock.RealClock&#123;&#125;) klet.runtimeState = newRuntimeState(maxWaitForContainerRuntime) klet.runtimeState.addHealthCheck(&quot;PLEG&quot;, klet.pleg.Healthy) if _, err := klet.updatePodCIDR(kubeCfg.PodCIDR); err != nil &#123; klog.Errorf(&quot;Pod CIDR update failed %v&quot;, err) &#125; // 10、初始化 containerGC、containerDeletor、imageManager、containerLogManager containerGC, err := kubecontainer.NewContainerGC(klet.containerRuntime, containerGCPolicy, klet.sourcesReady) if err != nil &#123; return nil, err &#125; klet.containerGC = containerGC klet.containerDeletor = newPodContainerDeletor(klet.containerRuntime, integer.IntMax(containerGCPolicy.MaxPerPodContainer, minDeadContainerInPod)) imageManager, err := images.NewImageGCManager(klet.containerRuntime, klet.StatsProvider, kubeDeps.Recorder, nodeRef, imageGCPolicy, crOptions. PodSandboxImage) if err != nil &#123; return nil, fmt.Errorf(&quot;failed to initialize image manager: %v&quot;, err) &#125; klet.imageManager = imageManager if containerRuntime == kubetypes.RemoteContainerRuntime &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.CRIContainerLogRotation) &#123; containerLogManager, err := logs.NewContainerLogManager( klet.runtimeService, kubeCfg.ContainerLogMaxSize, int(kubeCfg.ContainerLogMaxFiles), ) if err != nil &#123; return nil, fmt.Errorf(&quot;failed to initialize container log manager: %v&quot;, err) &#125; klet.containerLogManager = containerLogManager &#125; else &#123; klet.containerLogManager = logs.NewStubContainerLogManager() &#125; // 11、初始化 serverCertificateManager、probeManager、tokenManager、volumePluginMgr、pluginManager、volumeManager if kubeCfg.ServerTLSBootstrap &amp;&amp; kubeDeps.TLSOptions != nil &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.RotateKubeletServerCertificate) &#123; klet.serverCertificateManager, err = kubeletcertificate.NewKubeletServerCertificateManager(klet.kubeClient, kubeCfg, klet.nodeName, klet. getLastObservedNodeAddresses, certDirectory) if err != nil &#123; return nil, fmt.Errorf(&quot;failed to initialize certificate manager: %v&quot;, err) &#125; kubeDeps.TLSOptions.Config.GetCertificate = func(*tls.ClientHelloInfo) (*tls.Certificate, error) &#123; cert := klet.serverCertificateManager.Current() if cert == nil &#123; return nil, fmt.Errorf(&quot;no serving certificate available for the kubelet&quot;) &#125; return cert, nil &#125; &#125; klet.probeManager = prober.NewManager(......) tokenManager := token.NewManager(kubeDeps.KubeClient) klet.volumePluginMgr, err = NewInitializedVolumePluginMgr(klet, secretManager, configMapManager, tokenManager, kubeDeps.VolumePlugins, kubeDeps.DynamicPluginProber) if err != nil &#123; return nil, err &#125; klet.pluginManager = pluginmanager.NewPluginManager( klet.getPluginsRegistrationDir(), /* sockDir */ klet.getPluginsDir(), /* deprecatedSockDir */ kubeDeps.Recorder, ) if len(experimentalMounterPath) != 0 &#123; experimentalCheckNodeCapabilitiesBeforeMount = false klet.dnsConfigurer.SetupDNSinContainerizedMounter(experimentalMounterPath) &#125; klet.volumeManager = volumemanager.NewVolumeManager(......) // 12、初始化 workQueue、podWorkers、evictionManager klet.reasonCache = NewReasonCache() klet.workQueue = queue.NewBasicWorkQueue(klet.clock) klet.podWorkers = newPodWorkers(klet.syncPod, kubeDeps.Recorder, klet.workQueue, klet.resyncInterval, backOffPeriod, klet.podCache) klet.backOff = flowcontrol.NewBackOff(backOffPeriod, MaxContainerBackOff) klet.podKillingCh = make(chan *kubecontainer.PodPair, podKillingChannelCapacity) evictionManager, evictionAdmitHandler := eviction.NewManager(klet.resourceAnalyzer, evictionConfig, killPodNow(klet.podWorkers, kubeDeps.Recorder), klet.podManager.GetMirrorPodByPod, klet.imageManager, klet.containerGC, kubeDeps.Recorder, nodeRef, klet.clock) klet.evictionManager = evictionManager klet.admitHandlers.AddPodAdmitHandler(evictionAdmitHandler) if utilfeature.DefaultFeatureGate.Enabled(features.Sysctls) &#123; runtimeSupport, err := sysctl.NewRuntimeAdmitHandler(klet.containerRuntime) if err != nil &#123; return nil, err &#125; safeAndUnsafeSysctls := append(sysctlwhitelist.SafeSysctlWhitelist(), allowedUnsafeSysctls...) sysctlsWhitelist, err := sysctl.NewWhitelist(safeAndUnsafeSysctls) if err != nil &#123; return nil, err &#125; klet.admitHandlers.AddPodAdmitHandler(runtimeSupport) klet.admitHandlers.AddPodAdmitHandler(sysctlsWhitelist) &#125; // 13、为 pod 注册相关模块的 handler activeDeadlineHandler, err := newActiveDeadlineHandler(klet.statusManager, kubeDeps.Recorder, klet.clock) if err != nil &#123; return nil, err &#125; klet.AddPodSyncLoopHandler(activeDeadlineHandler) klet.AddPodSyncHandler(activeDeadlineHandler) if utilfeature.DefaultFeatureGate.Enabled(features.TopologyManager) &#123; klet.admitHandlers.AddPodAdmitHandler(klet.containerManager.GetTopologyPodAdmitHandler()) &#125; criticalPodAdmissionHandler := preemption.NewCriticalPodAdmissionHandler(klet.GetActivePods, killPodNow(klet.podWorkers, kubeDeps.Recorder),kubeDeps.Recorder) klet.admitHandlers.AddPodAdmitHandler(lifecycle.NewPredicateAdmitHandler(klet.getNodeAnyWay, criticalPodAdmissionHandler, klet.containerManager.UpdatePluginResources)) for _, opt := range kubeDeps.Options &#123; opt(klet) &#125; klet.appArmorValidator = apparmor.NewValidator(containerRuntime) klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewAppArmorAdmitHandler(klet.appArmorValidator)) klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewNoNewPrivsAdmitHandler(klet.containerRuntime)) if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123; klet.nodeLeaseController = nodelease.NewController(klet.clock, klet.heartbeatClient, string(klet.nodeName), kubeCfg.NodeLeaseDurationSeconds, klet.onRepeatedHeartbeatFailure) &#125; klet.softAdmitHandlers.AddPodAdmitHandler(lifecycle.NewProcMountAdmitHandler(klet.containerRuntime)) klet.kubeletConfiguration = *kubeCfg klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs() return klet, nil&#125; startKubelet在startKubelet 中通过调用 k.Run 来启动 kubelet 中的所有模块以及主流程，然后启动 kubelet 所需要的 http server，在 v1.16 中，kubelet 默认仅启动健康检查端口 10248 和 kubelet server 的端口 10250。 k8s.io/kubernetes/cmd/kubelet/app/server.go:1070 123456789101112131415161718func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableCAdvisorJSONEndpoints, enableServer bool) &#123; // start the kubelet go wait.Until(func() &#123; k.Run(podCfg.Updates()) &#125;, 0, wait.NeverStop) // start the kubelet server if enableServer &#123; go k.ListenAndServe(net.ParseIP(kubeCfg.Address), uint(kubeCfg.Port), kubeDeps.TLSOptions, kubeDeps.Auth, enableCAdvisorJSONEndpoints, kubeCfg. EnableDebuggingHandlers, kubeCfg.EnableContentionProfiling) &#125; if kubeCfg.ReadOnlyPort &gt; 0 &#123; go k.ListenAndServeReadOnly(net.ParseIP(kubeCfg.Address), uint(kubeCfg.ReadOnlyPort), enableCAdvisorJSONEndpoints) &#125; if utilfeature.DefaultFeatureGate.Enabled(features.KubeletPodResources) &#123; go k.ListenAndServePodResources() &#125;&#125; 至此，kubelet 对象以及其依赖模块在上面的几个方法中已经初始化完成了，除了单独启动了 gc 模块外其余的模块以及主逻辑最后都会在 Run 方法启动，Run 方法的主要逻辑在下文中会进行解释，此处总结一下 kubelet 启动逻辑中的调用关系如下所示： 1234567891011 |--&gt; NewMainKubelet | |--&gt; createAndInitKubelet --|--&gt; BirthCry | | |--&gt; RunKubelet --| |--&gt; StartGarbageCollection | | | |--&gt; startKubelet --&gt; k.Run |NewKubeletCommand --&gt; Run --&gt; run --|--&gt; http.ListenAndServe | |--&gt; daemon.SdNotify RunRun 方法是启动 kubelet 的核心方法，其中会启动 kubelet 的依赖模块以及主循环逻辑，该方法的主要逻辑为： 1、注册 logServer； 2、判断是否需要启动 cloud provider sync manager； 3、调用 kl.initializeModules 首先启动不依赖 container runtime 的一些模块； 4、启动 volume manager； 5、执行 kl.syncNodeStatus 定时同步 Node 状态； 6、调用 kl.fastStatusUpdateOnce 更新容器运行时启动时间以及执行首次状态同步； 7、判断是否启用 NodeLease 机制； 8、执行 kl.updateRuntimeUp 定时更新 Runtime 状态； 9、执行 kl.syncNetworkUtil 定时同步 iptables 规则； 10、执行 kl.podKiller 定时清理异常 pod，当 pod 没有被 podworker 正确处理的时候，启动一个goroutine 负责 kill 掉 pod； 11、启动 statusManager； 12、启动 probeManager； 13、启动 runtimeClassManager； 14、启动 pleg； 15、调用 kl.syncLoop 监听 pod 变化； 在 Run 方法中主要调用了两个方法 kl.initializeModules 和 kl.fastStatusUpdateOnce 来完成启动前的一些初始化，在初始化完所有的模块后会启动主循环。 k8s.io/kubernetes/pkg/kubelet/kubelet.go:1398 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123; // 1、注册 logServer if kl.logServer == nil &#123; kl.logServer = http.StripPrefix(&quot;/logs/&quot;, http.FileServer(http.Dir(&quot;/var/log/&quot;))) &#125; if kl.kubeClient == nil &#123; klog.Warning(&quot;No api server defined - no node status update will be sent.&quot;) &#125; // 2、判断是否需要启动 cloud provider sync manager if kl.cloudResourceSyncManager != nil &#123; go kl.cloudResourceSyncManager.Run(wait.NeverStop) &#125; // 3、调用 kl.initializeModules 首先启动不依赖 container runtime 的一些模块 if err := kl.initializeModules(); err != nil &#123; kl.recorder.Eventf(kl.nodeRef, v1.EventTypeWarning, events.KubeletSetupFailed, err.Error()) klog.Fatal(err) &#125; // 4、启动 volume manager go kl.volumeManager.Run(kl.sourcesReady, wait.NeverStop) if kl.kubeClient != nil &#123; // 5、执行 kl.syncNodeStatus 定时同步 Node 状态 go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop) // 6、调用 kl.fastStatusUpdateOnce 更新容器运行时启动时间以及执行首次状态同步 go kl.fastStatusUpdateOnce() // 7、判断是否启用 NodeLease 机制 if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123; go kl.nodeLeaseController.Run(wait.NeverStop) &#125; &#125; // 8、执行 kl.updateRuntimeUp 定时更新 Runtime 状态 go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) // 9、执行 kl.syncNetworkUtil 定时同步 iptables 规则 if kl.makeIPTablesUtilChains &#123; go wait.Until(kl.syncNetworkUtil, 1*time.Minute, wait.NeverStop) &#125; // 10、执行 kl.podKiller 定时清理异常 pod go wait.Until(kl.podKiller, 1*time.Second, wait.NeverStop) // 11、启动 statusManager、probeManager、runtimeClassManager kl.statusManager.Start() kl.probeManager.Start() if kl.runtimeClassManager != nil &#123; kl.runtimeClassManager.Start(wait.NeverStop) &#125; // 12、启动 pleg kl.pleg.Start() // 13、调用 kl.syncLoop 监听 pod 变化 kl.syncLoop(updates, kl)&#125; initializeModulesinitializeModules 中启动的模块是不依赖于 container runtime 的，并且不依赖于尚未初始化的模块，其主要逻辑为： 1、调用 kl.setupDataDirs 创建 kubelet 所需要的文件目录； 2、创建 ContainerLogsDir /var/log/containers； 3、启动 imageManager，image gc 的功能已经在 RunKubelet 中启动了，此处主要是监控 image 的变化； 4、启动 certificateManager，负责证书更新； 5、启动 oomWatcher，监听 oom 并记录事件； 6、启动 resourceAnalyzer； k8s.io/kubernetes/pkg/kubelet/kubelet.go:1319 1234567891011121314151617181920212223242526272829303132333435363738func (kl *Kubelet) initializeModules() error &#123; metrics.Register( kl.runtimeCache, collectors.NewVolumeStatsCollector(kl), collectors.NewLogMetricsCollector(kl.StatsProvider.ListPodStats), ) metrics.SetNodeName(kl.nodeName) servermetrics.Register() // 1、创建文件目录 if err := kl.setupDataDirs(); err != nil &#123; return err &#125; // 2、创建 ContainerLogsDir if _, err := os.Stat(ContainerLogsDir); err != nil &#123; if err := kl.os.MkdirAll(ContainerLogsDir, 0755); err != nil &#123; klog.Errorf(&quot;Failed to create directory %q: %v&quot;, ContainerLogsDir, err) &#125; &#125; // 3、启动 imageManager kl.imageManager.Start() // 4、启动 certificate manager if kl.serverCertificateManager != nil &#123; kl.serverCertificateManager.Start() &#125; // 5、启动 oomWatcher. if err := kl.oomWatcher.Start(kl.nodeRef); err != nil &#123; return fmt.Errorf(&quot;failed to start OOM watcher %v&quot;, err) &#125; // 6、启动 resource analyzer kl.resourceAnalyzer.Start() return nil&#125; fastStatusUpdateOncefastStatusUpdateOnce 会不断尝试更新 pod CIDR，一旦更新成功会立即执行updateRuntimeUp和syncNodeStatus来进行运行时的更新和节点状态更新。此方法只在 kubelet 启动时执行一次，目的是为了通过更新 pod CIDR，减少节点达到 ready 状态的时延，尽可能快的进行 runtime update 和 node status update。 k8s.io/kubernetes/pkg/kubelet/kubelet.go:2262 1234567891011121314151617181920func (kl *Kubelet) fastStatusUpdateOnce() &#123; for &#123; time.Sleep(100 * time.Millisecond) node, err := kl.GetNode() if err != nil &#123; klog.Errorf(err.Error()) continue &#125; if len(node.Spec.PodCIDRs) != 0 &#123; podCIDRs := strings.Join(node.Spec.PodCIDRs, &quot;,&quot;) if _, err := kl.updatePodCIDR(podCIDRs); err != nil &#123; klog.Errorf(&quot;Pod CIDR update to %v failed %v&quot;, podCIDRs, err) continue &#125; kl.updateRuntimeUp() kl.syncNodeStatus() return &#125; &#125;&#125; updateRuntimeUpupdateRuntimeUp 方法在容器运行时首次启动过程中初始化运行时依赖的模块，并在 kubelet 的runtimeState中更新容器运行时的启动时间。updateRuntimeUp 方法首先检查 network 以及 runtime 是否处于 ready 状态，如果 network 以及 runtime 都处于 ready 状态，然后调用 initializeRuntimeDependentModules 初始化 runtime 的依赖模块，包括 cadvisor、containerManager、evictionManager、containerLogManager、pluginManage等。 k8s.io/kubernetes/pkg/kubelet/kubelet.go:2168 123456789101112131415161718192021222324252627282930313233func (kl *Kubelet) updateRuntimeUp() &#123; kl.updateRuntimeMux.Lock() defer kl.updateRuntimeMux.Unlock() // 1、获取 containerRuntime Status s, err := kl.containerRuntime.Status() if err != nil &#123; klog.Errorf(&quot;Container runtime sanity check failed: %v&quot;, err) return &#125; if s == nil &#123; klog.Errorf(&quot;Container runtime status is nil&quot;) return &#125; // 2、检查 network 和 runtime 是否处于 ready 状态 networkReady := s.GetRuntimeCondition(kubecontainer.NetworkReady) if networkReady == nil || !networkReady.Status &#123; kl.runtimeState.setNetworkState(fmt.Errorf(&quot;runtime network not ready: %v&quot;, networkReady)) &#125; else &#123; kl.runtimeState.setNetworkState(nil) &#125; runtimeReady := s.GetRuntimeCondition(kubecontainer.RuntimeReady) if runtimeReady == nil || !runtimeReady.Status &#123; kl.runtimeState.setRuntimeState(err) return &#125; kl.runtimeState.setRuntimeState(nil) // 3、调用 kl.initializeRuntimeDependentModules 启动依赖模块 kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules) kl.runtimeState.setRuntimeSync(kl.clock.Now())&#125; initializeRuntimeDependentModules该方法的主要逻辑为： 1、启动 cadvisor； 2、获取 CgroupStats； 3、启动 containerManager、evictionManager、containerLogManager； 4、将 CSI Driver 和 Device Manager 注册到 pluginManager，然后启动 pluginManager； k8s.io/kubernetes/pkg/kubelet/kubelet.go:1361 1234567891011121314151617181920212223242526272829func (kl *Kubelet) initializeRuntimeDependentModules() &#123; // 1、启动 cadvisor if err := kl.cadvisor.Start(); err != nil &#123; ...... &#125; // 2、获取 CgroupStats kl.StatsProvider.GetCgroupStats(&quot;/&quot;, true) node, err := kl.getNodeAnyWay() if err != nil &#123; klog.Fatalf(&quot;Kubelet failed to get node info: %v&quot;, err) &#125; // 3、启动 containerManager、evictionManager、containerLogManager if err := kl.containerManager.Start(node, kl.GetActivePods, kl.sourcesReady, kl.statusManager, kl.runtimeService); err != nil &#123; klog.Fatalf(&quot;Failed to start ContainerManager %v&quot;, err) &#125; kl.evictionManager.Start(kl.StatsProvider, kl.GetActivePods, kl.podResourcesAreReclaimed, evictionMonitoringPeriod) kl.containerLogManager.Start() kl.pluginManager.AddHandler(pluginwatcherapi.CSIPlugin, plugincache.PluginHandler(csi.PluginHandler)) kl.pluginManager.AddHandler(pluginwatcherapi.DevicePlugin, kl.containerManager.GetPluginRegistrationHandler()) // 4、启动 pluginManager go kl.pluginManager.Run(kl.sourcesReady, wait.NeverStop)&#125; 小结在 Run 方法中可以看到，会直接调用 kl.syncNodeStatus和 kl.updateRuntimeUp，但在 kl.fastStatusUpdateOnce 中也调用了这两个方法，而在 kl.fastStatusUpdateOnce 中仅执行一次，在 Run 方法中会定期执行。在kl.fastStatusUpdateOnce 中调用的目的就是当 kubelet 首次启动时尽可能快的进行 runtime update 和 node status update，减少节点达到 ready 状态的时延。而在 kl.updateRuntimeUp 中调用的初始化 runtime 依赖模块的方法 kl.initializeRuntimeDependentModules 通过 sync.Once 调用仅仅会被执行一次。 syncLoopsyncLoop 是 kubelet 的主循环方法，它从不同的管道(file，http，apiserver)监听 pod 的变化，并把它们汇聚起来。当有新的变化发生时，它会调用对应的函数，保证 pod 处于期望的状态。 syncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现错误时，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。 k8s.io/kubernetes/pkg/kubelet/kubelet.go:1821 1234567891011121314151617181920212223242526func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123; syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for &#123; if err := kl.runtimeState.runtimeErrors(); err != nil &#123; time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue &#125; duration = base kl.syncLoopMonitor.Store(kl.clock.Now()) if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123; break &#125; kl.syncLoopMonitor.Store(kl.clock.Now()) &#125;&#125; syncLoopIterationsyncLoopIteration 方法会监听多个 channel，当发现任何一个 channel 有数据就交给 handler 去处理，在 handler 中通过调用 dispatchWork 分发任务。它会从以下几个 channel 中获取消息： 1、configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作； 2、syncCh：定时器，每隔一秒去同步最新保存的 pod 状态； 3、houseKeepingCh：housekeeping 事件的通道，做 pod 清理工作； 4、plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件； 5、liveness Manager：健康检查模块发现某个 pod 异常时，kubelet 将根据 pod 的 restartPolicy 自动执行正确的操作； k8s.io/kubernetes/pkg/kubelet/kubelet.go:1888 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566func (kl *Kubelet) syncLoopIteration(......) bool &#123; select &#123; case u, open := &lt;-configCh: if !open &#123; return false &#125; switch u.Op &#123; case kubetypes.ADD: handler.HandlePodAdditions(u.Pods) case kubetypes.UPDATE: handler.HandlePodUpdates(u.Pods) case kubetypes.REMOVE: handler.HandlePodRemoves(u.Pods) case kubetypes.RECONCILE: handler.HandlePodReconcile(u.Pods) case kubetypes.DELETE: handler.HandlePodUpdates(u.Pods) case kubetypes.RESTORE: handler.HandlePodAdditions(u.Pods) case kubetypes.SET: &#125; if u.Op != kubetypes.RESTORE &#123; kl.sourcesReady.AddSource(u.Source) &#125; case e := &lt;-plegCh: if isSyncPodWorthy(e) &#123; if pod, ok := kl.podManager.GetPodByUID(e.ID); ok &#123; klog.V(2).Infof(&quot;SyncLoop (PLEG): %q, event: %#v&quot;, format.Pod(pod), e) handler.HandlePodSyncs([]*v1.Pod&#123;pod&#125;) &#125; else &#123; klog.V(4).Infof(&quot;SyncLoop (PLEG): ignore irrelevant event: %#v&quot;, e) &#125; &#125; if e.Type == pleg.ContainerDied &#123; if containerID, ok := e.Data.(string); ok &#123; kl.cleanUpContainersInPod(e.ID, containerID) &#125; &#125; case &lt;-syncCh: podsToSync := kl.getPodsToSync() if len(podsToSync) == 0 &#123; break &#125; handler.HandlePodSyncs(podsToSync) case update := &lt;-kl.livenessManager.Updates(): if update.Result == proberesults.Failure &#123; pod, ok := kl.podManager.GetPodByUID(update.PodUID) if !ok &#123; break &#125; handler.HandlePodSyncs([]*v1.Pod&#123;pod&#125;) &#125; case &lt;-housekeepingCh: if !kl.sourcesReady.AllReady() &#123; klog.V(4).Infof(&quot;SyncLoop (housekeeping, skipped): sources aren&apos;t ready yet.&quot;) &#125; else &#123; if err := handler.HandlePodCleanups(); err != nil &#123; klog.Errorf(&quot;Failed cleaning pods: %v&quot;, err) &#125; &#125; &#125; return true&#125; 最后再总结一下启动 kubelet 以及其依赖模块 Run 方法中的调用流程： 1234567891011121314151617181920212223242526272829303132 |--&gt; kl.cloudResourceSyncManager.Run | | |--&gt; kl.setupDataDirs | |--&gt; kl.imageManager.StartRun --|--&gt; kl.initializeModules ---|--&gt; kl.serverCertificateManager.Start | |--&gt; kl.oomWatcher.Start | |--&gt; kl.resourceAnalyzer.Start | |--&gt; kl.volumeManager.Run | |--&gt; kl.containerRuntime.Status |--&gt; kl.syncNodeStatus | | |--&gt; kl.updateRuntimeUp --| |--&gt; kl.cadvisor.Start | | | | |--&gt; kl.fastStatusUpdateOnce --| |--&gt; kl.initializeRuntimeDependentModules --|--&gt; kl.containerManager.Start | | | | |--&gt; kl.syncNodeStatus |--&gt; kl.evictionManager.Start | | |--&gt; kl.updateRuntimeUp |--&gt; kl.containerLogManager.Start | | |--&gt; kl.syncNetworkUtil |--&gt; kl.pluginManager.Run | |--&gt; kl.podKiller | |--&gt; kl.statusManager.Start | |--&gt; kl.probeManager.Start | |--&gt; kl.runtimeClassManager.Start | |--&gt; kl.pleg.Start | |--&gt; kl.syncLoop --&gt; kl.syncLoopIteration 总结本文主要介绍了 kubelet 的启动流程，可以看到 kubelet 启动流程中的环节非常多，kubelet 中也包含了非常多的模块，后续在分享 kubelet 源码的文章中会先以 Run 方法中启动的所有模块为主，各个击破。]]></content>
      <tags>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[job controller 源码分析]]></title>
    <url>%2F2019%2F12%2F31%2Fjob_controller%2F</url>
    <content type="text"><![CDATA[job 在 kubernetes 中主要用来处理离线任务，job 直接管理 pod，可以创建一个或多个 pod 并会确保指定数量的 pod 运行完成。kubernetes 中有两种类型的 job，分别为 cronjob 和 batchjob，cronjob 类似于定时任务是定时触发的而 batchjob 创建后会直接运行，本文主要介绍 batchjob，下面简称为 job。 job 的基本功能创建job 的一个示例如下所示： 1234567891011121314151617181920apiVersion: batch/v1kind: Jobmetadata: name: pispec: backoffLimit: 6 // 标记为 failed 前的重试次数，默认为 6 completions: 4 // 要完成job 的 pod 数，若没有设定该值则默认等于 parallelism 的值 parallelism: 2 // 任意时间最多可以启动多少个 pod 同时运行，默认为 1 activeDeadlineSeconds: 120 // job 运行时间 ttlSecondsAfterFinished: 60 // job 在运行完成后 60 秒就会自动删除掉 template: spec: containers: - command: - sh - -c - &apos;echo &apos;&apos;scale=5000; 4*a(1)&apos;&apos; | bc -l &apos; image: resouer/ubuntu-bc name: pi restartPolicy: Never 扩缩容job 不支持运行时扩缩容，job 在创建后其 spec.completions 字段也不支持修改。 删除通常系统中已执行完成的 job 不再需要，将它们保留在系统中会占用一定的资源，需要进行回收，pod 在执行完任务后会进入到 Completed 状态，删除 job 也会清除其创建的 pod。 12345$ kubectl get podpi-gdrwr 0/1 Completed 0 10mpi-rjphf 0/1 Completed 0 10m$ kubectl delete job pi 自动清理机制每次 job 执行完成后手动回收非常麻烦，k8s 在 v1.12 版本中加入了 TTLAfterFinished feature-gates，启用该特性后会启动一个 TTL 控制器，在创建 job 时指定后可在 job 运行完成后自动回收相关联的 pod，如上文中的 yaml 所示，创建 job 时指定了 ttlSecondsAfterFinished: 60，job 在执行完成后停留 60s 会被自动回收， 若 ttlSecondsAfterFinished 设置为 0 则表示在 job 执行完成后立刻回收。当 TTL 控制器清理 job 时，它将级联删除 job，即 pod 和 job 一起被删除。不过该特性截止目前还是 Alpha 版本，请谨慎使用。 job controller 源码分析 kubernetes 版本：v1.16 在上节介绍了 job 的基本操作后，本节会继续深入源码了解其背后的设计与实现。 startJobController首先还是直接看 jobController 的启动方法 startJobController，该方法中调用 NewJobController 初始化 jobController 然后调用 Run 方法启动 jobController。从初始化流程中可以看到 JobController 监听 pod 和 job 两种资源，其中 ConcurrentJobSyncs 默认值为 5。 k8s.io/kubernetes/cmd/kube-controller-manager/app/batch.go:33 1234567891011func startJobController(ctx ControllerContext) (http.Handler, bool, error) &#123; if !ctx.AvailableResources[schema.GroupVersionResource&#123;Group: &quot;batch&quot;, Version: &quot;v1&quot;, Resource: &quot;jobs&quot;&#125;] &#123; return nil, false, nil &#125; go job.NewJobController( ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Batch().V1().Jobs(), ctx.ClientBuilder.ClientOrDie(&quot;job-controller&quot;), ).Run(int(ctx.ComponentConfig.JobController.ConcurrentJobSyncs), ctx.Stop) return nil, true, nil&#125; Run以下是 jobController 的 Run 方法，其中核心逻辑是调用 jm.worker 执行 syncLoop 操作，worker 方法是 syncJob 方法的别名，最终调用的是 syncJob。 k8s.io/kubernetes/pkg/controller/job/job_controller.go:139 1234567891011121314151617func (jm *JobController) Run(workers int, stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() defer jm.queue.ShutDown() klog.Infof(&quot;Starting job controller&quot;) defer klog.Infof(&quot;Shutting down job controller&quot;) if !cache.WaitForNamedCacheSync(&quot;job&quot;, stopCh, jm.podStoreSynced, jm.jobStoreSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; go wait.Until(jm.worker, time.Second, stopCh) &#125; &lt;-stopCh&#125; syncJobsyncJob 是 jobController 的核心方法，其主要逻辑为： 1、从 lister 中获取 job 对象； 2、判断 job 是否已经执行完成，当 job 的 .status.conditions 中有 Complete 或 Failed 的 type 且对应的 status 为 true 时表示该 job 已经执行完成，例如： 123456789status: completionTime: &quot;2019-12-18T14:16:47Z&quot; conditions: - lastProbeTime: &quot;2019-12-18T14:16:47Z&quot; lastTransitionTime: &quot;2019-12-18T14:16:47Z&quot; status: &quot;True&quot; // status 为 true type: Complete // Complete startTime: &quot;2019-12-18T14:15:35Z&quot; succeeded: 2 3、获取 job 重试的次数； 4、调用 jm.expectations.SatisfiedExpectations 判断 job 是否需能进行 sync 操作，Expectations 机制在之前写的” ReplicaSetController 源码分析“一文中详细讲解过，其主要判断条件如下： 1、该 key 在 ControllerExpectations 中的 adds 和 dels 都 &lt;= 0，即调用 apiserver 的创建和删除接口没有失败过； 2、该 key 在 ControllerExpectations 中已经超过 5min 没有更新了； 3、该 key 在 ControllerExpectations 中不存在，即该对象是新创建的； 4、调用 GetExpectations 方法失败，内部错误； 5、调用 jm.getPodsForJob 通过 selector 获取 job 关联的 pod，若有孤儿 pod 的 label 与 job 的能匹配则进行关联，若已关联的 pod label 有变化则解除与 job 的关联关系； 6、分别计算 active、succeeded、failed 状态的 pod 数； 7、判断 job 是否为首次启动，若首次启动其 job.Status.StartTime 为空，此时首先设置 startTime，然后检查是否有 job.Spec.ActiveDeadlineSeconds 是否为空，若不为空则将其再加入到延迟队列中，等待 ActiveDeadlineSeconds 时间后会再次触发 sync 操作； 8、判断 job 的重试次数是否超过了 job.Spec.BackoffLimit(默认是6次)，有两个判断方法一是 job 的重试次数以及 job 的状态，二是当 job 的 restartPolicy 为 OnFailure 时 container 的重启次数，两者任一个符合都说明 job 处于 failed 状态且原因为 BackoffLimitExceeded； 9、判断 job 的运行时间是否达到 job.Spec.ActiveDeadlineSeconds 中设定的值，若已达到则说明 job 此时处于 failed 状态且原因为 DeadlineExceeded； 10、根据以上判断如果 job 处于 failed 状态，则调用 jm.deleteJobPods 并发删除所有 active pods ； 11、若非 failed 状态，根据 jobNeedsSync 判断是否要进行同步，若需要同步则调用 jm.manageJob 进行同步； 12、通过检查 job.Spec.Completions 判断 job 是否已经运行完成，若 job.Spec.Completions 字段没有设置值则只要有一个 pod 运行完成该 job 就为 Completed 状态，若设置了 job.Spec.Completions 会通过判断已经运行完成状态的 pod 即 succeeded pod 数是否大于等于该值； 13、通过以上判断若 job 运行完成了，则更新 job.Status.Conditions 和 job.Status.CompletionTime 字段； 14、如果 job 的 status 有变化，将 job 的 status 更新到 apiserver； 在 syncJob 中又调用了 jm.manageJob 处理非 failed 状态下的 sync 操作，下面主要分析一下该方法。 k8s.io/kubernetes/pkg/controller/job/job_controller.go:436 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156func (jm *JobController) syncJob(key string) (bool, error) &#123; // 1、计算每次 sync 的运行时间 startTime := time.Now() defer func() &#123; klog.V(4).Infof(&quot;Finished syncing job %q (%v)&quot;, key, time.Since(startTime)) &#125;() ns, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return false, err &#125; if len(ns) == 0 || len(name) == 0 &#123; return false, fmt.Errorf(&quot;invalid job key %q: either namespace or name is missing&quot;, key) &#125; // 2、从 lister 中获取 job 对象 sharedJob, err := jm.jobLister.Jobs(ns).Get(name) if err != nil &#123; if errors.IsNotFound(err) &#123; klog.V(4).Infof(&quot;Job has been deleted: %v&quot;, key) jm.expectations.DeleteExpectations(key) return true, nil &#125; return false, err &#125; job := *sharedJob // 3、判断 job 是否已经执行完成 if IsJobFinished(&amp;job) &#123; return true, nil &#125; // 4、获取 job 重试的次数 previousRetry := jm.queue.NumRequeues(key) // 5、判断 job 是否能进行 sync 操作 jobNeedsSync := jm.expectations.SatisfiedExpectations(key) // 6、获取 job 关联的所有 pod pods, err := jm.getPodsForJob(&amp;job) if err != nil &#123; return false, err &#125; // 7、分别计算 active、succeeded、failed 状态的 pod 数 activePods := controller.FilterActivePods(pods) active := int32(len(activePods)) succeeded, failed := getStatus(pods) conditions := len(job.Status.Conditions) // 8、判断 job 是否为首次启动 if job.Status.StartTime == nil &#123; now := metav1.Now() job.Status.StartTime = &amp;now // 9、判断是否设定了 ActiveDeadlineSeconds 值 if job.Spec.ActiveDeadlineSeconds != nil &#123; klog.V(4).Infof(&quot;Job %s have ActiveDeadlineSeconds will sync after %d seconds&quot;, key, *job.Spec.ActiveDeadlineSeconds) jm.queue.AddAfter(key, time.Duration(*job.Spec.ActiveDeadlineSeconds)*time.Second) &#125; &#125; var manageJobErr error jobFailed := false var failureReason string var failureMessage string // 10、判断 job 的重启次数是否已达到上限，即处于 BackoffLimitExceeded jobHaveNewFailure := failed &gt; job.Status.Failed exceedsBackoffLimit := jobHaveNewFailure &amp;&amp; (active != *job.Spec.Parallelism) &amp;&amp; (int32(previousRetry)+1 &gt; *job.Spec.BackoffLimit) if exceedsBackoffLimit || pastBackoffLimitOnFailure(&amp;job, pods) &#123; jobFailed = true failureReason = &quot;BackoffLimitExceeded&quot; failureMessage = &quot;Job has reached the specified backoff limit&quot; &#125; else if pastActiveDeadline(&amp;job) &#123; jobFailed = true failureReason = &quot;DeadlineExceeded&quot; failureMessage = &quot;Job was active longer than specified deadline&quot; &#125; // 11、如果处于 failed 状态，则调用 jm.deleteJobPods 并发删除所有 active pods if jobFailed &#123; errCh := make(chan error, active) jm.deleteJobPods(&amp;job, activePods, errCh) select &#123; case manageJobErr = &lt;-errCh: if manageJobErr != nil &#123; break &#125; default: &#125; failed += active active = 0 job.Status.Conditions = append(job.Status.Conditions, newCondition(batch.JobFailed, failureReason, failureMessage)) jm.recorder.Event(&amp;job, v1.EventTypeWarning, failureReason, failureMessage) &#125; else &#123; // 12、若非 failed 状态，根据 jobNeedsSync 判断是否要进行同步 if jobNeedsSync &amp;&amp; job.DeletionTimestamp == nil &#123; active, manageJobErr = jm.manageJob(activePods, succeeded, &amp;job) &#125; // 13、检查 job.Spec.Completions 判断 job 是否已经运行完成 completions := succeeded complete := false if job.Spec.Completions == nil &#123; if succeeded &gt; 0 &amp;&amp; active == 0 &#123; complete = true &#125; &#125; else &#123; if completions &gt;= *job.Spec.Completions &#123; complete = true if active &gt; 0 &#123; jm.recorder.Event(&amp;job, v1.EventTypeWarning, &quot;TooManyActivePods&quot;, &quot;Too many active pods running after completion count reached&quot;) &#125; if completions &gt; *job.Spec.Completions &#123; jm.recorder.Event(&amp;job, v1.EventTypeWarning, &quot;TooManySucceededPods&quot;, &quot;Too many succeeded pods running after completion count reached&quot;) &#125; &#125; &#125; // 14、若 job 运行完成了，则更新 job.Status.Conditions 和 job.Status.CompletionTime 字段 if complete &#123; job.Status.Conditions = append(job.Status.Conditions, newCondition(batch.JobComplete, &quot;&quot;, &quot;&quot;)) now := metav1.Now() job.Status.CompletionTime = &amp;now &#125; &#125; forget := false if job.Status.Succeeded &lt; succeeded &#123; forget = true &#125; // 15、如果 job 的 status 有变化，将 job 的 status 更新到 apiserver if job.Status.Active != active || job.Status.Succeeded != succeeded || job.Status.Failed != failed || len(job.Status.Conditions) != conditions &#123; job.Status.Active = active job.Status.Succeeded = succeeded job.Status.Failed = failed if err := jm.updateHandler(&amp;job); err != nil &#123; return forget, err &#125; if jobHaveNewFailure &amp;&amp; !IsJobFinished(&amp;job) &#123; return forget, fmt.Errorf(&quot;failed pod(s) detected for job key %q&quot;, key) &#125; forget = true &#125; return forget, manageJobErr&#125; jm.manageJobjm.manageJob它主要做的事情就是根据 job 配置的并发数来确认当前处于 active 的 pods 数量是否合理，如果不合理的话则进行调整，其主要逻辑为： 1、首先获取 job 的 active pods 数与可运行的 pod 数即 job.Spec.Parallelism； 2、判断如果处于 active 状态的 pods 数大于 job 设置的并发数 job.Spec.Parallelism，则并发删除多余的 active pods，需要删除的 active pods 是有一定的优先级的，删除的优先级为： 1、判断是否绑定了 node：Unassigned &lt; assigned； 2、判断 pod phase：PodPending &lt; PodUnknown &lt; PodRunning； 3、判断 pod 状态：Not ready &lt; ready； 4、若 pod 都为 ready，则按运行时间排序，运行时间最短会被删除：empty time &lt; less time &lt; more time； 5、根据 pod 重启次数排序：higher restart counts &lt; lower restart counts； 6、按 pod 创建时间进行排序：Empty creation time pods &lt; newer pods &lt; older pods； 3、若处于 active 状态的 pods 数小于 job 设置的并发数，则需要根据 job 的配置计算 pod 的 diff 数并进行创建，计算方法与 completions、parallelism 以及 succeeded 的 pods 数有关，计算出 diff 数后会进行批量创建，创建的 pod 数依次为 1、2、4、8……，呈指数级增长，job 创建 pod 的方式与 rs 创建 pod 是类似的，但是此处并没有限制在一个 syncLoop 中创建 pod 的上限值，创建完 pod 后会将结果记录在 job 的 expectations 中，此处并非所有的 pod 都能创建成功，若超时错误会直接忽略，因其他错误创建失败的 pod 会记录在 expectations 中，expectations 机制的主要目的是减少不必要的 sync 操作，至于其详细的说明可以参考之前写的 ” ReplicaSetController 源码分析“ 一文； k8s.io/kubernetes/pkg/controller/job/job_controller.go:684 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124func (jm *JobController) manageJob(activePods []*v1.Pod, succeeded int32, job *batch.Job) (int32, error) &#123; // 1、获取 job 的 active pods 数与可运行的 pod 数 var activeLock sync.Mutex active := int32(len(activePods)) parallelism := *job.Spec.Parallelism jobKey, err := controller.KeyFunc(job) if err != nil &#123; utilruntime.HandleError(fmt.Errorf(&quot;Couldn&apos;t get key for job %#v: %v&quot;, job, err)) return 0, nil &#125; var errCh chan error // 2、如果处于 active 状态的 pods 数大于 job 设置的并发数 if active &gt; parallelism &#123; diff := active - parallelism errCh = make(chan error, diff) jm.expectations.ExpectDeletions(jobKey, int(diff)) klog.V(4).Infof(&quot;Too many pods running job %q, need %d, deleting %d&quot;, jobKey, parallelism, diff) // 3、对 activePods 按以上 6 种策略进行排序 sort.Sort(controller.ActivePods(activePods)) // 4、并发删除多余的 active pods active -= diff wait := sync.WaitGroup&#123;&#125; wait.Add(int(diff)) for i := int32(0); i &lt; diff; i++ &#123; go func(ix int32) &#123; defer wait.Done() if err := jm.podControl.DeletePod(job.Namespace, activePods[ix].Name, job); err != nil &#123; defer utilruntime.HandleError(err) klog.V(2).Infof(&quot;Failed to delete %v, decrementing expectations for job %q/%q&quot;, activePods[ix].Name, job.Namespace, job.Name) jm.expectations.DeletionObserved(jobKey) activeLock.Lock() active++ activeLock.Unlock() errCh &lt;- err &#125; &#125;(i) &#125; wait.Wait() // 5、若处于 active 状态的 pods 数小于 job 设置的并发数，则需要创建出新的 pod &#125; else if active &lt; parallelism &#123; // 6、首先计算出 diff 数 // 若 job.Spec.Completions == nil &amp;&amp; succeeded pods &gt; 0, 则diff = 0; // 若 job.Spec.Completions == nil &amp;&amp; succeeded pods = 0，则diff = Parallelism; // 若 job.Spec.Completions != nil 则diff等于(job.Spec.Completions - succeeded - active)和 parallelism 中的最小值(非负值)； wantActive := int32(0) if job.Spec.Completions == nil &#123; if succeeded &gt; 0 &#123; wantActive = active &#125; else &#123; wantActive = parallelism &#125; &#125; else &#123; wantActive = *job.Spec.Completions - succeeded if wantActive &gt; parallelism &#123; wantActive = parallelism &#125; &#125; diff := wantActive - active if diff &lt; 0 &#123; utilruntime.HandleError(fmt.Errorf(&quot;More active than wanted: job %q, want %d, have %d&quot;, jobKey, wantActive, active)) diff = 0 &#125; if diff == 0 &#123; return active, nil &#125; jm.expectations.ExpectCreations(jobKey, int(diff)) errCh = make(chan error, diff) klog.V(4).Infof(&quot;Too few pods running job %q, need %d, creating %d&quot;, jobKey, wantActive, diff) active += diff wait := sync.WaitGroup&#123;&#125; // 7、批量创建 pod，呈指数级增长 for batchSize := int32(integer.IntMin(int(diff), controller.SlowStartInitialBatchSize)); diff &gt; 0; batchSize = integer.Int32Min(2*batchSize, diff) &#123; errorCount := len(errCh) wait.Add(int(batchSize)) for i := int32(0); i &lt; batchSize; i++ &#123; go func() &#123; defer wait.Done() err := jm.podControl.CreatePodsWithControllerRef(job.Namespace, &amp;job.Spec.Template, job, metav1.NewControllerRef(job, controllerKind)) // 8、调用 apiserver 创建时忽略 Timeout 错误 if err != nil &amp;&amp; errors.IsTimeout(err) &#123; return &#125; if err != nil &#123; defer utilruntime.HandleError(err) klog.V(2).Infof(&quot;Failed creation, decrementing expectations for job %q/%q&quot;, job.Namespace, job.Name) jm.expectations.CreationObserved(jobKey) activeLock.Lock() active-- activeLock.Unlock() errCh &lt;- err &#125; &#125;() &#125; wait.Wait() // 9、若有创建失败的操作记录在 expectations 中 skippedPods := diff - batchSize if errorCount &lt; len(errCh) &amp;&amp; skippedPods &gt; 0 &#123; klog.V(2).Infof(&quot;Slow-start failure. Skipping creation of %d pods, decrementing expectations for job %q/%q&quot;, skippedPods, job.Namespace, job.Name) active -= skippedPods for i := int32(0); i &lt; skippedPods; i++ &#123; jm.expectations.CreationObserved(jobKey) &#125; break &#125; diff -= batchSize &#125; &#125; select &#123; case err := &lt;-errCh: if err != nil &#123; return active, err &#125; default: &#125; return active, nil&#125; 总结以上就是 jobController 源码中主要的逻辑，从上文分析可以看到 jobController 的代码比较清晰，若看过前面写的几个 controller 分析会发现每个 controller 在功能实现上有很多类似的地方。]]></content>
      <tags>
        <tag>kube-controller-manager</tag>
        <tag>job controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[garbage collector controller 源码分析]]></title>
    <url>%2F2019%2F12%2F28%2Fgarbagecollector_controller%2F</url>
    <content type="text"><![CDATA[在前面几篇关于 controller 源码分析的文章中多次提到了当删除一个对象时，其对应的 controller 并不会执行删除对象的操作，在 kubernetes 中对象的回收操作是由 GarbageCollectorController 负责的，其作用就是当删除一个对象时，会根据指定的删除策略回收该对象及其依赖对象，本文会深入分析垃圾收集背后的实现。 kubernetes 中的删除策略kubernetes 中有三种删除策略：Orphan、Foreground 和 Background，三种删除策略的意义分别为： Orphan 策略：非级联删除，删除对象时，不会自动删除它的依赖或者是子对象，这些依赖被称作是原对象的孤儿对象，例如当执行以下命令时会使用 Orphan 策略进行删除，此时 ds 的依赖对象 controllerrevision 不会被删除； 1$ kubectl delete ds/nginx-ds --cascade=false Background 策略：在该模式下，kubernetes 会立即删除该对象，然后垃圾收集器会在后台删除这些该对象的依赖对象； Foreground 策略：在该模式下，对象首先进入“删除中”状态，即会设置对象的 deletionTimestamp 字段并且对象的 metadata.finalizers 字段包含了值 “foregroundDeletion”，此时该对象依然存在，然后垃圾收集器会删除该对象的所有依赖对象，垃圾收集器在删除了所有“Blocking” 状态的依赖对象（指其子对象中 ownerReference.blockOwnerDeletion=true的对象）之后，然后才会删除对象本身； 在 v1.9 以前的版本中，大部分 controller 默认的删除策略为 Orphan，从 v1.9 开始，对于 apps/v1 下的资源默认使用 Background 模式。以上三种删除策略都可以在删除对象时通过设置 deleteOptions.propagationPolicy 字段进行指定，如下所示： 1$ curl -k -v -XDELETE -H &quot;Accept: application/json&quot; -H &quot;Content-Type: application/json&quot; -d &apos;&#123;&quot;propagationPolicy&quot;:&quot;Foreground&quot;&#125;&apos; &apos;https://192.168.99.108:8443/apis/apps/v1/namespaces/default/daemonsets/nginx-ds&apos; finalizer 机制finalizer 是在删除对象时设置的一个 hook，其目的是为了让对象在删除前确认其子对象已经被完全删除，k8s 中默认有两种 finalizer：OrphanFinalizer 和 ForegroundFinalizer，finalizer 存在于对象的 ObjectMeta 中，当一个对象的依赖对象被删除后其对应的 finalizers 字段也会被移除，只有 finalizers 字段为空时，apiserver 才会删除该对象。 12345678910&#123; ...... &quot;metadata&quot;: &#123; ...... &quot;finalizers&quot;: [ &quot;foregroundDeletion&quot; ] &#125; ......&#125; 此外，finalizer 不仅仅支持以上两种字段，在使用自定义 controller 时也可以在 CR 中设置自定义的 finalizer 标识。 GarbageCollectorController 源码分析 kubernetes 版本：v1.16 GarbageCollectorController 负责回收 kubernetes 中的资源，要回收 kubernetes 中所有资源首先得监控所有资源，GarbageCollectorController 会监听集群中所有可删除资源产生的所有事件，这些事件会被放入到一个队列中，然后 controller 会启动多个 goroutine 处理队列中的事件，若为删除事件会根据对象的删除策略删除关联的对象，对于非删除事件会更新对象之间的依赖关系。 startGarbageCollectorController首先还是看 GarbageCollectorController 的启动方法 startGarbageCollectorController，其主要逻辑为： 1、初始化 discoveryClient，discoveryClient 主要用来获取集群中的所有资源； 2、调用 garbagecollector.GetDeletableResources 获取集群内所有可删除的资源对象，支持 “delete”, “list”, “watch” 三种操作的 resource 称为 deletableResource； 3、调用 garbagecollector.NewGarbageCollector 初始化 garbageCollector 对象； 4、调用 garbageCollector.Run 启动 garbageCollector； 5、调用 garbageCollector.Sync 监听集群中的 DeletableResources ，当出现新的 DeletableResources 时同步到 monitors 中，确保监控集群中的所有资源； 6、调用 garbagecollector.NewDebugHandler 注册 debug 接口，用来提供集群内所有对象的关联关系； k8s.io/kubernetes/cmd/kube-controller-manager/app/core.go:443 1234567891011121314151617181920212223242526272829303132333435363738func startGarbageCollectorController(ctx ControllerContext) (http.Handler, bool, error) &#123; if !ctx.ComponentConfig.GarbageCollectorController.EnableGarbageCollector &#123; return nil, false, nil &#125; // 1、初始化 discoveryClient gcClientset := ctx.ClientBuilder.ClientOrDie(&quot;generic-garbage-collector&quot;) discoveryClient := cacheddiscovery.NewMemCacheClient(gcClientset.Discovery()) config := ctx.ClientBuilder.ConfigOrDie(&quot;generic-garbage-collector&quot;) metadataClient, err := metadata.NewForConfig(config) if err != nil &#123; return nil, true, err &#125; // 2、获取 deletableResource deletableResources := garbagecollector.GetDeletableResources(discoveryClient) ignoredResources := make(map[schema.GroupResource]struct&#123;&#125;) for _, r := range ctx.ComponentConfig.GarbageCollectorController.GCIgnoredResources &#123; ignoredResources[schema.GroupResource&#123;Group: r.Group, Resource: r.Resource&#125;] = struct&#123;&#125;&#123;&#125; &#125; // 3、初始化 garbageCollector 对象 garbageCollector, err := garbagecollector.NewGarbageCollector( ...... ) if err != nil &#123; return nil, true, fmt.Errorf(&quot;failed to start the generic garbage collector: %v&quot;, err) &#125; // 4、启动 garbage collector workers := int(ctx.ComponentConfig.GarbageCollectorController.ConcurrentGCSyncs) go garbageCollector.Run(workers, ctx.Stop) // 5、监听集群中的 DeletableResources go garbageCollector.Sync(gcClientset.Discovery(), 30*time.Second, ctx.Stop) // 6、注册 debug 接口 return garbagecollector.NewDebugHandler(garbageCollector), true, nil&#125; 在 startGarbageCollectorController 中主要调用了四种方法garbagecollector.NewGarbageCollector、garbageCollector.Run、garbageCollector.Sync 和 garbagecollector.NewDebugHandler 来完成核心功能，下面主要针对这四种方法进行说明。 garbagecollector.NewGarbageCollectorNewGarbageCollector 的主要功能是初始化 GarbageCollector 和 GraphBuilder 对象，并调用 gb.syncMonitors方法初始化 deletableResources 中所有 resource controller 的 informer。GarbageCollector 的主要作用是启动 GraphBuilder 以及启动所有的消费者，GraphBuilder 的主要作用是启动所有的生产者。 k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:74 123456789101112131415func NewGarbageCollector(......) (*GarbageCollector, error) &#123; ...... gc := &amp;GarbageCollector&#123; ...... &#125; gb := &amp;GraphBuilder&#123; ...... &#125; if err := gb.syncMonitors(deletableResources); err != nil &#123; utilruntime.HandleError(fmt.Errorf(&quot;failed to sync all monitors: %v&quot;, err)) &#125; gc.dependencyGraphBuilder = gb return gc, nil&#125; gb.syncMonitorssyncMonitors 的主要作用是初始化各个资源对象的 informer，并调用 gb.controllerFor 为每种资源注册 eventHandler，此处每种资源被称为 monitors，因为为每种资源注册 eventHandler 时，对于 AddFunc、UpdateFunc 和 DeleteFunc 都会将对应的 event push 到 graphChanges 队列中，每种资源对象的 informer 都作为生产者。 k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:179 123456789101112131415161718192021222324252627282930313233func (gb *GraphBuilder) syncMonitors(resources map[schema.GroupVersionResource]struct&#123;&#125;) error &#123; gb.monitorLock.Lock() defer gb.monitorLock.Unlock() ...... for resource := range resources &#123; if _, ok := gb.ignoredResources[resource.GroupResource()]; ok &#123; continue &#125; ...... kind, err := gb.restMapper.KindFor(resource) if err != nil &#123; errs = append(errs, fmt.Errorf(&quot;couldn&apos;t look up resource %q: %v&quot;, resource, err)) continue &#125; // 为 resource 的 controller 注册 eventHandler c, s, err := gb.controllerFor(resource, kind) if err != nil &#123; errs = append(errs, fmt.Errorf(&quot;couldn&apos;t start monitor for resource %q: %v&quot;, resource, err)) continue &#125; current[resource] = &amp;monitor&#123;store: s, controller: c&#125; added++ &#125; gb.monitors = current for _, monitor := range toRemove &#123; if monitor.stopCh != nil &#123; close(monitor.stopCh) &#125; &#125; return utilerrors.NewAggregate(errs)&#125; gb.controllerFor在 gb.controllerFor中主要是为每个 deletableResources 的 informer 注册 eventHandler，此处就可以看到真正的生产者了。 k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:127 1234567891011121314151617181920212223242526272829303132333435363738394041func (gb *GraphBuilder) controllerFor(resource schema.GroupVersionResource, kind schema.GroupVersionKind) (cache.Controller, cache.Store, error) &#123; handlers := cache.ResourceEventHandlerFuncs&#123; AddFunc: func(obj interface&#123;&#125;) &#123; event := &amp;event&#123; eventType: addEvent, obj: obj, gvk: kind, &#125; // 将对应的 event push 到 graphChanges 队列中 gb.graphChanges.Add(event) &#125;, UpdateFunc: func(oldObj, newObj interface&#123;&#125;) &#123; event := &amp;event&#123; eventType: updateEvent, obj: newObj, oldObj: oldObj, gvk: kind, &#125; // 将对应的 event push 到 graphChanges 队列中 gb.graphChanges.Add(event) &#125;, DeleteFunc: func(obj interface&#123;&#125;) &#123; if deletedFinalStateUnknown, ok := obj.(cache.DeletedFinalStateUnknown); ok &#123; obj = deletedFinalStateUnknown.Obj &#125; event := &amp;event&#123; eventType: deleteEvent, obj: obj, gvk: kind, &#125; // 将对应的 event push 到 graphChanges 队列中 gb.graphChanges.Add(event) &#125;, &#125; shared, err := gb.sharedInformers.ForResource(resource) if err != nil &#123; return nil, nil, err &#125; shared.Informer().AddEventHandlerWithResyncPeriod(handlers, ResourceResyncTime) return shared.Informer().GetController(), shared.Informer().GetStore(), nil&#125; 至此 NewGarbageCollector 的功能已经分析完了，在 NewGarbageCollector 中初始化了两个对象 GarbageCollector 和 GraphBuilder，然后在 gb.syncMonitors 中初始化了所有 deletableResources 的 informer，为每个 informer 添加 eventHandler 并将监听到的所有 event push 到 graphChanges 队列中，此处每个 informer 都被称为 monitor，所有 informer 都被称为生产者。graphChanges 是 GraphBuilder 中的一个对象，GraphBuilder 的主要功能是作为一个生产者，其会处理 graphChanges 中的所有事件并进行分类，将事件放入到 attemptToDelete 和 attemptToOrphan 两个队列中，具体处理逻辑下文讲述。 NewGarbageCollector 中的调用逻辑如下所示： 123456789101112131415161718192021 |--&gt; ctx.ClientBuilder. | ClientOrDie | | |--&gt; cacheddiscovery. | NewMemCacheClient | |--&gt; gb.sharedInformers. | | ForResource | |startGarbage ----|--&gt; garbagecollector. --&gt; gb.syncMonitors --&gt; gb.controllerFor --|CollectorController | NewGarbageCollector | | | | |--&gt; shared.Informer(). | AddEventHandlerWithResyncPeriod |--&gt; garbageCollector.Run | | |--&gt; garbageCollector.Sync | | |--&gt; garbagecollector.NewDebugHandler garbageCollector.Run上文已经详述了 NewGarbageCollector 的主要功能，然后继续分析 startGarbageCollectorController 中的第二个核心方法 garbageCollector.Run，garbageCollector.Run 的主要作用是启动所有的生产者和消费者，其首先会调用 gc.dependencyGraphBuilder.Run 启动所有的生产者，即 monitors，然后再启动一个 goroutine 处理 graphChanges 队列中的事件并分别放到 attemptToDelete 和 attemptToOrphan 两个队列中，dependencyGraphBuilder 即上文提到的 GraphBuilder，run 方法会调用 gc.runAttemptToDeleteWorker 和 gc.runAttemptToOrphanWorker 启动多个 goroutine 处理 attemptToDelete 和 attemptToOrphan 两个队列中的事件。 k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:124 12345678910111213141516171819202122232425func (gc *GarbageCollector) Run(workers int, stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() defer gc.attemptToDelete.ShutDown() defer gc.attemptToOrphan.ShutDown() defer gc.dependencyGraphBuilder.graphChanges.ShutDown() defer klog.Infof(&quot;Shutting down garbage collector controller&quot;) // 1、调用 gc.dependencyGraphBuilder.Run 启动所有的 monitors 即 informers，并且启动一个 goroutine 处理 graphChanges 中的事件将其分别放到 GraphBuilder 的 attemptToDelete 和 attemptToOrphan 两个 队列中； go gc.dependencyGraphBuilder.Run(stopCh) // 2、等待 informers 的 cache 同步完成 if !cache.WaitForNamedCacheSync(&quot;garbage collector&quot;, stopCh, gc.dependencyGraphBuilder.IsSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; // 3、启动多个 goroutine 调用 gc.runAttemptToDeleteWorker 处理 attemptToDelete 中的事件 go wait.Until(gc.runAttemptToDeleteWorker, 1*time.Second, stopCh) // 4、启动多个 goroutine 调用 gc.runAttemptToOrphanWorker 处理 attemptToDelete 中的事件 go wait.Until(gc.runAttemptToOrphanWorker, 1*time.Second, stopCh) &#125; &lt;-stopCh&#125; Run 方法中调用了 gc.dependencyGraphBuilder.Run 来完成 GraphBuilder 的启动。 gc.dependencyGraphBuilder.RunGraphBuilder 在 garbageCollector 整个环节中起到承上启下的作用，首先看一下 GraphBuilder 对象的结构： 123456789101112131415161718192021222324252627282930313233type GraphBuilder struct &#123; restMapper meta.RESTMapper // informers monitors monitors monitorLock sync.RWMutex // 当 kube-controller-manager 中所有的 controllers 都启动后，informersStarted 会被 close 掉 // informersStarted 会被 close 掉的调用程序在 kube-controller-manager 的启动流程中 informersStarted &lt;-chan struct&#123;&#125; stopCh &lt;-chan struct&#123;&#125; // 当调用 GraphBuilder 的 run 方法时，running 会被设置为 true running bool metadataClient metadata.Interface // informers 监听到的事件会放在 graphChanges 中 graphChanges workqueue.RateLimitingInterface // 维护所有对象的依赖关系 uidToNode *concurrentUIDToNode // GarbageCollector 作为消费者要处理 attemptToDelete 和 attemptToOrphan 两个队列中的事件 attemptToDelete workqueue.RateLimitingInterface attemptToOrphan workqueue.RateLimitingInterface absentOwnerCache *UIDCache sharedInformers controller.InformerFactory // 不需要被 gc 的资源 ignoredResources map[schema.GroupResource]struct&#123;&#125;&#125; uidToNode此处有必要先说明一下 uidToNode 的功能，uidToNode 数据结构中维护着所有对象的依赖关系，此处的依赖关系是指比如当创建一个 deployment 时会创建对应的 rs 以及 pod，pod 的 owner 就是 rs，rs 的 owner 是 deployment，rs 的 dependents 是其关联的所有 pod，deployment 的 dependents 是其关联的所有 rs。 uidToNode 中的 node 不是指 k8s 中的 node 节点，而是将 graphChanges 中的 event 转换为 node 对象，k8s 中所有 object 之间的级联关系是通过 node 的概念来维护的，garbageCollector 在后续的处理中会直接使用 node 对象，node 对象定义如下： 12345678910111213141516171819202122232425type concurrentUIDToNode struct &#123; uidToNodeLock sync.RWMutex uidToNode map[types.UID]*node&#125;type node struct &#123; identity objectReference dependentsLock sync.RWMutex // 其依赖项指 metadata.ownerReference 中的对象 dependents map[*node]struct&#123;&#125; deletingDependents bool deletingDependentsLock sync.RWMutex beingDeleted bool beingDeletedLock sync.RWMutex // 当 virtual 值为 true 时，此时不确定该对象是否存在于 apiserver 中 virtual bool virtualLock sync.RWMutex // 对象本身的 OwnerReference 列表 owners []metav1.OwnerReference&#125; GraphBuilder 主要有三个功能： 1、监控集群中所有的可删除资源； 2、基于 informers 中的资源在 uidToNode 数据结构中维护着所有对象的依赖关系； 3、处理 graphChanges 中的事件并放到 attemptToDelete 和 attemptToOrphan 两个队列中； 上文已经说了 gc.dependencyGraphBuilder.Run 的功能，启动所有的 informers 然后再启动一个 goroutine 处理 graphChanges 队列中的事件并分别放到 attemptToDelete 和 attemptToOrphan 两个队列中，代码如下所示： k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:281 1234567891011121314151617181920212223242526272829func (gb *GraphBuilder) Run(stopCh &lt;-chan struct&#123;&#125;) &#123; klog.Infof(&quot;GraphBuilder running&quot;) defer klog.Infof(&quot;GraphBuilder stopping&quot;) gb.monitorLock.Lock() gb.stopCh = stopCh gb.running = true gb.monitorLock.Unlock() gb.startMonitors() // 调用 gb.runProcessGraphChanges // 此处为死循环，除非收到 stopCh 信号，否则下面的代码不会被执行到 wait.Until(gb.runProcessGraphChanges, 1*time.Second, stopCh) // 若执行到此处说明收到了 stopCh 的信号，此时需要停止所有的 running monitors gb.monitorLock.Lock() defer gb.monitorLock.Unlock() monitors := gb.monitors stopped := 0 for _, monitor := range monitors &#123; if monitor.stopCh != nil &#123; stopped++ close(monitor.stopCh) &#125; &#125; gb.monitors = nil&#125; gc.dependencyGraphBuilder.Run的核心是调用了 gb.startMonitors 和 gb.runProcessGraphChanges 两个方法来完成主要功能，继续看这两个方法的主要逻辑。 gb.startMonitorsstartMonitors 的功能很简单就是启动所有的 informers，代码如下所示： k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:232 12345678910111213141516171819202122232425func (gb *GraphBuilder) startMonitors() &#123; gb.monitorLock.Lock() defer gb.monitorLock.Unlock() // 1、当 GraphBuilder 调用 run 方法后，running 会设置为 true if !gb.running &#123; return &#125; // 2、当 kube-controller-manager 中所有的 controllers 在启动流程中都启动后 // 会 close 掉 informersStarted &lt;-gb.informersStarted // 3、启动所有 informer monitors := gb.monitors started := 0 for _, monitor := range monitors &#123; if monitor.stopCh == nil &#123; monitor.stopCh = make(chan struct&#123;&#125;) gb.sharedInformers.Start(gb.stopCh) go monitor.Run() started++ &#125; &#125;&#125; gb.runProcessGraphChangesrunProcessGraphChanges 方法的主要功能是处理 graphChanges 中的事件将其分别放到 GraphBuilder 的 attemptToDelete 和 attemptToOrphan 两个队列中，代码主要逻辑为： 1、从 graphChanges 队列中取出一个 item 即 event； 2、获取 event 的 accessor，accessor 是一个 object 的 meta.Interface，里面包含访问 object meta 中所有字段的方法； 3、通过 accessor 获取 UID 判断 uidToNode 中是否存在该 object； 4、若 uidToNode 中不存在该 node 且该事件是 addEvent 或 updateEvent，则为该 object 创建对应的 node，并调用 gb.insertNode 将该 node 加到 uidToNode 中，然后将该 node 添加到其 owner 的 dependents 中，执行完 gb.insertNode 中的操作后再调用 gb.processTransitions 方法判断该对象是否处于删除状态，若处于删除状态会判断该对象是以 orphan 模式删除还是以 foreground 模式删除，若以 orphan 模式删除，则将该 node 加入到 attemptToOrphan 队列中，若以 foreground 模式删除则将该对象以及其所有 dependents 都加入到 attemptToDelete 队列中； 5、若 uidToNode 中存在该 node 且该事件是 addEvent 或 updateEvent 时，此时可能是一个 update 操作，调用 referencesDiffs 方法检查该对象的 OwnerReferences 字段是否有变化，若有变化(1)调用 gb.addUnblockedOwnersToDeleteQueue 将被删除以及更新的 owner 对应的 node 加入到 attemptToDelete 中，因为此时该 node 中已被删除或更新的 owner 可能处于删除状态且阻塞在该 node 处，此时有三种方式避免该 node 的 owner 处于删除阻塞状态，一是等待该 node 被删除，二是将该 node 自身对应 owner 的 OwnerReferences 字段删除，三是将该 node OwnerReferences 字段中对应 owner 的 BlockOwnerDeletion 设置为 false；(2)更新该 node 的 owners 列表；(3)若有新增的 owner，将该 node 加入到新 owner 的 dependents 中；(4) 若有被删除的 owner，将该 node 从已删除 owner 的 dependents 中删除；以上操作完成后，检查该 node 是否处于删除状态并进行标记，最后调用 gb.processTransitions 方法检查该 node 是否要被删除； 举个例子，若以 foreground 模式删除 deployment 时，deployment 的 dependents 列表中有对应的 rs，那么 deployment 的删除会阻塞住等待其依赖 rs 的删除，此时 rs 有三种方法不阻塞 deployment 的删除操作，一是 rs 对象被删除，二是删除 rs 对象 OwnerReferences 字段中对应的 deployment，三是将 rs 对象OwnerReferences 字段中对应的 deployment 配置 BlockOwnerDeletion 设置为 false，文末会有示例演示该操作。 6、若该事件为 deleteEvent，首先从 uidToNode 中删除该对象，然后从该 node 所有 owners 的 dependents 中删除该对象，将该 node 所有的 dependents 加入到 attemptToDelete 队列中，最后检查该 node 的所有 owners，若有处于删除状态的 owner，此时该 owner 可能处于删除阻塞状态正在等待该 node 的删除，将该 owner 加入到 attemptToDelete 中； 总结一下，当从 graphChanges 中取出 event 时，不管是什么 event，主要完成三件时，首先都会将 event 转化为 uidToNode 中的 node 对象，其次一是更新 uidToNode 中维护的依赖关系，二是更新该 node 的 owners 以及 owners 的 dependents，三是检查该 node 的 owners 是否要被删除以及该 node 的 dependents 是否要被删除，若需要删除则根据 node 的删除策略将其添加到 attemptToOrphan 或者 attemptToDelete 队列中； k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:526 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091func (gb *GraphBuilder) runProcessGraphChanges() &#123; for gb.processGraphChanges() &#123; &#125;&#125;func (gb *GraphBuilder) processGraphChanges() bool &#123; // 1、从 graphChanges 取出一个 event item, quit := gb.graphChanges.Get() if quit &#123; return false &#125; defer gb.graphChanges.Done(item) event, ok := item.(*event) if !ok &#123; utilruntime.HandleError(fmt.Errorf(&quot;expect a *event, got %v&quot;, item)) return true &#125; obj := event.obj accessor, err := meta.Accessor(obj) if err != nil &#123; utilruntime.HandleError(fmt.Errorf(&quot;cannot access obj: %v&quot;, err)) return true &#125; // 2、若存在 node 对象，从 uidToNode 中取出该 event 的 node 对象 existingNode, found := gb.uidToNode.Read(accessor.GetUID()) if found &#123; existingNode.markObserved() &#125; switch &#123; // 3、若 event 为 add 或 update 类型以及对应的 node 对象不存在时 case (event.eventType == addEvent || event.eventType == updateEvent) &amp;&amp; !found: // 4、为 node 创建 event 对象 newNode := &amp;node&#123; ...... &#125; // 5、在 uidToNode 中添加该 node 对象 gb.insertNode(newNode) // 6、检查并处理 node 的删除操作 gb.processTransitions(event.oldObj, accessor, newNode) // 7、若 event 为 add 或 update 类型以及对应的 node 对象存在时 case (event.eventType == addEvent || event.eventType == updateEvent) &amp;&amp; found: added, removed, changed := referencesDiffs(existingNode.owners, accessor.GetOwnerReferences()) // 8、若 node 的 owners 有变化 if len(added) != 0 || len(removed) != 0 || len(changed) != 0 &#123; gb.addUnblockedOwnersToDeleteQueue(removed, changed) // 9、更新 uidToNode 中的 owners existingNode.owners = accessor.GetOwnerReferences() // 10、添加更新后 Owners 对应的 dependent gb.addDependentToOwners(existingNode, added) // 11、移除旧 owners 对应的 dependents gb.removeDependentFromOwners(existingNode, removed) &#125; // 12、检查是否处于删除状态 if beingDeleted(accessor) &#123; existingNode.markBeingDeleted() &#125; // 13、检查并处理 node 的删除操作 gb.processTransitions(event.oldObj, accessor, existingNode) // 14、若为 delete event case event.eventType == deleteEvent: if !found &#123; return true &#125; // 15、从 uidToNode 中删除该 node gb.removeNode(existingNode) existingNode.dependentsLock.RLock() defer existingNode.dependentsLock.RUnlock() if len(existingNode.dependents) &gt; 0 &#123; gb.absentOwnerCache.Add(accessor.GetUID()) &#125; // 16、删除该 node 的 dependents for dep := range existingNode.dependents &#123; gb.attemptToDelete.Add(dep) &#125; // 17、删除该 node 处于删除阻塞状态的 owner for _, owner := range existingNode.owners &#123; ownerNode, found := gb.uidToNode.Read(owner.UID) if !found || !ownerNode.isDeletingDependents() &#123; continue &#125; gb.attemptToDelete.Add(ownerNode) &#125; &#125; return true&#125; processTransitions上述在处理 add 或 update event 时最后都调用了 processTransitions 方法检查 node 是否处于删除状态，若处于删除状态会通过其删除策略将 node 放到 attemptToOrphan 或 attemptToDelete 队列中。 k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:509 12345678910111213func (gb *GraphBuilder) processTransitions(oldObj interface&#123;&#125;, newAccessor metav1.Object, n *node) &#123; if startsWaitingForDependentsOrphaned(oldObj, newAccessor) &#123; gb.attemptToOrphan.Add(n) return &#125; if startsWaitingForDependentsDeleted(oldObj, newAccessor) &#123; n.markDeletingDependents() for dep := range n.dependents &#123; gb.attemptToDelete.Add(dep) &#125; gb.attemptToDelete.Add(n) &#125;&#125; gc.runAttemptToDeleteWorkerrunAttemptToDeleteWorker 是执行删除 attemptToDelete 中 node 的方法，其主要逻辑为： 1、调用 gc.attemptToDeleteItem 删除 node； 2、若删除失败则重新加入到 attemptToDelete 队列中进行重试； k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:280 12345678910111213141516171819202122232425262728293031func (gc *GarbageCollector) runAttemptToDeleteWorker() &#123; for gc.attemptToDeleteWorker() &#123; &#125;&#125;func (gc *GarbageCollector) attemptToDeleteWorker() bool &#123; item, quit := gc.attemptToDelete.Get() gc.workerLock.RLock() defer gc.workerLock.RUnlock() if quit &#123; return false &#125; defer gc.attemptToDelete.Done(item) n, ok := item.(*node) if !ok &#123; utilruntime.HandleError(fmt.Errorf(&quot;expect *node, got %#v&quot;, item)) return true &#125; err := gc.attemptToDeleteItem(n) if err != nil &#123; if _, ok := err.(*restMappingError); ok &#123; klog.V(5).Infof(&quot;error syncing item %s: %v&quot;, n, err) &#125; else &#123; utilruntime.HandleError(fmt.Errorf(&quot;error syncing item %s: %v&quot;, n, err)) &#125; gc.attemptToDelete.AddRateLimited(item) &#125; else if !n.isObserved() &#123; gc.attemptToDelete.AddRateLimited(item) &#125; return true&#125; gc.runAttemptToDeleteWorker 中调用了 gc.attemptToDeleteItem 执行实际的删除操作。 gc.attemptToDeleteItemgc.attemptToDeleteItem 的主要逻辑为： 1、判断 node 是否处于删除状态； 2、从 apiserver 获取该 node 最新的状态，该 node 可能为 virtual node，若为 virtual node 则从 apiserver 中获取不到该 node 的对象，此时会将该 node 重新加入到 graphChanges 队列中，再次处理该 node 时会将其从 uidToNode 中删除； 3、判断该 node 最新状态的 uid 是否等于本地缓存中的 uid，若不匹配说明该 node 已更新过此时将其设置为 virtual node 并重新加入到 graphChanges 队列中，再次处理该 node 时会将其从 uidToNode 中删除； 4、通过 node 的 deletingDependents 字段判断该 node 当前是否处于删除 dependents 的状态，若该 node 处于删除 dependents 的状态则调用 processDeletingDependentsItem 方法检查 node 的 blockingDependents 是否被完全删除，若 blockingDependents 已完全被删除则删除该 node 对应的 finalizer，若 blockingDependents 还未删除完，将未删除的 blockingDependents 加入到 attemptToDelete 中； 上文中在 GraphBuilder 处理 graphChanges 中的事件时，若发现 node 处于删除状态，会将 node 的 dependents 加入到 attemptToDelete 中并标记 node 的 deletingDependents 为 true； 5、调用 gc.classifyReferences 将 node 的 ownerReferences 分类为 solid, dangling, waitingForDependentsDeletion 三类：dangling(owner 不存在)、waitingForDependentsDeletion(owner 存在，owner 处于删除状态且正在等待其 dependents 被删除)、solid(至少有一个 owner 存在且不处于删除状态)； 6、对以上分类进行不同的处理，若 solid不为 0 即当前 node 至少存在一个 owner，该对象还不能被回收，此时需要将 dangling 和 waitingForDependentsDeletion 列表中的 owner 从 node 的 ownerReferences 删除，即已经被删除或等待删除的引用从对象中删掉； 7、第二种情况是该 node 的 owner 处于 waitingForDependentsDeletion 状态并且 node 的 dependents 未被完全删除，该 node 需要等待删除完所有的 dependents 后才能被删除； 8、第三种情况就是该 node 已经没有任何 dependents 了，此时按照 node 中声明的删除策略调用 apiserver 的接口删除即可； k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:404 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586func (gc *GarbageCollector) attemptToDeleteItem(item *node) error &#123; // 1、判断 node 是否处于删除状态 if item.isBeingDeleted() &amp;&amp; !item.isDeletingDependents() &#123; return nil &#125; // 2、从 apiserver 获取该 node 最新的状态 latest, err := gc.getObject(item.identity) switch &#123; case errors.IsNotFound(err): gc.dependencyGraphBuilder.enqueueVirtualDeleteEvent(item.identity) item.markObserved() return nil case err != nil: return err &#125; // 3、判断该 node 最新状态的 uid 是否等于本地缓存中的 uid if latest.GetUID() != item.identity.UID &#123; gc.dependencyGraphBuilder.enqueueVirtualDeleteEvent(item.identity) item.markObserved() return nil &#125; // 4、判断该 node 当前是否处于删除 dependents 状态中 if item.isDeletingDependents() &#123; return gc.processDeletingDependentsItem(item) &#125; // 5、检查 node 是否还存在 ownerReferences ownerReferences := latest.GetOwnerReferences() if len(ownerReferences) == 0 &#123; return nil &#125; // 6、对 ownerReferences 进行分类 solid, dangling, waitingForDependentsDeletion, err := gc.classifyReferences(item, ownerReferences) if err != nil &#123; return err &#125; switch &#123; // 7、存在不处于删除状态的 owner case len(solid) != 0: if len(dangling) == 0 &amp;&amp; len(waitingForDependentsDeletion) == 0 &#123; return nil &#125; ownerUIDs := append(ownerRefsToUIDs(dangling), ownerRefsToUIDs(waitingForDependentsDeletion)...) patch := deleteOwnerRefStrategicMergePatch(item.identity.UID, ownerUIDs...) _, err = gc.patch(item, patch, func(n *node) ([]byte, error) &#123; return gc.deleteOwnerRefJSONMergePatch(n, ownerUIDs...) &#125;) return err // 8、node 的 owner 处于 waitingForDependentsDeletion 状态并且 node // 的 dependents 未被完全删除 case len(waitingForDependentsDeletion) != 0 &amp;&amp; item.dependentsLength() != 0: deps := item.getDependents() // 9、删除 dependents for _, dep := range deps &#123; if dep.isDeletingDependents() &#123; patch, err := item.unblockOwnerReferencesStrategicMergePatch() if err != nil &#123; return err &#125; if _, err := gc.patch(item, patch, gc.unblockOwnerReferencesJSONMergePatch); err != nil &#123; return err &#125; break &#125; &#125; // 10、以 Foreground 模式删除 node 对象 policy := metav1.DeletePropagationForeground return gc.deleteObject(item.identity, &amp;policy) // 11、该 node 已经没有任何依赖了，按照 node 中声明的删除策略调用 apiserver 的接口删除 default: var policy metav1.DeletionPropagation switch &#123; case hasOrphanFinalizer(latest): policy = metav1.DeletePropagationOrphan case hasDeleteDependentsFinalizer(latest): policy = metav1.DeletePropagationForeground default: policy = metav1.DeletePropagationBackground &#125; return gc.deleteObject(item.identity, &amp;policy) &#125;&#125; gc.runAttemptToOrphanWorkerrunAttemptToOrphanWorker 是处理以 orphan 模式删除的 node，主要逻辑为： 1、调用 gc.orphanDependents 删除 owner 所有 dependents OwnerReferences 中的 owner 字段； 2、调用 gc.removeFinalizer 删除 owner 的 orphan Finalizer； 3、以上两步中若有失败的会进行重试； k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:574 1234567891011121314151617181920212223242526272829303132333435func (gc *GarbageCollector) runAttemptToOrphanWorker() &#123; for gc.attemptToOrphanWorker() &#123; &#125;&#125;func (gc *GarbageCollector) attemptToOrphanWorker() bool &#123; item, quit := gc.attemptToOrphan.Get() gc.workerLock.RLock() defer gc.workerLock.RUnlock() if quit &#123; return false &#125; defer gc.attemptToOrphan.Done(item) owner, ok := item.(*node) if !ok &#123; return true &#125; owner.dependentsLock.RLock() dependents := make([]*node, 0, len(owner.dependents)) for dependent := range owner.dependents &#123; dependents = append(dependents, dependent) &#125; owner.dependentsLock.RUnlock() err := gc.orphanDependents(owner.identity, dependents) if err != nil &#123; gc.attemptToOrphan.AddRateLimited(item) return true &#125; // 更新 owner, 从 finalizers 列表中移除 &quot;orphaningFinalizer&quot; err = gc.removeFinalizer(owner, metav1.FinalizerOrphanDependents) if err != nil &#123; gc.attemptToOrphan.AddRateLimited(item) &#125; return true&#125; garbageCollector.SyncgarbageCollector.Sync 是 startGarbageCollectorController 中的第三个核心方法，主要功能是周期性的查询集群中所有的资源，过滤出 deletableResources，然后对比已经监控的 deletableResources 和当前获取到的 deletableResources 是否一致，若不一致则更新 GraphBuilder 的 monitors 并重新启动 monitors 监控所有的 deletableResources，该方法的主要逻辑为： 1、通过调用 GetDeletableResources 获取集群内所有的 deletableResources 作为 newResources，deletableResources 指支持 “delete”, “list”, “watch” 三种操作的 resource，包括 CR； 2、检查 oldResources, newResources 是否一致，不一致则需要同步； 3、调用 gc.resyncMonitors 同步 newResources，在 gc.resyncMonitors 中会重新调用 GraphBuilder 的 syncMonitors 和 startMonitors 两个方法完成 monitors 的刷新； 4、等待 newResources informer 中的 cache 同步完成； 5、将 newResources 作为 oldResources，继续进行下一轮的同步； k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:164 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func (gc *GarbageCollector) Sync(discoveryClient discovery.ServerResourcesInterface, period time.Duration, stopCh &lt;-chan struct&#123;&#125;) &#123; oldResources := make(map[schema.GroupVersionResource]struct&#123;&#125;) wait.Until(func() &#123; // 1、获取集群内所有的 DeletableResources 作为 newResources newResources := GetDeletableResources(discoveryClient) if len(newResources) == 0 &#123; return &#125; // 2、判断集群中的资源是否有变化 if reflect.DeepEqual(oldResources, newResources) &#123; return &#125; gc.workerLock.Lock() defer gc.workerLock.Unlock() // 3、开始更新 GraphBuilder 中的 monitors attempt := 0 wait.PollImmediateUntil(100*time.Millisecond, func() (bool, error) &#123; attempt++ if attempt &gt; 1 &#123; newResources = GetDeletableResources(discoveryClient) if len(newResources) == 0 &#123; return false, nil &#125; &#125; gc.restMapper.Reset() // 4、调用 gc.resyncMonitors 同步 newResources if err := gc.resyncMonitors(newResources); err != nil &#123; return false, nil &#125; // 5、等待所有 monitors 的 cache 同步完成 if !cache.WaitForNamedCacheSync(&quot;garbage collector&quot;, waitForStopOrTimeout(stopCh, period), gc.dependencyGraphBuilder.IsSynced) &#123; return false, nil &#125; return true, nil &#125;, stopCh) // 6、更新 oldResources oldResources = newResources &#125;, period, stopCh)&#125; garbageCollector.Sync 中主要调用了两个方法，一是调用 GetDeletableResources 获取集群中所有的可删除资源，二是调用 gc.resyncMonitors 更新 GraphBuilder 中 monitors。 GetDeletableResources在 GetDeletableResources 中首先通过调用 discoveryClient.ServerPreferredResources 方法获取集群内所有的 resource 信息，然后通过调用 discovery.FilteredBy 过滤出支持 “delete”, “list”, “watch” 三种方法的 resource 作为 deletableResources。 k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:636 12345678910111213141516171819202122232425262728func GetDeletableResources(discoveryClient discovery.ServerResourcesInterface) map[schema.GroupVersionResource]struct&#123;&#125; &#123; // 1、调用 discoveryClient.ServerPreferredResources 方法获取集群内所有的 resource 信息 preferredResources, err := discoveryClient.ServerPreferredResources() if err != nil &#123; if discovery.IsGroupDiscoveryFailedError(err) &#123; ...... &#125; else &#123; ...... &#125; &#125; if preferredResources == nil &#123; return map[schema.GroupVersionResource]struct&#123;&#125;&#123;&#125; &#125; // 2、调用 discovery.FilteredBy 过滤出 deletableResources deletableResources := discovery.FilteredBy(discovery.SupportsAllVerbs&#123;Verbs: []string&#123;&quot;delete&quot;, &quot;list&quot;, &quot;watch&quot;&#125;&#125;, preferredResources) deletableGroupVersionResources := map[schema.GroupVersionResource]struct&#123;&#125;&#123;&#125; for _, rl := range deletableResources &#123; gv, err := schema.ParseGroupVersion(rl.GroupVersion) if err != nil &#123; continue &#125; for i := range rl.APIResources &#123; deletableGroupVersionResources[schema.GroupVersionResource&#123;Group: gv.Group, Version: gv.Version, Resource: rl.APIResources[i].Name&#125;] = struct&#123;&#125;&#123;&#125; &#125; &#125; return deletableGroupVersionResources&#125; ServerPreferredResourcesServerPreferredResources 的主要功能是获取集群内所有的 resource 以及其 group、version、verbs 信息，该方法的主要逻辑为： 1、调用 ServerGroups 方法获取集群内所有的 GroupList，ServerGroups 方法首先从 apiserver 通过 /api URL 获取当前版本下所有可用的 APIVersions，再通过 /apis URL 获取 所有可用的 APIVersions 以及其下的所有 APIGroupList； 2、调用 fetchGroupVersionResources 通过 serverGroupList 再获取到对应的 resource； 3、将获取到的 version、group、resource 构建成标准格式添加到 metav1.APIResourceList 中； k8s.io/kubernetes/staging/src/k8s.io/client-go/discovery/discovery_client.go:285 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859func ServerPreferredResources(d DiscoveryInterface) ([]*metav1.APIResourceList, error) &#123; // 1、获取集群内所有的 GroupList serverGroupList, err := d.ServerGroups() if err != nil &#123; return nil, err &#125; // 2、通过 serverGroupList 获取到对应的 resource groupVersionResources, failedGroups := fetchGroupVersionResources(d, serverGroupList) result := []*metav1.APIResourceList&#123;&#125; grVersions := map[schema.GroupResource]string&#123;&#125; // selected version of a GroupResource grAPIResources := map[schema.GroupResource]*metav1.APIResource&#123;&#125; // selected APIResource for a GroupResource gvAPIResourceLists := map[schema.GroupVersion]*metav1.APIResourceList&#123;&#125; // blueprint for a APIResourceList for later grouping // 3、格式化 resource for _, apiGroup := range serverGroupList.Groups &#123; for _, version := range apiGroup.Versions &#123; groupVersion := schema.GroupVersion&#123;Group: apiGroup.Name, Version: version.Version&#125; apiResourceList, ok := groupVersionResources[groupVersion] if !ok &#123; continue &#125; emptyAPIResourceList := metav1.APIResourceList&#123; GroupVersion: version.GroupVersion, &#125; gvAPIResourceLists[groupVersion] = &amp;emptyAPIResourceList result = append(result, &amp;emptyAPIResourceList) for i := range apiResourceList.APIResources &#123; apiResource := &amp;apiResourceList.APIResources[i] if strings.Contains(apiResource.Name, &quot;/&quot;) &#123; continue &#125; gv := schema.GroupResource&#123;Group: apiGroup.Name, Resource: apiResource.Name&#125; if _, ok := grAPIResources[gv]; ok &amp;&amp; version.Version != apiGroup.PreferredVersion.Version &#123; continue &#125; grVersions[gv] = version.Version grAPIResources[gv] = apiResource &#125; &#125; &#125; for groupResource, apiResource := range grAPIResources &#123; version := grVersions[groupResource] groupVersion := schema.GroupVersion&#123;Group: groupResource.Group, Version: version&#125; apiResourceList := gvAPIResourceLists[groupVersion] apiResourceList.APIResources = append(apiResourceList.APIResources, *apiResource) &#125; if len(failedGroups) == 0 &#123; return result, nil &#125; return result, &amp;ErrGroupDiscoveryFailed&#123;Groups: failedGroups&#125;&#125; GetDeletableResources 方法中的调用流程为： 12345678 |--&gt; d.ServerGroups | |--&gt; discoveryClient. --| | ServerPreferredResources | | |--&gt; fetchGroupVersionResourcesGetDeletableResources --| | |--&gt; discovery.FilteredBy gc.resyncMonitorsgc.resyncMonitors 的主要功能是更新 GraphBuilder 的 monitors 并重新启动 monitors 监控所有的 deletableResources，GraphBuilder 的 syncMonitors 和 startMonitors 方法在前面的流程中已经分析过，此处不再详细说明。 k8s.io/kubernetes/pkg/controller/garbagecollector/garbagecollector.go:116 1234567func (gc *GarbageCollector) resyncMonitors(deletableResources map[schema. GroupVersionResource]struct&#123;&#125;) error &#123; if err := gc.dependencyGraphBuilder.syncMonitors(deletableResources); err != nil &#123; return err &#125; gc.dependencyGraphBuilder.startMonitors() return nil&#125; garbagecollector.NewDebugHandlergarbagecollector.NewDebugHandler 主要功能是对外提供一个接口供用户查询当前集群中所有资源的依赖关系，依赖关系可以以图表的形式展示。 1234func startGarbageCollectorController(ctx ControllerContext) (http.Handler, bool, error) &#123; ...... return garbagecollector.NewDebugHandler(garbageCollector), true, nil&#125; 具体使用方法如下所示： 12345678$ curl http://192.168.99.108:10252/debug/controllers/garbagecollector/graph &gt; tmp.dot$ curl http://192.168.99.108:10252/debug/controllers/garbagecollector/graph\?uid=f9555d53-2b5f-4702-9717-54a313ed4fe8 &gt; tmp.dot// 生成 svg 文件$ dot -Tsvg -o graph.svg tmp.dot// 然后在浏览器中打开 svg 文件 依赖关系图如下所示： 示例在此处会有一个小示例验证一下源码中的删除阻塞逻辑，当以 Foreground 策略删除一个对象时，该对象会处于阻塞状态等待其依依赖被删除，此时有三种方式避免该对象处于删除阻塞状态，一是将依赖对象直接删除，二是将依赖对象自身的 OwnerReferences 中 owner 字段删除，三是将该依赖对象 OwnerReferences 字段中对应 owner 的 BlockOwnerDeletion 设置为 false，下面会验证下这三种方式，首先创建一个 deployment，deployment 创建出的 rs 默认不会有 foregroundDeletion finalizers，此时使用 kubectl edit 手动加上 foregroundDeletion finalizers，当 deployment 正常运行时，如下所示： 123456789101112131415161718192021222324252627282930313233343536$ kubectl get deployment nginx-deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 2/2 2 2 43s$ kubectl get rs nginx-deployment-69b6b4c5cdNAME DESIRED CURRENT READY AGEnginx-deployment-69b6b4c5cd 2 2 2 57s$ kubectl get podNAME READY STATUS RESTARTS AGEnginx-deployment-69b6b4c5cd-26dsn 1/1 Running 0 66snginx-deployment-69b6b4c5cd-6rqqc 1/1 Running 0 64s$ kubectl edit rs nginx-deployment-69b6b4c5cd // deployment 关联的 rs 对象 apiVersion: apps/v1kind: ReplicaSetmetadata: name: nginx-deployment-69b6b4c5cd namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: Deployment name: nginx-deployment uid: 40a1044e-03d1-48bc-8806-cb79d781c946 finalizers: - foregroundDeletion // 为 rs 手动添加的 Foreground 策略 ......spec: replicas: 2 ......status: ...... 当 deployment、rs、pod 都处于正常运行状态且 deployment 关联的 rs 使用 Foreground 删除策略时，然后验证源码中提到的三种方法，验证时需要模拟一个依赖对象无法删除的场景，当然这个也很好模拟，三种场景如下所示： 1、当 pod 所在的 node 处于 Ready 状态时，以 Foreground 策略删除 deploment，因为 rs 关联的 pod 会直接被删除，rs 也会被正常删除，此时 deployment 也会直接被删除； 2、当 pod 所在的 node 处于 NotReady 状态时，以 Foreground 策略删除 deploment，此时因 rs 关联的 pod 无法被删除，rs 会一直处于删除阻塞状态，deployment 由于 rs 无法被删除也会处于删除阻塞状态，此时更新 rs 去掉其 ownerReferences 中对应的 deployment 部分，deployment 会因无依赖对象被成功删除； 3、和 2 同样的场景，node 处于 NotReady 状态时，以 Foreground 策略删除 deploment，deployment 和 rs 将处于删除阻塞状态，此时将 rs ownerReferences 中关联 deployment 的 blockOwnerDeletion 字段设置为 false，可以看到 deployment 会因无 block 依赖对象被成功删除； 123456789$ systemctl stop kubelet// node 处于 NotReady 状态$ kubectl get nodeNAME STATUS ROLES AGE VERSIONminikube NotReady master 6d11h v1.16.2// 以 Foreground 策略删除 deployment$ curl -k -v -XDELETE -H &quot;Accept: application/json&quot; -H &quot;Content-Type: application/json&quot; -d &apos;&#123;&quot;propagationPolicy&quot;:&quot;Foreground&quot;&#125;&apos; &apos;https://192.168.99.108:8443/apis/apps/v1/namespaces/default/deployments/nginx-deployment&apos; 总结GarbageCollectorController 是一种典型的生产者消费者模型，所有 deletableResources 的 informer 都是生产者，每种资源的 informer 监听到变化后都会将对应的事件 push 到 graphChanges 中，graphChanges 是 GraphBuilder 对象中的一个数据结构，GraphBuilder 会启动另外的 goroutine 对 graphChanges 中的事件进行分类并放在其 attemptToDelete 和 attemptToOrphan 两个队列中，garbageCollector 会启动多个 goroutine 对 attemptToDelete 和 attemptToOrphan 两个队列中的事件进行处理，处理的结果就是回收一些需要被删除的对象。最后，再用一个流程图总结一下 GarbageCollectorController 的主要流程: 12345678910111213141516171819202122 monitors (producer) | | ∨ graphChanges queue | | ∨ processGraphChanges | | ∨ ------------------------------- | | | | ∨ ∨attemptToDelete queue attemptToOrphan queue | | | | ∨ ∨ AttemptToDeleteWorker AttemptToOrphanWorker (consumer) (consumer)]]></content>
      <tags>
        <tag>kube-controller-manager</tag>
        <tag>garbage collector controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet statusManager 源码分析]]></title>
    <url>%2F2019%2F12%2F25%2Fkubelet_status_manager%2F</url>
    <content type="text"><![CDATA[本篇文章没有接上篇继续更新 kube-controller-manager，kube-controller-manager 的源码阅读笔记也会继续更新，笔者会同时阅读多个组件的源码，阅读笔记也会按组件进行交叉更新，交叉更新的目的一是为了加深印象避免阅读完后又很快忘记，二是某些代码的功能难以理解，避免死磕，但整体目标是将每个组件的核心代码阅读完。 在前面的文章中已经介绍过 kubelet 的架构以及启动流程，本章会继续介绍 kubelet 中的核心功能，kubelet 中包含数十个 manager 以及对 CNI、CRI、CSI 的调用。每个 manager 的功能各不相同，manager 之间也会有依赖关系，本文会介绍比较简单的 statusManager。 statusManager 源码分析 kubernetes 版本：v1.16 statusManager 的主要功能是将 pod 状态信息同步到 apiserver，statusManage 并不会主动监控 pod 的状态，而是提供接口供其他 manager 进行调用。 statusManager 的初始化kubelet 在启动流程时会在 NewMainKubelet 方法中初始化其核心组件，包括各种 manager。 k8s.io/kubernetes/pkg/kubelet/kubelet.go:335 123456func NewMainKubelet() (*Kubelet, error) &#123; ...... // statusManager 的初始化 klet.statusManager = status.NewManager(klet.kubeClient, klet.podManager, klet) ......&#125; NewManager 是用来初始化 statusManager 对象的，其中参数的功能如下所示： kubeClient：用于和 apiserver 交互； podManager：负责内存中 pod 的维护； podStatuses：statusManager 的 cache，保存 pod 与状态的对应关系； podStatusesChannel：当其他组件调用 statusManager 更新 pod 状态时，会将 pod 的状态信息发送到podStatusesChannel 中； apiStatusVersions：维护最新的 pod status 版本号，每更新一次会加1； podDeletionSafety：删除 pod 的接口； k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:118 12345678910func NewManager(kubeClient clientset.Interface, podManager kubepod.Manager, podDeletionSafety PodDeletionSafetyProvider) Manager &#123; return &amp;manager&#123; kubeClient: kubeClient, podManager: podManager, podStatuses: make(map[types.UID]versionedPodStatus), podStatusChannel: make(chan podStatusSyncRequest, 1000), apiStatusVersions: make(map[kubetypes.MirrorPodUID]uint64), podDeletionSafety: podDeletionSafety, &#125;&#125; 在初始化完成后，kubelet 会在 Run 方法中会以 goroutine 的方式启动 statusManager。 k8s.io/kubernetes/pkg/kubelet/kubelet.go:1398 12345func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123; ...... kl.statusManager.Start() ......&#125; statusManager 的代码主要在 k8s.io/kubernetes/pkg/kubelet/status/ 目录中，其对外暴露的接口有以下几个： 12345678910111213141516171819202122type Manager interface &#123; // 一个 interface 用来暴露给其他组件获取 pod status 的 PodStatusProvider // 启动 statusManager 的方法 Start() // 设置 pod 的状态并会触发一个状态同步操作 SetPodStatus(pod *v1.Pod, status v1.PodStatus) // 设置 pod .status.containerStatuses 中 container 是否为 ready 状态并触发状态同步操作 SetContainerReadiness(podUID types.UID, containerID kubecontainer.ContainerID, ready bool) // 设置 pod .status.containerStatuses 中 container 是否为 started 状态并触发状态同步操作 SetContainerStartup(podUID types.UID, containerID kubecontainer.ContainerID, started bool) // 将 pod .status.containerStatuses 和 .status.initContainerStatuses 中 container 的 state 置为 Terminated 状态并触发状态同步操作 TerminatePod(pod *v1.Pod) // 从 statusManager 缓存 podStatuses 中删除对应的 pod RemoveOrphanedStatuses(podUIDs map[types.UID]bool)&#125; pod 对应的 status 字段如下所示： 1234567891011121314151617status: conditions: ...... containerStatuses: - containerID: containerd://64e9d88459b38e90c2a4b4d87db5acd180c820c855a55aabe38e4e11b9b83576 image: docker.io/library/nginx:1.9 imageID: sha256:f568d3158b1e871b713cb33aca5a9377bc21a1f644addf41368393d28c35e894 lastState: &#123;&#125; name: nginx-pod ready: true restartCount: 0 started: true state: running: startedAt: &quot;2019-12-15T16:13:29Z&quot; podIP: 10.15.225.15 ...... 然后继续看 statusManager 的启动方法 start, 其主要逻辑为： 1、设置定时器，syncPeriod 默认为 10s； 2、启动 wait.Forever goroutine 同步 pod 的状态，有两种同步方式，第一种是当监听到某个 pod 状态改变时会调用 m.syncPod 进行同步，第二种是当触发定时器时调用 m.syncBatch 进行批量同步； k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:147 12345678910111213141516171819202122func (m *manager) Start() &#123; // 1、检查 kubeClient 是否被初始化 if m.kubeClient == nil &#123; klog.Infof(&quot;Kubernetes client is nil, not starting status manager.&quot;) return &#125; // 2、设置定时器 syncTicker := time.Tick(syncPeriod) go wait.Forever(func() &#123; select &#123; // 3、监听 m.podStatusChannel channel，当接收到数据时触发同步操作 case syncRequest := &lt;-m.podStatusChannel: ...... m.syncPod(syncRequest.podUID, syncRequest.status) // 4、定时同步 case &lt;-syncTicker: m.syncBatch() &#125; &#125;, 0)&#125; syncPodsyncPod 是用来同步 pod 最新状态至 apiserver 的方法，主要逻辑为： 1、调用 m.needsUpdate 判断是否需要同步状态，若 apiStatusVersions 中的 status 版本号小于当前接收到的 status 版本号或者 apistatusVersions 中不存在该 status 版本号则需要同步，若不需要同步则继续检查 pod 是否处于删除状态，若处于删除状态调用 m.podDeletionSafety.PodResourcesAreReclaimed 将 pod 完全删除； 2、从 apiserver 获取 pod 的 oldStatus； 3、检查 pod oldStatus 与 currentStatus 的 uid 是否相等，若不相等则说明 pod 被重建过； 4、调用 statusutil.PatchPodStatus 同步 pod 最新的 status 至 apiserver，并将返回的 pod 作为 newPod； 5、检查 newPod 是否处于 terminated 状态，若处于 terminated 状态则调用 apiserver 接口进行删除并从 cache 中清除，删除后 pod 会进行重建； k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:514 1234567891011121314151617181920212223242526272829303132333435363738394041424344func (m *manager) syncPod(uid types.UID, status versionedPodStatus) &#123; // 1、判断是否需要同步状态 if !m.needsUpdate(uid, status) &#123; klog.V(1).Infof(&quot;Status for pod %q is up-to-date; skipping&quot;, uid) return &#125; // 2、获取 pod 的 oldStatus pod, err := m.kubeClient.CoreV1().Pods(status.podNamespace).Get(status.podName, metav1.GetOptions&#123;&#125;) if errors.IsNotFound(err) &#123; return &#125; if err != nil &#123; return &#125; translatedUID := m.podManager.TranslatePodUID(pod.UID) // 3、检查 pod UID 是否已经改变 if len(translatedUID) &gt; 0 &amp;&amp; translatedUID != kubetypes.ResolvedPodUID(uid) &#123; return &#125; // 4、同步 pod 最新的 status 至 apiserver oldStatus := pod.Status.DeepCopy() newPod, patchBytes, err := statusutil.PatchPodStatus(m.kubeClient, pod.Namespace, pod.Name, *oldStatus, mergePodStatus(*oldStatus, status.status)) if err != nil &#123; return &#125; pod = newPod m.apiStatusVersions[kubetypes.MirrorPodUID(pod.UID)] = status.version // 5、若 newPod 处于 terminated 状态则调用 apiserver 删除该 pod，删除后 pod 会重建 if m.canBeDeleted(pod, status.status) &#123; deleteOptions := metav1.NewDeleteOptions(0) deleteOptions.Preconditions = metav1.NewUIDPreconditions(string(pod.UID)) err = m.kubeClient.CoreV1().Pods(pod.Namespace).Delete(pod.Name, deleteOptions) if err != nil &#123; return &#125; // 6、从 cache 中清除 m.deletePodStatus(uid) &#125;&#125; needsUpdateneedsUpdate 方法主要是检查 pod 的状态是否需要更新，以及检查当 pod 处于 terminated 状态时保证 pod 被完全删除。 k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:570 12345678910111213141516171819func (m *manager) needsUpdate(uid types.UID, status versionedPodStatus) bool &#123; latest, ok := m.apiStatusVersions[kubetypes.MirrorPodUID(uid)] if !ok || latest &lt; status.version &#123; return true &#125; pod, ok := m.podManager.GetPodByUID(uid) if !ok &#123; return false &#125; return m.canBeDeleted(pod, status.status)&#125;func (m *manager) canBeDeleted(pod *v1.Pod, status v1.PodStatus) bool &#123; if pod.DeletionTimestamp == nil || kubepod.IsMirrorPod(pod) &#123; return false &#125; // 此处说明 pod 已经处于删除状态了 return m.podDeletionSafety.PodResourcesAreReclaimed(pod, status)&#125; PodResourcesAreReclaimedPodResourcesAreReclaimed 检查 pod 在 node 上占用的所有资源是否已经被回收，其主要逻辑为： 1、检查 pod 中的所有 container 是否都处于非 running 状态； 2、从 podCache 中获取 podStatus，通过 podStatus 检查 pod 中的 container 是否已被完全删除； 3、检查 pod 的 volume 是否被清理； 4、检查 pod 的 cgroup 是否被清理； 5、若以上几个检查项都通过说明在 kubelet 端 pod 已被完全删除； k8s.io/kubernetes/pkg/kubelet/kubelet_pods.go:900 123456789101112131415161718192021222324252627282930313233func (kl *Kubelet) PodResourcesAreReclaimed(pod *v1.Pod, status v1.PodStatus) bool &#123; // 1、检查 pod 中的所有 container 是否都处于非 running 状态 if !notRunning(status.ContainerStatuses) &#123; return false &#125; // 2、从 podCache 中获取 podStatus，通过 podStatus 检查 pod 中的 container 是否已被完全删除 runtimeStatus, err := kl.podCache.Get(pod.UID) if err != nil &#123; return false &#125; if len(runtimeStatus.ContainerStatuses) &gt; 0 &#123; var statusStr string for _, status := range runtimeStatus.ContainerStatuses &#123; statusStr += fmt.Sprintf(&quot;%+v &quot;, *status) &#125; return false &#125; // 3、检查 pod 的 volume 是否被清理 if kl.podVolumesExist(pod.UID) &amp;&amp; !kl.keepTerminatedPodVolumes &#123; return false &#125; // 4、检查 pod 的 cgroup 是否被清理 if kl.kubeletConfiguration.CgroupsPerQOS &#123; pcm := kl.containerManager.NewPodContainerManager() if pcm.Exists(pod) &#123; return false &#125; &#125; return true&#125; syncBatchsyncBatch 是定期将 statusManager 缓存 podStatuses 中的数据同步到 apiserver 的方法，主要逻辑为： 1、调用 m.podManager.GetUIDTranslations 从 podManager 中获取 mirrorPod uid 与 staticPod uid 的对应关系； 2、从 apiStatusVersions 中清理已经不存在的 pod，遍历 apiStatusVersions，检查 podStatuses 以及 mirrorToPod 中是否存在该对应的 pod，若不存在则从 apiStatusVersions 中删除； 3、遍历 podStatuses，首先调用 needsUpdate 检查 pod 的状态是否与 apiStatusVersions 中的一致，然后调用 needsReconcile 检查 pod 的状态是否与 podManager 中的一致，若不一致则将需要同步的 pod 加入到 updatedStatuses 列表中； 4、遍历 updatedStatuses 列表，调用 m.syncPod 方法同步状态； syncBatch 主要是将 statusManage cache 中的数据与 apiStatusVersions 和 podManager 中的数据进行对比是否一致，若不一致则以 statusManage cache 中的数据为准同步至 apiserver。 k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:469 12345678910111213141516171819202122232425262728293031323334353637383940func (m *manager) syncBatch() &#123; var updatedStatuses []podStatusSyncRequest // 1、获取 mirrorPod 与 staticPod 的对应关系 podToMirror, mirrorToPod := m.podManager.GetUIDTranslations() func() &#123; m.podStatusesLock.RLock() defer m.podStatusesLock.RUnlock() // 2、从 apiStatusVersions 中清理已经不存在的 pod for uid := range m.apiStatusVersions &#123; _, hasPod := m.podStatuses[types.UID(uid)] _, hasMirror := mirrorToPod[uid] if !hasPod &amp;&amp; !hasMirror &#123; delete(m.apiStatusVersions, uid) &#125; &#125; // 3、遍历 podStatuses，将需要同步状态的 pod 加入到 updatedStatuses 列表中 for uid, status := range m.podStatuses &#123; syncedUID := kubetypes.MirrorPodUID(uid) if mirrorUID, ok := podToMirror[kubetypes.ResolvedPodUID(uid)]; ok &#123; if mirrorUID == &quot;&quot; &#123; continue &#125; syncedUID = mirrorUID &#125; if m.needsUpdate(types.UID(syncedUID), status) &#123; updatedStatuses = append(updatedStatuses, podStatusSyncRequest&#123;uid, status&#125;) &#125; else if m.needsReconcile(uid, status.status) &#123; delete(m.apiStatusVersions, syncedUID) updatedStatuses = append(updatedStatuses, podStatusSyncRequest&#123;uid, status&#125;) &#125; &#125; &#125;() // 4、调用 m.syncPod 同步 pod 状态 for _, update := range updatedStatuses &#123; m.syncPod(update.podUID, update.status) &#125;&#125; syncBatch 中主要调用了两个方法 needsUpdate 和 needsReconcile ，needsUpdate 在上文中已经介绍过了，下面介绍 needsReconcile 方法的主要逻辑。 needsReconcileneedsReconcile 对比当前 pod 的状态与 podManager 中的状态是否一致，podManager 中保存了 node 上 pod 的 object，podManager 中的数据与 apiserver 是一致的，needsReconcile 主要逻辑为： 1、通过 uid 从 podManager 中获取 pod 对象； 2、检查 pod 是否为 static pod，若为 static pod 则获取其对应的 mirrorPod； 3、格式化 pod status subResource； 4、检查 podManager 中的 status 与 statusManager cache 中的 status 是否一致，若不一致则以 statusManager 为准进行同步； k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:598 12345678910111213141516171819202122232425262728func (m *manager) needsReconcile(uid types.UID, status v1.PodStatus) bool &#123; // 1、从 podManager 中获取 pod 对象 pod, ok := m.podManager.GetPodByUID(uid) if !ok &#123; return false &#125; // 2、检查 pod 是否为 static pod，若为 static pod 则获取其对应的 mirrorPod if kubetypes.IsStaticPod(pod) &#123; mirrorPod, ok := m.podManager.GetMirrorPodByPod(pod) if !ok &#123; return false &#125; pod = mirrorPod &#125; podStatus := pod.Status.DeepCopy() // 3、格式化 pod status subResource normalizeStatus(pod, podStatus) // 4、检查 podManager 中的 status 与 statusManager cache 中的 status 是否一致 if isPodStatusByKubeletEqual(podStatus, &amp;status) &#123; return false &#125; return true&#125; 以上就是 statusManager 同步 pod status 的主要逻辑，下面再了解一下 statusManager 对其他组件暴露的方法。 SetPodStatusSetPodStatus 是为 pod 设置 status subResource 并会触发同步操作，主要逻辑为： 1、检查 pod.Status.Conditions 中的类型是否为 kubelet 创建的，kubelet 会创建 ContainersReady、Initialized、Ready、PodScheduled 和 Unschedulable 五种类型的 conditions； 2、调用 m.updateStatusInternal 触发更新操作； k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:179 123456789101112131415func (m *manager) SetPodStatus(pod *v1.Pod, status v1.PodStatus) &#123; m.podStatusesLock.Lock() defer m.podStatusesLock.Unlock() for _, c := range pod.Status.Conditions &#123; if !kubetypes.PodConditionByKubelet(c.Type) &#123; klog.Errorf(&quot;Kubelet is trying to update pod condition %q for pod %q. &quot;+ &quot;But it is not owned by kubelet.&quot;, string(c.Type), format.Pod(pod)) &#125; &#125; status = *status.DeepCopy() m.updateStatusInternal(pod, status, pod.DeletionTimestamp != nil)&#125; updateStatusInternalstatusManager 对外暴露的方法中触发状态同步的操作都是由 updateStatusInternal 完成的，updateStatusInternal 会更新 statusManager 的 cache 并会将 newStatus 发送到 m.podStatusChannel 中，然后 statusManager 会调用 syncPod 方法同步到 apiserver。 1、从 cache 中获取 oldStatus； 2、检查 ContainerStatuses 和 InitContainerStatuses 是否合法； 3、为 status 设置 ContainersReady、PodReady、PodInitialized、PodScheduled conditions； 4、设置 status 的 StartTime； 5、格式化 status； 6、将 newStatus 添加到 statusManager 的 cache podStatuses 中； 7、将 newStatus 发送到 m.podStatusChannel 中； k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:362 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func (m *manager) updateStatusInternal(pod *v1.Pod, status v1.PodStatus, forceUpdate bool) bool &#123; var oldStatus v1.PodStatus // 1、从 cache 中获取 oldStatus cachedStatus, isCached := m.podStatuses[pod.UID] if isCached &#123; oldStatus = cachedStatus.status &#125; else if mirrorPod, ok := m.podManager.GetMirrorPodByPod(pod); ok &#123; oldStatus = mirrorPod.Status &#125; else &#123; oldStatus = pod.Status &#125; // 2、检查 ContainerStatuses 和 InitContainerStatuses 是否合法 if err := checkContainerStateTransition(oldStatus.ContainerStatuses, status.ContainerStatuses, pod.Spec.RestartPolicy); err != nil &#123; return false &#125; if err := checkContainerStateTransition(oldStatus.InitContainerStatuses, status.InitContainerStatuses, pod.Spec.RestartPolicy); err != nil &#123; klog.Errorf(&quot;Status update on pod %v/%v aborted: %v&quot;, pod.Namespace, pod.Name, err) return false &#125; // 3、为 status 设置 ContainersReady、PodReady、PodInitialized、PodScheduled conditions updateLastTransitionTime(&amp;status, &amp;oldStatus, v1.ContainersReady) updateLastTransitionTime(&amp;status, &amp;oldStatus, v1.PodReady) updateLastTransitionTime(&amp;status, &amp;oldStatus, v1.PodInitialized) updateLastTransitionTime(&amp;status, &amp;oldStatus, v1.PodScheduled) // 4、设置 status 的 StartTime if oldStatus.StartTime != nil &amp;&amp; !oldStatus.StartTime.IsZero() &#123; status.StartTime = oldStatus.StartTime &#125; else if status.StartTime.IsZero() &#123; now := metav1.Now() status.StartTime = &amp;now &#125; // 5、格式化 status normalizeStatus(pod, &amp;status) if isCached &amp;&amp; isPodStatusByKubeletEqual(&amp;cachedStatus.status, &amp;status) &amp;&amp; !forceUpdate &#123; return false &#125; // 6、将 newStatus 添加到 statusManager 的 cache podStatuses 中 newStatus := versionedPodStatus&#123; status: status, version: cachedStatus.version + 1, podName: pod.Name, podNamespace: pod.Namespace, &#125; m.podStatuses[pod.UID] = newStatus // 7、将 newStatus 发送到 m.podStatusChannel 中 select &#123; case m.podStatusChannel &lt;- podStatusSyncRequest&#123;pod.UID, newStatus&#125;: return true default: return false &#125;&#125; SetPodStatus 方法主要会用在 kubelet 的主 syncLoop 中，并在 syncPod 方法中创建 pod 时使用。 SetContainerReadinessSetContainerReadiness 方法会设置 pod status subResource 中 container 是否为 ready 状态，其主要逻辑为： 1、获取 pod 对象； 2、从 m.podStatuses 获取 oldStatus； 3、通过 containerID 从 pod 中获取 containerStatus； 4、若 container status 为 Ready 直接返回，此时该 container 状态无须更新； 5、添加 PodReadyCondition 和 ContainersReadyCondition； 6、调用 m.updateStatusInternal 触发同步操作； k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:198 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152func (m *manager) SetContainerReadiness(podUID types.UID, containerID kubecontainer.ContainerID, ready bool) &#123; m.podStatusesLock.Lock() defer m.podStatusesLock.Unlock() // 1、获取 pod 对象 pod, ok := m.podManager.GetPodByUID(podUID) if !ok &#123; return &#125; // 2、从 m.podStatuses 获取 oldStatus oldStatus, found := m.podStatuses[pod.UID] if !found &#123; return &#125; // 3、通过 containerID 从 pod 中获取 containerStatus containerStatus, _, ok := findContainerStatus(&amp;oldStatus.status, containerID.String()) if !ok &#123; return &#125; // 4、若 container status 为 Ready 直接返回，此时该 container 状态无须更新 if containerStatus.Ready == ready &#123; return &#125; status := *oldStatus.status.DeepCopy() containerStatus, _, _ = findContainerStatus(&amp;status, containerID.String()) containerStatus.Ready = ready updateConditionFunc := func(conditionType v1.PodConditionType, condition v1.PodCondition) &#123; conditionIndex := -1 for i, condition := range status.Conditions &#123; if condition.Type == conditionType &#123; conditionIndex = i break &#125; &#125; if conditionIndex != -1 &#123; status.Conditions[conditionIndex] = condition &#125; else &#123; status.Conditions = append(status.Conditions, condition) &#125; &#125; // 5、添加 PodReadyCondition 和 ContainersReadyCondition updateConditionFunc(v1.PodReady, GeneratePodReadyCondition(&amp;pod.Spec, status.Conditions, status.ContainerStatuses, status.Phase)) updateConditionFunc(v1.ContainersReady, GenerateContainersReadyCondition(&amp;pod.Spec, status.ContainerStatuses, status.Phase)) // 6、调用 m.updateStatusInternal 触发同步操作 m.updateStatusInternal(pod, status, false)&#125; SetContainerReadiness 方法主要被用在 proberManager 中，关于 proberManager 的功能会在后文说明。 SetContainerStartupSetContainerStartup 方法会设置 pod status subResource 中 container 是否为 started 状态，主要逻辑为： 1、通过 podUID 从 podManager 中获取 pod 对象； 2、从 statusManager 中获取 pod 的 oldStatus； 3、检查要更新的 container 是否存在； 4、检查目标 container 的 started 状态是否已为期望值； 5、设置目标 container 的 started 状态； 6、调用 m.updateStatusInternal 触发同步操作； k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:255 12345678910111213141516171819202122232425262728293031323334353637func (m *manager) SetContainerStartup(podUID types.UID, containerID kubecontainer.ContainerID, started bool) &#123; m.podStatusesLock.Lock() defer m.podStatusesLock.Unlock() // 1、通过 podUID 从 podManager 中获取 pod 对象 pod, ok := m.podManager.GetPodByUID(podUID) if !ok &#123; return &#125; // 2、从 statusManager 中获取 pod 的 oldStatus oldStatus, found := m.podStatuses[pod.UID] if !found &#123; return &#125; // 3、检查要更新的 container 是否存在 containerStatus, _, ok := findContainerStatus(&amp;oldStatus.status, containerID.String()) if !ok &#123; klog.Warningf(&quot;Container startup changed for unknown container: %q - %q&quot;, format.Pod(pod), containerID.String()) return &#125; // 4、检查目标 container 的 started 状态是否已为期望值 if containerStatus.Started != nil &amp;&amp; *containerStatus.Started == started &#123; return &#125; // 5、设置目标 container 的 started 状态 status := *oldStatus.status.DeepCopy() containerStatus, _, _ = findContainerStatus(&amp;status, containerID.String()) containerStatus.Started = &amp;started // 6、触发同步操作 m.updateStatusInternal(pod, status, false)&#125; SetContainerStartup 方法也是主要被用在 proberManager 中。 TerminatePodTerminatePod 方法的主要逻辑是把 pod .status.containerStatuses 和 .status.initContainerStatuses 中 container 的 state 置为 Terminated 状态并触发状态同步操作。 k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:312 1234567891011121314151617181920func (m *manager) TerminatePod(pod *v1.Pod) &#123; m.podStatusesLock.Lock() defer m.podStatusesLock.Unlock() oldStatus := &amp;pod.Status if cachedStatus, ok := m.podStatuses[pod.UID]; ok &#123; oldStatus = &amp;cachedStatus.status &#125; status := *oldStatus.DeepCopy() for i := range status.ContainerStatuses &#123; status.ContainerStatuses[i].State = v1.ContainerState&#123; Terminated: &amp;v1.ContainerStateTerminated&#123;&#125;, &#125; &#125; for i := range status.InitContainerStatuses &#123; status.InitContainerStatuses[i].State = v1.ContainerState&#123; Terminated: &amp;v1.ContainerStateTerminated&#123;&#125;, &#125; &#125; m.updateStatusInternal(pod, status, true)&#125; TerminatePod 方法主要会用在 kubelet 的主 syncLoop 中。 RemoveOrphanedStatusesRemoveOrphanedStatuses 的主要逻辑是从 statusManager 缓存 podStatuses 中删除对应的 pod。 k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:457 12345678910func (m *manager) RemoveOrphanedStatuses(podUIDs map[types.UID]bool) &#123; m.podStatusesLock.Lock() defer m.podStatusesLock.Unlock() for key := range m.podStatuses &#123; if _, ok := podUIDs[key]; !ok &#123; klog.V(5).Infof(&quot;Removing %q from status map.&quot;, key) delete(m.podStatuses, key) &#125; &#125;&#125; 总结本文主要介绍了 statusManager 的功能以及使用，其功能其实非常简单，当 pod 状态改变时 statusManager 会将状态同步到 apiserver，statusManager 也提供了多个接口供其他组件调用，当其他组件需要改变 pod 的状态时会将 pod 的 status 信息发送到 statusManager 进行同步。]]></content>
      <tags>
        <tag>kubelet</tag>
        <tag>statusManager</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[daemonset controller 源码分析]]></title>
    <url>%2F2019%2F12%2F18%2Fdaemonset_controller%2F</url>
    <content type="text"><![CDATA[在前面的文章中已经分析过 deployment、statefulset 两个重要对象了，本文会继续分析 kubernetes 中另一个重要的对象 daemonset，在 kubernetes 中 daemonset 类似于 linux 上的守护进程会运行在每一个 node 上，在实际场景中，一般会将日志采集或者网络插件采用 daemonset 的方式部署。 DaemonSet 的基本操作创建daemonset 在创建后会在每个 node 上都启动一个 pod。 1$ kubectl create -f nginx-ds.yaml 扩缩容由于 daemonset 是在每个 node 上启动一个 pod，其不存在扩缩容操作，副本数量跟 node 数量保持一致。 更新daemonset 有两种更新策略 OnDelete 和 RollingUpdate，默认为 RollingUpdate。滚动更新时，需要指定 .spec.updateStrategy.rollingUpdate.maxUnavailable（默认为1）和 .spec.minReadySeconds（默认为 0）。 12345// 更新镜像$ kubectl set image ds/nginx-ds nginx-ds=nginx:1.16// 查看更新状态$ kubectl rollout status ds/nginx-ds 回滚在 statefulset 源码分析一节已经提到过 controllerRevision 这个对象了，其主要用来保存历史版本信息，在更新以及回滚操作时使用，daemonset controller 也是使用 controllerrevision 保存历史版本信息，在回滚时会使用历史 controllerrevision 中的信息替换 daemonset 中 Spec.Template。 1234567891011// 查看 ds 历史版本信息$ kubectl get controllerrevisionNAME CONTROLLER REVISION AGEnginx-ds-5c4b75bdbb daemonset.apps/nginx-ds 2 122mnginx-ds-7cd7798dcd daemonset.apps/nginx-ds 1 133m// 回滚到版本 1$ kubectl rollout undo daemonset nginx-ds --to-revision=1// 查看回滚状态$ kubectl rollout status ds/nginx-ds 暂停daemonset 目前不支持暂停操作。 删除daemonset 也支持两种删除操作。 12345// 非级联删除$ kubectl delete ds/nginx-ds --cascade=false// 级联删除$ kubectl delete ds/nginx-ds DaemonSetController 源码分析 kubernetes 版本：v1.16 首先还是看 startDaemonSetController 方法，在此方法中会初始化 DaemonSetsController 对象并调用 Run方法启动 daemonset controller，从该方法中可以看出 daemonset controller 会监听 daemonsets、controllerRevision、pod 和 node 四种对象资源的变动。其中 ConcurrentDaemonSetSyncs的默认值为 2。 k8s.io/kubernetes/cmd/kube-controller-manager/app/apps.go:36 123456789101112131415161718func startDaemonSetController(ctx ControllerContext) (http.Handler, bool, error) &#123; if !ctx.AvailableResources[schema.GroupVersionResource&#123;Group: &quot;apps&quot;, Version: &quot;v1&quot;, Resource: &quot;daemonsets&quot;&#125;] &#123; return nil, false, nil &#125; dsc, err := daemon.NewDaemonSetsController( ctx.InformerFactory.Apps().V1().DaemonSets(), ctx.InformerFactory.Apps().V1().ControllerRevisions(), ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Core().V1().Nodes(), ctx.ClientBuilder.ClientOrDie(&quot;daemon-set-controller&quot;), flowcontrol.NewBackOff(1*time.Second, 15*time.Minute), ) if err != nil &#123; return nil, true, fmt.Errorf(&quot;error creating DaemonSets controller: %v&quot;, err) &#125; go dsc.Run(int(ctx.ComponentConfig.DaemonSetController.ConcurrentDaemonSetSyncs), ctx.Stop) return nil, true, nil&#125; 在 Run 方法中会启动两个操作，一个就是 dsc.runWorker 执行的 sync 操作，另一个就是 dsc.failedPodsBackoff.GC 执行的 gc 操作，主要逻辑为： 1、等待 informer 缓存同步完成； 2、启动两个 goroutine 分别执行 dsc.runWorker； 3、启动一个 goroutine 每分钟执行一次 dsc.failedPodsBackoff.GC，从 startDaemonSetController 方法中可以看到 failedPodsBackoff 的 duration为1s，max duration为15m，failedPodsBackoff 的主要作用是当发现 daemon pod 状态为 failed 时，会定时重启该 pod； k8s.io/kubernetes/pkg/controller/daemon/daemon_controller.go:263 1234567891011121314151617181920func (dsc *DaemonSetsController) Run(workers int, stopCh &lt;-chan struct&#123;&#125;) &#123; defer utilruntime.HandleCrash() defer dsc.queue.ShutDown() defer klog.Infof(&quot;Shutting down daemon sets controller&quot;) if !cache.WaitForNamedCacheSync(&quot;daemon sets&quot;, stopCh, dsc.podStoreSynced, dsc.nodeStoreSynced, dsc.historyStoreSynced, dsc.dsStoreSynced) &#123; return &#125; for i := 0; i &lt; workers; i++ &#123; // sync 操作 go wait.Until(dsc.runWorker, time.Second, stopCh) &#125; // GC 操作 go wait.Until(dsc.failedPodsBackoff.GC, BackoffGCInterval, stopCh) &lt;-stopCh&#125; syncDaemonSetdaemonset 中 pod 的创建与删除是与 node 相关联的，所以每次执行 sync 操作时需要遍历所有的 node 进行判断。syncDaemonSet 的主要逻辑为： 1、通过 key 获取 ns 和 name； 2、从 dsLister 中获取 ds 对象； 3、从 nodeLister 获取所有 node； 4、获取 dsKey； 5、判断 ds 是否处于删除状态； 6、调用 constructHistory 获取 current 和 old controllerRevision； 7、调用 dsc.expectations.SatisfiedExpectations 判断是否满足 expectations 机制，expectations 机制的目的就是减少不必要的 sync 操作，关于 expectations 机制的详细说明可以参考笔者以前写的 “replicaset controller 源码分析”一文； 8、调用 dsc.manage 执行实际的 sync 操作； 9、判断是否为更新操作，并执行对应的更新操作逻辑； 10、调用 dsc.cleanupHistory 根据 spec.revisionHistoryLimit字段清理过期的 controllerrevision； 11、调用 dsc.updateDaemonSetStatus 更新 ds 状态； k8s.io/kubernetes/pkg/controller/daemon/daemon_controller.go:1212 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576func (dsc *DaemonSetsController) syncDaemonSet(key string) error &#123; ...... // 1、通过 key 获取 ns 和 name namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; // 2、从 dsLister 中获取 ds 对象 ds, err := dsc.dsLister.DaemonSets(namespace).Get(name) if errors.IsNotFound(err) &#123; dsc.expectations.DeleteExpectations(key) return nil &#125; ...... // 3、从 nodeLister 获取所有 node nodeList, err := dsc.nodeLister.List(labels.Everything()) ...... everything := metav1.LabelSelector&#123;&#125; if reflect.DeepEqual(ds.Spec.Selector, &amp;everything) &#123; dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, SelectingAllReason, &quot;This daemon set is selecting all pods. A non-empty selector is required. &quot;) return nil &#125; // 4、获取 dsKey dsKey, err := controller.KeyFunc(ds) if err != nil &#123; return fmt.Errorf(&quot;couldn&apos;t get key for object %#v: %v&quot;, ds, err) &#125; // 5、判断 ds 是否处于删除状态 if ds.DeletionTimestamp != nil &#123; return nil &#125; // 6、获取 current 和 old controllerRevision cur, old, err := dsc.constructHistory(ds) if err != nil &#123; return fmt.Errorf(&quot;failed to construct revisions of DaemonSet: %v&quot;, err) &#125; hash := cur.Labels[apps.DefaultDaemonSetUniqueLabelKey] // 7、判断是否满足 expectations 机制 if !dsc.expectations.SatisfiedExpectations(dsKey) &#123; return dsc.updateDaemonSetStatus(ds, nodeList, hash, false) &#125; // 8、执行实际的 sync 操作 err = dsc.manage(ds, nodeList, hash) if err != nil &#123; return err &#125; // 9、判断是否为更新操作，并执行对应的更新操作 if dsc.expectations.SatisfiedExpectations(dsKey) &#123; switch ds.Spec.UpdateStrategy.Type &#123; case apps.OnDeleteDaemonSetStrategyType: case apps.RollingUpdateDaemonSetStrategyType: err = dsc.rollingUpdate(ds, nodeList, hash) &#125; if err != nil &#123; return err &#125; &#125; // 10、清理过期的 controllerrevision err = dsc.cleanupHistory(ds, old) if err != nil &#123; return fmt.Errorf(&quot;failed to clean up revisions of DaemonSet: %v&quot;, err) &#125; // 11、更新 ds 状态 return dsc.updateDaemonSetStatus(ds, nodeList, hash, true)&#125; syncDaemonSet 中主要有 manage、rollingUpdate和updateDaemonSetStatus 三个方法，分别对应创建、更新与状态同步，下面主要来分析这三个方法。 managemanage 主要是用来保证 ds 的 pod 数正常运行在每一个 node 上，其主要逻辑为： 1、调用 dsc.getNodesToDaemonPods 获取已存在 daemon pod 与 node 的映射关系； 2、遍历所有 node，调用 dsc.podsShouldBeOnNode 方法来确定在给定的节点上需要创建还是删除 daemon pod； 3、判断是否启动了 ScheduleDaemonSetPodsfeature-gates 特性，若启动了则需要删除通过默认调度器已经调度到不存在 node 上的 daemon pod； 4、调用 dsc.syncNodes 为对应的 node 创建 daemon pod 以及删除多余的 pods； k8s.io/kubernetes/pkg/controller/daemon/daemon_controller.go:952 1234567891011121314151617181920212223242526272829303132func (dsc *DaemonSetsController) manage(ds *apps.DaemonSet, nodeList []*v1.Node, hash string) error &#123; // 1、获取已存在 daemon pod 与 node 的映射关系 nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds) ...... // 2、判断每一个 node 是否需要运行 daemon pod var nodesNeedingDaemonPods, podsToDelete []string for _, node := range nodeList &#123; nodesNeedingDaemonPodsOnNode, podsToDeleteOnNode, err := dsc.podsShouldBeOnNode( node, nodeToDaemonPods, ds) if err != nil &#123; continue &#125; nodesNeedingDaemonPods = append(nodesNeedingDaemonPods, nodesNeedingDaemonPodsOnNode...) podsToDelete = append(podsToDelete, podsToDeleteOnNode...) &#125; // 3、判断是否启动了 ScheduleDaemonSetPods feature-gates 特性，若启用了则对不存在 node 上的 // daemon pod 进行删除 if utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) &#123; podsToDelete = append(podsToDelete, getUnscheduledPodsWithoutNode(nodeList, nodeToDaemonPods)...) &#125; // 4、为对应的 node 创建 daemon pod 以及删除多余的 pods if err = dsc.syncNodes(ds, podsToDelete, nodesNeedingDaemonPods, hash); err != nil &#123; return err &#125; return nil&#125; 在 manage 方法中又调用了 getNodesToDaemonPods、podsShouldBeOnNode 和 syncNodes 三个方法，继续来看这几种方法的作用。 getNodesToDaemonPodsgetNodesToDaemonPods 是用来获取已存在 daemon pod 与 node 的映射关系，并且会通过 adopt/orphan 方法关联以及释放对应的 pod。 k8s.io/kubernetes/pkg/controller/daemon/daemon_controller.go:820 12345678910111213141516171819func (dsc *DaemonSetsController) getNodesToDaemonPods(ds *apps.DaemonSet) (map[string][]*v1.Pod, error) &#123; claimedPods, err := dsc.getDaemonPods(ds) if err != nil &#123; return nil, err &#125; nodeToDaemonPods := make(map[string][]*v1.Pod) for _, pod := range claimedPods &#123; nodeName, err := util.GetTargetNodeName(pod) if err != nil &#123; klog.Warningf(&quot;Failed to get target node name of Pod %v/%v in DaemonSet %v/%v&quot;, pod.Namespace, pod.Name, ds.Namespace, ds.Name) continue &#125; nodeToDaemonPods[nodeName] = append(nodeToDaemonPods[nodeName], pod) &#125; return nodeToDaemonPods, nil&#125; podsShouldBeOnNodepodsShouldBeOnNode 方法用来确定在给定的节点上需要创建还是删除 daemon pod，主要逻辑为： 1、调用 dsc.nodeShouldRunDaemonPod 判断该 node 是否需要运行 daemon pod 以及 pod 能不能调度成功，该方法返回三个值 wantToRun, shouldSchedule, shouldContinueRunning； 2、通过判断 wantToRun, shouldSchedule, shouldContinueRunning 将需要创建 daemon pod 的 node 列表以及需要删除的 pod 列表获取到， wantToRun主要检查的是 selector、taints 等是否匹配，shouldSchedule 主要检查 node 上的资源是否充足，shouldContinueRunning 默认为 true； k8s.io/kubernetes/pkg/controller/daemon/daemon_controller.go:866 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768func (dsc *DaemonSetsController) podsShouldBeOnNode(...) (nodesNeedingDaemonPods, podsToDelete []string, err error) &#123; // 1、判断该 node 是否需要运行 daemon pod 以及能不能调度成功 wantToRun, shouldSchedule, shouldContinueRunning, err := dsc.nodeShouldRunDaemonPod(node, ds) if err != nil &#123; return &#125; // 2、获取该节点上的指定ds的pod列表 daemonPods, exists := nodeToDaemonPods[node.Name] dsKey, err := cache.MetaNamespaceKeyFunc(ds) if err != nil &#123; utilruntime.HandleError(err) return &#125; // 3、从 suspended list 中移除在该节点上 ds 的 pod dsc.removeSuspendedDaemonPods(node.Name, dsKey) switch &#123; // 4、对于需要创建 pod 但是不能调度 pod 的 node，先把 pod 放入到 suspended 队列中 case wantToRun &amp;&amp; !shouldSchedule: dsc.addSuspendedDaemonPods(node.Name, dsKey) // 5、需要创建 pod 且 pod 未运行，则创建 pod case shouldSchedule &amp;&amp; !exists: nodesNeedingDaemonPods = append(nodesNeedingDaemonPods, node.Name) // 6、需要 pod 一直运行 case shouldContinueRunning: var daemonPodsRunning []*v1.Pod for _, pod := range daemonPods &#123; if pod.DeletionTimestamp != nil &#123; continue &#125; // 7、如果 pod 运行状态为 failed，则删除该 pod if pod.Status.Phase == v1.PodFailed &#123; backoffKey := failedPodsBackoffKey(ds, node.Name) now := dsc.failedPodsBackoff.Clock.Now() inBackoff := dsc.failedPodsBackoff.IsInBackOffSinceUpdate(backoffKey, now) if inBackoff &#123; delay := dsc.failedPodsBackoff.Get(backoffKey) dsc.enqueueDaemonSetAfter(ds, delay) continue &#125; dsc.failedPodsBackoff.Next(backoffKey, now) podsToDelete = append(podsToDelete, pod.Name) &#125; else &#123; daemonPodsRunning = append(daemonPodsRunning, pod) &#125; &#125; // 8、如果节点上已经运行 daemon pod 数 &gt; 1，保留运行时间最长的 pod，其余的删除 if len(daemonPodsRunning) &gt; 1 &#123; sort.Sort(podByCreationTimestampAndPhase(daemonPodsRunning)) for i := 1; i &lt; len(daemonPodsRunning); i++ &#123; podsToDelete = append(podsToDelete, daemonPodsRunning[i].Name) &#125; &#125; // 9、如果 pod 不需要继续运行但 pod 已存在则需要删除 pod case !shouldContinueRunning &amp;&amp; exists: for _, pod := range daemonPods &#123; if pod.DeletionTimestamp != nil &#123; continue &#125; podsToDelete = append(podsToDelete, pod.Name) &#125; &#125; return nodesNeedingDaemonPods, podsToDelete, nil&#125; 然后继续看 nodeShouldRunDaemonPod 方法的主要逻辑： 1、调用 NewPod 为该 node 构建一个 daemon pod object； 2、判断 ds 是否指定了 .Spec.Template.Spec.NodeName 字段； 3、调用 dsc.simulate 执行 GeneralPredicates 预选算法检查该 node 是否能够调度成功； 4、判断 GeneralPredicates 预选算法执行后的 reasons 确定 wantToRun, shouldSchedule, shouldContinueRunning 的值； k8s.io/kubernetes/pkg/controller/daemon/daemon_controller.go:1337 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768func (dsc *DaemonSetsController) nodeShouldRunDaemonPod(node *v1.Node, ds *apps.DaemonSet) (wantToRun, shouldSchedule, shouldContinueRunning bool, err error) &#123; // 1、构建 daemon pod object newPod := NewPod(ds, node.Name) wantToRun, shouldSchedule, shouldContinueRunning = true, true, true // 2、判断 ds 是否指定了 node，若指定了且不为当前 node 直接返回 false if !(ds.Spec.Template.Spec.NodeName == &quot;&quot; || ds.Spec.Template.Spec.NodeName == node.Name) &#123; return false, false, false, nil &#125; // 3、执行 GeneralPredicates 预选算法 reasons, nodeInfo, err := dsc.simulate(newPod, node, ds) if err != nil &#123; ...... &#125; // 4、检查预选算法执行的结果 var insufficientResourceErr error for _, r := range reasons &#123; switch reason := r.(type) &#123; case *predicates.InsufficientResourceError: insufficientResourceErr = reason case *predicates.PredicateFailureError: var emitEvent bool switch reason &#123; case predicates.ErrNodeSelectorNotMatch, predicates.ErrPodNotMatchHostName, predicates.ErrNodeLabelPresenceViolated, predicates.ErrPodNotFitsHostPorts: return false, false, false, nil case predicates.ErrTaintsTolerationsNotMatch: fitsNoExecute, _, err := predicates.PodToleratesNodeNoExecuteTaints(newPod, nil, nodeInfo) if err != nil &#123; return false, false, false, err &#125; if !fitsNoExecute &#123; return false, false, false, nil &#125; wantToRun, shouldSchedule = false, false case predicates.ErrDiskConflict, predicates.ErrVolumeZoneConflict, predicates.ErrMaxVolumeCountExceeded, predicates.ErrNodeUnderMemoryPressure, predicates.ErrNodeUnderDiskPressure: shouldSchedule = false emitEvent = true case predicates.ErrPodAffinityNotMatch, predicates.ErrServiceAffinityViolated: return false, false, false, fmt.Errorf(&quot;unexpected reason: DaemonSet Predicates should not return reason %s&quot;, reason.GetReason()) default: wantToRun, shouldSchedule, shouldContinueRunning = false, false, false emitEvent = true &#125; ...... &#125; &#125; if shouldSchedule &amp;&amp; insufficientResourceErr != nil &#123; dsc.eventRecorder.Eventf(ds, v1.EventTypeWarning, FailedPlacementReason, &quot;failed to place pod on %q: %s&quot;, node.ObjectMeta.Name, insufficientResourceErr.Error()) shouldSchedule = false &#125; return&#125; syncNodessyncNodes 方法主要是为需要 daemon pod 的 node 创建 pod 以及删除多余的 pod，其主要逻辑为： 1、将 createDiff 和 deleteDiff 与 burstReplicas 进行比较，burstReplicas 默认值为 250 即每个 syncLoop 中创建或者删除的 pod 数最多为 250 个，若超过其值则剩余需要创建或者删除的 pod 在下一个 syncLoop 继续操作； 2、将 createDiff 和 deleteDiff 写入到 expectations 中； 3、并发创建 pod，创建 pod 有两种方法:（1）创建的 pod 不经过默认调度器，直接指定了 pod 的运行节点(即设定pod.Spec.NodeName)；（2）若启用了 ScheduleDaemonSetPods feature-gates 特性，则使用默认调度器进行创建 pod，通过 nodeAffinity来保证每个节点都运行一个 pod； 4、并发删除 deleteDiff 中的所有 pod； ScheduleDaemonSetPods 是一个 feature-gates 特性，其出现在 v1.11 中，在 v1.12 中处于 Beta 版本，v1.17 为 GA 版。最初 daemonset controller 只有一种创建 pod 的方法，即直接指定 pod 的 spec.NodeName 字段，但是目前这种方式已经暴露了许多问题，在以后的发展中社区还是希望能通过默认调度器进行调度，所以才出现了第二种方式，原因主要有以下五点： 1、DaemonSet 无法感知 node 上资源的变化 (#46935, #58868)：当 pod 第一次因资源不够无法创建时，若其他 pod 退出后资源足够时 DaemonSet 无法感知到； 2、Daemonset 无法支持 Pod Affinity 和 Pod AntiAffinity 的功能(#29276)； 3、在某些功能上需要实现和 scheduler 重复的代码逻辑, 例如：critical pods (#42028), tolerant/taint； 4、当 DaemonSet 的 Pod 创建失败时难以 debug，例如：资源不足时，对于 pending pod 最好能打一个 event 说明； 5、多个组件同时调度时难以实现抢占机制：这也是无法通过横向扩展调度器提高调度吞吐量的一个原因； 更详细的原因可以参考社区的文档：schedule-DS-pod-by-scheduler.md。 k8s.io/kubernetes/pkg/controller/daemon/daemon_controller.go:990 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192func (dsc *DaemonSetsController) syncNodes(ds *apps.DaemonSet, podsToDelete, nodesNeedingDaemonPods []string, hash string) error &#123; ...... // 1、设置 burstReplicas createDiff := len(nodesNeedingDaemonPods) deleteDiff := len(podsToDelete) if createDiff &gt; dsc.burstReplicas &#123; createDiff = dsc.burstReplicas &#125; if deleteDiff &gt; dsc.burstReplicas &#123; deleteDiff = dsc.burstReplicas &#125; // 2、写入到 expectations 中 dsc.expectations.SetExpectations(dsKey, createDiff, deleteDiff) errCh := make(chan error, createDiff+deleteDiff) createWait := sync.WaitGroup&#123;&#125; generation, err := util.GetTemplateGeneration(ds) if err != nil &#123; generation = nil &#125; template := util.CreatePodTemplate(ds.Spec.Template, generation, hash) // 3、并发创建 pod，创建的 pod 数依次为 1, 2, 4, 8, ... batchSize := integer.IntMin(createDiff, controller.SlowStartInitialBatchSize) for pos := 0; createDiff &gt; pos; batchSize, pos = integer.IntMin(2*batchSize, createDiff-(pos+batchSize)), pos+batchSize &#123; errorCount := len(errCh) createWait.Add(batchSize) for i := pos; i &lt; pos+batchSize; i++ &#123; go func(ix int) &#123; defer createWait.Done() var err error podTemplate := template.DeepCopy() // 4、若启动了 ScheduleDaemonSetPods 功能，则通过 kube-scheduler 创建 pod if utilfeature.DefaultFeatureGate.Enabled(features.ScheduleDaemonSetPods) &#123; podTemplate.Spec.Affinity = util.ReplaceDaemonSetPodNodeNameNodeAffinity( podTemplate.Spec.Affinity, nodesNeedingDaemonPods[ix]) err = dsc.podControl.CreatePodsWithControllerRef(ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind)) &#125; else &#123; // 5、否则直接设置 pod 的 .spec.NodeName 创建 pod err = dsc.podControl.CreatePodsOnNode(nodesNeedingDaemonPods[ix], ds.Namespace, podTemplate, ds, metav1.NewControllerRef(ds, controllerKind)) &#125; // 6、创建 pod 时忽略 timeout err if err != nil &amp;&amp; errors.IsTimeout(err) &#123; return &#125; if err != nil &#123; dsc.expectations.CreationObserved(dsKey) errCh &lt;- err utilruntime.HandleError(err) &#125; &#125;(i) &#125; createWait.Wait() // 7、将创建失败的 pod 数记录到 expectations 中 skippedPods := createDiff - (batchSize + pos) if errorCount &lt; len(errCh) &amp;&amp; skippedPods &gt; 0 &#123; dsc.expectations.LowerExpectations(dsKey, skippedPods, 0) break &#125; &#125; // 8、并发删除 deleteDiff 中的 pod deleteWait := sync.WaitGroup&#123;&#125; deleteWait.Add(deleteDiff) for i := 0; i &lt; deleteDiff; i++ &#123; go func(ix int) &#123; defer deleteWait.Done() if err := dsc.podControl.DeletePod(ds.Namespace, podsToDelete[ix], ds); err != nil &#123; dsc.expectations.DeletionObserved(dsKey) errCh &lt;- err utilruntime.HandleError(err) &#125; &#125;(i) &#125; deleteWait.Wait() errors := []error&#123;&#125; close(errCh) for err := range errCh &#123; errors = append(errors, err) &#125; return utilerrors.NewAggregate(errors)&#125; RollingUpdatedaemonset update 的方式有两种 OnDelete 和 RollingUpdate，当为 OnDelete 时需要用户手动删除每一个 pod 后完成更新操作，当为 RollingUpdate 时，daemonset controller 会自动控制升级进度。 当为 RollingUpdate 时，主要逻辑为： 1、获取 daemonset pod 与 node 的映射关系； 2、根据 controllerrevision 的 hash 值获取所有未更新的 pods； 3、获取 maxUnavailable, numUnavailable 的 pod 数值，maxUnavailable 是从 ds 的 rollingUpdate 字段中获取的默认值为 1，numUnavailable 的值是通过 daemonset pod 与 node 的映射关系计算每个 node 下是否有 available pod 得到的； 4、通过 oldPods 获取 oldAvailablePods, oldUnavailablePods 的 pod 列表； 5、遍历 oldUnavailablePods 列表将需要删除的 pod 追加到 oldPodsToDelete 数组中。oldUnavailablePods 列表中的 pod 分为两种，一种处于更新中，即删除状态，一种处于未更新且异常状态，处于异常状态的都需要被删除； 6、遍历 oldAvailablePods 列表，此列表中的 pod 都处于正常运行状态，根据 maxUnavailable 值确定是否需要删除该 pod 并将需要删除的 pod 追加到 oldPodsToDelete 数组中； 7、调用 dsc.syncNodes 删除 oldPodsToDelete 数组中的 pods，syncNodes 方法在 manage 阶段已经分析过，此处不再详述； rollingUpdate 的结果是找出需要删除的 pods 并进行删除，被删除的 pod 在下一个 syncLoop 中会通过 manage 方法使用最新版本的 daemonset template 进行创建，整个滚动更新的过程是通过先删除再创建的方式一步步完成更新的，每次操作都是严格按照 maxUnavailable 的值确定需要删除的 pod 数。 k8s.io/kubernetes/pkg/controller/daemon/update.go:43 12345678910111213141516171819202122232425262728293031323334func (dsc *DaemonSetsController) rollingUpdate(......) error &#123; // 1、获取 daemonset pod 与 node 的映射关系 nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds) ...... // 2、获取所有未更新的 pods _, oldPods := dsc.getAllDaemonSetPods(ds, nodeToDaemonPods, hash) // 3、计算 maxUnavailable, numUnavailable 的 pod 数值 maxUnavailable, numUnavailable, err := dsc.getUnavailableNumbers(ds, nodeList, nodeToDaemonPods) if err != nil &#123; return fmt.Errorf(&quot;couldn&apos;t get unavailable numbers: %v&quot;, err) &#125; oldAvailablePods, oldUnavailablePods := util.SplitByAvailablePods(ds.Spec.MinReadySeconds, oldPods) // 4、将非 running 状态的 pods 加入到 oldPodsToDelete 中 var oldPodsToDelete []string for _, pod := range oldUnavailablePods &#123; if pod.DeletionTimestamp != nil &#123; continue &#125; oldPodsToDelete = append(oldPodsToDelete, pod.Name) &#125; // 5、根据 maxUnavailable 值确定是否需要删除 pod for _, pod := range oldAvailablePods &#123; if numUnavailable &gt;= maxUnavailable &#123; break &#125; oldPodsToDelete = append(oldPodsToDelete, pod.Name) numUnavailable++ &#125; // 6、调用 syncNodes 方法删除 oldPodsToDelete 数组中的 pods return dsc.syncNodes(ds, oldPodsToDelete, []string&#123;&#125;, hash)&#125; 总结一下，manage 方法中的主要流程为： 1234567 |-&gt; dsc.getNodesToDaemonPods | |manage ---- |-&gt; dsc.podsShouldBeOnNode ---&gt; dsc.nodeShouldRunDaemonPod | | |-&gt; dsc.syncNodes updateDaemonSetStatusupdateDaemonSetStatus 是 syncDaemonSet 中最后执行的方法，主要是用来计算 ds status subresource 中的值并更新其 status。status 如下所示： 123456789status: currentNumberScheduled: 1 // 已经运行了 DaemonSet Pod的节点数量 desiredNumberScheduled: 1 // 需要运行该DaemonSet Pod的节点数量 numberMisscheduled: 0 // 不需要运行 DeamonSet Pod 但是已经运行了的节点数量 numberReady: 0 // DaemonSet Pod状态为Ready的节点数量 numberAvailable: 1 // DaemonSet Pod状态为Ready且运行时间超过 // Spec.MinReadySeconds 的节点数量 numberUnavailable: 0 // desiredNumberScheduled - numberAvailable 的节点数量 observedGeneration: 3 updatedNumberScheduled: 1 // 已经完成DaemonSet Pod更新的节点数量 updateDaemonSetStatus 主要逻辑为： 1、调用 dsc.getNodesToDaemonPods 获取已存在 daemon pod 与 node 的映射关系； 2、遍历所有 node，调用 dsc.nodeShouldRunDaemonPod 判断该 node 是否需要运行 daemon pod，然后计算 status 中的部分字段值； 3、调用 storeDaemonSetStatus 更新 ds status subresource； 4、判断 ds 是否需要 resync； k8s.io/kubernetes/pkg/controller/daemon/daemon_controller.go:1152 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556func (dsc *DaemonSetsController) updateDaemonSetStatus(......) error &#123; // 1、获取已存在 daemon pod 与 node 的映射关系 nodeToDaemonPods, err := dsc.getNodesToDaemonPods(ds) ...... var desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable int for _, node := range nodeList &#123; // 2、判断该 node 是否需要运行 daemon pod wantToRun, _, _, err := dsc.nodeShouldRunDaemonPod(node, ds) if err != nil &#123; return err &#125; scheduled := len(nodeToDaemonPods[node.Name]) &gt; 0 // 3、计算 status 中的字段值 if wantToRun &#123; desiredNumberScheduled++ if scheduled &#123; currentNumberScheduled++ daemonPods, _ := nodeToDaemonPods[node.Name] sort.Sort(podByCreationTimestampAndPhase(daemonPods)) pod := daemonPods[0] if podutil.IsPodReady(pod) &#123; numberReady++ if podutil.IsPodAvailable(pod, ds.Spec.MinReadySeconds, metav1.Now()) &#123; numberAvailable++ &#125; &#125; generation, err := util.GetTemplateGeneration(ds) if err != nil &#123; generation = nil &#125; if util.IsPodUpdated(pod, hash, generation) &#123; updatedNumberScheduled++ &#125; &#125; &#125; else &#123; if scheduled &#123; numberMisscheduled++ &#125; &#125; &#125; numberUnavailable := desiredNumberScheduled - numberAvailable // 4、更新 daemonset status subresource err = storeDaemonSetStatus(dsc.kubeClient.AppsV1().DaemonSets(ds.Namespace), ds, desiredNumberScheduled, currentNumberScheduled, numberMisscheduled, numberReady, updatedNumberScheduled, numberAvailable, numberUnavailable, updateObservedGen) if err != nil &#123; return fmt.Errorf(&quot;error storing status for daemon set %#v: %v&quot;, ds, err) &#125; // 5、判断 ds 是否需要 resync if ds.Spec.MinReadySeconds &gt; 0 &amp;&amp; numberReady != numberAvailable &#123; dsc.enqueueDaemonSetAfter(ds, time.Duration(ds.Spec.MinReadySeconds)*time.Second) &#125; return nil&#125; 最后，再总结一下 syncDaemonSet 方法的主要流程： 123456789101112 |-&gt; dsc.getNodesToDaemonPods | | |-&gt; manage --&gt;|-&gt; dsc.podsShouldBeOnNode ---&gt; dsc.nodeShouldRunDaemonPod | | | |syncDaemonSet --&gt; | |-&gt; dsc.syncNodes | |-&gt; rollingUpdate | | |-&gt; updateDaemonSetStatus 总结在 daemonset controller 中可以看到许多功能都是 deployment 和 statefulset 已有的。在创建 pod 的流程与 replicaset controller 创建 pod 的流程是相似的，都使用了 expectations 机制并且限制了在一个 syncLoop 中最多创建或删除的 pod 数。更新方式与 statefulset 一样都有 OnDelete 和 RollingUpdate 两种， OnDelete 方式与 statefulset 相似，都需要手动删除对应的 pod，而 RollingUpdate 方式与 statefulset 和 deployment 都有点区别， RollingUpdate方式更新时不支持暂停操作并且 pod 是先删除再创建的顺序进行。版本控制方式与 statefulset 的一样都是使用 controllerRevision。最后要说的一点是在 v1.12 及以后的版本中，使用 daemonset 创建的 pod 已不再使用直接指定 .spec.nodeName的方式绕过调度器进行调度，而是走默认调度器通过 nodeAffinity 的方式调度到每一个节点上。 参考： https://yq.aliyun.com/articles/702305]]></content>
      <tags>
        <tag>kube-controller-manager</tag>
        <tag>daemonset controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[statefulset controller 源码分析]]></title>
    <url>%2F2019%2F12%2F11%2Fstatefulset_controller%2F</url>
    <content type="text"><![CDATA[Statefulset 的基本功能statefulset 旨在与有状态的应用及分布式系统一起使用，statefulset 中的每个 pod 拥有一个唯一的身份标识，并且所有 pod 名都是按照 {0..N-1} 的顺序进行编号。本文会主要分析 statefulset controller 的设计与实现，在分析源码前先介绍一下 statefulset 的基本使用。 创建对于一个拥有 N 个副本的 statefulset，pod 是按照 {0..N-1}的序号顺序创建的，并且会等待前一个 pod 变为 Running &amp; Ready 后才会启动下一个 pod。 12345678910$ kubectl create -f sts.yaml$ kubectl get pod -o wide -wNAME READY STATUS RESTARTS AGE IP NODEweb-0 0/1 ContainerCreating 0 20s &lt;none&gt; minikubeweb-0 1/1 Running 0 3m1s 10.1.0.8 minikubeweb-1 0/1 Pending 0 0s &lt;none&gt; &lt;none&gt;web-1 0/1 ContainerCreating 0 2s &lt;none&gt; minikubeweb-1 1/1 Running 0 4s 10.1.0.9 minikube 扩容statefulset 扩容时 pod 也是顺序创建的，编号与前面的 pod 相接。 123456789101112$ kubectl scale sts web --replicas=4statefulset.apps/web scaled$ kubectl get pod -o wide -w......web-2 0/1 Pending 0 0s &lt;none&gt; &lt;none&gt;web-2 0/1 ContainerCreating 0 1s &lt;none&gt; minikubeweb-2 1/1 Running 0 4s 10.1.0.10 minikubeweb-3 0/1 Pending 0 0s &lt;none&gt; &lt;none&gt;web-3 0/1 ContainerCreating 0 1s &lt;none&gt; minikubeweb-3 1/1 Running 0 4s 10.1.0.11 minikube 缩容缩容时控制器会按照与 pod 序号索引相反的顺序每次删除一个 pod，在删除下一个 pod 前会等待上一个被完全删除。 123456789$ kubectl scale sts web --replicas=2$ kubectl get pod -o wide -w......web-3 1/1 Terminating 0 8m25s 10.1.0.11 minikubeweb-3 0/1 Terminating 0 8m27s &lt;none&gt; minikubeweb-2 1/1 Terminating 0 8m31s 10.1.0.10 minikubeweb-2 0/1 Terminating 0 8m33s 10.1.0.10 minikube 更新更新策略由 statefulset 中的 spec.updateStrategy.type 字段决定，可以指定为 OnDelete 或者 RollingUpdate , 默认的更新策略为 RollingUpdate。当使用RollingUpdate 更新策略更新所有 pod 时采用与序号索引相反的顺序进行更新，即最先删除序号最大的 pod 并根据更新策略中的 partition 参数来进行分段更新，控制器会更新所有序号大于或等于 partition 的 pod，等该区间内的 pod 更新完成后需要再次设定 partition 的值以此来更新剩余的 pod，最终 partition 被设置为 0 时代表更新完成了所有的 pod。在更新过程中，如果一个序号小于 partition 的 pod 被删除或者终止，controller 依然会使用更新前的配置重新创建。 12345678910// 使用 RollingUpdate 策略更新$ kubectl patch statefulset web --type=&apos;json&apos; -p=&apos;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;nginx:1.16&quot;&#125;]&apos;statefulset.apps/web patched$ kubectl rollout status sts/webWaiting for 1 pods to be ready...Waiting for partitioned roll out to finish: 1 out of 2 new pods have been updated...Waiting for 1 pods to be ready...partitioned roll out complete: 2 new pods have been updated... 如果 statefulset 的 .spec.updateStrategy.type 字段被设置为 OnDelete，在更新 statefulset 时，statefulset controller 将不会自动更新其 pod。你必须手动删除 pod，此时 statefulset controller 在重新创建 pod 时，使用修改过的 .spec.template 的内容创建新 pod。 12345678910// 使用 OnDelete 方式更新$ kubectl patch statefulset nginx --type=&apos;json&apos; -p=&apos;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;nginx:1.9&quot;&#125;]&apos;// 删除 web-1$ kubectl delete pod web-1// 查看 web-0 与 web-1 的镜像版本，此时发现 web-1 已经变为最新版本 nginx:1.9 了$ kubectl get pod -l app=nginx -o jsonpath=&apos;&#123;range .items[*]&#125;&#123;.metadata.name&#125;&#123;&quot;\t&quot;&#125;&#123;.spec.containers[0].image&#125;&#123;&quot;\n&quot;&#125;&#123;end&#125;&apos;web-0 nginx:1.16web-1 nginx:1.9 使用滚动更新策略时你必须以某种策略不断更新 partition 值来进行升级，类似于金丝雀部署方式，升级对于 pod 名称来说是逆序。使用非滚动更新方式，需要手动删除对应的 pod，升级可以是无序的。 回滚statefulset 和 deployment 一样也支持回滚操作，statefulset 也保存了历史版本，和 deployment 一样利用.spec.revisionHistoryLimit 字段设置保存多少个历史版本，但 statefulset 的回滚并不是自动进行的，回滚操作也仅仅是进行了一次发布更新，和发布更新的策略一样，更新 statefulset 后需要按照对应的策略手动删除 pod 或者修改 partition 字段以达到回滚 pod 的目的。 12345678910111213141516// 查看 sts 的历史版本$ kubectl rollout history statefulset webstatefulset.apps/webREVISION0056$ kubectl get controllerrevisionNAME CONTROLLER REVISION AGEweb-6c4c79564f statefulset.apps/web 6 11mweb-c47b9997f statefulset.apps/web 5 4h13m// 回滚至最近的一个版本$ kubectl rollout undo statefulset web --to-revision=5 因为 statefulset 的使用对象是有状态服务，大部分有状态副本集都会用到持久存储，statefulset 下的每个 pod 正常情况下都会关联一个 pv 对象，对 statefulset 对象回滚非常容易，但其使用的 pv 中保存的数据无法回滚，所以在生产环境中进行回滚时需要谨慎操作，statefulset、pod、pvc 和 pv 关系图如下所示： 删除statefulset 同时支持级联和非级联删除。使用非级联方式删除 statefulset 时，statefulset 的 pod 不会被删除。使用级联删除时，statefulset 和它关联的 pod 都会被删除。对于级联与非级联删除，在删除时需要指定删除选项(orphan、background 或者 foreground)进行区分。 123456789101112131415// 1、非级联删除$ kubectl delete statefulset web --cascade=false// 删除 sts 后 pod 依然处于运行中$ kubectl get podNAME READY STATUS RESTARTS AGEweb-0 1/1 Running 0 4m38sweb-1 1/1 Running 0 17m// 重新创建 sts 后，会再次关联所有的 pod$ kubectl create -f sts.yaml$ kubectl get stsNAME READY AGEweb 2/2 28s 在级联删除 statefulset 时，会将所有的 pod 同时删掉，statefulset 控制器会首先进行一个类似缩容的操作，pod 按照和他们序号索引相反的顺序每次终止一个。在终止一个 pod 前，statefulset 控制器会等待 pod 后继者被完全终止。 123456789// 2、级联删除$ kubectl delete statefulset web$ kubectl get pod -o wide -w......web-0 1/1 Terminating 0 17m 10.1.0.18 minikube &lt;none&gt; &lt;none&gt;web-1 1/1 Terminating 0 36m 10.1.0.15 minikube &lt;none&gt; &lt;none&gt;web-1 0/1 Terminating 0 36m 10.1.0.15 minikube &lt;none&gt; &lt;none&gt;web-0 0/1 Terminating 0 17m 10.1.0.18 minikube &lt;none&gt; &lt;none&gt; Pod 管理策略statefulset 的默认管理策略是 OrderedReady，该策略遵循上文展示的顺序性保证。statefulset 还有另外一种管理策略 Parallel，Parallel 管理策略告诉 statefulset 控制器并行的终止所有 pod，在启动或终止另一个 pod 前，不必等待这些 pod 变成 Running &amp; Ready 或者完全终止状态，但是 Parallel 仅仅支持在 OnDelete 策略下生效，下文会在源码中具体分析。 StatefulSetController 源码分析 kubernetes 版本：v1.16 startStatefulSetController 是 statefulSetController 的启动方法，其中调用 NewStatefulSetController 进行初始化 controller 对象然后调用 Run 方法启动 controller。其中 ConcurrentStatefulSetSyncs 默认值为 5。 k8s.io/kubernetes/cmd/kube-controller-manager/app/apps.go:55 12345678910111213func startStatefulSetController(ctx ControllerContext) (http.Handler, bool, error) &#123; if !ctx.AvailableResources[schema.GroupVersionResource&#123;Group: &quot;apps&quot;, Version: &quot;v1&quot;, Resource: &quot;statefulsets&quot;&#125;] &#123; return nil, false, nil &#125; go statefulset.NewStatefulSetController( ctx.InformerFactory.Core().V1().Pods(), ctx.InformerFactory.Apps().V1().StatefulSets(), ctx.InformerFactory.Core().V1().PersistentVolumeClaims(), ctx.InformerFactory.Apps().V1().ControllerRevisions(), ctx.ClientBuilder.ClientOrDie(&quot;statefulset-controller&quot;), ).Run(int(ctx.ComponentConfig.StatefulSetController.ConcurrentStatefulSetSyncs), ctx.Stop) return nil, true, nil&#125; 当 controller 启动后会通过 informer 同步 cache 并监听 pod 和 statefulset 对象的变更事件，informer 的处理流程此处不再详细讲解，最后会执行 sync 方法，sync 方法是每个 controller 的核心方法，下面直接看 statefulset controller 的 sync 方法。 syncsync 方法的主要逻辑为： 1、根据 ns/name 获取 sts 对象； 2、获取 sts 的 selector； 3、调用 ssc.adoptOrphanRevisions 检查是否有孤儿 controllerrevisions 对象，若有且能匹配 selector 的则添加 ownerReferences 进行关联，已关联但 label 不匹配的则进行释放； 4、调用 ssc.getPodsForStatefulSet 通过 selector 获取 sts 关联的 pod，若有孤儿 pod 的 label 与 sts 的能匹配则进行关联，若已关联的 pod label 有变化则解除与 sts 的关联关系； 5、最后调用 ssc.syncStatefulSet 执行真正的 sync 操作； k8s.io/kubernetes/pkg/controller/statefulset/stateful_set.go:408 12345678910111213141516171819202122232425262728func (ssc *StatefulSetController) sync(key string) error &#123; ...... namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; // 1、获取 sts 对象 set, err := ssc.setLister.StatefulSets(namespace).Get(name) ...... selector, err := metav1.LabelSelectorAsSelector(set.Spec.Selector) ...... // 2、关联以及释放 sts 的 controllerrevisions if err := ssc.adoptOrphanRevisions(set); err != nil &#123; return err &#125; // 3、获取 sts 所关联的 pod pods, err := ssc.getPodsForStatefulSet(set, selector) if err != nil &#123; return err &#125; return ssc.syncStatefulSet(set, pods)&#125; syncStatefulSet在 syncStatefulSet 中仅仅是调用了 ssc.control.UpdateStatefulSet 方法进行处理。ssc.control.UpdateStatefulSet 会调用 defaultStatefulSetControl 的 UpdateStatefulSet 方法，defaultStatefulSetControl 是 statefulset controller 中另外一个对象，主要负责处理 statefulset 的更新。 k8s.io/kubernetes/pkg/controller/statefulset/stateful_set.go:448 12345678func (ssc *StatefulSetController) syncStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error &#123; ...... if err := ssc.control.UpdateStatefulSet(set.DeepCopy(), pods); err != nil &#123; return err &#125; ...... return nil&#125; UpdateStatefulSet 方法的主要逻辑如下所示： 1、获取历史 revisions； 2、计算 currentRevision 和 updateRevision，若 sts 处于更新过程中则 currentRevision 和 updateRevision 值不同； 3、调用 ssc.updateStatefulSet 执行实际的 sync 操作； 4、调用 ssc.updateStatefulSetStatus 更新 status subResource； 5、根据 sts 的 spec.revisionHistoryLimit字段清理过期的 controllerrevision； 在基本操作的回滚阶段提到了过，sts 通过 controllerrevision 保存历史版本，类似于 deployment 的 replicaset，与 replicaset 不同的是 controllerrevision 仅用于回滚阶段，在 sts 的滚动升级过程中是通过 currentRevision 和 updateRevision来进行控制并不会用到 controllerrevision。 k8s.io/kubernetes/pkg/controller/statefulset/stateful_set_control.go:75 12345678910111213141516171819202122232425262728293031func (ssc *defaultStatefulSetControl) UpdateStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error &#123; // 1、获取历史 revisions revisions, err := ssc.ListRevisions(set) if err != nil &#123; return err &#125; history.SortControllerRevisions(revisions) // 2、计算 currentRevision 和 updateRevision currentRevision, updateRevision, collisionCount, err := ssc.getStatefulSetRevisions(set, revisions) if err != nil &#123; return err &#125; // 3、执行实际的 sync 操作 status, err := ssc.updateStatefulSet(set, currentRevision, updateRevision, collisionCount, pods) if err != nil &#123; return err &#125; // 4、更新 sts 状态 err = ssc.updateStatefulSetStatus(set, status) if err != nil &#123; return err &#125; ...... // 5、清理过期的历史版本 return ssc.truncateHistory(set, pods, revisions, currentRevision, updateRevision)&#125; updateStatefulSetupdateStatefulSet 是 sync 操作中的核心方法，对于 statefulset 的创建、扩缩容、更新、删除等操作都会在这个方法中完成，以下是其主要逻辑： 1、分别获取 currentRevision 和 updateRevision 对应的的 statefulset object； 2、构建 status 对象； 3、将 statefulset 的 pods 按 ord(ord 为 pod name 中的序号)的值分到 replicas 和 condemned 两个数组中，0 &lt;= ord &lt; Spec.Replicas 的放到 replicas 组，ord &gt;= Spec.Replicas 的放到 condemned 组，replicas 组代表可用的 pod，condemned 组是需要删除的 pod； 4、找出 replicas 和 condemned 组中的 unhealthy pod，healthy pod 指 running &amp; ready 并且不处于删除状态； 5、判断 sts 是否处于删除状态； 6、遍历 replicas 数组，确保 replicas 数组中的容器处于 running &amp; ready状态，其中处于 failed 状态的容器删除重建，未创建的容器则直接创建，最后检查 pod 的信息是否与 statefulset 的匹配，若不匹配则更新 pod 的状态。在此过程中每一步操作都会检查 monotonic 的值，即 sts 是否设置了 Parallel 参数，若设置了则循环处理 replicas 中的所有 pod，否则每次处理一个 pod，剩余 pod 则在下一个 syncLoop 继续进行处理； 7、按 pod 名称逆序删除 condemned 数组中的 pod，删除前也要确保 pod 处于 running &amp; ready状态，在此过程中也会检查 monotonic 的值，以此来判断是顺序删除还是在下一个 syncLoop 中继续进行处理； 8、判断 sts 的更新策略 .Spec.UpdateStrategy.Type，若为 OnDelete 则直接返回； 9、此时更新策略为 RollingUpdate，更新序号大于等于 .Spec.UpdateStrategy.RollingUpdate.Partition 的 pod，在 RollingUpdate 时，并不会关注 monotonic 的值，都是顺序进行处理且等待当前 pod 删除成功后才继续删除小于上一个 pod 序号的 pod，所以 Parallel 的策略在滚动更新时无法使用。 updateStatefulSet 这个方法中包含了 statefulset 的创建、删除、扩缩容、更新等操作，在源码层面对于各个功能无法看出明显的界定，没有 deployment sync 方法中写的那么清晰，下面还是按 statefulset 的功能再分析一下具体的操作： 创建：在创建 sts 后，sts 对象已被保存至 etcd 中，此时 sync 操作仅仅是创建出需要的 pod，即执行到第 6 步就会结束； 扩缩容：对于扩若容操作仅仅是创建或者删除对应的 pod，在操作前也会判断所有 pod 是否处于 running &amp; ready状态，然后进行对应的创建/删除操作，在上面的步骤中也会执行到第 6 步就结束了； 更新：可以看出在第六步之后的所有操作就是与更新相关的了，所以更新操作会执行完整个方法，在更新过程中通过 pod 的 currentRevision 和 updateRevision 来计算 currentReplicas、updatedReplicas 的值，最终完成所有 pod 的更新； 删除：删除操作就比较明显了，会止于第五步，但是在此之前检查 pod 状态以及分组的操作确实是多余的； k8s.io/kubernetes/pkg/controller/statefulset/stateful_set_control.go:255 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236func (ssc *defaultStatefulSetControl) updateStatefulSet(......) (*apps.StatefulSetStatus, error) &#123; // 1、分别获取 currentRevision 和 updateRevision 对应的的 statefulset object currentSet, err := ApplyRevision(set, currentRevision) if err != nil &#123; return nil, err &#125; updateSet, err := ApplyRevision(set, updateRevision) if err != nil &#123; return nil, err &#125; // 2、计算 status status := apps.StatefulSetStatus&#123;&#125; status.ObservedGeneration = set.Generation status.CurrentRevision = currentRevision.Name status.UpdateRevision = updateRevision.Name status.CollisionCount = new(int32) *status.CollisionCount = collisionCount // 3、将 statefulset 的 pods 按 ord(ord 为 pod name 中的序数)的值 // 分到 replicas 和 condemned 两个数组中 replicaCount := int(*set.Spec.Replicas) replicas := make([]*v1.Pod, replicaCount) condemned := make([]*v1.Pod, 0, len(pods)) unhealthy := 0 firstUnhealthyOrdinal := math.MaxInt32 var firstUnhealthyPod *v1.Pod // 4、计算 status 字段中的值，将 pod 分配到 replicas和condemned两个数组中 for i := range pods &#123; status.Replicas++ if isRunningAndReady(pods[i]) &#123; status.ReadyReplicas++ &#125; if isCreated(pods[i]) &amp;&amp; !isTerminating(pods[i]) &#123; if getPodRevision(pods[i]) == currentRevision.Name &#123; status.CurrentReplicas++ &#125; if getPodRevision(pods[i]) == updateRevision.Name &#123; status.UpdatedReplicas++ &#125; &#125; if ord := getOrdinal(pods[i]); 0 &lt;= ord &amp;&amp; ord &lt; replicaCount &#123; replicas[ord] = pods[i] &#125; else if ord &gt;= replicaCount &#123; condemned = append(condemned, pods[i]) &#125; &#125; // 5、检查 replicas数组中 [0,set.Spec.Replicas) 下标是否有缺失的 pod，若有缺失的则创建对应的 pod object // 在 newVersionedStatefulSetPod 中会判断是使用 currentSet 还是 updateSet 来创建 for ord := 0; ord &lt; replicaCount; ord++ &#123; if replicas[ord] == nil &#123; replicas[ord] = newVersionedStatefulSetPod( currentSet, updateSet, currentRevision.Name, updateRevision.Name, ord) &#125; &#125; // 6、对 condemned 数组进行排序 sort.Sort(ascendingOrdinal(condemned)) // 7、根据 ord 在 replicas 和 condemned 数组中找出 first unhealthy Pod for i := range replicas &#123; if !isHealthy(replicas[i]) &#123; unhealthy++ if ord := getOrdinal(replicas[i]); ord &lt; firstUnhealthyOrdinal &#123; firstUnhealthyOrdinal = ord firstUnhealthyPod = replicas[i] &#125; &#125; &#125; for i := range condemned &#123; if !isHealthy(condemned[i]) &#123; unhealthy++ if ord := getOrdinal(condemned[i]); ord &lt; firstUnhealthyOrdinal &#123; firstUnhealthyOrdinal = ord firstUnhealthyPod = condemned[i] &#125; &#125; &#125; ...... // 8、判断是否处于删除中 if set.DeletionTimestamp != nil &#123; return &amp;status, nil &#125; // 9、默认设置为非并行模式 monotonic := !allowsBurst(set) // 10、确保 replicas 数组中所有的 pod 是 running 的 for i := range replicas &#123; // 11、对于 failed 的 pod 删除并重新构建 pod object if isFailed(replicas[i]) &#123; ...... if err := ssc.podControl.DeleteStatefulPod(set, replicas[i]); err != nil &#123; return &amp;status, err &#125; if getPodRevision(replicas[i]) == currentRevision.Name &#123; status.CurrentReplicas-- &#125; if getPodRevision(replicas[i]) == updateRevision.Name &#123; status.UpdatedReplicas-- &#125; status.Replicas-- replicas[i] = newVersionedStatefulSetPod( currentSet, updateSet, currentRevision.Name, updateRevision.Name, i) &#125; // 12、如果 pod.Status.Phase 不为“” 说明该 pod 未创建，则直接重新创建该 pod if !isCreated(replicas[i]) &#123; if err := ssc.podControl.CreateStatefulPod(set, replicas[i]); err != nil &#123; return &amp;status, err &#125; status.Replicas++ if getPodRevision(replicas[i]) == currentRevision.Name &#123; status.CurrentReplicas++ &#125; if getPodRevision(replicas[i]) == updateRevision.Name &#123; status.UpdatedReplicas++ &#125; // 13、如果为Parallel，直接return status结束；如果为OrderedReady，循环处理下一个pod。 if monotonic &#123; return &amp;status, nil &#125; continue &#125; // 14、如果pod正在删除(pod.DeletionTimestamp不为nil)，且Spec.PodManagementPolicy不 // 为Parallel，直接return status结束，结束后会在下一个 syncLoop 继续进行处理， // pod 状态的改变会触发下一次 syncLoop if isTerminating(replicas[i]) &amp;&amp; monotonic &#123; ...... return &amp;status, nil &#125; // 15、如果pod状态不是Running &amp; Ready，且Spec.PodManagementPolicy不为Parallel， // 直接return status结束 if !isRunningAndReady(replicas[i]) &amp;&amp; monotonic &#123; ...... return &amp;status, nil &#125; // 16、检查 pod 的信息是否与 statefulset 的匹配，若不匹配则更新 pod 的状态 if identityMatches(set, replicas[i]) &amp;&amp; storageMatches(set, replicas[i]) &#123; continue &#125; replica := replicas[i].DeepCopy() if err := ssc.podControl.UpdateStatefulPod(updateSet, replica); err != nil &#123; return &amp;status, err &#125; &#125; // 17、逆序处理 condemned 中的 pod for target := len(condemned) - 1; target &gt;= 0; target-- &#123; // 18、如果pod正在删除，检查 Spec.PodManagementPolicy 的值，如果为Parallel， // 循环处理下一个pod 否则直接退出 if isTerminating(condemned[target]) &#123; ...... if monotonic &#123; return &amp;status, nil &#125; continue &#125; // 19、不满足以下条件说明该 pod 是更新前创建的，正处于创建中 if !isRunningAndReady(condemned[target]) &amp;&amp; monotonic &amp;&amp; condemned[target] != firstUnhealthyPod &#123; ...... return &amp;status, nil &#125; // 20、否则直接删除该 pod if err := ssc.podControl.DeleteStatefulPod(set, condemned[target]); err != nil &#123; return &amp;status, err &#125; if getPodRevision(condemned[target]) == currentRevision.Name &#123; status.CurrentReplicas-- &#125; if getPodRevision(condemned[target]) == updateRevision.Name &#123; status.UpdatedReplicas-- &#125; // 21、如果为 OrderedReady 方式则返回否则继续处理下一个 pod if monotonic &#123; return &amp;status, nil &#125; &#125; // 22、对于 OnDelete 策略直接返回 if set.Spec.UpdateStrategy.Type == apps.OnDeleteStatefulSetStrategyType &#123; return &amp;status, nil &#125; // 23、若为 RollingUpdate 策略，则倒序处理 replicas数组中下标大于等于 // Spec.UpdateStrategy.RollingUpdate.Partition 的 pod updateMin := 0 if set.Spec.UpdateStrategy.RollingUpdate != nil &#123; updateMin = int(*set.Spec.UpdateStrategy.RollingUpdate.Partition) &#125; for target := len(replicas) - 1; target &gt;= updateMin; target-- &#123; // 24、如果Pod的Revision 不等于 updateRevision，且 pod 没有处于删除状态则直接删除 pod if getPodRevision(replicas[target]) != updateRevision.Name &amp;&amp; !isTerminating(replicas[target]) &#123; ...... err := ssc.podControl.DeleteStatefulPod(set, replicas[target]) status.CurrentReplicas-- return &amp;status, err &#125; // 25、如果 pod 非 healthy 状态直接返回 if !isHealthy(replicas[target]) &#123; return &amp;status, nil &#125; &#125; return &amp;status, nil&#125; 总结本文分析了 statefulset controller 的主要功能，statefulset 在设计上有很多功能与 deployment 是类似的，但其主要是用来部署有状态应用的，statefulset 中的 pod 名称存在顺序性和唯一性，同时每个 pod 都使用了 pv 和 pvc 来存储状态，在创建、删除、更新操作中都会按照 pod 的顺序进行。 参考： https://github.com/kubernetes/kubernetes/issues/78007 https://github.com/kubernetes/kubernetes/issues/67250 https://www.cnblogs.com/linuxk/p/9767736.html]]></content>
      <tags>
        <tag>kube-controller-manager</tag>
        <tag>statefulset controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[replicaset controller 源码分析]]></title>
    <url>%2F2019%2F12%2F08%2Freplicaset_controller%2F</url>
    <content type="text"><![CDATA[在前面的文章中已经介绍了 deployment controller 的设计与实现，deployment 控制的是 replicaset，而 replicaset 控制 pod 的创建与删除，deployment 通过控制 replicaset 实现了滚动更新、回滚等操作。而 replicaset 会直接控制 pod 的创建与删除，本文会继续从源码层面分析 replicaset 的设计与实现。 在分析源码前先考虑一下 replicaset 的使用场景，在平时的操作中其实我们并不会直接操作 replicaset，replicaset 也仅有几个简单的操作，创建、删除、更新等，但其地位是非常重要的，replicaset 的主要功能就是通过 add/del pod 来达到期望的状态。 ReplicaSetController 源码分析 kubernetes 版本: v1.16 启动流程首先来看 replicaSetController 对象初始化以及启动的代码，在 startReplicaSetController 中有两个比较重要的变量： BurstReplicas：用来控制在一个 syncLoop 过程中 rs 最多能创建的 pod 数量，设置上限值是为了避免单个 rs 影响整个系统，默认值为 500； ConcurrentRSSyncs：指的是需要启动多少个 goroutine 处理 informer 队列中的对象，默认值为 5； k8s.io/kubernetes/cmd/kube-controller-manager/app/apps.go:69123456789101112func startReplicaSetController(ctx ControllerContext) (http.Handler, bool, error) &#123; if !ctx.AvailableResources[schema.GroupVersionResource&#123;Group: &quot;apps&quot;, Version: &quot;v1&quot;, Resource: &quot;replicasets&quot;&#125;] &#123; return nil, false, nil &#125; go replicaset.NewReplicaSetController( ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(&quot;replicaset-controller&quot;), replicaset.BurstReplicas, ).Run(int(ctx.ComponentConfig.ReplicaSetController.ConcurrentRSSyncs), ctx.Stop) return nil, true, nil&#125; 下面是 replicaSetController 初始化的具体步骤，可以看到其会监听 pod 以及 rs 两个对象的事件。 k8s.io/kubernetes/pkg/controller/replicaset/replica_set.go:109 123456789101112131415161718192021222324252627282930313233343536373839404142434445func NewReplicaSetController(......) *ReplicaSetController &#123; ...... // 1、此处调用 NewBaseController return NewBaseController(rsInformer, podInformer, kubeClient, burstReplicas, apps.SchemeGroupVersion.WithKind(&quot;ReplicaSet&quot;), &quot;replicaset_controller&quot;, &quot;replicaset&quot;, controller.RealPodControl&#123; KubeClient: kubeClient, Recorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: &quot;replicaset-controller&quot;&#125;), &#125;, )&#125;func NewBaseController(......) *ReplicaSetController &#123; ...... // 2、ReplicaSetController 初始化 rsc := &amp;ReplicaSetController&#123; GroupVersionKind: gvk, kubeClient: kubeClient, podControl: podControl, burstReplicas: burstReplicas, // 3、expectations 的初始化 expectations: controller.NewUIDTrackingControllerExpectations(controller.NewControllerExpectations()), queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), queueName), &#125; // 4、rsInformer 中注册的 EventHandler rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: rsc.enqueueReplicaSet, UpdateFunc: rsc.updateRS, DeleteFunc: rsc.enqueueReplicaSet, &#125;) ...... // 5、podInformer 中注册的 EventHandler podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: rsc.addPod, UpdateFunc: rsc.updatePod, DeleteFunc: rsc.deletePod, &#125;) ...... return rsc&#125; replicaSetController 初始化完成后会调用 Run 方法启动 5 个 goroutine 处理 informer 队列中的事件并进行 sync 操作，kube-controller-manager 中每个 controller 的启动操作都是如下所示流程。 k8s.io/kubernetes/pkg/controller/replicaset/replica_set.go:177 123456789101112131415161718192021222324252627282930313233343536func (rsc *ReplicaSetController) Run(workers int, stopCh &lt;-chan struct&#123;&#125;) &#123; ...... // 1、等待 informer 同步缓存 if !cache.WaitForNamedCacheSync(rsc.Kind, stopCh, rsc.podListerSynced, rsc.rsListerSynced) &#123; return &#125; // 2、启动 5 个 goroutine 执行 worker 方法 for i := 0; i &lt; workers; i++ &#123; go wait.Until(rsc.worker, time.Second, stopCh) &#125; &lt;-stopCh&#125;// 3、worker 方法中调用 rocessNextWorkItemfunc (rsc *ReplicaSetController) worker() &#123; for rsc.processNextWorkItem() &#123; &#125;&#125;func (rsc *ReplicaSetController) processNextWorkItem() bool &#123; // 4、从队列中取出对象 key, quit := rsc.queue.Get() if quit &#123; return false &#125; defer rsc.queue.Done(key) // 5、执行 sync 操作 err := rsc.syncHandler(key.(string)) ...... return true&#125; EventHandler初始化 replicaSetController 时，其中有一个 expectations 字段，这是 rs 中一个比较特殊的机制，为了说清楚 expectations，先来看一下 controller 中所注册的 eventHandler，replicaSetController 会 watch pod 和 replicaSet 两个对象，eventHandler 中注册了对这两种对象的 add、update、delete 三个操作。 addPod 1、判断 pod 是否处于删除状态； 2、获取该 pod 关联的 rs 以及 rsKey，入队 rs 并更新 rsKey 的 expectations； 3、若 pod 对象没体现出关联的 rs 则为孤儿 pod，遍历 rsList 查找匹配的 rs，若该 rs.Namespace == pod.Namespace 并且 rs.Spec.Selector 匹配 pod.Labels，则说明该 pod 应该与此 rs 关联，将匹配的 rs 入队； k8s.io/kubernetes/pkg/controller/replicaset/replica_set.go:255 12345678910111213141516171819202122232425262728293031323334func (rsc *ReplicaSetController) addPod(obj interface&#123;&#125;) &#123; pod := obj.(*v1.Pod) if pod.DeletionTimestamp != nil &#123; rsc.deletePod(pod) return &#125; // 1、获取 pod 所关联的 rs if controllerRef := metav1.GetControllerOf(pod); controllerRef != nil &#123; rs := rsc.resolveControllerRef(pod.Namespace, controllerRef) if rs == nil &#123; return &#125; rsKey, err := controller.KeyFunc(rs) if err != nil &#123; return &#125; // 2、更新 expectations，rsKey 的 add - 1 rsc.expectations.CreationObserved(rsKey) rsc.enqueueReplicaSet(rs) return &#125; rss := rsc.getPodReplicaSets(pod) if len(rss) == 0 &#123; return &#125; for _, rs := range rss &#123; rsc.enqueueReplicaSet(rs) &#125;&#125; updatePod 1、如果 pod label 改变或者处于删除状态，则直接删除； 2、如果 pod 的 OwnerReference 发生改变，此时 oldRS 需要创建 pod，将 oldRS 入队； 3、获取 pod 关联的 rs，入队 rs，若 pod 当前处于 ready 并非 available 状态，则会再次将该 rs 加入到延迟队列中，因为 pod 从 ready 到 available 状态需要触发一次 status 的更新； 4、否则为孤儿 pod，遍历 rsList 查找匹配的 rs，若找到则将 rs 入队； k8s.io/kubernetes/pkg/controller/replicaset/replica_set.go:298 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253func (rsc *ReplicaSetController) updatePod(old, cur interface&#123;&#125;) &#123; curPod := cur.(*v1.Pod) oldPod := old.(*v1.Pod) if curPod.ResourceVersion == oldPod.ResourceVersion &#123; return &#125; // 1、如果 pod label 改变或者处于删除状态，则直接删除 labelChanged := !reflect.DeepEqual(curPod.Labels, oldPod.Labels) if curPod.DeletionTimestamp != nil &#123; rsc.deletePod(curPod) if labelChanged &#123; rsc.deletePod(oldPod) &#125; return &#125; // 2、如果 pod 的 OwnerReference 发生改变，将 oldRS 入队 curControllerRef := metav1.GetControllerOf(curPod) oldControllerRef := metav1.GetControllerOf(oldPod) controllerRefChanged := !reflect.DeepEqual(curControllerRef, oldControllerRef) if controllerRefChanged &amp;&amp; oldControllerRef != nil &#123; if rs := rsc.resolveControllerRef(oldPod.Namespace, oldControllerRef); rs != nil &#123; rsc.enqueueReplicaSet(rs) &#125; &#125; // 3、获取 pod 关联的 rs，入队 rs if curControllerRef != nil &#123; rs := rsc.resolveControllerRef(curPod.Namespace, curControllerRef) if rs == nil &#123; return &#125; rsc.enqueueReplicaSet(rs) if !podutil.IsPodReady(oldPod) &amp;&amp; podutil.IsPodReady(curPod) &amp;&amp; rs.Spec.MinReadySeconds &gt; 0 &#123; rsc.enqueueReplicaSetAfter(rs, (time.Duration(rs.Spec.MinReadySeconds)*time.Second)+time.Second) &#125; return &#125; // 4、查找匹配的 rs if labelChanged || controllerRefChanged &#123; rss := rsc.getPodReplicaSets(curPod) if len(rss) == 0 &#123; return &#125; for _, rs := range rss &#123; rsc.enqueueReplicaSet(rs) &#125; &#125;&#125; deletePod 1、确认该对象是否为 pod； 2、判断是否为孤儿 pod； 3、获取其对应的 rs 以及 rsKey； 4、更新 expectations 中 rsKey 的 del 值； 5、将 rs 入队； k8s.io/kubernetes/pkg/controller/replicaset/replica_set.go:372 1234567891011121314151617181920212223func (rsc *ReplicaSetController) deletePod(obj interface&#123;&#125;) &#123; pod, ok := obj.(*v1.Pod) if !ok &#123; ...... &#125; controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil &#123; return &#125; rs := rsc.resolveControllerRef(pod.Namespace, controllerRef) if rs == nil &#123; return &#125; rsKey, err := controller.KeyFunc(rs) if err != nil &#123; return &#125; // 更新 expectations，该 rsKey 的 del - 1 rsc.expectations.DeletionObserved(rsKey, controller.PodKey(pod)) rsc.enqueueReplicaSet(rs)&#125; AddRS 和 DeleteRS以上两个操作仅仅是将对应的 rs 入队。 #####UpdateRS 其实 updateRS 也仅仅是将对应的 rs 进行入队，不过多了一个打印日志的操作，如下所示： k8s.io/kubernetes/pkg/controller/replicaset/replica_set.go:232 123456789func (rsc *ReplicaSetController) updateRS(old, cur interface&#123;&#125;) &#123; oldRS := old.(*apps.ReplicaSet) curRS := cur.(*apps.ReplicaSet) if *(oldRS.Spec.Replicas) != *(curRS.Spec.Replicas) &#123; klog.V(4).Infof(&quot;%v %v updated. Desired pod count change: %d-&gt;%d&quot;, rsc.Kind, curRS.Name, *(oldRS.Spec.Replicas), *(curRS.Spec.Replicas)) &#125; rsc.enqueueReplicaSet(cur)&#125; 至于 expectations 机制会在下文进行分析。 syncReplicaSetsyncReplicaSet 是 controller 的核心方法，它会驱动 controller 所控制的对象达到期望状态，主要逻辑如下所示： 1、根据 ns/name 获取 rs 对象； 2、调用 expectations.SatisfiedExpectations 判断是否需要执行真正的 sync 操作； 3、获取所有 pod list； 4、根据 pod label 进行过滤获取与该 rs 关联的 pod 列表，对于其中的孤儿 pod 若与该 rs label 匹配则进行关联，若已关联的 pod 与 rs label 不匹配则解除关联关系； 5、调用 manageReplicas 进行同步 pod 操作，add/del pod； 6、计算 rs 当前的 status 并进行更新； 7、若 rs 设置了 MinReadySeconds 字段则将该 rs 加入到延迟队列中； k8s.io/kubernetes/pkg/controller/replicaset/replica_set.go:562 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556func (rsc *ReplicaSetController) syncReplicaSet(key string) error &#123; ...... namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; // 1、根据 ns/name 从 informer cache 中获取 rs 对象， // 若 rs 已经被删除则直接删除 expectations 中的对象 rs, err := rsc.rsLister.ReplicaSets(namespace).Get(name) if errors.IsNotFound(err) &#123; rsc.expectations.DeleteExpectations(key) return nil &#125; ...... // 2、判断该 rs 是否需要执行 sync 操作 rsNeedsSync := rsc.expectations.SatisfiedExpectations(key) selector, err := metav1.LabelSelectorAsSelector(rs.Spec.Selector) if err != nil &#123; ...... &#125; // 3、获取所有 pod list allPods, err := rsc.podLister.Pods(rs.Namespace).List(labels.Everything()) ...... // 4、过滤掉异常 pod，处于删除状态或者 failed 状态的 pod 都为非 active 状态 filteredPods := controller.FilterActivePods(allPods) // 5、检查所有 pod，根据 pod 并进行 adopt 与 release 操作，最后获取与该 rs 关联的 pod list filteredPods, err = rsc.claimPods(rs, selector, filteredPods) ...... // 6、若需要 sync 则执行 manageReplicas 创建/删除 pod var manageReplicasErr error if rsNeedsSync &amp;&amp; rs.DeletionTimestamp == nil &#123; manageReplicasErr = rsc.manageReplicas(filteredPods, rs) &#125; rs = rs.DeepCopy() // 7、计算 rs 当前的 status newStatus := calculateStatus(rs, filteredPods, manageReplicasErr) // 8、更新 rs status updatedRS, err := updateReplicaSetStatus(rsc.kubeClient.AppsV1().ReplicaSets(rs.Namespace), rs, newStatus) // 9、判断是否需要将 rs 加入到延迟队列中 if manageReplicasErr == nil &amp;&amp; updatedRS.Spec.MinReadySeconds &gt; 0 &amp;&amp; updatedRS.Status.ReadyReplicas == *(updatedRS.Spec.Replicas) &amp;&amp; updatedRS.Status.AvailableReplicas != *(updatedRS.Spec.Replicas) &#123; rsc.enqueueReplicaSetAfter(updatedRS, time.Duration(updatedRS.Spec.MinReadySeconds)*time.Second) &#125; return manageReplicasErr&#125; 在 syncReplicaSet 方法中有几个重要的操作分别为：rsc.expectations.SatisfiedExpectations、rsc.manageReplicas、calculateStatus，下面一一进行分析。 SatisfiedExpectations该方法主要判断 rs 是否需要执行真正的同步操作，若需要 add/del pod 或者 expectations 已过期则需要进行同步操作。 k8s.io/kubernetes/pkg/controller/controller_utils.go:181 12345678910111213141516171819202122232425262728func (r *ControllerExpectations) SatisfiedExpectations(controllerKey string) bool &#123; // 1、若该 key 存在时，判断是否满足条件或者是否超过同步周期 if exp, exists, err := r.GetExpectations(controllerKey); exists &#123; if exp.Fulfilled() &#123; return true &#125; else if exp.isExpired() &#123; return true &#125; else &#123; return false &#125; &#125; else if err != nil &#123; ...... &#125; else &#123; // 2、该 rs 可能为新创建的，需要进行 sync ...... &#125; return true&#125;// 3、若 add &lt;= 0 且 del &lt;= 0 说明本地观察到的状态已经为期望状态了func (e *ControlleeExpectations) Fulfilled() bool &#123; return atomic.LoadInt64(&amp;e.add) &lt;= 0 &amp;&amp; atomic.LoadInt64(&amp;e.del) &lt;= 0&#125;// 4、判断 key 是否过期，ExpectationsTimeout 默认值为 5 * time.Minutefunc (exp *ControlleeExpectations) isExpired() bool &#123; return clock.RealClock&#123;&#125;.Since(exp.timestamp) &gt; ExpectationsTimeout&#125; manageReplicasmanageReplicas 是最核心的方法，它会计算 replicaSet 需要创建或者删除多少个 pod 并调用 apiserver 的接口进行操作，在此阶段仅仅是调用 apiserver 的接口进行创建，并不保证 pod 成功运行，如果在某一轮，未能成功创建的所有 Pod 对象，则不再创建剩余的 pod。一个周期内最多只能创建或删除 500 个 pod，若超过上限值未创建完成的 pod 数会在下一个 syncLoop 继续进行处理。 该方法主要逻辑如下所示： 1、计算已存在 pod 数与期望数的差异； 2、如果 diff &lt; 0 说明 rs 实际的 pod 数未达到期望值需要继续创建 pod，首先会将需要创建的 pod 数在 expectations 中进行记录，然后调用 slowStartBatch 创建所需要的 pod，slowStartBatch 以指数级增长的方式批量创建 pod，创建 pod 过程中若出现 timeout err 则忽略，若为其他 err 则终止创建操作并更新 expectations； 3、如果 diff &gt; 0 说明可能是一次缩容操作需要删除多余的 pod，如果需要删除全部的 pod 则直接进行删除，否则会通过 getPodsToDelete 方法筛选出需要删除的 pod，具体的筛选策略在下文会讲到，然后并发删除这些 pod，对于删除失败操作也会记录在 expectations 中； 在 slowStartBatch 中会调用 rsc.podControl.CreatePodsWithControllerRef 方法创建 pod，若创建 pod 失败会判断是否为创建超时错误，或者可能是超时后失败，但此时认为超时并不影响后续的批量创建动作，大家知道，创建 pod 操作提交到 apiserver 后会经过认证、鉴权、以及动态访问控制三个步骤，此过程有可能会超时，即使真的创建失败了，等到 expectations 过期后在下一个 syncLoop 时会重新创建。 k8s.io/kubernetes/pkg/controller/replicaset/replica_set.go:459 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677func (rsc *ReplicaSetController) manageReplicas(......) error &#123; // 1、计算已存在 pod 数与期望数的差异 diff := len(filteredPods) - int(*(rs.Spec.Replicas)) rsKey, err := controller.KeyFunc(rs) if err != nil &#123; ...... &#125; 2、如果 &lt;0，则需要创建 pod if diff &lt; 0 &#123; diff *= -1 3、判断需要创建的 pod 数是否超过单次 sync 上限值 500 if diff &gt; rsc.burstReplicas &#123; diff = rsc.burstReplicas &#125; 4、在 expectations 中进行记录，若该 key 已经存在会进行覆盖 rsc.expectations.ExpectCreations(rsKey, diff) 5、调用 slowStartBatch 创建所需要的 pod successfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error &#123; err := rsc.podControl.CreatePodsWithControllerRef(rs.Namespace, &amp;rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind)) // 6、若为 timeout err 则忽略 if err != nil &amp;&amp; errors.IsTimeout(err) &#123; return nil &#125; return err &#125;) // 7、计算未创建的 pod 数，并记录在 expectations 中 // 若 pod 创建成功，informer watch 到事件后会在 addPod handler 中更新 expectations if skippedPods := diff - successfulCreations; skippedPods &gt; 0 &#123; for i := 0; i &lt; skippedPods; i++ &#123; rsc.expectations.CreationObserved(rsKey) &#125; &#125; return err &#125; else if diff &gt; 0 &#123; // 8、若 diff &gt;0 说明需要删除多创建的 pod if diff &gt; rsc.burstReplicas &#123; diff = rsc.burstReplicas &#125; // 9、getPodsToDelete 会按照一定的策略找出需要删除的 pod 列表 podsToDelete := getPodsToDelete(filteredPods, diff) // 10、在 expectations 中进行记录，若该 key 已经存在会进行覆盖 rsc.expectations.ExpectDeletions(rsKey, getPodKeys(podsToDelete)) // 11、进行并发删除的操作 errCh := make(chan error, diff) var wg sync.WaitGroup wg.Add(diff) for _, pod := range podsToDelete &#123; go func(targetPod *v1.Pod) &#123; defer wg.Done() if err := rsc.podControl.DeletePod(rs.Namespace, targetPod.Name, rs); err != nil &#123; podKey := controller.PodKey(targetPod) // 12、某次删除操作若失败会记录在 expectations 中 rsc.expectations.DeletionObserved(rsKey, podKey) errCh &lt;- err &#125; &#125;(pod) &#125; wg.Wait() // 13、返回其中一条 err select &#123; case err := &lt;-errCh: if err != nil &#123; return err &#125; default: &#125; &#125; return nil&#125; slowStartBatch 会批量创建出已计算出的 diff pod 数，创建的 pod 数依次为 1、2、4、8……，呈指数级增长，其方法如下所示： k8s.io/kubernetes/pkg/controller/replicaset/replica_set.go:658 12345678910111213141516171819202122232425func slowStartBatch(count int, initialBatchSize int, fn func() error) (int, error) &#123; remaining := count successes := 0 for batchSize := integer.IntMin(remaining, initialBatchSize); batchSize &gt; 0; batchSize = integer.IntMin(2*batchSize, remaining) &#123; errCh := make(chan error, batchSize) var wg sync.WaitGroup wg.Add(batchSize) for i := 0; i &lt; batchSize; i++ &#123; go func() &#123; defer wg.Done() if err := fn(); err != nil &#123; errCh &lt;- err &#125; &#125;() &#125; wg.Wait() curSuccesses := batchSize - len(errCh) successes += curSuccesses if len(errCh) &gt; 0 &#123; return successes, &lt;-errCh &#125; remaining -= batchSize &#125; return successes, nil&#125; 若 diff &gt; 0 时再删除 pod 阶段会调用getPodsToDelete 对 pod 进行筛选操作，此阶段会选出最劣质的 pod，下面是用到的 6 种筛选方法： 1、判断是否绑定了 node：Unassigned &lt; assigned； 2、判断 pod phase：PodPending &lt; PodUnknown &lt; PodRunning； 3、判断 pod 状态：Not ready &lt; ready； 4、若 pod 都为 ready，则按运行时间排序，运行时间最短会被删除：empty time &lt; less time &lt; more time； 5、根据 pod 重启次数排序：higher restart counts &lt; lower restart counts； 6、按 pod 创建时间进行排序：Empty creation time pods &lt; newer pods &lt; older pods； 上面的几个排序规则遵循互斥原则，从上到下进行匹配，符合条件则排序完成，代码如下所示： k8s.io/kubernetes/pkg/controller/replicaset/replica_set.go:684 123456func getPodsToDelete(filteredPods []*v1.Pod, diff int) []*v1.Pod &#123; if diff &lt; len(filteredPods) &#123; sort.Sort(controller.ActivePods(filteredPods)) &#125; return filteredPods[:diff]&#125; k8s.io/kubernetes/pkg/controller/controller_utils.go:735 1234567891011121314151617181920212223242526272829303132333435363738type ActivePods []*v1.Podfunc (s ActivePods) Len() int &#123; return len(s) &#125;func (s ActivePods) Swap(i, j int) &#123; s[i], s[j] = s[j], s[i] &#125;func (s ActivePods) Less(i, j int) bool &#123; // 1. Unassigned &lt; assigned if s[i].Spec.NodeName != s[j].Spec.NodeName &amp;&amp; (len(s[i].Spec.NodeName) == 0 || len(s[j].Spec.NodeName) == 0) &#123; return len(s[i].Spec.NodeName) == 0 &#125; // 2. PodPending &lt; PodUnknown &lt; PodRunning m := map[v1.PodPhase]int&#123;v1.PodPending: 0, v1.PodUnknown: 1, v1.PodRunning: 2&#125; if m[s[i].Status.Phase] != m[s[j].Status.Phase] &#123; return m[s[i].Status.Phase] &lt; m[s[j].Status.Phase] &#125; // 3. Not ready &lt; ready if podutil.IsPodReady(s[i]) != podutil.IsPodReady(s[j]) &#123; return !podutil.IsPodReady(s[i]) &#125; // 4. Been ready for empty time &lt; less time &lt; more time if podutil.IsPodReady(s[i]) &amp;&amp; podutil.IsPodReady(s[j]) &amp;&amp; !podReadyTime(s[i]).Equal(podReadyTime(s[j])) &#123; return afterOrZero(podReadyTime(s[i]), podReadyTime(s[j])) &#125; // 5. Pods with containers with higher restart counts &lt; lower restart counts if maxContainerRestarts(s[i]) != maxContainerRestarts(s[j]) &#123; return maxContainerRestarts(s[i]) &gt; maxContainerRestarts(s[j]) &#125; // 6. Empty creation time pods &lt; newer pods &lt; older pods if !s[i].CreationTimestamp.Equal(&amp;s[j].CreationTimestamp) &#123; return afterOrZero(&amp;s[i].CreationTimestamp, &amp;s[j].CreationTimestamp) &#125; return false&#125; calculateStatuscalculateStatus 会通过当前 pod 的状态计算出 rs 中 status 字段值，status 字段如下所示： 123456status: availableReplicas: 10 fullyLabeledReplicas: 10 observedGeneration: 1 readyReplicas: 10 replicas: 10 k8s.io/kubernetes/pkg/controller/replicaset/replica_set_utils.go:85 1234567891011121314151617181920212223242526272829303132333435363738func calculateStatus(......) apps.ReplicaSetStatus &#123; newStatus := rs.Status fullyLabeledReplicasCount := 0 readyReplicasCount := 0 availableReplicasCount := 0 templateLabel := labels.Set(rs.Spec.Template.Labels).AsSelectorPreValidated() for _, pod := range filteredPods &#123; if templateLabel.Matches(labels.Set(pod.Labels)) &#123; fullyLabeledReplicasCount++ &#125; if podutil.IsPodReady(pod) &#123; readyReplicasCount++ if podutil.IsPodAvailable(pod, rs.Spec.MinReadySeconds, metav1.Now()) &#123; availableReplicasCount++ &#125; &#125; &#125; failureCond := GetCondition(rs.Status, apps.ReplicaSetReplicaFailure) if manageReplicasErr != nil &amp;&amp; failureCond == nil &#123; var reason string if diff := len(filteredPods) - int(*(rs.Spec.Replicas)); diff &lt; 0 &#123; reason = &quot;FailedCreate&quot; &#125; else if diff &gt; 0 &#123; reason = &quot;FailedDelete&quot; &#125; cond := NewReplicaSetCondition(apps.ReplicaSetReplicaFailure, v1.ConditionTrue, reason, manageReplicasErr.Error()) SetCondition(&amp;newStatus, cond) &#125; else if manageReplicasErr == nil &amp;&amp; failureCond != nil &#123; RemoveCondition(&amp;newStatus, apps.ReplicaSetReplicaFailure) &#125; newStatus.Replicas = int32(len(filteredPods)) newStatus.FullyLabeledReplicas = int32(fullyLabeledReplicasCount) newStatus.ReadyReplicas = int32(readyReplicasCount) newStatus.AvailableReplicas = int32(availableReplicasCount) return newStatus&#125; expectations 机制通过上面的分析可知，在 rs 每次入队后进行 sync 操作时，首先需要判断该 rs 是否满足 expectations 机制，那么这个 expectations 的目的是什么？其实，rs 除了有 informer 的缓存外，还有一个本地缓存就是 expectations，expectations 会记录 rs 所有对象需要 add/del 的 pod 数量，若两者都为 0 则说明该 rs 所期望创建的 pod 或者删除的 pod 数已经被满足，若不满足则说明某次在 syncLoop 中创建或者删除 pod 时有失败的操作，则需要等待 expectations 过期后再次同步该 rs。 通过上面对 eventHandler 的分析，再来总结一下触发 replicaSet 对象发生同步事件的条件： 1、与 rs 相关的：AddRS、UpdateRS、DeleteRS； 2、与 pod 相关的：AddPod、UpdatePod、DeletePod； 3、informer 二级缓存的同步； 但是所有的更新事件是否都需要执行 sync 操作？对于除 rs.Spec.Replicas 之外的更新操作其实都没必要执行 sync 操作，因为 spec 其他字段和 status 的更新都不需要创建或者删除 pod。 在 sync 操作真正开始之前，依据 expectations 机制进行判断，确定是否要真正地启动一次 sync，因为在 eventHandler 阶段也会更新 expectations 值，从上面的 eventHandler 中可以看到在 addPod 中会调用 rsc.expectations.CreationObserved 更新 rsKey 的 expectations，将其 add 值 -1，在 deletePod 中调用 rsc.expectations.DeletionObserved 将其 del 值 -1。所以等到 sync 时，若 controllerKey(name 或者 ns/name)满足 expectations 机制则进行 sync 操作，而 updatePod 并不会修改 expectations，所以，expectations 的设计就是当需要创建或删除 pod 才会触发对应的 sync 操作，expectations 机制的目的就是减少不必要的 sync 操作。 什么条件下 expectations 机制会满足？ 1、当 expectations 中不存在 rsKey 时，也就说首次创建 rs 时； 2、当 expectations 中 del 以及 add 值都为 0 时，即 rs 所需要创建或者删除的 pod 数都已满足； 3、当 expectations 过期时，即超过 5 分钟未进行 sync 操作； 最后再看一下 expectations 中用到的几个方法： 1234567891011121314// 创建了一个 pod 说明 expectations 中对应的 key add 期望值需要减少一个 pod， add -1CreationObserved(controllerKey string)// 删除了一个 pod 说明 expectations 中对应的 key del 期望值需要减少一个 pod， del - 1DeletionObserved(controllerKey string)// 写入 key 需要 add 的 pod 数量ExpectCreations(controllerKey string, adds int) error// 写入 key 需要 del 的 pod 数量ExpectDeletions(controllerKey string, dels int) error// 删除该 keyDeleteExpectations(controllerKey string) 当在 syncLoop 中发现满足条件时，会执行 manageReplicas 方法，在 manageReplicas 中无论是为 rs 创建还是删除 pod 都会调用 ExpectCreations 或 ExpectDeletions 为 rsKey 创建 expectations 对象。 总结本文主要从源码层面分析了 replicaSetController 的设计与实现，但是不得不说其在设计方面考虑了很多因素，文中只提到了笔者理解了或者思考后稍有了解的一些机制，至于其他设计思想还得自行阅读代码体会。 下面以一个流程图总结下创建 rs 的主要流程。 1234567891011121314151617181920 SatisfiedExpectations (expectations 中不存在 rsKey，rsNeedsSync 为 true) | 判断 add/del pod | | | ∨ | 创建 expectations 对象, | 并设置 add/del 值 ∨ |create rs --&gt; syncReplicaSet --&gt; manageReplicas --&gt; ∨ (为 rs 创建 pod) 调用 slowStartBatch 批量创建 pod/ | 删除筛选出的多余 pod | | | ∨ | 更新 expectations 对象 ∨ updateReplicaSetStatus (更新 rs 的 status subResource) 参考： https://keyla.vip/k8s/3-master/controller/replica-set/]]></content>
      <tags>
        <tag>kube-controller-manager</tag>
        <tag>replicaset controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deployment controller 源码分析]]></title>
    <url>%2F2019%2F11%2F28%2Fdeployment_controller%2F</url>
    <content type="text"><![CDATA[在前面的文章中已经分析过 kubernetes 中多个组件的源码了，本章会继续解读 kube-controller-manager 源码，kube-controller-manager 中有数十个 controller，本文会分析最常用到的 deployment controller。 deployment 的功能deployment 是 kubernetes 中用来部署无状态应用的一个对象，也是最常用的一种对象。 deployment、replicaSet 和 pod 之间的关系deployment 的本质是控制 replicaSet，replicaSet 会控制 pod，然后由 controller 驱动各个对象达到期望状态。 DeploymentController 是 Deployment 资源的控制器，其通过 DeploymentInformer、ReplicaSetInformer、PodInformer 监听三种资源，当三种资源变化时会触发 DeploymentController 中的 syncLoop 操作。 deployment 的基本功能下面通过命令行操作展示一下 deployment 的基本功能。 以下是 deployment 的一个示例文件： 123456789101112131415161718192021222324252627apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: progressDeadlineSeconds: 600 // 执行操作的超时时间 replicas: 20 revisionHistoryLimit: 10 // 保存的历史版本数量 selector: matchLabels: app: nginx-deployment strategy: rollingUpdate: maxSurge: 25% // 升级过程中最多可以比原先设置多出的 pod 数量 maxUnavailable: 25% // 升级过程中最多有多少个 pod 处于无法提供服务的状态 type: RollingUpdate // 更新策略 template: metadata: labels: app: nginx-deployment spec: containers: - name: nginx-deployment image: nginx:1.9 imagePullPolicy: IfNotPresent ports: - containerPort: 80 创建123456789$ kubectl create -f nginx-dep.yaml --record$ kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEnginx-deployment 20/20 20 20 22h$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-68b649bd8b 20 20 20 22h 滚动更新123$ kubectl set image deploy/nginx-deployment nginx-deployment=nginx:1.9.3$ kubectl rollout status deployment/nginx-deployment 回滚123456789// 查看历史版本$ kubectl rollout history deployment/nginx-deploymentdeployment.extensions/nginx-deploymentREVISION CHANGE-CAUSE4 &lt;none&gt;5 &lt;none&gt;// 指定版本回滚$ kubectl rollout undo deployment/nginx-deployment --to-revision=2 扩缩容12$ kubectl scale deployment nginx-deployment --replicas 10deployment.extensions/nginx-deployment scaled 暂停与恢复12$ kubectl rollout pause deployment/nginx-deployment$ kubectl rollout resume deploy nginx-deployment 删除12345// 级联删除$ kubectl delete deployment nginx-deployment// 非级联删除$ kubectl delete deployment nginx-deployment --cascade=false 以上是 deployment 的几个常用操作，下面会结合源码分析这几个操作都是如何实现的。 deployment controller 源码分析 kubernetes 版本：v1.16 在控制器模式下，每次操作对象都会触发一次事件，然后 controller 会进行一次 syncLoop 操作，controller 是通过 informer 监听事件以及进行 ListWatch 操作的，关于 informer 的基础知识可以参考以前写的文章。 deployment controller 启动流程kube-controller-manager 中所有 controller 的启动都是在 Run 方法中完成初始化并启动的。在 Run 中会调用 run 函数，run 函数的主要流程有： 1、调用 NewControllerInitializers 初始化所有 controller 2、调用 StartControllers 启动所有 controller k8s.io/kubernetes/cmd/kube-controller-manager/app/controllermanager.go:158 1234567891011121314func Run(c *config.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;) error &#123; ...... run := func(ctx context.Context) &#123; ...... // 1.调用 NewControllerInitializers 初始化所有 controller // 2.调用 StartControllers 启动所有 controller if err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux); err != nil &#123; klog.Fatalf(&quot;error starting controllers: %v&quot;, err) &#125; ...... select &#123;&#125; &#125; ......&#125; NewControllerInitializers 中定义了所有的 controller 以及 start controller 对应的方法。deployment controller 对应的启动方法是 startDeploymentController。 k8s.io/kubernetes/cmd/kube-controller-manager/app/controllermanager.go:373 1234567func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc &#123; controllers := map[string]InitFunc&#123;&#125; ...... controllers[&quot;deployment&quot;] = startDeploymentController ......&#125; 在startDeploymentController 中对 deploymentController 进行了初始化，并执行 dc.Run() 方法启动了 controller。 k8s.io/kubernetes/cmd/kube-controller-manager/app/apps.go:82 12345678910111213141516func startDeploymentController(ctx ControllerContext) (http.Handler, bool, error) &#123; ...... // 初始化 controller dc, err := deployment.NewDeploymentController( ctx.InformerFactory.Apps().V1().Deployments(), ctx.InformerFactory.Apps().V1().ReplicaSets(), ctx.InformerFactory.Core().V1().Pods(), ctx.ClientBuilder.ClientOrDie(&quot;deployment-controller&quot;), ) ...... // 启动 controller go dc.Run(int(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop) return nil, true, nil&#125; ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs 指定了 deployment controller 中工作的 goroutine 数量，默认值为 5，即会启动五个 goroutine 从 workqueue 中取出 object 并进行 sync 操作，该参数的默认值定义在 k8s.io/kubernetes/pkg/controller/deployment/config/v1alpha1/defaults.go 中。 dc.Run 方法会执行 ListWatch 操作并根据对应的事件执行 syncLoop。 k8s.io/kubernetes/pkg/controller/deployment/deployment_controller.go:148 12345678910111213141516func (dc *DeploymentController) Run(workers int, stopCh &lt;-chan struct&#123;&#125;) &#123; ...... // 1、等待 informer cache 同步完成 if !cache.WaitForNamedCacheSync(&quot;deployment&quot;, stopCh, dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) &#123; return &#125; // 2、启动 5 个 goroutine for i := 0; i &lt; workers; i++ &#123; // 3、在每个 goroutine 中每秒执行一次 dc.worker 方法 go wait.Until(dc.worker, time.Second, stopCh) &#125; &lt;-stopCh&#125; dc.worker 会调用 syncHandler 进行 sync 操作。 123456789101112131415161718func (dc *DeploymentController) worker() &#123; for dc.processNextWorkItem() &#123; &#125;&#125;func (dc *DeploymentController) processNextWorkItem() bool &#123; key, quit := dc.queue.Get() if quit &#123; return false &#125; defer dc.queue.Done(key) // 若 workQueue 中有任务则进行处理 err := dc.syncHandler(key.(string)) dc.handleErr(err, key) return true&#125; syncHandler 是 controller 的核心逻辑，下面会进行详细说明。至此，对于 deployment controller 的启动流程已经分析完，再来看一下 deployment controller 启动过程中的整个调用链，如下所示： 12Run() --&gt; run() --&gt; NewControllerInitializers() --&gt; StartControllers() --&gt; startDeploymentController() --&gt; deployment.NewDeploymentController() --&gt; deployment.Run()--&gt; deployment.syncDeployment() deployment controller 在初始化时指定了 dc.syncHandler = dc.syncDeployment，所以该函数名为 syncDeployment，本文开头介绍 deployment 中的基本操作都是在 syncDeployment 中完成的。 syncDeployment 的主要流程如下所示： 1、调用 getReplicaSetsForDeployment 获取集群中与 Deployment 相关的 ReplicaSet，若发现匹配但没有关联 deployment 的 rs 则通过设置 ownerReferences 字段与 deployment 关联，已关联但不匹配的则删除对应的 ownerReferences； 2、调用 getPodMapForDeployment 获取当前 Deployment 对象关联的 pod，并根据 rs.UID 对上述 pod 进行分类； 3、通过判断 deployment 的 DeletionTimestamp 字段确认是否为删除操作； 4、执行 checkPausedConditions检查 deployment 是否为pause状态并添加合适的condition； 5、调用 getRollbackTo 函数检查 Deployment 是否有Annotations：&quot;deprecated.deployment.rollback.to&quot;字段，如果有，调用 dc.rollback 方法执行 rollback 操作； 6、调用 dc.isScalingEvent 方法检查是否处于 scaling 状态中； 7、最后检查是否为更新操作，并根据更新策略 Recreate 或 RollingUpdate 来执行对应的操作； k8s.io/kubernetes/pkg/controller/deployment/deployment_controller.go:56212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273func (dc *DeploymentController) syncDeployment(key string) error &#123; ...... namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil &#123; return err &#125; // 1、从 informer cache 中获取 deployment 对象 deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) &#123; ...... &#125; ...... d := deployment.DeepCopy() // 2、判断 selecor 是否为空 everything := metav1.LabelSelector&#123;&#125; if reflect.DeepEqual(d.Spec.Selector, &amp;everything) &#123; ...... return nil &#125; // 3、获取 deployment 对应的所有 rs，通过 LabelSelector 进行匹配 rsList, err := dc.getReplicaSetsForDeployment(d) if err != nil &#123; return err &#125; // 4、获取当前 Deployment 对象关联的 pod，并根据 rs.UID 对 pod 进行分类 podMap, err := dc.getPodMapForDeployment(d, rsList) if err != nil &#123; return err &#125; // 5、如果该 deployment 处于删除状态，则更新其 status if d.DeletionTimestamp != nil &#123; return dc.syncStatusOnly(d, rsList) &#125; // 6、检查是否处于 pause 状态 if err = dc.checkPausedConditions(d); err != nil &#123; return err &#125; if d.Spec.Paused &#123; return dc.sync(d, rsList) &#125; // 7、检查是否为回滚操作 if getRollbackTo(d) != nil &#123; return dc.rollback(d, rsList) &#125; // 8、检查 deployment 是否处于 scale 状态 scalingEvent, err := dc.isScalingEvent(d, rsList) if err != nil &#123; return err &#125; if scalingEvent &#123; return dc.sync(d, rsList) &#125; // 9、更新操作 switch d.Spec.Strategy.Type &#123; case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(d, rsList) &#125; return fmt.Errorf(&quot;unexpected deployment strategy type: %s&quot;, d.Spec.Strategy.Type)&#125; 可以看出对于 deployment 的删除、暂停恢复、扩缩容以及更新操作都是在 syncDeployment 方法中进行处理的，最终是通过调用 syncStatusOnly、sync、rollback、rolloutRecreate、rolloutRolling 这几个方法来处理的，其中 syncStatusOnly 和 sync 都是更新 Deployment 的 Status，rollback 是用来回滚的，rolloutRecreate 和 rolloutRolling 是根据不同的更新策略来更新 Deployment 的，下面就来看看这些操作的具体实现。 从 syncDeployment 中也可知以上几个操作的优先级为： 1delete &gt; pause &gt; rollback &gt; scale &gt; rollout 举个例子，当在 rollout 操作时可以执行 pause 操作，在 pause 状态时也可直接执行删除操作。 删除syncDeployment 中首先处理的是删除操作，删除操作是由客户端发起的，首先会在对象的 metadata 中设置 DeletionTimestamp 字段。 func (dc *DeploymentController) syncDeployment(key string) error { ...... if d.DeletionTimestamp != nil { return dc.syncStatusOnly(d, rsList) } ...... } 当 controller 检查到该对象有了 DeletionTimestamp 字段时会调用 dc.syncStatusOnly 执行对应的删除逻辑，该方法首先获取 newRS 以及所有的 oldRSs，然后会调用 syncDeploymentStatus 方法。 k8s.io/kubernetes/pkg/controller/deployment/sync.go:48123456789func (dc *DeploymentController) syncStatusOnly(d *apps.Deployment, rsList []*apps.ReplicaSet) error &#123; newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil &#123; return err &#125; allRSs := append(oldRSs, newRS) return dc.syncDeploymentStatus(allRSs, newRS, d)&#125; syncDeploymentStatus 首先通过 newRS 和 allRSs 计算 deployment 当前的 status，然后和 deployment 中的 status 进行比较，若二者有差异则更新 deployment 使用最新的 status，syncDeploymentStatus 在后面的多种操作中都会被用到。 k8s.io/kubernetes/pkg/controller/deployment/sync.go:469123456789101112func (dc *DeploymentController) syncDeploymentStatus(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error &#123; newStatus := calculateStatus(allRSs, newRS, d) if reflect.DeepEqual(d.Status, newStatus) &#123; return nil &#125; newDeployment := d newDeployment.Status = newStatus _, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(newDeployment) return err&#125; calculateStatus 如下所示，主要是通过 allRSs 以及 deployment 的状态计算出最新的 status。 k8s.io/kubernetes/pkg/controller/deployment/sync.go:483 1234567891011121314151617181920212223242526272829303132func calculateStatus(allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) apps.DeploymentStatus &#123; availableReplicas := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs) totalReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs) unavailableReplicas := totalReplicas - availableReplicas if unavailableReplicas &lt; 0 &#123; unavailableReplicas = 0 &#125; status := apps.DeploymentStatus&#123; ObservedGeneration: deployment.Generation, Replicas: deploymentutil.GetActualReplicaCountForReplicaSets(allRSs), UpdatedReplicas: deploymentutil.GetActualReplicaCountForReplicaSets([]*apps.ReplicaSet&#123;newRS&#125;), ReadyReplicas: deploymentutil.GetReadyReplicaCountForReplicaSets(allRSs), AvailableReplicas: availableReplicas, UnavailableReplicas: unavailableReplicas, CollisionCount: deployment.Status.CollisionCount, &#125; conditions := deployment.Status.Conditions for i := range conditions &#123; status.Conditions = append(status.Conditions, conditions[i]) &#125; conditions := deployment.Status.Conditions for i := range conditions &#123; status.Conditions = append(status.Conditions, conditions[i]) &#125; ...... return status&#125; 以上就是 controller 中处理删除逻辑的主要流程，通过上述代码可知，当删除 deployment 对象时，仅仅是判断该对象中是否存在 metadata.DeletionTimestamp 字段，然后进行一次状态同步，并没有看到删除 deployment、rs、pod 对象的操作，其实删除对象并不是在此处进行而是在 kube-controller-manager 的垃圾回收器(garbagecollector controller)中完成的，对于 garbagecollector controller 会在后面的文章中进行说明，此外在删除对象时还需要指定一个删除选项(orphan、background 或者 foreground)来说明该对象如何删除。 暂停和恢复暂停以及恢复两个操作都是通过更新 deployment spec.paused 字段实现的，下面直接看它的具体实现。 123456789101112131415161718192021func (dc *DeploymentController) syncDeployment(key string) error &#123; ...... // pause 操作 if d.Spec.Paused &#123; return dc.sync(d, rsList) &#125; if getRollbackTo(d) != nil &#123; return dc.rollback(d, rsList) &#125; // scale 操作 scalingEvent, err := dc.isScalingEvent(d, rsList) if err != nil &#123; return err &#125; if scalingEvent &#123; return dc.sync(d, rsList) &#125; ......&#125; 当触发暂停操作时，会调用 sync 方法进行操作，sync 方法的主要逻辑如下所示： 1、获取 newRS 和 oldRSs； 2、根据 newRS 和 oldRSs 判断是否需要 scale 操作； 3、若处于暂停状态且没有执行回滚操作，则根据 deployment 的 .spec.revisionHistoryLimit 中的值清理多余的 rs； 4、最后执行 syncDeploymentStatus 更新 status； 123456789101112131415161718func (dc *DeploymentController) sync(d *apps.Deployment, rsList []*apps.ReplicaSet) error &#123; newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil &#123; return err &#125; if err := dc.scale(d, newRS, oldRSs); err != nil &#123; return err &#125; if d.Spec.Paused &amp;&amp; getRollbackTo(d) == nil &#123; if err := dc.cleanupDeployment(oldRSs, d); err != nil &#123; return err &#125; &#125; allRSs := append(oldRSs, newRS) return dc.syncDeploymentStatus(allRSs, newRS, d)&#125; 上文已经提到过 deployment controller 在一个 syncLoop 中各种操作是有优先级，而 pause &gt; rollback &gt; scale &gt; rollout，通过文章开头的命令行参数也可以看出，暂停和恢复操作只有在 rollout 时才会生效，再结合源码分析，虽然暂停操作下不会执行到 scale 相关的操作，但是 pause 与 scale 都是调用 sync 方法完成的，且在 sync 方法中会首先检查 scale 操作是否完成，也就是说在 pause 操作后并不是立即暂停所有操作，例如，当执行滚动更新操作后立即执行暂停操作，此时滚动更新的第一个周期并不会立刻停止而是会等到滚动更新的第一个周期完成后才会处于暂停状态，在下文的滚动更新一节会有例子进行详细的分析，至于 scale 操作在下文也会进行详细分析。 syncDeploymentStatus 方法以及相关的代码在上文的删除操作中已经解释过了，此处不再进行分析。 回滚kubernetes 中的每一个 Deployment 资源都包含有 revision 这个概念，并且其 .spec.revisionHistoryLimit 字段指定了需要保留的历史版本数，默认为10，每个版本都会对应一个 rs，若发现集群中有大量 0/0 rs 时请不要删除它，这些 rs 对应的都是 deployment 的历史版本，否则会导致无法回滚。当一个 deployment 的历史 rs 数超过指定数时，deployment controller 会自动清理。 当在客户端触发回滚操作时，controller 会调用 getRollbackTo 进行判断并调用 rollback 执行对应的回滚操作。 1234567func (dc *DeploymentController) syncDeployment(key string) error &#123; ...... if getRollbackTo(d) != nil &#123; return dc.rollback(d, rsList) &#125; ......&#125; getRollbackTo 通过判断 deployment 是否存在 rollback 对应的注解然后获取其值作为目标版本。 1234567891011121314func getRollbackTo(d *apps.Deployment) *extensions.RollbackConfig &#123; // annotations 为 "deprecated.deployment.rollback.to" revision := d.Annotations[apps.DeprecatedRollbackTo] if revision == "" &#123; return nil &#125; revision64, err := strconv.ParseInt(revision, 10, 64) if err != nil &#123; return nil &#125; return &amp;extensions.RollbackConfig&#123; Revision: revision64, &#125;&#125; rollback 方法的主要逻辑如下： 1、获取 newRS 和 oldRSs； 2、调用 getRollbackTo 获取 rollback 的 revision； 3、判断 revision 以及对应的 rs 是否存在，若 revision 为 0，则表示回滚到上一个版本； 4、若存在对应的 rs，则调用 rollbackToTemplate 方法将 rs.Spec.Template 赋值给 d.Spec.Template，否则放弃回滚操作； k8s.io/kubernetes/pkg/controller/deployment/rollback.go:32123456789101112131415161718192021222324252627282930313233343536func (dc *DeploymentController) rollback(d *apps.Deployment, rsList []*apps.ReplicaSet) error &#123; // 1、获取 newRS 和 oldRSs newRS, allOldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil &#123; return err &#125; allRSs := append(allOldRSs, newRS) // 2、调用 getRollbackTo 获取 rollback 的 revision rollbackTo := getRollbackTo(d) // 3、判断 revision 以及对应的 rs 是否存在，若 revision 为 0，则表示回滚到最新的版本 if rollbackTo.Revision == 0 &#123; if rollbackTo.Revision = deploymentutil.LastRevision(allRSs); rollbackTo.Revision == 0 &#123; // 4、清除回滚标志放弃回滚操作 return dc.updateDeploymentAndClearRollbackTo(d) &#125; &#125; for _, rs := range allRSs &#123; v, err := deploymentutil.Revision(rs) if err != nil &#123; ...... &#125; if v == rollbackTo.Revision &#123; // 5、调用 rollbackToTemplate 进行回滚操作 performedRollback, err := dc.rollbackToTemplate(d, rs) if performedRollback &amp;&amp; err == nil &#123; ...... &#125; return err &#125; &#125; return dc.updateDeploymentAndClearRollbackTo(d)&#125; rollbackToTemplate 会判断 deployment.Spec.Template 和 rs.Spec.Template 是否相等，若相等则无需回滚，否则使用 rs.Spec.Template 替换 deployment.Spec.Template，然后更新 deployment 的 spec 并清除回滚标志。 k8s.io/kubernetes/pkg/controller/deployment/rollback.go:751234567891011121314151617func (dc *DeploymentController) rollbackToTemplate(d *apps.Deployment, rs *apps.ReplicaSet) (bool, error) &#123; performedRollback := false // 1、比较 d.Spec.Template 和 rs.Spec.Template 是否相等 if !deploymentutil.EqualIgnoreHash(&amp;d.Spec.Template, &amp;rs.Spec.Template) &#123; // 2、替换 d.Spec.Template deploymentutil.SetFromReplicaSetTemplate(d, rs.Spec.Template) // 3、设置 annotation deploymentutil.SetDeploymentAnnotationsTo(d, rs) performedRollback = true &#125; else &#123; dc.emitRollbackWarningEvent(d, deploymentutil.RollbackTemplateUnchanged, eventMsg) &#125; // 4、更新 deployment 并清除回滚标志 return performedRollback, dc.updateDeploymentAndClearRollbackTo(d)&#125; 回滚操作其实就是通过 revision 找到对应的 rs，然后使用 rs.Spec.Template 替换 deployment.Spec.Template 最后驱动 replicaSet 和 pod 达到期望状态即完成了回滚操作，在最新版中，这种使用注解方式指定回滚版本的方法即将被废弃。 扩缩容当执行 scale 操作时，首先会通过 isScalingEvent 方法判断是否为扩缩容操作，然后通过 dc.sync 方法来执行实际的扩缩容动作。 123456789101112func (dc *DeploymentController) syncDeployment(key string) error &#123; ...... // scale 操作 scalingEvent, err := dc.isScalingEvent(d, rsList) if err != nil &#123; return err &#125; if scalingEvent &#123; return dc.sync(d, rsList) &#125; ......&#125; isScalingEvent 的主要逻辑如下所示： 1、获取所有的 rs； 2、过滤出 activeRS，rs.Spec.Replicas &gt; 0 的为 activeRS； 3、判断 rs 的 desired 值是否等于 deployment.Spec.Replicas，若不等于则需要为 rs 进行 scale 操作； k8s.io/kubernetes/pkg/controller/deployment/sync.go:52612345678910111213141516171819202122func (dc *DeploymentController) isScalingEvent(......) (bool, error) &#123; // 1、获取所有 rs newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil &#123; return false, err &#125; allRSs := append(oldRSs, newRS) // 2、过滤出 activeRS 并进行比较 for _, rs := range controller.FilterActiveReplicaSets(allRSs) &#123; // 3、获取 rs annotation 中 deployment.kubernetes.io/desired-replicas 的值 desired, ok := deploymentutil.GetDesiredReplicasAnnotation(rs) if !ok &#123; continue &#125; // 4、判断是否需要 scale 操作 if desired != *(d.Spec.Replicas) &#123; return true, nil &#125; &#125; return false, nil&#125; 在通过 isScalingEvent 判断为 scale 操作时会调用 sync 方法执行，主要逻辑如下： 1、获取 newRS 和 oldRSs； 2、调用 scale 方法进行扩缩容操作； 3、同步 deployment 的状态； 12345678910111213func (dc *DeploymentController) sync(d *apps.Deployment, rsList []*apps.ReplicaSet) error &#123; newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil &#123; return err &#125; if err := dc.scale(d, newRS, oldRSs); err != nil &#123; return err &#125; ...... allRSs := append(oldRSs, newRS) return dc.syncDeploymentStatus(allRSs, newRS, d)&#125; sync 方法中会调用 scale 方法执行扩容操作，其主要逻辑为： 1、通过 FindActiveOrLatest 获取 activeRS 或者最新的 rs，此时若只有一个 rs 说明本次操作仅为 scale 操作，则调用 scaleReplicaSetAndRecordEvent 对 rs 进行 scale 操作，否则此时存在多个 activeRS； 2、判断 newRS 是否已达到期望副本数，若达到则将所有的 oldRS 缩容到 0； 3、若 newRS 还未达到期望副本数，且存在多个 activeRS，说明此时的操作有可能是升级与扩缩容操作同时进行，若 deployment 的更新操作为 RollingUpdate 那么 scale 操作也需要按比例进行： 通过 FilterActiveReplicaSets 获取所有活跃的 ReplicaSet 对象； 调用 GetReplicaCountForReplicaSets 计算当前 Deployment 对应 ReplicaSet 持有的全部 Pod 副本个数； 计算 Deployment 允许创建的最大 Pod 数量； 判断是扩容还是缩容并对 allRSs 按时间戳进行正向或者反向排序； 计算每个 rs 需要增加或者删除的副本数； 更新 rs 对象； 4、若为 recreat 则需要等待更新完成后再进行 scale 操作； k8s.io/kubernetes/pkg/controller/deployment/sync.go:2941234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192func (dc *DeploymentController) scale(......) error &#123; // 1、在滚动更新过程中 第一个 rs 的 replicas 数量= maxSuger + dep.spec.Replicas ， // 更新完成后 pod 数量会多出 maxSurge 个，此处若检测到则应缩减回去 if activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil &#123; if *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) &#123; return nil &#125; // 2、只更新 rs annotation 以及为 deployment 设置 events _, _, err := dc.scaleReplicaSetAndRecordEvent(activeOrLatest, *(deployment.Spec.Replicas), deployment) return err &#125; // 3、当调用 IsSaturated 方法发现当前的 Deployment 对应的副本数量已经达到期望状态时就 // 将所有历史版本 rs 持有的副本缩容为 0 if deploymentutil.IsSaturated(deployment, newRS) &#123; for _, old := range controller.FilterActiveReplicaSets(oldRSs) &#123; if _, _, err := dc.scaleReplicaSetAndRecordEvent(old, 0, deployment); err != nil &#123; return err &#125; &#125; return nil &#125; // 4、此时说明 当前的 rs 副本并没有达到期望状态并且存在多个活跃的 rs 对象， // 若 deployment 的更新策略为滚动更新，需要按照比例分别对各个活跃的 rs 进行扩容或者缩容 if deploymentutil.IsRollingUpdate(deployment) &#123; allRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS)) allRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs) allowedSize := int32(0) // 5、计算最大可以创建出的 pod 数 if *(deployment.Spec.Replicas) &gt; 0 &#123; allowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment) &#125; // 6、计算需要扩容的 pod 数 deploymentReplicasToAdd := allowedSize - allRSsReplicas // 7、如果 deploymentReplicasToAdd &gt; 0，ReplicaSet 将按照从新到旧的顺序依次进行扩容； // 如果 deploymentReplicasToAdd &lt; 0，ReplicaSet 将按照从旧到新的顺序依次进行缩容； // 若 &gt; 0，则需要先扩容 newRS，但当在先扩容然后立刻缩容时，若 &lt;0,则需要先删除 oldRS 的 pod var scalingOperation string switch &#123; case deploymentReplicasToAdd &gt; 0: sort.Sort(controller.ReplicaSetsBySizeNewer(allRSs)) scalingOperation = &quot;up&quot; case deploymentReplicasToAdd &lt; 0: sort.Sort(controller.ReplicaSetsBySizeOlder(allRSs)) scalingOperation = &quot;down&quot; &#125; deploymentReplicasAdded := int32(0) nameToSize := make(map[string]int32) // 8、遍历所有的 rs，计算每个 rs 需要扩容或者缩容到的期望副本数 for i := range allRSs &#123; rs := allRSs[i] if deploymentReplicasToAdd != 0 &#123; // 9、调用 GetProportion 估算出 rs 需要扩容或者缩容的副本数 proportion := deploymentutil.GetProportion(rs, *deployment, deploymentReplicasToAdd, deploymentReplicasAdded) nameToSize[rs.Name] = *(rs.Spec.Replicas) + proportion deploymentReplicasAdded += proportion &#125; else &#123; nameToSize[rs.Name] = *(rs.Spec.Replicas) &#125; &#125; // 10、遍历所有的 rs，第一个最活跃的 rs.Spec.Replicas 加上上面循环中计算出 // 其他 rs 要加或者减的副本数，然后更新所有 rs 的 rs.Spec.Replicas for i := range allRSs &#123; rs := allRSs[i] // 11、要扩容或者要删除的 rs 已经达到了期望状态 if i == 0 &amp;&amp; deploymentReplicasToAdd != 0 &#123; leftover := deploymentReplicasToAdd - deploymentReplicasAdded nameToSize[rs.Name] = nameToSize[rs.Name] + leftover if nameToSize[rs.Name] &lt; 0 &#123; nameToSize[rs.Name] = 0 &#125; &#125; // 12、对 rs 进行 scale 操作 if _, _, err := dc.scaleReplicaSet(rs, nameToSize[rs.Name], deployment, scalingOperation); err != nil &#123; return err &#125; &#125; &#125; return nil&#125; 上述方法中有一个重要的操作就是在第 9 步调用 GetProportion 方法估算出 rs 需要扩容或者缩容的副本数，该方法中计算副本数的逻辑如下所示： k8s.io/kubernetes/pkg/controller/deployment/util/deployment_util.go:4661234567891011121314151617181920212223242526272829303132func GetProportion(rs *apps.ReplicaSet, d apps.Deployment, deploymentReplicasToAdd, deploymentReplicasAdded int32) int32 &#123; if rs == nil || *(rs.Spec.Replicas) == 0 || deploymentReplicasToAdd == 0 || deploymentReplicasToAdd == deploymentReplicasAdded &#123; return int32(0) &#125; // 调用 getReplicaSetFraction 方法 rsFraction := getReplicaSetFraction(*rs, d) allowed := deploymentReplicasToAdd - deploymentReplicasAdded if deploymentReplicasToAdd &gt; 0 &#123; return integer.Int32Min(rsFraction, allowed) &#125; return integer.Int32Max(rsFraction, allowed)&#125;func getReplicaSetFraction(rs apps.ReplicaSet, d apps.Deployment) int32 &#123; if *(d.Spec.Replicas) == int32(0) &#123; return -*(rs.Spec.Replicas) &#125; deploymentReplicas := *(d.Spec.Replicas) + MaxSurge(d) annotatedReplicas, ok := getMaxReplicasAnnotation(&amp;rs) if !ok &#123; annotatedReplicas = d.Status.Replicas &#125; // 计算 newRSSize 的公式 newRSsize := (float64(*(rs.Spec.Replicas) * deploymentReplicas)) / float64(annotatedReplicas) // 返回最终计算出的结果 return integer.RoundToInt32(newRSsize) - *(rs.Spec.Replicas)&#125; 滚动更新deployment 的更新方式有两种，其中滚动更新是最常用的，下面就看看其具体的实现。 1234567891011func (dc *DeploymentController) syncDeployment(key string) error &#123; ...... switch d.Spec.Strategy.Type &#123; case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: // 调用 rolloutRolling 执行滚动更新 return dc.rolloutRolling(d, rsList) &#125; ......&#125; 通过判断 d.Spec.Strategy.Type ，当更新操作为 rolloutRolling 时，会调用 rolloutRolling 方法进行操作，具体的逻辑如下所示： 1、调用 getAllReplicaSetsAndSyncRevision 获取所有的 rs，若没有 newRS 则创建； 2、调用 reconcileNewReplicaSet 判断是否需要对 newRS 进行 scaleUp 操作； 3、如果需要 scaleUp，更新 Deployment 的 status，添加相关的 condition，直接返回； 4、调用 reconcileOldReplicaSets 判断是否需要为 oldRS 进行 scaleDown 操作； 5、如果两者都不是则滚动升级很可能已经完成，此时需要检查 deployment status 是否已经达到期望状态，并且根据 deployment.Spec.RevisionHistoryLimit 的值清理 oldRSs； 123456789101112131415161718192021222324252627282930313233343536func (dc *DeploymentController) rolloutRolling(......) error &#123; // 1、获取所有的 rs，若没有 newRS 则创建 newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil &#123; return err &#125; allRSs := append(oldRSs, newRS) // 2、执行 scale up 操作 scaledUp, err := dc.reconcileNewReplicaSet(allRSs, newRS, d) if err != nil &#123; return err &#125; if scaledUp &#123; return dc.syncRolloutStatus(allRSs, newRS, d) &#125; // 3、执行 scale down 操作 scaledDown, err := dc.reconcileOldReplicaSets(allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if err != nil &#123; return err &#125; if scaledDown &#123; return dc.syncRolloutStatus(allRSs, newRS, d) &#125; // 4、清理过期的 rs if deploymentutil.DeploymentComplete(d, &amp;d.Status) &#123; if err := dc.cleanupDeployment(oldRSs, d); err != nil &#123; return err &#125; &#125; // 5、同步 deployment status return dc.syncRolloutStatus(allRSs, newRS, d)&#125; reconcileNewReplicaSet 主要逻辑如下： 1、判断 newRS.Spec.Replicas 和 deployment.Spec.Replicas 是否相等，如果相等则直接返回，说明已经达到期望状态； 2、若 newRS.Spec.Replicas &gt; deployment.Spec.Replicas ，则说明 newRS 副本数已经超过期望值，调用 dc.scaleReplicaSetAndRecordEvent 进行 scale down； 3、此时 newRS.Spec.Replicas &lt; deployment.Spec.Replicas ，调用 NewRSNewReplicas 为 newRS 计算所需要的副本数，计算原则遵守 maxSurge 和 maxUnavailable 的约束； 4、调用 scaleReplicaSetAndRecordEvent 更新 newRS 对象，设置 rs.Spec.Replicas、rs.Annotations[DesiredReplicasAnnotation] 以及 rs.Annotations[MaxReplicasAnnotation] ； k8s.io/kubernetes/pkg/controller/deployment/rolling.go:6912345678910111213141516171819202122func (dc *DeploymentController) reconcileNewReplicaSet(......) (bool, error) &#123; // 1、判断副本数是否已达到了期望值 if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) &#123; return false, nil &#125; // 2、判断是否需要 scale down 操作 if *(newRS.Spec.Replicas) &gt; *(deployment.Spec.Replicas) &#123; scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, *(deployment.Spec.Replicas), deployment) return scaled, err &#125; // 3、计算 newRS 所需要的副本数 newReplicasCount, err := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS) if err != nil &#123; return false, err &#125; // 4、如果需要 scale ，则更新 rs 的 annotation 以及 rs.Spec.Replicas scaled, _, err := dc.scaleReplicaSetAndRecordEvent(newRS, newReplicasCount, deployment) return scaled, err&#125; NewRSNewReplicas 是为 newRS 计算所需要的副本数，该方法主要逻辑为： 1、判断更新策略； 2、计算 maxSurge 值； 3、通过 allRSs 计算 currentPodCount 的值； 4、最后计算 scaleUpCount 值； k8s.io/kubernetes/pkg/controller/deployment/util/deployment_util.go:814123456789101112131415161718192021222324252627func NewRSNewReplicas(......) (int32, error) &#123; switch deployment.Spec.Strategy.Type &#123; case apps.RollingUpdateDeploymentStrategyType: // 1、计算 maxSurge 值 maxSurge, err := intstrutil.GetValueFromIntOrPercent(deployment.Spec.Strategy.RollingUpdate.MaxSurge, int(*(deployment.Spec.Replicas)), true) if err != nil &#123; return 0, err &#125; // 2、累加 rs.Spec.Replicas 获取 currentPodCount currentPodCount := GetReplicaCountForReplicaSets(allRSs) maxTotalPods := *(deployment.Spec.Replicas) + int32(maxSurge) if currentPodCount &gt;= maxTotalPods &#123; return *(newRS.Spec.Replicas), nil &#125; // 3、计算 scaleUpCount scaleUpCount := maxTotalPods - currentPodCount scaleUpCount = int32(integer.IntMin(int(scaleUpCount), int(*(deployment.Spec.Replicas)-*(newRS.Spec.Replicas)))) return *(newRS.Spec.Replicas) + scaleUpCount, nil case apps.RecreateDeploymentStrategyType: return *(deployment.Spec.Replicas), nil default: return 0, fmt.Errorf(&quot;deployment type %v isn&apos;t supported&quot;, deployment.Spec.Strategy.Type) &#125;&#125; reconcileOldReplicaSets 的主要逻辑如下： 1、通过 oldRSs 和 allRSs 获取 oldPodsCount 和 allPodsCount； 2、计算 deployment 的 maxUnavailable、minAvailable、newRSUnavailablePodCount、maxScaledDown 值，当 deployment 的 maxSurge 和 maxUnavailable 值为百分数时，计算 maxSurge 向上取整而 maxUnavailable 则向下取整； 3、清理异常的 rs； 4、计算 oldRS 的 scaleDownCount； 123456789101112131415161718192021222324252627282930313233343536func (dc *DeploymentController) reconcileOldReplicaSets(......) (bool, error) &#123; // 1、计算 oldPodsCount oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs) if oldPodsCount == 0 &#123; return false, nil &#125; // 2、计算 allPodsCount allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs) // 3、计算 maxScaledDown maxUnavailable := deploymentutil.MaxUnavailable(*deployment) minAvailable := *(deployment.Spec.Replicas) - maxUnavailable newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount if maxScaledDown &lt;= 0 &#123; return false, nil &#125; // 4、清理异常的 rs oldRSs, cleanupCount, err := dc.cleanupUnhealthyReplicas(oldRSs, deployment, maxScaledDown) if err != nil &#123; return false, nil &#125; allRSs = append(oldRSs, newRS) // 5、缩容 old rs scaledDownCount, err := dc.scaleDownOldReplicaSetsForRollingUpdate(allRSs, oldRSs, deployment) if err != nil &#123; return false, nil &#125; totalScaledDown := cleanupCount + scaledDownCount return totalScaledDown &gt; 0, nil&#125; 通过上面的代码可以看出，滚动更新过程中主要是通过调用reconcileNewReplicaSet对 newRS 不断扩容，调用 reconcileOldReplicaSets 对 oldRS 不断缩容，最终达到期望状态，并且在整个升级过程中，都严格遵守 maxSurge 和 maxUnavailable 的约束。 不论是在 scale up 或者 scale down 中都是调用 scaleReplicaSetAndRecordEvent 执行，而 scaleReplicaSetAndRecordEvent 又会调用 scaleReplicaSet 来执行，两个操作都是更新 rs 的 annotations 以及 rs.Spec.Replicas。 12345scale down or --&gt; dc.scaleReplicaSetAndRecordEvent() --&gt; dc.scaleReplicaSet()scale up 滚动更新示例上面的代码看起来非常的枯燥，只看源码其实并不能完全理解整个滚动升级的流程，此处举个例子说明一下： 创建一个 nginx-deployment 有10 个副本，等 10 个 pod 都启动完成后如下所示：12345$ kubectl create -f nginx-dep.yaml$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-68b649bd8b 10 10 10 72m 然后更新 nginx-deployment 的镜像，默认使用滚动更新的方式：1$ kubectl set image deploy/nginx-deployment nginx-deployment=nginx:1.9.3 此时通过源码可知会计算该 deployment 的 maxSurge、maxUnavailable 和 maxAvailable 的值，分别为 3、2 和 13，计算方法如下所示：1234567// 向上取整为 3maxSurge = replicas * deployment.spec.strategy.rollingUpdate.maxSurge(25%)= 2.5// 向下取整为 2maxUnavailable = replicas * deployment.spec.strategy.rollingUpdate.maxUnavailable(25%)= 2.5maxAvailable = replicas(10) + MaxSurge（3） = 13 如上面代码所说，更新时首先创建 newRS，然后为其设定 replicas，计算 newRS replicas 值的方法在NewRSNewReplicas 中，此时计算出 replicas 结果为 3，然后更新 deployment 的 annotation，创建 events，本次 syncLoop 完成。等到下一个 syncLoop 时，所有 rs 的 replicas 已经达到最大值 10 + 3 = 13，此时需要 scale down oldRSs 了，scale down 的数量是通过以下公式得到的：1234567891011// 13 = 10 + 3allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs)// 8 = 10 - 2minAvailable := *(deployment.Spec.Replicas) - maxUnavailable// ???newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas// 13 - 8 - ???maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount allPodsCount 是 allRSs 的 replicas 之和此时为 13，minAvailable 为 8 ，newRSUnavailablePodCount 此时不确定，但是值在 [0,3] 中，此时假设 newRS 的三个 pod 还处于 containerCreating 状态，则newRSUnavailablePodCount 为 3，根据以上公式计算所知 maxScaledDown 为 2，则 oldRS 需要 scale down 2 个 pod，其 replicas 需要改为 8，此时该 syncLoop 完成。下一个 syncLoop 时在 scaleUp 处计算得知 scaleUpCount = maxTotalPods - currentPodCount，13-3-8=2， 此时 newRS 需要更新 replicase 增加 2。以此轮询直到 newRS replicas 扩容到 10，oldRSs replicas 缩容至 0。 对于上面的示例，可以使用 kubectl get rs -w 进行观察，以下为输出：12345678910111213141516171819202122$ kubectl get rs -wNAME DESIRED CURRENT READY AGEnginx-deployment-68b649bd8b 10 0 0 0snginx-deployment-68b649bd8b 10 10 0 0snginx-deployment-68b649bd8b 10 10 10 13snginx-deployment-689bff574f 3 0 0 0snginx-deployment-68b649bd8b 8 10 10 14snginx-deployment-689bff574f 3 0 0 0snginx-deployment-689bff574f 3 3 3 1snginx-deployment-689bff574f 5 3 0 0snginx-deployment-68b649bd8b 8 8 8 14snginx-deployment-689bff574f 5 3 0 0snginx-deployment-689bff574f 5 5 0 0snginx-deployment-689bff574f 5 5 5 6s...... 重新创建deployment 的另一种更新策略recreate 就比较简单粗暴了，当更新策略为 Recreate 时，deployment 先将所有旧的 rs 缩容到 0，并等待所有 pod 都删除后，再创建新的 rs。 12345678910func (dc *DeploymentController) syncDeployment(key string) error &#123; ...... switch d.Spec.Strategy.Type &#123; case apps.RecreateDeploymentStrategyType: return dc.rolloutRecreate(d, rsList, podMap) case apps.RollingUpdateDeploymentStrategyType: return dc.rolloutRolling(d, rsList) &#125; ......&#125; rolloutRecreate 方法主要逻辑为： 1、获取 newRS 和 oldRSs； 2、缩容 oldRS replicas 至 0； 3、创建 newRS； 4、扩容 newRS； 5、同步 deployment 状态； 123456789101112131415161718192021222324252627282930313233343536373839404142434445func (dc *DeploymentController) rolloutRecreate(......) error &#123; // 1、获取所有 rs newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(d, rsList, false) if err != nil &#123; return err &#125; allRSs := append(oldRSs, newRS) activeOldRSs := controller.FilterActiveReplicaSets(oldRSs) // 2、缩容 oldRS scaledDown, err := dc.scaleDownOldReplicaSetsForRecreate(activeOldRSs, d) if err != nil &#123; return err &#125; if scaledDown &#123; return dc.syncRolloutStatus(allRSs, newRS, d) &#125; if oldPodsRunning(newRS, oldRSs, podMap) &#123; return dc.syncRolloutStatus(allRSs, newRS, d) &#125; // 3、创建 newRS if newRS == nil &#123; newRS, oldRSs, err = dc.getAllReplicaSetsAndSyncRevision(d, rsList, true) if err != nil &#123; return err &#125; allRSs = append(oldRSs, newRS) &#125; // 4、扩容 newRS if _, err := dc.scaleUpNewReplicaSetForRecreate(newRS, d); err != nil &#123; return err &#125; // 5、清理过期的 RS if util.DeploymentComplete(d, &amp;d.Status) &#123; if err := dc.cleanupDeployment(oldRSs, d); err != nil &#123; return err &#125; &#125; // 6、同步 deployment 状态 return dc.syncRolloutStatus(allRSs, newRS, d)&#125; 判断 deployment 是否存在 newRS 是在 deploymentutil.FindNewReplicaSet 方法中进行判断的，对比 rs.Spec.Template 和 deployment.Spec.Template 中字段的 hash 值是否相等以此进行确定，在上面的几个操作中也多次用到了该方法，此处说明一下。 1dc.getAllReplicaSetsAndSyncRevision() --&gt; dc.getNewReplicaSet() --&gt; deploymentutil.FindNewReplicaSet() --&gt; EqualIgnoreHash() EqualIgnoreHash 方法如下所示： k8s.io/kubernetes/pkg/controller/deployment/util/deployment_util.go:63312345678func EqualIgnoreHash(template1, template2 *v1.PodTemplateSpec) bool &#123; t1Copy := template1.DeepCopy() t2Copy := template2.DeepCopy() // Remove hash labels from template.Labels before comparing delete(t1Copy.Labels, apps.DefaultDeploymentUniqueLabelKey) delete(t2Copy.Labels, apps.DefaultDeploymentUniqueLabelKey) return apiequality.Semantic.DeepEqual(t1Copy, t2Copy)&#125; 以上就是对 deployment recreate 更新策略源码的分析，需要注意的是，该策略会导致服务一段时间不可用，当 oldRS 缩容为 0，newRS 才开始创建，此时无可用的 pod，所以在生产环境中请慎用该更新策略。 总结本文主要介绍了 deployment 的基本功能以及从源码角度分析其实现，deployment 主要有更新、回滚、扩缩容、暂停与恢复几个主要的功能。从源码中可以看到 deployment 在升级过程中一直会修改 rs 的 replicas 以及 annotation 最终达到最终期望的状态，但是整个过程中并没有体现出 pod 的创建与删除，从开头三者的关系图中可知是 rs 控制 pod 的变化，在下篇文章中会继续介绍 rs 是如何控制 pod 的变化。 参考：https://my.oschina.net/u/3797264/blog/2966086https://draveness.me/kubernetes-deployment]]></content>
      <tags>
        <tag>kube-controller-manager</tag>
        <tag>deployment controller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-proxy ipvs 模式源码分析]]></title>
    <url>%2F2019%2F11%2F18%2Fkube_proxy_ipvs%2F</url>
    <content type="text"><![CDATA[前几篇文章已经分析了 service 的原理以及 kube-proxy iptables 模式的原理与实现，本篇文章会继续分析 kube-proxy ipvs 模式的原理与实现。 ipvsipvs (IP Virtual Server) 是基于 Netfilter 的，作为 linux 内核的一部分实现了传输层负载均衡，ipvs 集成在LVS(Linux Virtual Server)中，它在主机中运行，并在真实服务器集群前充当负载均衡器。ipvs 可以将对 TCP/UDP 服务的请求转发给后端的真实服务器，因此 ipvs 天然支持 Kubernetes Service。ipvs 也包含了多种不同的负载均衡算法，例如轮询、最短期望延迟、最少连接以及各种哈希方法等，ipvs 的设计就是用来为大规模服务进行负载均衡的。 ipvs 的负载均衡方式ipvs 有三种负载均衡方式，分别为： NAT TUN DR 关于三种模式的原理可以参考：LVS 配置小结。 上面的负载均衡方式中只有 NAT 模式可以进行端口映射，因此 kubernetes 中 ipvs 的实现使用了 NAT 模式，用来将 service IP 和 service port 映射到后端的 container ip 和container port。 NAT 模式下的工作流程如下所示： 1234567891011121314151617181920 +--------+ | Client | +--------+ (CIP) &lt;-- Client&apos;s IP address | | &#123; internet &#125; | | (VIP) &lt;-- Virtual IP address +----------+ | Director | +----------+ (PIP) &lt;-- (Director&apos;s Private IP address) | | (RIP) &lt;-- Real (server&apos;s) IP address+-------------+| Real server |+-------------+ 其具体流程为：当用户发起一个请求时，请求从 VIP 接口流入，此时数据源地址是 CIP，目标地址是 VIP，当接收到请求后拆掉 mac 地址封装后看到目标 IP 地址就是自己，按照正常流程会通过 INPUT 转入用户空间，但此时工作在 INPUT 链上的 ipvs 会强行将数据转到 POSTROUTING 链上，并根据相应的负载均衡算法选择后端具体的服务器，再通过 DNAT 转发给 Real server，此时源地址 CIP，目标地址变成了 RIP。 ipvs 与 iptables 的区别与联系区别： 底层数据结构：iptables 使用链表，ipvs 使用哈希表 负载均衡算法：iptables 只支持随机、轮询两种负载均衡算法而 ipvs 支持的多达 8 种； 操作工具：iptables 需要使用 iptables 命令行工作来定义规则，ipvs 需要使用 ipvsadm 来定义规则。 此外 ipvs 还支持 realserver 运行状况检查、连接重试、端口映射、会话保持等功能。 联系： ipvs 和 iptables 都是基于 netfilter内核模块，两者都是在内核中的五个钩子函数处工作，下图是 ipvs 所工作的几个钩子函数： 关于 kube-proxy iptables 与 ipvs 模式的区别，更多详细信息可以查看官方文档：https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md。 ipsetIP sets 是 Linux 内核中的一个框架，可以由 ipset 命令进行管理。根据不同的类型，IP set 可以以某种方式保存 IP地址、网络、(TCP/UDP)端口号、MAC地址、接口名或它们的组合，并且能够快速匹配。 根据官网的介绍，若有以下使用场景： 在保存了多个 IP 地址或端口号的 iptables 规则集合中想使用哈希查找; 根据 IP 地址或端口动态更新 iptables 规则时希望在性能上无损； 在使用 iptables 工具创建一个基于 IP 地址和端口的复杂规则时觉得非常繁琐； 此时，使用 ipset 工具可能是你最好的选择。 ipset 是 iptables 的一种扩展，在 iptables 中可以使用-m set启用 ipset 模块，具体来说，ipvs 使用 ipset 来存储需要 NAT 或 masquared 时的 ip 和端口列表。在数据包过滤过程中，首先遍历 iptables 规则，在定义了使用 ipset 的条件下会跳转到 ipset 列表中进行匹配。 kube-proxy ipvs 模式kube-proxy 的 ipvs 模式是在 2015 年由 k8s 社区的大佬 thockin 提出的(Try kube-proxy via ipvs instead of iptables or userspace)，在 2017 年由华为云团队实现的(Implement IPVS-based in-cluster service load balancing)。前面的文章已经提到了，在kubernetes v1.8 中已经引入了 ipvs 模式。 kube-proxy 在 ipvs 模式下自定义了八条链，分别为 KUBE-SERVICES、KUBE-FIREWALL、KUBE-POSTROUTING、KUBE-MARK-MASQ、KUBE-NODE-PORT、KUBE-MARK-DROP、KUBE-FORWARD、KUBE-LOAD-BALANCER ，如下所示： NAT 表： Filter 表： 此外，由于 linux 内核原生的 ipvs 模式只支持 DNAT，不支持 SNAT，所以，在以下几种场景中 ipvs 仍需要依赖 iptables 规则： 1、kube-proxy 启动时指定 –-masquerade-all=true 参数，即集群中所有经过 kube-proxy 的包都做一次 SNAT； 2、kube-proxy 启动时指定 --cluster-cidr= 参数； 3、对于 Load Balancer 类型的 service，用于配置白名单； 4、对于 NodePort 类型的 service，用于配置 MASQUERADE； 5、对于 externalIPs 类型的 service； 但对于 ipvs 模式的 kube-proxy，无论有多少 pod/service，iptables 的规则数都是固定的。 ipvs 模式的启用1、首先要加载 IPVS 所需要的 kernel module 123456$ modprobe -- ip_vs$ modprobe -- ip_vs_rr$ modprobe -- ip_vs_wrr$ modprobe -- ip_vs_sh$ modprobe -- nf_conntrack_ipv4$ cut -f1 -d &quot; &quot; /proc/modules | grep -e ip_vs -e nf_conntrack_ipv4 2、在启动 kube-proxy 时，指定 proxy-mode 参数 1--proxy-mode=ipvs (如果要使用其他负载均衡算法，可以指定 --ipvs-scheduler= 参数，默认为 rr) 当创建 ClusterIP type 的 service 时，IPVS proxier 会执行以下三个操作： 确保本机已创建 dummy 网卡，默认为 kube-ipvs0。为什么要创建 dummy 网卡？因为 ipvs netfilter 的 DNAT 钩子挂载在 INPUT 链上，当访问 ClusterIP 时，将 ClusterIP 绑定在 dummy 网卡上为了让内核识别该 IP 就是本机 IP，进而进入 INPUT 链，然后通过钩子函数 ip_vs_in 转发到 POSTROUTING 链； 将 ClusterIP 绑定到 dummy 网卡； 为每个 ClusterIP 创建 IPVS virtual servers 和 real server，分别对应 service 和 endpoints； 例如下面的示例： 123456789101112131415161718192021222324252627// kube-ipvs0 dummy 网卡$ ip addr......4: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default link/ether de:be:c0:73:bc:c7 brd ff:ff:ff:ff:ff:ff inet 10.96.0.1/32 brd 10.96.0.1 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.96.0.10/32 brd 10.96.0.10 scope global kube-ipvs0 valid_lft forever preferred_lft forever inet 10.97.4.140/32 brd 10.97.4.140 scope global kube-ipvs0 valid_lft forever preferred_lft forever ......// 10.97.4.140 为 CLUSTER-IP 挂载在 kube-ipvs0 上$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtenant-service ClusterIP 10.97.4.140 &lt;none&gt; 7000/TCP 23s// 10.97.4.140 后端的 realserver 分别为 10.244.1.2 和 10.244.1.3$ ipvsadm -L -nIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.97.4.140:7000 rr -&gt; 10.244.1.2:7000 Masq 1 0 0 -&gt; 10.244.1.3:7000 Masq 1 0 0 ipvs 模式下数据包的流向clusterIP 访问方式1PREROUTING --&gt; KUBE-SERVICES --&gt; KUBE-CLUSTER-IP --&gt; INPUT --&gt; KUBE-FIREWALL --&gt; POSTROUTING 首先进入 PREROUTING 链 从 PREROUTING 链会转到 KUBE-SERVICES 链，10.244.0.0/16 为 ClusterIP 网段 在 KUBE-SERVICES 链打标记 从 KUBE-SERVICES 链再进入到 KUBE-CLUSTER-IP 链 KUBE-CLUSTER-IP 为 ipset 集合，在此处会进行 DNAT 然后会进入 INPUT 链 从 INPUT 链会转到 KUBE-FIREWALL 链，在此处检查标记 在 INPUT 链处，ipvs 的 LOCAL_IN Hook 发现此包在 ipvs 规则中则直接转发到 POSTROUTING 链 12345678-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES-A KUBE-SERVICES ! -s 10.244.0.0/16 -m comment --comment &quot;Kubernetes service cluster ip + port for masquerade purpose&quot; -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ// 执行完 PREROUTING 规则,数据打上0x4000/0x4000的标记-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000-A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT KUBE-CLUSTER-IP 为 ipset 列表： 123456789101112# ipset list | grep -A 20 KUBE-CLUSTER-IPName: KUBE-CLUSTER-IPType: hash:ip,portRevision: 5Header: family inet hashsize 1024 maxelem 65536Size in memory: 352References: 2Members:10.96.0.10,17:5310.96.0.10,6:5310.96.0.1,6:44310.96.0.10,6:9153 然后会进入 INPUT： 123-A INPUT -j KUBE-FIREWALL-A KUBE-FIREWALL -m comment --comment &quot;kubernetes firewall for dropping marked packets&quot; -m mark --mark 0x8000/0x8000 -j DROP 如果进来的数据带有 0x8000/0x8000 标记则丢弃，若有 0x4000/0x4000 标记则正常执行： 12-A POSTROUTING -m comment --comment &quot;kubernetes postrouting rules&quot; -j KUBE-POSTROUTING-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -m mark --mark 0x4000/0x4000 -j MASQUERADE nodePort 方式1PREROUTING --&gt; KUBE-SERVICES --&gt; KUBE-NODE-PORT --&gt; INPUT --&gt; KUBE-FIREWALL --&gt; POSTROUTING 首先进入 PREROUTING 链 从 PREROUTING 链会转到 KUBE-SERVICES 链 在 KUBE-SERVICES 链打标记 从 KUBE-SERVICES 链再进入到 KUBE-NODE-PORT 链 KUBE-NODE-PORT 为 ipset 集合，在此处会进行 DNAT 然后会进入 INPUT 链 从 INPUT 链会转到 KUBE-FIREWALL 链，在此处检查标记 在 INPUT 链处，ipvs 的 LOCAL_IN Hook 发现此包在 ipvs 规则中则直接转发到 POSTROUTING 链 1234567-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES-A KUBE-SERVICES ! -s 10.244.0.0/16 -m comment --comment &quot;Kubernetes service cluster ip + port for masquerade purpose&quot; -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT KUBE-NODE-PORT 对应的 ipset 列表： 12345678# ipset list | grep -B 10 KUBE-NODE-PORTName: KUBE-NODE-PORT-TCPType: bitmap:portRevision: 3Header: range 0-65535Size in memory: 8268References: 0Members: 流入 INPUT 后与 ClusterIP 的访问方式相同。 kube-proxy ipvs 源码分析 kubernetes 版本：v1.16 在前面的文章中已经介绍过 ipvs 的初始化了，下面直接看其核心方法：proxier.syncRunner。 12345func NewProxier(......) &#123; ...... proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) ......&#125; proxier.syncRunner() 执行流程： 通过 iptables-save 获取现有的 Filter 和 NAT 表存在的链数据 创建自定义链与规则 创建 Dummy 接口和 ipset 默认列表 为每个服务生成 ipvs 规则 对 serviceMap 内的每个服务进行遍历处理，对不同的服务类型(clusterip/nodePort/externalIPs/load-balancer)进行不同的处理(ipset 列表/vip/ipvs 后端服务器) 根据 endpoint 列表，更新 KUBE-LOOP-BACK 的 ipset 列表 若为 clusterIP 类型更新对应的 ipset 列表 KUBE-CLUSTER-IP 若为 externalIPs 类型更新对应的 ipset 列表 KUBE-EXTERNAL-IP 若为 load-balancer 类型更新对应的 ipset 列表 KUBE-LOAD-BALANCER、KUBE-LOAD-BALANCER-LOCAL、KUBE-LOAD-BALANCER-FW、KUBE-LOAD-BALANCER-SOURCE-CIDR、KUBE-LOAD-BALANCER-SOURCE-IP 若为 NodePort 类型更新对应的 ipset 列表 KUBE-NODE-PORT-TCP、KUBE-NODE-PORT-LOCAL-TCP、KUBE-NODE-PORT-LOCAL-SCTP-HASH、KUBE-NODE-PORT-LOCAL-UDP、KUBE-NODE-PORT-SCTP-HASH、KUBE-NODE-PORT-UDP 同步 ipset 记录 刷新 iptables 规则 12345678910111213141516171819func (proxier *Proxier) syncProxyRules() &#123; proxier.mu.Lock() defer proxier.mu.Unlock() serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges) endpointUpdateResult := proxier.endpointsMap.Update(proxier.endpointsChanges) staleServices := serviceUpdateResult.UDPStaleClusterIP // 合并 service 列表 for _, svcPortName := range endpointUpdateResult.StaleServiceNames &#123; if svcInfo, ok := proxier.serviceMap[svcPortName]; ok &amp;&amp; svcInfo != nil &amp;&amp; svcInfo.Protocol() == v1.ProtocolUDP &#123; staleServices.Insert(svcInfo.ClusterIP().String()) for _, extIP := range svcInfo.ExternalIPStrings() &#123; staleServices.Insert(extIP) &#125; &#125; &#125; ...... 读取系统 iptables 到内存，创建自定义链以及 iptables 规则，创建 dummy interface kube-ipvs0，创建默认的 ipset 规则。 12345678910111213141516171819202122232425 proxier.natChains.Reset() proxier.natRules.Reset() proxier.filterChains.Reset() proxier.filterRules.Reset() writeLine(proxier.filterChains, &quot;*filter&quot;) writeLine(proxier.natChains, &quot;*nat&quot;) // 创建kubernetes的表连接链数据 proxier.createAndLinkeKubeChain()// 创建 dummy interface kube-ipvs0 _, err := proxier.netlinkHandle.EnsureDummyDevice(DefaultDummyDevice) if err != nil &#123; ...... return &#125;// 创建默认的 ipset 规则 for _, set := range proxier.ipsetList &#123; if err := ensureIPSet(set); err != nil &#123; return &#125; set.resetEntries() &#125; 对每一个服务创建 ipvs 规则。根据 endpoint 列表，更新 KUBE-LOOP-BACK 的 ipset 列表。 12345678910111213141516171819for svcName, svc := range proxier.serviceMap &#123; svcInfo, ok := svc.(*serviceInfo) if !ok &#123; ...... &#125; for _, e := range proxier.endpointsMap[svcName] &#123; ep, ok := e.(*proxy.BaseEndpointInfo) if !ok &#123; klog.Errorf(&quot;Failed to cast BaseEndpointInfo %q&quot;, e.String()) continue &#125; ...... if valid := proxier.ipsetList[kubeLoopBackIPSet].validateEntry(entry); !valid &#123; ...... &#125; proxier.ipsetList[kubeLoopBackIPSet].activeEntries.Insert(entry.String()) &#125; 对于 clusterIP 类型更新对应的 ipset 列表 KUBE-CLUSTER-IP。 1234567891011121314151617if valid := proxier.ipsetList[kubeClusterIPSet].validateEntry(entry); !valid &#123; ......&#125;proxier.ipsetList[kubeClusterIPSet].activeEntries.Insert(entry.String())......if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP &#123; ......&#125;// 绑定 ClusterIP to dummy interfaceif err := proxier.syncService(svcNameString, serv, true); err == nil &#123; // 同步 endpoints 信息 if err := proxier.syncEndpoint(svcName, false, serv); err != nil &#123; ...... &#125;&#125; else &#123; ......&#125; 为 externalIP 创建 ipvs 规则。 12345678910111213141516171819202122232425262728293031323334for _, externalIP := range svcInfo.ExternalIPStrings() &#123; if local, err := utilproxy.IsLocalIP(externalIP); err != nil &#123; ...... &#125; else if local &amp;&amp; (svcInfo.Protocol() != v1.ProtocolSCTP) &#123; ...... if proxier.portsMap[lp] != nil &#123; ...... &#125; else &#123; socket, err := proxier.portMapper.OpenLocalPort(&amp;lp) if err != nil &#123; ...... &#125; replacementPortsMap[lp] = socket &#125; &#125; ...... if valid := proxier.ipsetList[kubeExternalIPSet].validateEntry(entry); !valid &#123; ...... &#125; proxier.ipsetList[kubeExternalIPSet].activeEntries.Insert(entry.String()) ...... if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP &#123; ...... &#125; if err := proxier.syncService(svcNameString, serv, true); err == nil &#123; ...... if err := proxier.syncEndpoint(svcName, false, serv); err != nil &#123; ...... &#125; &#125; else &#123; ...... &#125;&#125; 为 load-balancer类型创建 ipvs 规则。 1234567891011121314151617181920212223242526272829303132for _, ingress := range svcInfo.LoadBalancerIPStrings() &#123; if ingress != &quot;&quot; &#123; ...... if valid := proxier.ipsetList[kubeLoadBalancerSet].validateEntry(entry); !valid &#123; ...... &#125; proxier.ipsetList[kubeLoadBalancerSet].activeEntries.Insert(entry.String()) if svcInfo.OnlyNodeLocalEndpoints() &#123; ...... &#125; if len(svcInfo.LoadBalancerSourceRanges()) != 0 &#123; ...... for _, src := range svcInfo.LoadBalancerSourceRanges() &#123; ...... &#125; ...... &#125; ...... if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP &#123; ...... &#125; if err := proxier.syncService(svcNameString, serv, true); err == nil &#123; ...... if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil &#123; ...... &#125; &#125; else &#123; ...... &#125; &#125;&#125; 为 nodePort 类型创建 ipvs 规则。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374 if svcInfo.NodePort() != 0 &#123; ...... var lps []utilproxy.LocalPort for _, address := range nodeAddresses &#123; ...... lps = append(lps, lp) &#125; for _, lp := range lps &#123; if proxier.portsMap[lp] != nil &#123; ...... &#125; else if svcInfo.Protocol() != v1.ProtocolSCTP &#123; socket, err := proxier.portMapper.OpenLocalPort(&amp;lp) if err != nil &#123; ...... &#125; if lp.Protocol == &quot;udp&quot; &#123; ...... &#125; &#125; &#125; switch protocol &#123; case &quot;tcp&quot;: ...... case &quot;udp&quot;: ...... case &quot;sctp&quot;: ...... default: ...... &#125; if nodePortSet != nil &#123; for _, entry := range entries &#123; ...... nodePortSet.activeEntries.Insert(entry.String()) &#125; &#125; if svcInfo.OnlyNodeLocalEndpoints() &#123; var nodePortLocalSet *IPSet switch protocol &#123; case &quot;tcp&quot;: nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetTCP] case &quot;udp&quot;: nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetUDP] case &quot;sctp&quot;: nodePortLocalSet = proxier.ipsetList[kubeNodePortLocalSetSCTP] default: ...... &#125; if nodePortLocalSet != nil &#123; entryInvalidErr := false for _, entry := range entries &#123; ...... nodePortLocalSet.activeEntries.Insert(entry.String()) &#125; ...... &#125; &#125; for _, nodeIP := range nodeIPs &#123; ...... if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP &#123; ...... &#125; if err := proxier.syncService(svcNameString, serv, false); err == nil &#123; if err := proxier.syncEndpoint(svcName, svcInfo.OnlyNodeLocalEndpoints(), serv); err != nil &#123; ...... &#125; &#125; else &#123; ...... &#125; &#125; &#125;&#125; 同步 ipset 记录，清理 conntrack。 12345678910111213141516171819 for _, set := range proxier.ipsetList &#123; set.syncIPSetEntries() &#125; proxier.writeIptablesRules() proxier.iptablesData.Reset() proxier.iptablesData.Write(proxier.natChains.Bytes()) proxier.iptablesData.Write(proxier.natRules.Bytes()) proxier.iptablesData.Write(proxier.filterChains.Bytes()) proxier.iptablesData.Write(proxier.filterRules.Bytes()) err = proxier.iptables.RestoreAll(proxier.iptablesData.Bytes(), utiliptables.NoFlushTables, utiliptables.RestoreCounters) if err != nil &#123; ...... &#125; ...... proxier.deleteEndpointConnections(endpointUpdateResult.StaleEndpoints)&#125; 总结本文主要讲述了 kube-proxy ipvs 模式的原理与实现，iptables 模式与 ipvs 模式下在源码实现上有许多相似之处，但二者原理不同，理解了原理分析代码则更加容易，笔者对于 ipvs 的知识也是现学的，文中如有不当之处望指正。虽然 ipvs 的性能要比 iptables 更好，但社区中已有相关的文章指出 BPF(Berkeley Packet Filter) 比 ipvs 的性能更好，且 BPF 将要取代 iptables，至于下一步如何发展，让我们拭目以待。 参考： http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.filter_rules.html https://bestsamina.github.io/posts/2018-10-19-ipvs-based-kube-proxy-4-scaled-k8s-lb/ https://www.bookstack.cn/read/k8s-source-code-analysis/core-kube-proxy-ipvs.md https://blog.51cto.com/goome/2369150 https://xigang.github.io/2019/07/21/kubernetes-service/ https://segmentfault.com/a/1190000016333317 https://cilium.io/blog/2018/04/17/why-is-the-kernel-community-replacing-iptables/]]></content>
      <tags>
        <tag>kube-proxy</tag>
        <tag>ipvs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-proxy iptables 模式源码分析]]></title>
    <url>%2F2019%2F11%2F06%2Fkube_proxy_iptables%2F</url>
    <content type="text"><![CDATA[iptables 的功能在前面的文章中已经介绍过 iptable 的一些基本信息，本文会深入介绍 kube-proxy iptables 模式下的工作原理，本文中多处会与 iptables 的知识相关联，若没有 iptables 基础，请先自行补充。 iptables 的功能： 流量转发：DNAT 实现 IP 地址和端口的映射； 负载均衡：statistic 模块为每个后端设置权重； 会话保持：recent 模块设置会话保持时间； iptables 有五张表和五条链，五条链分别对应为： PREROUTING 链：数据包进入路由之前，可以在此处进行 DNAT； INPUT 链：一般处理本地进程的数据包，目的地址为本机； FORWARD 链：一般处理转发到其他机器或者 network namespace 的数据包； OUTPUT 链：原地址为本机，向外发送，一般处理本地进程的输出数据包； POSTROUTING 链：发送到网卡之前，可以在此处进行 SNAT； 五张表分别为： filter 表：用于控制到达某条链上的数据包是继续放行、直接丢弃(drop)还是拒绝(reject)； nat 表：network address translation 网络地址转换，用于修改数据包的源地址和目的地址； mangle 表：用于修改数据包的 IP 头信息； raw 表：iptables 是有状态的，其对数据包有链接追踪机制，连接追踪信息在 /proc/net/nf_conntrack 中可以看到记录，而 raw 是用来去除链接追踪机制的； security 表：最不常用的表，用在 SELinux 上； 这五张表是对 iptables 所有规则的逻辑集群且是有顺序的，当数据包到达某一条链时会按表的顺序进行处理，表的优先级为：raw、mangle、nat、filter、security。 iptables 的工作流程如下图所示： kube-proxy 的 iptables 模式kube-proxy 组件负责维护 node 节点上的防火墙规则和路由规则，在 iptables 模式下，会根据 service 以及 endpoints 对象的改变来实时刷新规则，kube-proxy 使用了 iptables 的 filter 表和 nat 表，并对 iptables 的链进行了扩充，自定义了 KUBE-SERVICES、KUBE-EXTERNAL-SERVICES、KUBE-NODEPORTS、KUBE-POSTROUTING、KUBE-MARK-MASQ、KUBE-MARK-DROP、KUBE-FORWARD 七条链，另外还新增了以“KUBE-SVC-xxx”和“KUBE-SEP-xxx”开头的数个链，除了创建自定义的链以外还将自定义链插入到已有链的后面以便劫持数据包。 在 nat 表中自定义的链以及追加的链如下所示： 在 filter 表定义的链以及追加的链如下所示如下所示： 对于 KUBE-MARK-MASQ 链中所有规则设置了 kubernetes 独有的 MARK 标记，在 KUBE-POSTROUTING 链中对 node 节点上匹配 kubernetes 独有 MARK 标记的数据包，进行 SNAT 处理。 1-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 Kube-proxy 接着为每个服务创建 KUBE-SVC-xxx 链，并在 nat 表中将 KUBE-SERVICES 链中每个目标地址是service 的数据包导入这个 KUBE-SVC-xxx 链，如果 endpoint 尚未创建，则 KUBE-SVC-xxx 链中没有规则，任何 incomming packets 在规则匹配失败后会被 KUBE-MARK-DROP 进行标记然后再 FORWARD 链中丢弃。 这些自定义链与 iptables 的表结合后如下所示，笔者只画出了 PREROUTING 和 OUTPUT 链中追加的链以及部分自定义链，因为 PREROUTING 和 OUTPUT 的首条 NAT 规则都先将所有流量导入KUBE-SERVICE 链中，这样就截获了所有的入流量和出流量，进而可以对 k8s 相关流量进行重定向处理。 kubernetes 自定义链中数据包的详细流转可以参考： iptables 规则分析clusterIP 访问方式创建一个 clusterIP 访问方式的 service 以及带有两个副本，从 pod 中访问 clusterIP 的 iptables 规则流向为： 1PREROUTING --&gt; KUBE-SERVICE --&gt; KUBE-SVC-XXX --&gt; KUBE-SEP-XXX 访问流程如下所示： 1、对于进入 PREROUTING 链的都转到 KUBE-SERVICES 链进行处理； 2、在 KUBE-SERVICES 链，对于访问 clusterIP 为 10.110.243.155 的转发到 KUBE-SVC-5SB6FTEHND4GTL2W； 3、访问 KUBE-SVC-5SB6FTEHND4GTL2W 的使用随机数负载均衡，并转发到 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-OVNLTDWFHTHII4SC 上； 4、KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-OVNLTDWFHTHII4SC 对应 endpoint 中的 pod 192.168.137.147 和 192.168.98.213，设置 mark 标记，进行 DNAT 并转发到具体的 pod 上，如果某个 service 的 endpoints 中没有 pod，那么针对此 service 的请求将会被 drop 掉； 1234567891011121314151617// 1.-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES// 2.-A KUBE-SERVICES -d 10.110.243.155/32 -p tcp -m comment --comment &quot;pks-system/tenant-service: cluster IP&quot; -m tcp --dport 7000 -j KUBE-SVC-5SB6FTEHND4GTL2W// 3.-A KUBE-SVC-5SB6FTEHND4GTL2W -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-CI5ZO3FTK7KBNRMG-A KUBE-SVC-5SB6FTEHND4GTL2W -j KUBE-SEP-OVNLTDWFHTHII4SC// 4.-A KUBE-SEP-CI5ZO3FTK7KBNRMG -s 192.168.137.147/32 -j KUBE-MARK-MASQ-A KUBE-SEP-CI5ZO3FTK7KBNRMG -p tcp -m tcp -j DNAT --to-destination 192.168.137.147:7000-A KUBE-SEP-OVNLTDWFHTHII4SC -s 192.168.98.213/32 -j KUBE-MARK-MASQ-A KUBE-SEP-OVNLTDWFHTHII4SC -p tcp -m tcp -j DNAT --to-destination 192.168.98.213:7000 nodePort 方式在 nodePort 方式下，会用到 KUBE-NODEPORTS 规则链，通过 iptables -t nat -L -n 可以看到 KUBE-NODEPORTS 位于 KUBE-SERVICE 链的最后一个，iptables 在处理报文时会优先处理目的 IP 为clusterIP 的报文，在前面的 KUBE-SVC-XXX 都匹配失败之后再去使用 nodePort 方式进行匹配。 创建一个 nodePort 访问方式的 service 以及带有两个副本，访问 nodeport 的 iptables 规则流向为： 1、非本机访问 1PREROUTING --&gt; KUBE-SERVICE --&gt; KUBE-NODEPORTS --&gt; KUBE-SVC-XXX --&gt; KUBE-SEP-XXX 2、本机访问 1OUTPUT --&gt; KUBE-SERVICE --&gt; KUBE-NODEPORTS --&gt; KUBE-SVC-XXX --&gt; KUBE-SEP-XXX 该服务的 nodePort 端口为 30070，其 iptables 访问规则和使用 clusterIP 方式访问有点类似，不过 nodePort 方式会比 clusterIP 的方式多走一条链 KUBE-NODEPORTS，其会在 KUBE-NODEPORTS 链设置 mark 标记并转发到 KUBE-SVC-5SB6FTEHND4GTL2W，nodeport 与 clusterIP 访问方式最后都是转发到了 KUBE-SVC-xxx 链。 1、经过 PREROUTING 转到 KUBE-SERVICES 2、经过 KUBE-SERVICES 转到 KUBE-NODEPORTS 3、经过 KUBE-NODEPORTS 转到 KUBE-SVC-5SB6FTEHND4GTL2W 4、经过 KUBE-SVC-5SB6FTEHND4GTL2W 转到 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-VR562QDKF524UNPV 5、经过 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-VR562QDKF524UNPV 分别转到 192.168.137.147:7000 和 192.168.89.11:7000 12345678910111213141516171819202122// 1.-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES// 2.......-A KUBE-SERVICES xxx......-A KUBE-SERVICES -m comment --comment &quot;kubernetes service nodeports; NOTE: this must be the last rule in this chain&quot; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS// 3.-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;pks-system/tenant-service:&quot; -m tcp --dport 30070 -j KUBE-MARK-MASQ-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;pks-system/tenant-service:&quot; -m tcp --dport 30070 -j KUBE-SVC-5SB6FTEHND4GTL2W// 4、-A KUBE-SVC-5SB6FTEHND4GTL2W -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-CI5ZO3FTK7KBNRMG-A KUBE-SVC-5SB6FTEHND4GTL2W -j KUBE-SEP-VR562QDKF524UNPV// 5、-A KUBE-SEP-CI5ZO3FTK7KBNRMG -s 192.168.137.147/32 -j KUBE-MARK-MASQ-A KUBE-SEP-CI5ZO3FTK7KBNRMG -p tcp -m tcp -j DNAT --to-destination 192.168.137.147:7000-A KUBE-SEP-VR562QDKF524UNPV -s 192.168.89.11/32 -j KUBE-MARK-MASQ-A KUBE-SEP-VR562QDKF524UNPV -p tcp -m tcp -j DNAT --to-destination 192.168.89.11:7000 其他访问方式对应的 iptables 规则可自行分析。 iptables 模式源码分析 kubernetes 版本：v1.16 上篇文章已经在源码方面做了许多铺垫，下面就直接看 kube-proxy iptables 模式的核心方法。首先回顾一下 iptables 模式的调用流程，kube-proxy 根据给定的 proxyMode 初始化对应的 proxier 后会调用 Proxier.SyncLoop() 执行 proxier 的主循环，而其最终会调用 proxier.syncProxyRules() 刷新 iptables 规则。 1proxier.SyncLoop() --&gt; proxier.syncRunner.Loop()--&gt;bfr.tryRun()--&gt;bfr.fn()--&gt;proxier.syncProxyRules() proxier.syncProxyRules()这个函数比较长，大约 800 行，其中有许多冗余的代码，代码可读性不佳，我们只需理解其基本流程即可，该函数的主要功能为： 更新proxier.endpointsMap，proxier.servieMap 创建自定义链 将当前内核中 filter 表和 nat 表中的全部规则导入到内存中 为每个 service 创建规则 为 clusterIP 设置访问规则 为 externalIP 设置访问规则 为 ingress 设置访问规则 为 nodePort 设置访问规则 为 endpoint 生成规则链 写入 DNAT 规则 删除不再使用的服务自定义链 使用 iptables-restore 同步规则 首先是更新 proxier.endpointsMap，proxier.servieMap 两个对象。 k8s.io/kubernetes/pkg/proxy/iptables/proxier.go:677 123456789101112131415func (proxier *Proxier) syncProxyRules() &#123; ...... serviceUpdateResult := proxy.UpdateServiceMap(proxier.serviceMap, proxier.serviceChanges) endpointUpdateResult := proxier.endpointsMap.Update(proxier.endpointsChanges) staleServices := serviceUpdateResult.UDPStaleClusterIP for _, svcPortName := range endpointUpdateResult.StaleServiceNames &#123; if svcInfo, ok := proxier.serviceMap[svcPortName]; ok &amp;&amp; svcInfo != nil &amp;&amp; svcInfo.Protocol() == v1.ProtocolUDP &#123; staleServices.Insert(svcInfo.ClusterIP().String()) for _, extIP := range svcInfo.ExternalIPStrings() &#123; staleServices.Insert(extIP) &#125; &#125; &#125; ...... 然后创建所需要的 iptable 链： 12345678910111213for _, jump := range iptablesJumpChains &#123; // 创建自定义链 if _, err := proxier.iptables.EnsureChain(jump.table, jump.dstChain); err != nil &#123; ..... &#125; args := append(jump.extraArgs, ...... ) //插入到已有的链 if _, err := proxier.iptables.EnsureRule(utiliptables.Prepend, jump.table, jump.srcChain, args...); err != nil &#123; ...... &#125;&#125; 将当前内核中 filter 表和 nat 表中的全部规则临时导出到 buffer 中： 1234567891011121314151617err := proxier.iptables.SaveInto(utiliptables.TableFilter, proxier.existingFilterChainsData)if err != nil &#123; &#125; else &#123; existingFilterChains = utiliptables.GetChainLines(utiliptables.TableFilter, proxier.existingFilterChainsData.Bytes())&#125; ......err = proxier.iptables.SaveInto(utiliptables.TableNAT, proxier.iptablesData)if err != nil &#123; &#125; else &#123; existingNATChains = utiliptables.GetChainLines(utiliptables.TableNAT, proxier.iptablesData.Bytes())&#125;writeLine(proxier.filterChains, &quot;*filter&quot;)writeLine(proxier.natChains, &quot;*nat&quot;) 检查已经创建出的表是否存在： 1234567891011121314for _, chainName := range []utiliptables.Chain&#123;kubeServicesChain, kubeExternalServicesChain, kubeForwardChain&#125; &#123; if chain, ok := existingFilterChains[chainName]; ok &#123; writeBytesLine(proxier.filterChains, chain) &#125; else &#123; writeLine(proxier.filterChains, utiliptables.MakeChainLine(chainName)) &#125;&#125;for _, chainName := range []utiliptables.Chain&#123;kubeServicesChain, kubeNodePortsChain, kubePostroutingChain, KubeMarkMasqChain&#125; &#123; if chain, ok := existingNATChains[chainName]; ok &#123; writeBytesLine(proxier.natChains, chain) &#125; else &#123; writeLine(proxier.natChains, utiliptables.MakeChainLine(chainName)) &#125;&#125; 写入 SNAT 地址伪装规则，在 POSTROUTING 阶段对地址进行 MASQUERADE 处理，原始请求源 IP 将被丢失，被请求 pod 的应用看到为 NodeIP 或 CNI 设备 IP(bridge/vxlan设备)： 12345678910111213masqRule := []string&#123; ......&#125;if proxier.iptables.HasRandomFully() &#123; masqRule = append(masqRule, &quot;--random-fully&quot;)&#125; else &#123;&#125;writeLine(proxier.natRules, masqRule...)writeLine(proxier.natRules, []string&#123; ......&#125;...) 为每个 service 创建规则，创建 KUBE-SVC-xxx 和 KUBE-XLB-xxx 链、创建 service portal 规则、为 clusterIP 创建规则： 123456789101112131415161718for svcName, svc := range proxier.serviceMap &#123; svcInfo, ok := svc.(*serviceInfo) ...... if hasEndpoints &#123; ...... &#125; svcXlbChain := svcInfo.serviceLBChainName if svcInfo.OnlyNodeLocalEndpoints() &#123; ...... &#125; if hasEndpoints &#123; ...... &#125; else &#123; ...... &#125; 若服务使用了 externalIP，创建对应的规则： 123456789101112131415for _, externalIP := range svcInfo.ExternalIPStrings() &#123; if local, err := utilproxy.IsLocalIP(externalIP); err != nil &#123; ...... if proxier.portsMap[lp] != nil &#123; ...... &#125; else &#123; ...... &#125; &#125; if hasEndpoints &#123; ...... &#125; else &#123; ...... &#125;&#125; 若服务使用了 ingress，创建对应的规则： 1234567891011121314151617181920for _, ingress := range svcInfo.LoadBalancerIPStrings() &#123; if ingress != &quot;&quot; &#123; if hasEndpoints &#123; ...... if !svcInfo.OnlyNodeLocalEndpoints() &#123; ...... &#125; if len(svcInfo.LoadBalancerSourceRanges()) == 0 &#123; ...... &#125; else &#123; ...... &#125; ...... &#125; else &#123; ...... &#125; &#125;&#125; 若使用了 nodePort，创建对应的规则： 123456789101112131415161718192021222324252627if svcInfo.NodePort() != 0 &#123; addresses, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer) lps := make([]utilproxy.LocalPort, 0) for address := range addresses &#123; ...... lps = append(lps, lp) &#125; for _, lp := range lps &#123; if proxier.portsMap[lp] != nil &#123; &#125; else if svcInfo.Protocol() != v1.ProtocolSCTP &#123; socket, err := proxier.portMapper.OpenLocalPort(&amp;lp) ...... if lp.Protocol == &quot;udp&quot; &#123; ...... &#125; replacementPortsMap[lp] = socket &#125; &#125; if hasEndpoints &#123; ...... &#125; else &#123; ...... &#125;&#125; 为 endpoint 生成规则链 KUBE-SEP-XXX： 12345678910111213endpoints = endpoints[:0]endpointChains = endpointChains[:0]var endpointChain utiliptables.Chainfor _, ep := range proxier.endpointsMap[svcName] &#123; epInfo, ok := ep.(*endpointsInfo) ...... if chain, ok := existingNATChains[utiliptables.Chain(endpointChain)]; ok &#123; writeBytesLine(proxier.natChains, chain) &#125; else &#123; writeLine(proxier.natChains, utiliptables.MakeChainLine(endpointChain)) &#125; activeNATChains[endpointChain] = true&#125; 如果创建 service 时指定了 SessionAffinity 为 clientIP 则使用 recent 创建保持会话连接的规则： 12345if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP &#123; for _, endpointChain := range endpointChains &#123; ...... &#125;&#125; 写入负载均衡和 DNAT 规则，对于 endpoints 中的 pod 使用随机访问负载均衡策略。 在 iptables 规则中加入该 service 对应的自定义链“KUBE-SVC-xxx”，如果该服务对应的 endpoints 大于等于2，则添加负载均衡规则； 针对非本地 Node 上的 pod，需进行 DNAT，将请求的目标地址设置成候选的 pod 的 IP 后进行路由，KUBE-MARK-MASQ 将重设(伪装)源地址； 123456789101112131415161718192021for i, endpointChain := range endpointChains &#123; ...... if svcInfo.OnlyNodeLocalEndpoints() &amp;&amp; endpoints[i].IsLocal &#123; ...... &#125; ...... epIP := endpoints[i].IP() if epIP == &quot;&quot; &#123; ...... &#125; ...... args = append(args, &quot;-j&quot;, string(endpointChain)) writeLine(proxier.natRules, args...) ...... if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP &#123; ...... &#125; ...... writeLine(proxier.natRules, args...)&#125; 若启用了 clusterCIDR 则生成对应的规则链： 1234if len(proxier.clusterCIDR) &gt; 0 &#123; ...... writeLine(proxier.natRules, args...)&#125; 为本机的 pod 开启会话保持： 123456789101112131415161718192021 args = append(args[:0], &quot;-A&quot;, string(svcXlbChain)) writeLine(proxier.natRules, ......) numLocalEndpoints := len(localEndpointChains) if numLocalEndpoints == 0 &#123; ...... writeLine(proxier.natRules, args...) &#125; else &#123; if svcInfo.SessionAffinityType() == v1.ServiceAffinityClientIP &#123; for _, endpointChain := range localEndpointChains &#123; ...... &#125; &#125; ...... for i, endpointChain := range localEndpointChains &#123; ...... args = append(args, &quot;-j&quot;, string(endpointChain)) writeLine(proxier.natRules, args...) &#125; &#125;&#125; 删除不存在服务的自定义链，KUBE-SVC-xxx、KUBE-SEP-xxx、KUBE-FW-xxx、KUBE-XLB-xxx： 123456789101112for chain := range existingNATChains &#123; if !activeNATChains[chain] &#123; ...... if !strings.HasPrefix(chainString, &quot;KUBE-SVC-&quot;) &amp;&amp; !strings.HasPrefix(chainString, &quot;KUBE-SEP-&quot;) &amp;&amp; !strings.HasPrefix(chainString, &quot;KUBE-FW-&quot;) &amp;&amp; ! strings.HasPrefix(chainString, &quot;KUBE-XLB-&quot;) &#123; ...... continue &#125; writeBytesLine(proxier.natChains, existingNATChains[chain]) writeLine(proxier.natRules, &quot;-X&quot;, chainString) &#125;&#125; 在 KUBE-SERVICES 链最后添加 nodePort 规则： 123456789101112131415addresses, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer)if err != nil &#123; ......&#125; else &#123; for address := range addresses &#123; if utilproxy.IsZeroCIDR(address) &#123; ...... &#125; if isIPv6 &amp;&amp; !utilnet.IsIPv6String(address) || !isIPv6 &amp;&amp; utilnet.IsIPv6String(address) &#123; ...... &#125; ..... writeLine(proxier.natRules, args...) &#125;&#125; 为 INVALID 状态的包添加规则，为 KUBE-FORWARD 链添加对应的规则： 12345678910111213141516writeLine(proxier.filterRules, ......)writeLine(proxier.filterRules, ......)if len(proxier.clusterCIDR) != 0 &#123; writeLine(proxier.filterRules, ...... ) writeLine(proxier.filterRules, ...... )&#125; 在结尾添加标志： 12writeLine(proxier.filterRules, &quot;COMMIT&quot;)writeLine(proxier.natRules, &quot;COMMIT&quot;) 使用 iptables-restore 同步规则： 12345678910proxier.iptablesData.Reset()proxier.iptablesData.Write(proxier.filterChains.Bytes())proxier.iptablesData.Write(proxier.filterRules.Bytes())proxier.iptablesData.Write(proxier.natChains.Bytes())proxier.iptablesData.Write(proxier.natRules.Bytes())err = proxier.iptables.RestoreAll(proxier.iptablesData.Bytes(), utiliptables.NoFlushTables, utiliptables.RestoreCounters)if err != nil &#123; ......&#125; 以上就是对 kube-proxy iptables 代理模式核心源码的一个走读。 总结本文主要讲了 kube-proxy iptables 模式的实现，可以看到其中的 iptables 规则是相当复杂的，在实际环境中尽量根据已有服务再来梳理整个 iptables 规则链就比较清楚了，笔者对于 iptables 的知识也是现学的，文中如有不当之处望指正。上面分析完了整个 iptables 模式的功能，但是 iptable 存在一些性能问题，比如有规则线性匹配时延、规则更新时延、可扩展性差等，为了解决这些问题于是有了 ipvs 模式，在下篇文章中会继续介绍 ipvs 模式的实现。 参考： https://www.jianshu.com/p/a978af8e5dd8 https://blog.csdn.net/ebay/article/details/52798074 https://blog.csdn.net/horsefoot/article/details/51249161 https://rootdeep.github.io/posts/kube-proxy-code-analysis/ https://www.cnblogs.com/charlieroro/p/9588019.html]]></content>
      <tags>
        <tag>kube-proxy</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-proxy 源码分析]]></title>
    <url>%2F2019%2F11%2F01%2Fkube_proxy_process%2F</url>
    <content type="text"><![CDATA[上篇文章 kubernetes service 原理解析 已经分析了 service 原理以 kube-proxy 中三种模式的原理，本篇文章会从源码角度分析 kube-proxy 的设计与实现。 kubernetes 版本: v1.16 kube-proxy 启动流程前面的文章已经说过 kubernetes 中所有组件都是通过其 run() 方法启动主逻辑的，run() 方法调用之前会进行解析命令行参数、添加默认值等。下面就直接看 kube-proxy 的 run() 方法： 若启动时指定了 --write-config-to 参数，kube-proxy 只将启动的默认参数写到指定的配置文件中，然后退出 初始化 ProxyServer 对象 如果启动参数 --cleanup 设置为 true，则清理 iptables 和 ipvs 规则并退出 k8s.io/kubernetes/cmd/kube-proxy/app/server.go:290 123456789101112131415161718192021func (o *Options) Run() error &#123; defer close(o.errCh) // 1.如果指定了 --write-config-to 参数，则将默认的配置文件写到指定文件并退出 if len(o.WriteConfigTo) &gt; 0 &#123; return o.writeConfigFile() &#125; // 2.初始化 ProxyServer 对象 proxyServer, err := NewProxyServer(o) if err != nil &#123; return err &#125; // 3.如果启动参数 --cleanup 设置为 true，则清理 iptables 和 ipvs 规则并退出 if o.CleanupAndExit &#123; return proxyServer.CleanupAndExit() &#125; o.proxyServer = proxyServer return o.runLoop()&#125; Run() 方法中主要调用了 NewProxyServer() 方法来初始化 ProxyServer，然后会调用 runLoop() 启动主循环，继续看初始化 ProxyServer 的具体实现： 初始化 iptables、ipvs 相关的 interface 若启用了 ipvs 则检查内核版本、ipvs 依赖的内核模块、ipset 版本，内核模块主要包括：ip_vs，ip_vs_rr,ip_vs_wrr,ip_vs_sh,nf_conntrack_ipv4,nf_conntrack，若没有相关模块，kube-proxy 会尝试使用 modprobe 自动加载 根据 proxyMode 初始化 proxier，kube-proxy 启动后只运行一种 proxier k8s.io/kubernetes/cmd/kube-proxy/app/server_others.go:57 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114func NewProxyServer(o *Options) (*ProxyServer, error) &#123; return newProxyServer(o.config, o.CleanupAndExit, o.master)&#125;func newProxyServer( config *proxyconfigapi.KubeProxyConfiguration, cleanupAndExit bool, master string) (*ProxyServer, error) &#123; ...... if c, err := configz.New(proxyconfigapi.GroupName); err == nil &#123; c.Set(config) &#125; else &#123; return nil, fmt.Errorf(&quot;unable to register configz: %s&quot;, err) &#125; ...... // 1.关键依赖工具 iptables/ipvs/ipset/dbus var iptInterface utiliptables.Interface var ipvsInterface utilipvs.Interface var kernelHandler ipvs.KernelHandler var ipsetInterface utilipset.Interface var dbus utildbus.Interface // 2.执行 linux 命令行的工具 execer := exec.New() // 3.初始化 iptables/ipvs/ipset/dbus 对象 dbus = utildbus.New() iptInterface = utiliptables.New(execer, dbus, protocol) kernelHandler = ipvs.NewLinuxKernelHandler() ipsetInterface = utilipset.New(execer) // 4.检查该机器是否支持使用 ipvs 模式 canUseIPVS, _ := ipvs.CanUseIPVSProxier(kernelHandler, ipsetInterface) if canUseIPVS &#123; ipvsInterface = utilipvs.New(execer) &#125; if cleanupAndExit &#123; return &amp;ProxyServer&#123; ...... &#125;, nil &#125; // 5.初始化 kube client 和 event client client, eventClient, err := createClients(config.ClientConnection, master) if err != nil &#123; return nil, err &#125; ...... // 6.初始化 healthzServer var healthzServer *healthcheck.HealthzServer var healthzUpdater healthcheck.HealthzUpdater if len(config.HealthzBindAddress) &gt; 0 &#123; healthzServer = healthcheck.NewDefaultHealthzServer(config.HealthzBindAddress, 2*config.IPTables.SyncPeriod.Duration, recorder, nodeRef) healthzUpdater = healthzServer &#125; // 7.proxier 是一个 interface，每种模式都是一个 proxier var proxier proxy.Provider // 8.根据 proxyMode 初始化 proxier proxyMode := getProxyMode(string(config.Mode), kernelHandler, ipsetInterface, iptables.LinuxKernelCompatTester&#123;&#125;) ...... if proxyMode == proxyModeIPTables &#123; klog.V(0).Info(&quot;Using iptables Proxier.&quot;) if config.IPTables.MasqueradeBit == nil &#123; return nil, fmt.Errorf(&quot;unable to read IPTables MasqueradeBit from config&quot;) &#125; // 9.初始化 iptables 模式的 proxier proxier, err = iptables.NewProxier( ....... ) if err != nil &#123; return nil, fmt.Errorf(&quot;unable to create proxier: %v&quot;, err) &#125; metrics.RegisterMetrics() &#125; else if proxyMode == proxyModeIPVS &#123; // 10.判断是够启用了 ipv6 双栈 if utilfeature.DefaultFeatureGate.Enabled(features.IPv6DualStack) &#123; ...... // 11.初始化 ipvs 模式的 proxier proxier, err = ipvs.NewDualStackProxier( ...... ) &#125; else &#123; proxier, err = ipvs.NewProxier( ...... ) &#125; if err != nil &#123; return nil, fmt.Errorf(&quot;unable to create proxier: %v&quot;, err) &#125; metrics.RegisterMetrics() &#125; else &#123; // 12.初始化 userspace 模式的 proxier proxier, err = userspace.NewProxier( ...... ) if err != nil &#123; return nil, fmt.Errorf(&quot;unable to create proxier: %v&quot;, err) &#125; &#125; iptInterface.AddReloadFunc(proxier.Sync) return &amp;ProxyServer&#123; ...... &#125;, nil&#125; runLoop() 方法主要是启动 proxyServer。 k8s.io/kubernetes/cmd/kube-proxy/app/server.go:311 12345678910111213141516171819func (o *Options) runLoop() error &#123; // 1.watch 配置文件变化 if o.watcher != nil &#123; o.watcher.Run() &#125; // 2.以 goroutine 方式启动 proxyServer go func() &#123; err := o.proxyServer.Run() o.errCh &lt;- err &#125;() for &#123; err := &lt;-o.errCh if err != nil &#123; return err &#125; &#125;&#125; o.proxyServer.Run() 中会启动已经初始化好的所有服务： 设定进程 OOMScore，可通过命令行配置，默认值为 --oom-score-adj=&quot;-999&quot; 启动 metric server 和 healthz server，两者分别监听 10256 和 10249 端口 设置内核参数 nf_conntrack_tcp_timeout_established 和 nf_conntrack_tcp_timeout_close_wait 将 proxier 注册到 serviceEventHandler、endpointsEventHandler 中 启动 informer 监听 service 和 endpoints 变化 执行 s.Proxier.SyncLoop()，启动 proxier 主循环 k8s.io/kubernetes/cmd/kube-proxy/app/server.go:527 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687func (s *ProxyServer) Run() error &#123; ...... // 1.进程 OOMScore，避免进程因 oom 被杀掉，此处默认值为 -999 var oomAdjuster *oom.OOMAdjuster if s.OOMScoreAdj != nil &#123; oomAdjuster = oom.NewOOMAdjuster() if err := oomAdjuster.ApplyOOMScoreAdj(0, int(*s.OOMScoreAdj)); err != nil &#123; klog.V(2).Info(err) &#125; &#125; ...... // 2.启动 healthz server if s.HealthzServer != nil &#123; s.HealthzServer.Run() &#125; // 3.启动 metrics server if len(s.MetricsBindAddress) &gt; 0 &#123; ...... go wait.Until(func() &#123; err := http.ListenAndServe(s.MetricsBindAddress, proxyMux) if err != nil &#123; utilruntime.HandleError(fmt.Errorf(&quot;starting metrics server failed: %v&quot;, err)) &#125; &#125;, 5*time.Second, wait.NeverStop) &#125; // 4.配置 conntrack，设置内核参数 nf_conntrack_tcp_timeout_established 和 nf_conntrack_tcp_timeout_close_wait if s.Conntracker != nil &#123; max, err := getConntrackMax(s.ConntrackConfiguration) if err != nil &#123; return err &#125; if max &gt; 0 &#123; err := s.Conntracker.SetMax(max) ...... &#125; if s.ConntrackConfiguration.TCPEstablishedTimeout != nil &amp;&amp; s.ConntrackConfiguration.TCPEstablishedTimeout.Duration &gt; 0 &#123; timeout := int(s.ConntrackConfiguration.TCPEstablishedTimeout.Duration / time.Second) if err := s.Conntracker.SetTCPEstablishedTimeout(timeout); err != nil &#123; return err &#125; &#125; if s.ConntrackConfiguration.TCPCloseWaitTimeout != nil &amp;&amp; s.ConntrackConfiguration.TCPCloseWaitTimeout.Duration &gt; 0 &#123; timeout := int(s.ConntrackConfiguration.TCPCloseWaitTimeout.Duration / time.Second) if err := s.Conntracker.SetTCPCloseWaitTimeout(timeout); err != nil &#123; return err &#125; &#125; &#125; ...... // 5.启动 informer 监听 Services 和 Endpoints 或者 EndpointSlices 信息 informerFactory := informers.NewSharedInformerFactoryWithOptions(s.Client, s.ConfigSyncPeriod, informers.WithTweakListOptions(func(options *metav1.ListOptions) &#123; options.LabelSelector = labelSelector.String() &#125;)) // 6.将 proxier 注册到 serviceConfig、endpointsConfig 中 serviceConfig := config.NewServiceConfig(informerFactory.Core().V1().Services(), s.ConfigSyncPeriod) serviceConfig.RegisterEventHandler(s.Proxier) go serviceConfig.Run(wait.NeverStop) if utilfeature.DefaultFeatureGate.Enabled(features.EndpointSlice) &#123; endpointSliceConfig := config.NewEndpointSliceConfig(informerFactory.Discovery().V1alpha1().EndpointSlices(), s.ConfigSyncPeriod) endpointSliceConfig.RegisterEventHandler(s.Proxier) go endpointSliceConfig.Run(wait.NeverStop) &#125; else &#123; endpointsConfig := config.NewEndpointsConfig(informerFactory.Core().V1().Endpoints(), s.ConfigSyncPeriod) endpointsConfig.RegisterEventHandler(s.Proxier) go endpointsConfig.Run(wait.NeverStop) &#125; // 7.启动 informer informerFactory.Start(wait.NeverStop) s.birthCry() // 8.启动 proxier 主循环 s.Proxier.SyncLoop() return nil&#125; 回顾一下整个启动逻辑： 1o.Run() --&gt; o.runLoop() --&gt; o.proxyServer.Run() --&gt; s.Proxier.SyncLoop() o.Run() 中调用了 NewProxyServer() 来初始化 proxyServer 对象，其中包括初始化每种模式对应的 proxier，该方法最终会调用 s.Proxier.SyncLoop() 执行 proxier 的主循环。 proxier 的初始化看完了启动流程的逻辑代码，接着再看一下各代理模式的初始化，上文已经提到每种模式都是一个 proxier，即要实现 proxy.Provider 对应的 interface，如下所示： 12345678type Provider interface &#123; config.EndpointsHandler config.EndpointSliceHandler config.ServiceHandler Sync() SyncLoop()&#125; 首先要实现 service、endpoints 和 endpointSlice 对应的 handler，也就是对 OnAdd、OnUpdate、OnDelete 、OnSynced 四种方法的处理，详细的代码在下文进行讲解。EndpointSlice 是在 v1.16 中新加入的一个 API。Sync() 和 SyncLoop() 是主要用来处理iptables 规则的方法。 iptables proxier 初始化首先看 iptables 模式的 NewProxier()方法，其函数的具体执行逻辑为： 设置相关的内核参数route_localnet、bridge-nf-call-iptables 生成 masquerade 标记 设置默认调度算法 rr 初始化 proxier 对象 使用 BoundedFrequencyRunner 初始化 proxier.syncRunner，将 proxier.syncProxyRules 方法注入，BoundedFrequencyRunner 是一个管理器用于执行用户注入的函数，可以指定运行的时间策略。 k8s.io/kubernetes/pkg/proxy/iptables/proxier.go:249 123456789101112131415161718192021222324252627282930313233343536func NewProxier(ipt utiliptables.Interface, ......) (*Proxier, error) &#123; // 1.设置相关的内核参数 if val, _ := sysctl.GetSysctl(sysctlRouteLocalnet); val != 1 &#123; ...... &#125; if val, err := sysctl.GetSysctl(sysctlBridgeCallIPTables); err == nil &amp;&amp; val != 1 &#123; ...... &#125; // 2.设置 masqueradeMark，默认为 0x00004000/0x00004000 // 用来标记 k8s 管理的报文，masqueradeBit 默认为 14 // 标记 0x4000 的报文（即 POD 发出的报文)，在离开 Node 的时候需要进行 SNAT 转换 masqueradeValue := 1 &lt;&lt; uint(masqueradeBit) masqueradeMark := fmt.Sprintf(&quot;%#08x/%#08x&quot;, masqueradeValue, masqueradeValue) ...... endpointSlicesEnabled := utilfeature.DefaultFeatureGate.Enabled(features.EndpointSlice) healthChecker := healthcheck.NewServer(hostname, recorder, nil, nil) // 3.初始化 proxier isIPv6 := ipt.IsIpv6() proxier := &amp;Proxier&#123; ...... &#125; burstSyncs := 2 // 4.初始化 syncRunner，BoundedFrequencyRunner 是一个定时执行器，会定时执行 // proxier.syncProxyRules 方法,syncProxyRules 是每个 proxier 实际刷新iptables 规则的方法 proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) return proxier, nil&#125; ipvs proxier 初始化ipvs NewProxier() 方法主要逻辑为： 设定内核参数，route_localnet、br_netfilter、bridge-nf-call-iptables、conntrack、conn_reuse_mode、ip_forward、arp_ignore、arp_announce 等 和 iptables 一样，对于 SNAT iptables 规则生成 masquerade 标记 设置默认调度算法 rr 初始化 proxier 对象 初始化 ipset 规则 初始化 syncRunner 将 proxier.syncProxyRules 方法注入 启动 gracefuldeleteManager 定时清理 RS (realServer) 记录 k8s.io/kubernetes/pkg/proxy/ipvs/proxier.go:316 1234567891011121314151617181920212223242526272829303132333435363738394041func NewProxier(ipt utiliptables.Interface, ......) (*Proxier, error) &#123; // 1.设定内核参数 if val, _ := sysctl.GetSysctl(sysctlRouteLocalnet); val != 1 &#123; ...... &#125; ...... // 2.生成 masquerade 标记 masqueradeValue := 1 &lt;&lt; uint(masqueradeBit) masqueradeMark := fmt.Sprintf(&quot;%#08x/%#08x&quot;, masqueradeValue, masqueradeValue) // 3.设置默认调度算法 rr if len(scheduler) == 0 &#123; scheduler = DefaultScheduler &#125; healthChecker := healthcheck.NewServer(hostname, recorder, nil, nil) // use default implementations of deps endpointSlicesEnabled := utilfeature.DefaultFeatureGate.Enabled(features.EndpointSlice) // 4.初始化 proxier proxier := &amp;Proxier&#123; ...... &#125; // 5.初始化 ipset 规则 proxier.ipsetList = make(map[string]*IPSet) for _, is := range ipsetInfo &#123; proxier.ipsetList[is.name] = NewIPSet(ipset, is.name, is.setType, isIPv6, is.comment) &#125; burstSyncs := 2 // 6.初始化 syncRunner proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) // 7.启动 gracefuldeleteManager proxier.gracefuldeleteManager.Run() return proxier, nil&#125; userspace proxier 初始化userspace NewProxier() 方法主要逻辑为： 初始化 iptables 规则 初始化 proxier 初始化 syncRunner 将 proxier.syncProxyRules 方法注入 k8s.io/kubernetes/pkg/proxy/userspace/proxier.go:187 1234567891011121314151617181920212223242526272829303132333435363738394041func NewProxier(......) (*Proxier, error) &#123; return NewCustomProxier(loadBalancer, listenIP, iptables, exec, pr, syncPeriod, minSyncPeriod, udpIdleTimeout, nodePortAddresses, newProxySocket)&#125;func NewCustomProxier(......) (*Proxier, error) &#123; ...... // 1.设置打开文件数 err = setRLimit(64 * 1000) if err != nil &#123; return nil, fmt.Errorf(&quot;failed to set open file handler limit: %v&quot;, err) &#125; proxyPorts := newPortAllocator(pr) return createProxier(loadBalancer, listenIP, iptables, exec, hostIP, proxyPorts, syncPeriod, minSyncPeriod, udpIdleTimeout, makeProxySocket)&#125;func createProxier(loadBalancer LoadBalancer, listenIP net.IP, iptables iptables.Interface, exec utilexec.Interface, hostIP net.IP, proxyPorts PortAllocator, syncPeriod, minSyncPeriod, udpIdleTimeout time.Duration, makeProxySocket ProxySocketFunc) (*Proxier, error) &#123; if proxyPorts == nil &#123; proxyPorts = newPortAllocator(utilnet.PortRange&#123;&#125;) &#125; // 2.初始化 iptables 规则 if err := iptablesInit(iptables); err != nil &#123; return nil, fmt.Errorf(&quot;failed to initialize iptables: %v&quot;, err) &#125; if err := iptablesFlush(iptables); err != nil &#123; return nil, fmt.Errorf(&quot;failed to flush iptables: %v&quot;, err) &#125; // 3.初始化 proxier proxier := &amp;Proxier&#123; ...... &#125; // 4.初始化 syncRunner proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;userspace-proxy-sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, numBurstSyncs) return proxier, nil&#125; proxier 接口实现handler 的实现上文已经提到过每种 proxier 都需要实现 interface 中的几个方法，首先看一下 ServiceHandler、EndpointsHandler 和 EndpointSliceHandler 相关的，对于 service、endpoints 和 endpointSlices 三种对象都实现了 OnAdd、OnUpdate、OnDelete 和 OnSynced 方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 1.service 相关的方法func (proxier *Proxier) OnServiceAdd(service *v1.Service) &#123; proxier.OnServiceUpdate(nil, service)&#125;func (proxier *Proxier) OnServiceUpdate(oldService, service *v1.Service) &#123; if proxier.serviceChanges.Update(oldService, service) &amp;&amp; proxier.isInitialized() &#123; proxier.syncRunner.Run() &#125;&#125;func (proxier *Proxier) OnServiceDelete(service *v1.Service) &#123; proxier.OnServiceUpdate(service, nil)&#125;func (proxier *Proxier) OnServiceSynced()&#123; ...... proxier.syncProxyRules()&#125;// 2.endpoints 相关的方法func (proxier *Proxier) OnEndpointsAdd(endpoints *v1.Endpoints) &#123; proxier.OnEndpointsUpdate(nil, endpoints)&#125;func (proxier *Proxier) OnEndpointsUpdate(oldEndpoints, endpoints *v1.Endpoints) &#123; if proxier.endpointsChanges.Update(oldEndpoints, endpoints) &amp;&amp; proxier.isInitialized() &#123; proxier.Sync() &#125;&#125;func (proxier *Proxier) OnEndpointsDelete(endpoints *v1.Endpoints) &#123; proxier.OnEndpointsUpdate(endpoints, nil)&#125;func (proxier *Proxier) OnEndpointsSynced() &#123; ...... proxier.syncProxyRules()&#125;// 3.endpointSlice 相关的方法func (proxier *Proxier) OnEndpointSliceAdd(endpointSlice *discovery.EndpointSlice) &#123; if proxier.endpointsChanges.EndpointSliceUpdate(endpointSlice, false) &amp;&amp; proxier.isInitialized() &#123; proxier.Sync() &#125;&#125;func (proxier *Proxier) OnEndpointSliceUpdate(_, endpointSlice *discovery.EndpointSlice) &#123; if proxier.endpointsChanges.EndpointSliceUpdate(endpointSlice, false) &amp;&amp; proxier.isInitialized() &#123; proxier.Sync() &#125;&#125;func (proxier *Proxier) OnEndpointSliceDelete(endpointSlice *discovery.EndpointSlice) &#123; if proxier.endpointsChanges.EndpointSliceUpdate(endpointSlice, true) &amp;&amp; proxier.isInitialized() &#123; proxier.Sync() &#125;&#125;func (proxier *Proxier) OnEndpointSlicesSynced() &#123; ...... proxier.syncProxyRules()&#125; 在启动逻辑的 Run() 方法中 proxier 已经被注册到了 serviceConfig、endpointsConfig、endpointSliceConfig 中，当启动 informer，cache 同步完成后会调用 OnSynced() 方法，之后当 watch 到变化后会调用 proxier 中对应的 OnUpdate() 方法进行处理，OnSynced() 会直接调用 proxier.syncProxyRules() 来刷新iptables 规则，而 OnUpdate() 会调用 proxier.syncRunner.Run() 方法，其最终也是调用 proxier.syncProxyRules() 方法刷新规则的，这种转换是在 BoundedFrequencyRunner 中体现出来的，下面看一下具体实现。 Sync() 以及 SyncLoop() 的实现每种 proxier 的 Sync() 以及 SyncLoop() 方法如下所示，都是调用 syncRunner 中的相关方法，而 syncRunner 在前面的 NewProxier() 中已经说过了，syncRunner 是调用 async.NewBoundedFrequencyRunner() 方法初始化，至此，基本上可以确定了所有的核心都是在 BoundedFrequencyRunner 中实现的。 123456789101112131415161718func NewProxier() (*Proxier, error) &#123; ...... proxier.syncRunner = async.NewBoundedFrequencyRunner(&quot;sync-runner&quot;, proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) ......&#125;// Sync()func (proxier *Proxier) Sync() &#123; proxier.syncRunner.Run()&#125;// SyncLoop()func (proxier *Proxier) SyncLoop() &#123; if proxier.healthzServer != nil &#123; proxier.healthzServer.UpdateTimestamp() &#125; proxier.syncRunner.Loop(wait.NeverStop)&#125; NewBoundedFrequencyRunner()是其初始化的函数，其中的参数 minInterval和 maxInterval 分别对应 proxier 中的 minSyncPeriod 和 syncPeriod，两者的默认值分别为 0s 和 30s，其值可以使用 --iptables-min-sync-period 和 --iptables-sync-period 启动参数来指定。 k8s.io/kubernetes/pkg/util/async/bounded_frequency_runner.go:134 12345678910111213141516171819202122232425262728293031323334func NewBoundedFrequencyRunner(name string, fn func(), minInterval, maxInterval time.Duration, burstRuns int) *BoundedFrequencyRunner &#123; timer := realTimer&#123;Timer: time.NewTimer(0)&#125; // 执行定时器 &lt;-timer.C() // 调用 construct() 函数 return construct(name, fn, minInterval, maxInterval, burstRuns, timer)&#125;func construct(name string, fn func(), minInterval, maxInterval time.Duration, burstRuns int, timer timer) *BoundedFrequencyRunner &#123; if maxInterval &lt; minInterval &#123; panic(fmt.Sprintf(&quot;%s: maxInterval (%v) must be &gt;= minInterval (%v)&quot;, name, maxInterval, minInterval)) &#125; if timer == nil &#123; panic(fmt.Sprintf(&quot;%s: timer must be non-nil&quot;, name)) &#125; bfr := &amp;BoundedFrequencyRunner&#123; name: name, fn: fn, // 被调用的函数，proxier.syncProxyRules minInterval: minInterval, maxInterval: maxInterval, run: make(chan struct&#123;&#125;, 1), timer: timer, &#125; // 由于默认的 minInterval = 0，此处使用 nullLimiter if minInterval == 0 &#123; bfr.limiter = nullLimiter&#123;&#125; &#125; else &#123; // 采用“令牌桶”算法实现流控机制 qps := float32(time.Second) / float32(minInterval) bfr.limiter = flowcontrol.NewTokenBucketRateLimiterWithClock(qps, burstRuns, timer) &#125; return bfr&#125; 在启动流程 Run() 方法最后调用的 s.Proxier.SyncLoop() 最终调用的是 BoundedFrequencyRunner 的 Loop()方法，如下所示： k8s.io/kubernetes/pkg/util/async/bounded_frequency_runner.go:169 1234567891011121314func (bfr *BoundedFrequencyRunner) Loop(stop &lt;-chan struct&#123;&#125;) &#123; bfr.timer.Reset(bfr.maxInterval) for &#123; select &#123; case &lt;-stop: bfr.stop() return case &lt;-bfr.timer.C(): // 定时器 bfr.tryRun() case &lt;-bfr.run: // 接收 channel bfr.tryRun() &#125; &#125;&#125; proxier 的 OnUpdate() 中调用的 syncRunner.Run() 其实只是在 bfr.run 这个带 buffer 的 channel 中发送了一条数据，在 BoundedFrequencyRunner 的 Loop()方法中接收到该数据后会调用 bfr.tryRun() 进行处理： k8s.io/kubernetes/pkg/util/async/bounded_frequency_runner.go:191 123456func (bfr *BoundedFrequencyRunner) Run() &#123; select &#123; case bfr.run &lt;- struct&#123;&#125;&#123;&#125;: // 向 channel 发送信号 default: &#125;&#125; 而 tryRun() 方法才是最终调用 syncProxyRules() 刷新iptables 规则的。 k8s.io/kubernetes/pkg/util/async/bounded_frequency_runner.go:211 12345678910111213141516171819202122func (bfr *BoundedFrequencyRunner) tryRun() &#123; bfr.mu.Lock() defer bfr.mu.Unlock() if bfr.limiter.TryAccept() &#123; // 执行 fn() 即 syncProxyRules() 刷新iptables 规则 bfr.fn() bfr.lastRun = bfr.timer.Now() bfr.timer.Stop() bfr.timer.Reset(bfr.maxInterval) return &#125; elapsed := bfr.timer.Since(bfr.lastRun) // how long since last run nextPossible := bfr.minInterval - elapsed // time to next possible run nextScheduled := bfr.maxInterval - elapsed // time to next periodic run if nextPossible &lt; nextScheduled &#123; bfr.timer.Stop() bfr.timer.Reset(nextPossible) &#125;&#125; 通过以上分析可知，syncProxyRules() 是每个 proxier 的核心方法，启动 informer cache 同步完成后会直接调用 proxier.syncProxyRules() 刷新iptables 规则，之后如果 informer watch 到相关对象的变化后会调用 BoundedFrequencyRunner 的 tryRun()来刷新iptables 规则，定时器每 30s 会执行一次iptables 规则的刷新。 总结本文主要介绍了 kube-proxy 的启动逻辑以及三种模式 proxier 的初始化，还有最终调用刷新iptables 规则的 BoundedFrequencyRunner，可以看到其中的代码写的很巧妙。而每种模式下的iptables 规则是如何创建、刷新以及转发的是如何实现的会在后面的文章中进行分析。]]></content>
      <tags>
        <tag>kube-proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes service 原理解析]]></title>
    <url>%2F2019%2F10%2F31%2Fk8s_service_theory%2F</url>
    <content type="text"><![CDATA[为什么需要 service在 kubernetes 中，当创建带有多个副本的 deployment 时，kubernetes 会创建出多个 pod，此时即一个服务后端有多个容器，那么在 kubernetes 中负载均衡怎么做，容器漂移后 ip 也会发生变化，如何做服务发现以及会话保持？这就是 service 的作用，service 是一组具有相同 label pod 集合的抽象，集群内外的各个服务可以通过 service 进行互相通信，当创建一个 service 对象时也会对应创建一个 endpoint 对象，endpoint 是用来做容器发现的，service 只是将多个 pod 进行关联，实际的路由转发都是由 kubernetes 中的 kube-proxy 组件来实现，因此，service 必须结合 kube-proxy 使用，kube-proxy 组件可以运行在 kubernetes 集群中的每一个节点上也可以只运行在单独的几个节点上，其会根据 service 和 endpoints 的变动来改变节点上 iptables 或者 ipvs 中保存的路由规则。 service 的工作原理 endpoints controller 是负责生成和维护所有 endpoints 对象的控制器，监听 service 和对应 pod 的变化，更新对应 service 的 endpoints 对象。当用户创建 service 后 endpoints controller 会监听 pod 的状态，当 pod 处于 running 且准备就绪时，endpoints controller 会将 pod ip 记录到 endpoints 对象中，因此，service 的容器发现是通过 endpoints 来实现的。而 kube-proxy 会监听 service 和 endpoints 的更新并调用其代理模块在主机上刷新路由转发规则。 service 的负载均衡上文已经提到 service 实际的路由转发都是由 kube-proxy 组件来实现的，service 仅以一种 VIP（ClusterIP） 的形式存在，kube-proxy 主要实现了集群内部从 pod 到 service 和集群外部从 nodePort 到 service 的访问，kube-proxy 的路由转发规则是通过其后端的代理模块实现的，kube-proxy 的代理模块目前有四种实现方案，userspace、iptables、ipvs、kernelspace，其发展历程如下所示： kubernetes v1.0：services 仅是一个“4层”代理，代理模块只有 userspace kubernetes v1.1：Ingress API 出现，其代理“7层”服务，并且增加了 iptables 代理模块 kubernetes v1.2：iptables 成为默认代理模式 kubernetes v1.8：引入 ipvs 代理模块 kubernetes v1.9：ipvs 代理模块成为 beta 版本 kubernetes v1.11：ipvs 代理模式 GA 在每种模式下都有自己的负载均衡策略，下文会详解介绍。 userspace 模式在 userspace 模式下，访问服务的请求到达节点后首先进入内核 iptables，然后回到用户空间，由 kube-proxy 转发到后端的 pod，这样流量从用户空间进出内核带来的性能损耗是不可接受的，所以也就有了 iptables 模式。 为什么 userspace 模式要建立 iptables 规则，因为 kube-proxy 监听的端口在用户空间，这个端口不是服务的访问端口也不是服务的 nodePort，因此需要一层 iptables 把访问服务的连接重定向给 kube-proxy 服务。 iptables 模式iptables 模式是目前默认的代理方式，基于 netfilter 实现。当客户端请求 service 的 ClusterIP 时，根据 iptables 规则路由到各 pod 上，iptables 使用 DNAT 来完成转发，其采用了随机数实现负载均衡。 iptables 模式与 userspace 模式最大的区别在于，iptables 模块使用 DNAT 模块实现了 service 入口地址到 pod 实际地址的转换，免去了一次内核态到用户态的切换，另一个与 userspace 代理模式不同的是，如果 iptables 代理最初选择的那个 pod 没有响应，它不会自动重试其他 pod。 iptables 模式最主要的问题是在 service 数量大的时候会产生太多的 iptables 规则，使用非增量式更新会引入一定的时延，大规模情况下有明显的性能问题。 ipvs 模式当集群规模比较大时，iptables 规则刷新会非常慢，难以支持大规模集群，因其底层路由表的实现是链表，对路由规则的增删改查都要涉及遍历一次链表，ipvs 的问世正是解决此问题的，ipvs 是 LVS 的负载均衡模块，与 iptables 比较像的是，ipvs 的实现虽然也基于 netfilter 的钩子函数，但是它却使用哈希表作为底层的数据结构并且工作在内核态，也就是说 ipvs 在重定向流量和同步代理规则有着更好的性能，几乎允许无限的规模扩张。 ipvs 支持三种负载均衡模式：DR模式（Direct Routing）、NAT 模式（Network Address Translation）、Tunneling（也称 ipip 模式）。三种模式中只有 NAT 支持端口映射，所以 ipvs 使用 NAT 模式。linux 内核原生的 ipvs 只支持 DNAT，当在数据包过滤，SNAT 和支持 NodePort 类型的服务这几个场景中ipvs 还是会使用 iptables。 此外，ipvs 也支持更多的负载均衡算法，例如： rr：round-robin/轮询 lc：least connection/最少连接 dh：destination hashing/目标哈希 sh：source hashing/源哈希 sed：shortest expected delay/预计延迟时间最短 nq：never queue/从不排队 userspace、iptables、ipvs 三种模式中默认的负载均衡策略都是通过 round-robin 算法来选择后端 pod 的，在 service 中可以通过设置 service.spec.sessionAffinity 的值实现基于客户端 ip 的会话亲和性，service.spec.sessionAffinity 的值默认为”None”，可以设置为 “ClientIP”，此外也可以使用 service.spec.sessionAffinityConfig.clientIP.timeoutSeconds 设置会话保持时间。kernelspace 主要是在 windows 下使用的，本文暂且不谈。 service 的类型service 支持的类型也就是 kubernetes 中服务暴露的方式，默认有四种 ClusterIP、NodePort、LoadBalancer、ExternelName，此外还有 Ingress，下面会详细介绍每种类型 service 的具体使用场景。 ClusterIPClusterIP 类型的 service 是 kubernetes 集群默认的服务暴露方式，它只能用于集群内部通信，可以被各 pod 访问，其访问方式为： 1pod ---&gt; ClusterIP:ServicePort --&gt; (iptables)DNAT --&gt; PodIP:containePort ClusterIP Service 类型的结构如下图所示: NodePort如果你想要在集群外访问集群内部的服务，可以使用这种类型的 service，NodePort 类型的 service 会在集群内部署了 kube-proxy 的节点打开一个指定的端口，之后所有的流量直接发送到这个端口，然后会被转发到 service 后端真实的服务进行访问。Nodeport 构建在 ClusterIP 上，其访问链路如下所示： 1client ---&gt; NodeIP:NodePort ---&gt; ClusterIP:ServicePort ---&gt; (iptables)DNAT ---&gt; PodIP:containePort 其对应具体的 iptables 规则会在后文进行讲解。 NodePort service 类型的结构如下图所示: LoadBalancerLoadBalancer 类型的 service 通常和云厂商的 LB 结合一起使用，用于将集群内部的服务暴露到外网，云厂商的 LoadBalancer 会给用户分配一个 IP，之后通过该 IP 的流量会转发到你的 service 上。 LoadBalancer service 类型的结构如下图所示: ExternelName通过 CNAME 将 service 与 externalName 的值(比如：foo.bar.example.com)映射起来，这种方式用的比较少。 IngressIngress 其实不是 service 的一个类型，但是它可以作用于多个 service，被称为 service 的 service，作为集群内部服务的入口，Ingress 作用在七层，可以根据不同的 url，将请求转发到不同的 service 上。 Ingress 的结构如下图所示: service 的服务发现虽然 service 的 endpoints 解决了容器发现问题，但不提前知道 service 的 Cluster IP，怎么发现 service 服务呢？service 当前支持两种类型的服务发现机制，一种是通过环境变量，另一种是通过 DNS。在这两种方案中，建议使用后者。 环境变量当一个 pod 创建完成之后，kubelet 会在该 pod 中注册该集群已经创建的所有 service 相关的环境变量，但是需要注意的是，在 service 创建之前的所有 pod 是不会注册该环境变量的，所以在平时使用时，建议通过 DNS 的方式进行 service 之间的服务发现。 DNS可以在集群中部署 CoreDNS 服务(旧版本的 kubernetes 群使用的是 kubeDNS)， 来达到集群内部的 pod 通过DNS 的方式进行集群内部各个服务之间的通讯。 当前 kubernetes 集群默认使用 CoreDNS 作为默认的 DNS 服务，主要原因是 CoreDNS 是基于 Plugin 的方式进行扩展的，简单，灵活，并且不完全被Kubernetes所捆绑。 service 的使用ClusterIP 方式1234567891011121314apiVersion: v1kind: Servicemetadata: name: my-nginxspec: clusterIP: 10.105.146.177 ports: - port: 80 protocol: TCP targetPort: 8080 selector: app: my-nginx sessionAffinity: None type: ClusterIP NodePort 方式1234567891011121314apiVersion: v1kind: Servicemetadata: name: my-nginxspec: ports: - nodePort: 30090 port: 80 protocol: TCP targetPort: 8080 selector: app: my-nginx sessionAffinity: None type: NodePort 其中 nodeport 字段表示通过 nodeport 方式访问的端口，port 表示通过 service 方式访问的端口，targetPort 表示 container port。 Headless service(就是没有 Cluster IP 的 service )当不需要负载均衡以及单独的 ClusterIP 时，可以通过指定 spec.clusterIP 的值为 None 来创建 Headless service，它会给一个集群内部的每个成员提供一个唯一的 DNS 域名来作为每个成员的网络标识，集群内部成员之间使用域名通信。 12345678910111213apiVersion: v1kind: Servicemetadata: name: my-nginxspec: clusterIP: None ports: - nodePort: 30090 port: 80 protocol: TCP targetPort: 8080 selector: app: my-nginx 总结本文主要讲了 kubernetes 中 service 的原理、实现以及使用方式，service 目前主要有 5 种服务暴露方式，service 的容器发现是通过 endpoints 来实现的，其服务发现主要是通过 DNS 实现的，其负载均衡以及流量转发是通过 kube-proxy 实现的。在后面的文章我会继续介绍 kube-proxy 的设计及实现。 参考： https://www.cnblogs.com/xzkzzz/p/9559362.html https://xigang.github.io/2019/07/21/kubernetes-service/]]></content>
      <tags>
        <tag>kubernetes service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-scheduler 优先级与抢占机制源码分析]]></title>
    <url>%2F2019%2F10%2F24%2Fkube_scheduler_preempt%2F</url>
    <content type="text"><![CDATA[前面已经分析了 kube-scheduler 的代码逻辑以及 predicates 与 priorities 算法，本节会继续讲 scheduler 中的一个重要机制，pod 优先级与抢占机制(Pod Priority and Preemption)，该功能是在 v1.8 中引入的，v1.11 中该功能为 beta 版本且默认启用了，v1.14 为 stable 版本。 kube-scheduler 源码分析 kube-scheduler predicates 与 priorities 调度算法源码分析 为什么要有优先级与抢占机制正常情况下，当一个 pod 调度失败后，就会被暂时 “搁置” 处于 pending 状态，直到 pod 被更新或者集群状态发生变化，调度器才会对这个 pod 进行重新调度。但在实际的业务场景中会存在在线与离线业务之分，若在线业务的 pod 因资源不足而调度失败时，此时就需要离线业务下掉一部分为在线业务提供资源，即在线业务要抢占离线业务的资源，此时就需要 scheduler 的优先级和抢占机制了，该机制解决的是 pod 调度失败时该怎么办的问题，若该 pod 的优先级比较高此时并不会被”搁置”，而是会”挤走”某个 node 上的一些低优先级的 pod，这样就可以保证高优先级的 pod 调度成功。 优先级与抢占机制源码分析 kubernetes 版本: v1.16 抢占发生的原因，一定是一个高优先级的 pod 调度失败，我们称这个 pod 为“抢占者”，称被抢占的 pod 为“牺牲者”(victims)。而 kubernetes 调度器实现抢占算法的一个最重要的设计，就是在调度队列的实现里，使用了两个不同的队列。 第一个队列叫作 activeQ，凡是在 activeQ 里的 pod，都是下一个调度周期需要调度的对象。所以，当你在 kubernetes 集群里新创建一个 pod 的时候，调度器会将这个 pod 入队到 activeQ 里面，调度器不断从队列里出队(pop)一个 pod 进行调度，实际上都是从 activeQ 里出队的。 第二个队列叫作 unschedulableQ，专门用来存放调度失败的 pod，当一个 unschedulableQ 里的 pod 被更新之后，调度器会自动把这个 pod 移动到 activeQ 里，从而给这些调度失败的 pod “重新做人”的机会。 当 pod 拥有了优先级之后，高优先级的 pod 就可能会比低优先级的 pod 提前出队，从而尽早完成调度过程。 k8s.io/kubernetes/pkg/scheduler/internal/queue/scheduling_queue.go123456789101112131415161718192021222324252627282930313233343536373839// NewSchedulingQueue initializes a priority queue as a new scheduling queue.func NewSchedulingQueue(stop &lt;-chan struct&#123;&#125;, fwk framework.Framework) SchedulingQueue &#123; return NewPriorityQueue(stop, fwk)&#125;// NewPriorityQueue creates a PriorityQueue object.func NewPriorityQueue(stop &lt;-chan struct&#123;&#125;, fwk framework.Framework) *PriorityQueue &#123; return NewPriorityQueueWithClock(stop, util.RealClock&#123;&#125;, fwk)&#125;// NewPriorityQueueWithClock creates a PriorityQueue which uses the passed clock for time.func NewPriorityQueueWithClock(stop &lt;-chan struct&#123;&#125;, clock util.Clock, fwk framework.Framework) *PriorityQueue &#123; comp := activeQComp if fwk != nil &#123; if queueSortFunc := fwk.QueueSortFunc(); queueSortFunc != nil &#123; comp = func(podInfo1, podInfo2 interface&#123;&#125;) bool &#123; pInfo1 := podInfo1.(*framework.PodInfo) pInfo2 := podInfo2.(*framework.PodInfo) return queueSortFunc(pInfo1, pInfo2) &#125; &#125; &#125; pq := &amp;PriorityQueue&#123; clock: clock, stop: stop, podBackoff: NewPodBackoffMap(1*time.Second, 10*time.Second), activeQ: util.NewHeapWithRecorder(podInfoKeyFunc, comp, metrics.NewActivePodsRecorder()), unschedulableQ: newUnschedulablePodsMap(metrics.NewUnschedulablePodsRecorder()), nominatedPods: newNominatedPodMap(), moveRequestCycle: -1, &#125; pq.cond.L = &amp;pq.lock pq.podBackoffQ = util.NewHeapWithRecorder(podInfoKeyFunc, pq.podsCompareBackoffCompleted, metrics.NewBackoffPodsRecorder()) pq.run() return pq&#125; 前面的文章已经说了 scheduleOne() 是执行调度算法的主逻辑，其主要功能有： 调用 sched.schedule()，即执行 predicates 算法和 priorities 算法 若执行失败，会返回 core.FitError 若开启了抢占机制，则执行抢占机制 …… k8s.io/kubernetes/pkg/scheduler/scheduler.go:516 1234567891011121314151617181920212223func (sched *Scheduler) scheduleOne() &#123; ...... scheduleResult, err := sched.schedule(pod, pluginContext) // predicates 算法和 priorities 算法执行失败 if err != nil &#123; if fitError, ok := err.(*core.FitError); ok &#123; // 是否开启抢占机制 if sched.DisablePreemption &#123; ....... &#125; else &#123; // 执行抢占机制 preemptionStartTime := time.Now() sched.preempt(pluginContext, fwk, pod, fitError) ...... &#125; ...... &#125; else &#123; ...... &#125; return &#125; ......&#125; 我们主要来看其中的抢占机制，sched.preempt() 是执行抢占机制的主逻辑，主要功能有： 从 apiserver 获取 pod info 调用 sched.Algorithm.Preempt()执行抢占逻辑，该函数会返回抢占成功的 node、被抢占的 pods(victims) 以及需要被移除已提名的 pods 更新 scheduler 缓存，为抢占者绑定 nodeName，即设定 pod.Status.NominatedNodeName 将 pod info 提交到 apiserver 删除被抢占的 pods 删除被抢占 pods 的 NominatedNodeName 字段 可以看到当上述抢占过程发生时，抢占者并不会立刻被调度到被抢占的 node 上，调度器只会将抢占者的 status.nominatedNodeName 字段设置为被抢占的 node 的名字。然后，抢占者会重新进入下一个调度周期，在新的调度周期里来决定是不是要运行在被抢占的节点上，当然，即使在下一个调度周期，调度器也不会保证抢占者一定会运行在被抢占的节点上。 这样设计的一个重要原因是调度器只会通过标准的 DELETE API 来删除被抢占的 pod，所以，这些 pod 必然是有一定的“优雅退出”时间（默认是 30s）的。而在这段时间里，其他的节点也是有可能变成可调度的，或者直接有新的节点被添加到这个集群中来。所以，鉴于优雅退出期间集群的可调度性可能会发生的变化，把抢占者交给下一个调度周期再处理，是一个非常合理的选择。而在抢占者等待被调度的过程中，如果有其他更高优先级的 pod 也要抢占同一个节点，那么调度器就会清空原抢占者的 status.nominatedNodeName 字段，从而允许更高优先级的抢占者执行抢占，并且，这也使得原抢占者本身也有机会去重新抢占其他节点。以上这些都是设置 nominatedNodeName 字段的主要目的。 k8s.io/kubernetes/pkg/scheduler/scheduler.go:35212345678910111213141516171819202122232425262728293031323334353637383940414243func (sched *Scheduler) preempt(pluginContext *framework.PluginContext, fwk framework.Framework, preemptor *v1.Pod, scheduleErr error) (string, error) &#123; // 获取 pod info preemptor, err := sched.PodPreemptor.GetUpdatedPod(preemptor) if err != nil &#123; klog.Errorf(&quot;Error getting the updated preemptor pod object: %v&quot;, err) return &quot;&quot;, err &#125; // 执行抢占算法 node, victims, nominatedPodsToClear, err := sched.Algorithm.Preempt(pluginContext, preemptor, scheduleErr) if err != nil &#123; ...... &#125; var nodeName = &quot;&quot; if node != nil &#123; nodeName = node.Name // 更新 scheduler 缓存，为抢占者绑定 nodename，即设定 pod.Status.NominatedNodeName sched.SchedulingQueue.UpdateNominatedPodForNode(preemptor, nodeName) // 将 pod info 提交到 apiserver err = sched.PodPreemptor.SetNominatedNodeName(preemptor, nodeName) if err != nil &#123; sched.SchedulingQueue.DeleteNominatedPodIfExists(preemptor) return &quot;&quot;, err &#125; // 删除被抢占的 pods for _, victim := range victims &#123; if err := sched.PodPreemptor.DeletePod(victim); err != nil &#123; return &quot;&quot;, err &#125; ...... &#125; &#125; // 删除被抢占 pods 的 NominatedNodeName 字段 for _, p := range nominatedPodsToClear &#123; rErr := sched.PodPreemptor.RemoveNominatedNodeName(p) if rErr != nil &#123; ...... &#125; &#125; return nodeName, err&#125; preempt()中会调用 sched.Algorithm.Preempt()来执行实际抢占的算法，其主要功能有： 判断 err 是否为 FitError 调用podEligibleToPreemptOthers()确认 pod 是否有抢占其他 pod 的资格，若 pod 已经抢占了低优先级的 pod，被抢占的 pod 处于 terminating 状态中，则不会继续进行抢占 如果确定抢占可以发生，调度器会把自己缓存的所有节点信息复制一份，然后使用这个副本来模拟抢占过程 过滤预选失败的 node 列表，此处会检查 predicates 失败的原因，若存在 NodeSelectorNotMatch、PodNotMatchHostName 这些 error 则不能成为抢占者，如果过滤出的候选 node 为空则返回抢占者作为 nominatedPodsToClear 获取 PodDisruptionBudget 对象 从预选失败的 node 列表中并发计算可以被抢占的 nodes，得到 nodeToVictims 若声明了 extenders 则调用 extenders 再次过滤 nodeToVictims 调用 pickOneNodeForPreemption() 从 nodeToVictims 中选出一个节点作为最佳候选人 移除低优先级 pod 的 Nominated，更新这些 pod，移动到 activeQ 队列中，让调度器为这些 pod 重新 bind node k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:32012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455func (g *genericScheduler) Preempt(pluginContext *framework.PluginContext, pod *v1.Pod, scheduleErr error) (*v1.Node, []*v1.Pod, []*v1.Pod, error) &#123; fitError, ok := scheduleErr.(*FitError) if !ok || fitError == nil &#123; return nil, nil, nil, nil &#125; // 判断 pod 是否支持抢占，若 pod 已经抢占了低优先级的 pod，被抢占的 pod 处于 terminating 状态中，则不会继续进行抢占 if !podEligibleToPreemptOthers(pod, g.nodeInfoSnapshot.NodeInfoMap, g.enableNonPreempting) &#123; return nil, nil, nil, nil &#125; // 从缓存中获取 node list allNodes := g.cache.ListNodes() if len(allNodes) == 0 &#123; return nil, nil, nil, ErrNoNodesAvailable &#125; // 过滤 predicates 算法执行失败的 node 作为抢占的候选 node potentialNodes := nodesWherePreemptionMightHelp(allNodes, fitError) // 如果过滤出的候选 node 为空则返回抢占者作为 nominatedPodsToClear if len(potentialNodes) == 0 &#123; return nil, nil, []*v1.Pod&#123;pod&#125;, nil &#125; // 获取 PodDisruptionBudget objects pdbs, err := g.pdbLister.List(labels.Everything()) if err != nil &#123; return nil, nil, nil, err &#125; // 过滤出可以抢占的 node 列表 nodeToVictims, err := g.selectNodesForPreemption(pluginContext, pod, g.nodeInfoSnapshot.NodeInfoMap, potentialNodes, g.predicates, g.predicateMetaProducer, g.schedulingQueue, pdbs) if err != nil &#123; return nil, nil, nil, err &#125; // 若有 extender 则执行 nodeToVictims, err = g.processPreemptionWithExtenders(pod, nodeToVictims) if err != nil &#123; return nil, nil, nil, err &#125; // 选出最佳的 node candidateNode := pickOneNodeForPreemption(nodeToVictims) if candidateNode == nil &#123; return nil, nil, nil, nil &#125; // 移除低优先级 pod 的 Nominated，更新这些 pod，移动到 activeQ 队列中，让调度器 // 为这些 pod 重新 bind node nominatedPods := g.getLowerPriorityNominatedPods(pod, candidateNode.Name) if nodeInfo, ok := g.nodeInfoSnapshot.NodeInfoMap[candidateNode.Name]; ok &#123; return nodeInfo.Node(), nodeToVictims[candidateNode].Pods, nominatedPods, nil &#125; return nil, nil, nil, fmt.Errorf( &quot;preemption failed: the target node %s has been deleted from scheduler cache&quot;, candidateNode.Name)&#125; 该函数中调用了多个函数：nodesWherePreemptionMightHelp()：过滤 predicates 算法执行失败的 nodeselectNodesForPreemption()：过滤出可以抢占的 node 列表pickOneNodeForPreemption()：选出最佳的 nodegetLowerPriorityNominatedPods()：移除低优先级 pod 的 Nominated selectNodesForPreemption() 从 prediacates 算法执行失败的 node 列表中来寻找可以被抢占的 node，通过workqueue.ParallelizeUntil()并发执行checkNode()函数检查 node。 k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:996123456789101112131415161718192021222324252627282930func (g *genericScheduler) selectNodesForPreemption( ...... ) (map[*v1.Node]*schedulerapi.Victims, error) &#123; nodeToVictims := map[*v1.Node]*schedulerapi.Victims&#123;&#125; var resultLock sync.Mutex meta := metadataProducer(pod, nodeNameToInfo) // checkNode 函数 checkNode := func(i int) &#123; nodeName := potentialNodes[i].Name var metaCopy predicates.PredicateMetadata if meta != nil &#123; metaCopy = meta.ShallowCopy() &#125; // 调用 selectVictimsOnNode 函数进行检查 pods, numPDBViolations, fits := g.selectVictimsOnNode(pluginContext, pod, metaCopy, nodeNameToInfo[nodeName], fitPredicates, queue, pdbs) if fits &#123; resultLock.Lock() victims := schedulerapi.Victims&#123; Pods: pods, NumPDBViolations: numPDBViolations, &#125; nodeToVictims[potentialNodes[i]] = &amp;victims resultLock.Unlock() &#125; &#125; // 启动 16 个 goroutine 并发执行 workqueue.ParallelizeUntil(context.TODO(), 16, len(potentialNodes), checkNode) return nodeToVictims, nil&#125; 其中调用的selectVictimsOnNode()是来获取每个 node 上 victims pod 的，首先移除所有低优先级的 pod 尝试抢占者是否可以调度成功，如果能够调度成功，然后基于 pod 是否有 PDB 被分为两组 violatingVictims 和 nonViolatingVictims，再对每一组的 pod 按优先级进行排序。PDB(pod 中断预算)是 kubernetes 保证副本高可用的一个对象。 然后开始逐一”删除“ pod 即要删掉最少的 pod 数来完成这次抢占即可，先从 violatingVictims(有PDB)的一组中进行”删除“ pod，并且记录删除有 PDB pod 的数量，然后再“删除” nonViolatingVictims 组中的 pod，每次”删除“一个 pod 都要检查一下抢占者是否能够运行在该 node 上即执行一次预选策略，若执行预选策略失败则该 node 当前不满足抢占需要继续”删除“ pod 并将该 pod 加入到 victims 中，直到”删除“足够多的 pod 可以满足抢占，最后返回 victims 以及删除有 PDB pod 的数量。 k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:1086123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869func (g *genericScheduler) selectVictimsOnNode( ......) ([]*v1.Pod, int, bool) &#123; if nodeInfo == nil &#123; return nil, 0, false &#125; potentialVictims := util.SortableList&#123;CompFunc: util.MoreImportantPod&#125; nodeInfoCopy := nodeInfo.Clone() removePod := func(rp *v1.Pod) &#123; nodeInfoCopy.RemovePod(rp) if meta != nil &#123; meta.RemovePod(rp, nodeInfoCopy.Node()) &#125; &#125; addPod := func(ap *v1.Pod) &#123; nodeInfoCopy.AddPod(ap) if meta != nil &#123; meta.AddPod(ap, nodeInfoCopy) &#125; &#125; // 先删除所有的低优先级 pod 检查是否能满足抢占 pod 的调度需求 podPriority := util.GetPodPriority(pod) for _, p := range nodeInfoCopy.Pods() &#123; if util.GetPodPriority(p) &lt; podPriority &#123; potentialVictims.Items = append(potentialVictims.Items, p) removePod(p) &#125; &#125; // 如果删除所有低优先级的 pod 不符合要求则直接过滤掉该 node // podFitsOnNode 就是前文讲过用来执行预选函数的 if fits, _, _, err := g.podFitsOnNode(pluginContext, pod, meta, nodeInfoCopy, fitPredicates, queue, false); !fits &#123; if err != nil &#123; ...... &#125; return nil, 0, false &#125; var victims []*v1.Pod numViolatingVictim := 0 potentialVictims.Sort() // 尝试尽量多地“删除”这些 pods，先从 PDB violating victims 中“删除”，再从 PDB non-violating victims 中“删除” violatingVictims, nonViolatingVictims := filterPodsWithPDBViolation(potentialVictims.Items, pdbs) // reprievePod 是“删除” pods 的函数 reprievePod := func(p *v1.Pod) bool &#123; addPod(p) // 同样也会调用 podFitsOnNode 再次执行 predicates 算法 fits, _, _, _ := g.podFitsOnNode(pluginContext, pod, meta, nodeInfoCopy, fitPredicates, queue, false) if !fits &#123; removePod(p) // 加入到 victims 中 victims = append(victims, p) &#125; return fits &#125; // 删除 violatingVictims 中的 pod，同时也记录删除了多少个 for _, p := range violatingVictims &#123; if !reprievePod(p) &#123; numViolatingVictim++ &#125; &#125; // 删除 nonViolatingVictims 中的 pod for _, p := range nonViolatingVictims &#123; reprievePod(p) &#125; return victims, numViolatingVictim, true&#125; pickOneNodeForPreemption() 用来选出最佳的 node 作为抢占者的 node，该函数主要基于 6 个原则： PDB violations 值最小的 node 挑选具有高优先级较少的 node 对每个 node 上所有 victims 的优先级进项累加，选取最小的 如果多个 node 优先级总和相等，选择具有最小 victims 数量的 node 如果多个 node 优先级总和相等，选择具有高优先级且 pod 运行时间最短的 如果依据以上策略仍然选出了多个 node 则直接返回第一个 node k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:867123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107func pickOneNodeForPreemption(nodesToVictims map[*v1.Node]*schedulerapi.Victims) *v1.Node &#123; if len(nodesToVictims) == 0 &#123; return nil &#125; minNumPDBViolatingPods := math.MaxInt32 var minNodes1 []*v1.Node lenNodes1 := 0 for node, victims := range nodesToVictims &#123; if len(victims.Pods) == 0 &#123; // 若该 node 没有 victims 则返回 return node &#125; numPDBViolatingPods := victims.NumPDBViolations if numPDBViolatingPods &lt; minNumPDBViolatingPods &#123; minNumPDBViolatingPods = numPDBViolatingPods minNodes1 = nil lenNodes1 = 0 &#125; if numPDBViolatingPods == minNumPDBViolatingPods &#123; minNodes1 = append(minNodes1, node) lenNodes1++ &#125; &#125; if lenNodes1 == 1 &#123; return minNodes1[0] &#125; // 选出 PDB violating pods 数量最少的或者高优先级 victim 数量少的 minHighestPriority := int32(math.MaxInt32) var minNodes2 = make([]*v1.Node, lenNodes1) lenNodes2 := 0 for i := 0; i &lt; lenNodes1; i++ &#123; node := minNodes1[i] victims := nodesToVictims[node] highestPodPriority := util.GetPodPriority(victims.Pods[0]) if highestPodPriority &lt; minHighestPriority &#123; minHighestPriority = highestPodPriority lenNodes2 = 0 &#125; if highestPodPriority == minHighestPriority &#123; minNodes2[lenNodes2] = node lenNodes2++ &#125; &#125; if lenNodes2 == 1 &#123; return minNodes2[0] &#125; // 若多个 node 高优先级的 pod 同样少，则选出加权得分最小的 minSumPriorities := int64(math.MaxInt64) lenNodes1 = 0 for i := 0; i &lt; lenNodes2; i++ &#123; var sumPriorities int64 node := minNodes2[i] for _, pod := range nodesToVictims[node].Pods &#123; sumPriorities += int64(util.GetPodPriority(pod)) + int64(math.MaxInt32+1) &#125; if sumPriorities &lt; minSumPriorities &#123; minSumPriorities = sumPriorities lenNodes1 = 0 &#125; if sumPriorities == minSumPriorities &#123; minNodes1[lenNodes1] = node lenNodes1++ &#125; &#125; if lenNodes1 == 1 &#123; return minNodes1[0] &#125; // 若多个 node 高优先级的 pod 数量同等且加权分数相等，则选出 pod 数量最少的 minNumPods := math.MaxInt32 lenNodes2 = 0 for i := 0; i &lt; lenNodes1; i++ &#123; node := minNodes1[i] numPods := len(nodesToVictims[node].Pods) if numPods &lt; minNumPods &#123; minNumPods = numPods lenNodes2 = 0 &#125; if numPods == minNumPods &#123; minNodes2[lenNodes2] = node lenNodes2++ &#125; &#125; if lenNodes2 == 1 &#123; return minNodes2[0] &#125; // 若多个 node 的 pod 数量相等，则选出高优先级 pod 启动时间最短的 latestStartTime := util.GetEarliestPodStartTime(nodesToVictims[minNodes2[0]]) if latestStartTime == nil &#123; return minNodes2[0] &#125; nodeToReturn := minNodes2[0] for i := 1; i &lt; lenNodes2; i++ &#123; node := minNodes2[i] earliestStartTimeOnNode := util.GetEarliestPodStartTime(nodesToVictims[node]) if earliestStartTimeOnNode == nil &#123; klog.Errorf(&quot;earliestStartTime is nil for node %s. Should not reach here.&quot;, node) continue &#125; if earliestStartTimeOnNode.After(latestStartTime.Time) &#123; latestStartTime = earliestStartTimeOnNode nodeToReturn = node &#125; &#125; return nodeToReturn&#125; 以上就是对抢占机制代码的一个通读。 优先级与抢占机制的使用1、创建 PriorityClass 对象： 1234567apiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata: name: high-priorityvalue: 1000000globalDefault: falsedescription: &quot;This priority class should be used for XYZ service pods only.&quot; 2、在 deployment、statefulset 或者 pod 中声明使用已有的 priorityClass 对象即可 在 pod 中使用： 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: labels: app: nginx-a name: nginx-aspec: containers: - image: nginx:1.7.9 imagePullPolicy: IfNotPresent name: nginx-a ports: - containerPort: 80 protocol: TCP resources: requests: memory: &quot;64Mi&quot; cpu: 5 limits: memory: &quot;128Mi&quot; cpu: 5 priorityClassName: high-priority 在 deployment 中使用： 123456template: spec: containers: - image: nginx name: nginx-deployment priorityClassName: high-priority 3、测试过程中可以看到高优先级的 nginx-a 会抢占 nginx-5754944d6c 的资源： 12345678910111213$ kubectl get pod -o wide -wNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-5754944d6c-9mnxa 1/1 Running 0 37s 10.244.1.4 test-worker &lt;none&gt; &lt;none&gt;nginx-a 0/1 Pending 0 0s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;nginx-a 0/1 Pending 0 0s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;nginx-a 0/1 Pending 0 0s &lt;none&gt; &lt;none&gt; test-worker &lt;none&gt;nginx-5754944d6c-9mnxa 1/1 Terminating 0 45s 10.244.1.4 test-worker &lt;none&gt; &lt;none&gt;nginx-5754944d6c-9mnxa 0/1 Terminating 0 46s 10.244.1.4 test-worker &lt;none&gt; &lt;none&gt;nginx-5754944d6c-9mnxa 0/1 Terminating 0 47s 10.244.1.4 test-worker &lt;none&gt; &lt;none&gt;nginx-5754944d6c-9mnxa 0/1 Terminating 0 47s 10.244.1.4 test-worker &lt;none&gt; &lt;none&gt;nginx-a 0/1 Pending 0 2s &lt;none&gt; test-worker test-worker &lt;none&gt;nginx-a 0/1 ContainerCreating 0 2s &lt;none&gt; test-worker &lt;none&gt; &lt;none&gt;nginx-a 1/1 Running 0 4s 10.244.1.5 test-worker &lt;none&gt; &lt;none&gt; 总结这篇文章主要讲述 kube-scheduler 中的优先级与抢占机制，可以看到抢占机制比 predicates 与 priorities 算法都要复杂，其中的许多细节仍然没有提到，本文只是通读了大部分代码，某些代码的实现需要精读，限于笔者时间的关系，对于 kube-scheduler 的代码暂时分享到此处。 参考： https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/]]></content>
      <tags>
        <tag>kube-scheduler</tag>
        <tag>preempt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-scheduler predicates 与 priorities 调度算法源码分析]]></title>
    <url>%2F2019%2F10%2F22%2Fkube_scheduler_algorithm%2F</url>
    <content type="text"><![CDATA[在上篇文章kube-scheduler 源码分析中已经介绍了 kube-scheduler 的设计以及从源码角度分析了其执行流程，这篇文章会专注介绍调度过程中 predicates 和 priorities 这两个调度策略主要发生作用的阶段。 kubernetes 版本: v1.16 predicates 调度算法源码分析predicates 算法主要是对集群中的 node 进行过滤，选出符合当前 pod 运行的 nodes。 调度算法说明上节已经提到默认的调度算法在pkg/scheduler/algorithmprovider/defaults/defaults.go中定义了： 123456789101112131415161718func defaultPredicates() sets.String &#123; return sets.NewString( predicates.NoVolumeZoneConflictPred, predicates.MaxEBSVolumeCountPred, predicates.MaxGCEPDVolumeCountPred, predicates.MaxAzureDiskVolumeCountPred, predicates.MaxCSIVolumeCountPred, predicates.MatchInterPodAffinityPred, predicates.NoDiskConflictPred, predicates.GeneralPred, predicates.CheckNodeMemoryPressurePred, predicates.CheckNodeDiskPressurePred, predicates.CheckNodePIDPressurePred, predicates.CheckNodeConditionPred, predicates.PodToleratesNodeTaintsPred, predicates.CheckVolumeBindingPred, )&#125; 下面是对默认调度算法的一些说明： predicates 算法 说明 GeneralPred GeneralPred 包含 PodFitsResources、PodFitsHost,、PodFitsHostPorts、PodMatchNodeSelector 四种算法 NoDiskConflictPred 检查多个 Pod 声明挂载的持久化 Volume 是否有冲突 MaxGCEPDVolumeCountPred 检查 GCE 持久化 Volume 是否超过了一定数目 MaxAzureDiskVolumeCountPred 检查 Azure 持久化 Volume 是否超过了一定数目 MaxCSIVolumeCountPred 检查 CSI 持久化 Volume 是否超过了一定数目（已废弃） MaxEBSVolumeCountPred 检查 EBS 持久化 Volume 是否超过了一定数目 NoVolumeZoneConflictPred 检查持久化 Volume 的 Zone（高可用域）标签是否与节点的 Zone 标签相匹配 CheckVolumeBindingPred 检查该 Pod 对应 PV 的 nodeAffinity 字段是否跟某个节点的标签相匹配，Local Persistent Volume(本地持久化卷)必须使用 nodeAffinity 来跟某个具体的节点绑定 PodToleratesNodeTaintsPred 检查 Node 的 Taint 机制，只有当 Pod 的 Toleration 字段与 Node 的 Taint 字段能够匹配时，这个 Pod 才能被调度到该节点上 MatchInterPodAffinityPred 检查待调度 Pod 与 Node 上的已有 Pod 之间的亲密（affinity）和反亲密（anti-affinity）关系 CheckNodeConditionPred 检查 NodeCondition CheckNodePIDPressurePred 检查 NodePIDPressure CheckNodeDiskPressurePred 检查 NodeDiskPressure CheckNodeMemoryPressurePred 检查 NodeMemoryPressure 默认的 predicates 调度算法主要分为五种类型： 1、第一种类型叫作 GeneralPredicates，包含 PodFitsResources、PodFitsHost、PodFitsHostPorts、PodMatchNodeSelector 四种策略，其具体含义如下所示： PodFitsHost：检查宿主机的名字是否跟 Pod 的 spec.nodeName 一致 PodFitsHostPorts：检查 Pod 申请的宿主机端口（spec.nodePort）是不是跟已经被使用的端口有冲突 PodMatchNodeSelector：检查 Pod 的 nodeSelector 或者 nodeAffinity 指定的节点是否与节点匹配等 PodFitsResources：检查主机的资源是否满足 Pod 的需求，根据实际已经分配（Request）的资源量做调度 kubelet 在启动 Pod 前，会执行一个 Admit 操作来进行二次确认，这里二次确认的规则就是执行一遍 GeneralPredicates。 2、第二种类型是与 Volume 相关的过滤规则，主要有NoDiskConflictPred、MaxGCEPDVolumeCountPred、MaxAzureDiskVolumeCountPred、MaxCSIVolumeCountPred、MaxEBSVolumeCountPred、NoVolumeZoneConflictPred、CheckVolumeBindingPred。 3、第三种类型是宿主机相关的过滤规则，主要是 PodToleratesNodeTaintsPred。 4、第四种类型是 Pod 相关的过滤规则，主要是 MatchInterPodAffinityPred。 5、第五种类型是新增的过滤规则，与宿主机的运行状况有关，主要有 CheckNodeCondition、 CheckNodeMemoryPressure、CheckNodePIDPressure、CheckNodeDiskPressure 四种。若启用了 TaintNodesByCondition FeatureGates 则在 predicates 算法中会将该四种算法移除，TaintNodesByCondition 基于 node conditions 当 node 出现 pressure 时自动为 node 打上 taints 标签，该功能在 v1.8 引入，v1.12 成为 beta 版本，目前 v1.16 中也是 beta 版本，但在 v1.13 中该功能已默认启用。 predicates 调度算法也有一个顺序，要不然在一台资源已经严重不足的宿主机上，上来就开始计算 PodAffinityPredicate 是没有实际意义的，其默认顺序如下所示： k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/predicates.go:146 123456789var ( predicatesOrdering = []string&#123;CheckNodeConditionPred, CheckNodeUnschedulablePred, GeneralPred, HostNamePred, PodFitsHostPortsPred, MatchNodeSelectorPred, PodFitsResourcesPred, NoDiskConflictPred, PodToleratesNodeTaintsPred, PodToleratesNodeNoExecuteTaintsPred, CheckNodeLabelPresencePred, CheckServiceAffinityPred, MaxEBSVolumeCountPred, MaxGCEPDVolumeCountPred, MaxCSIVolumeCountPred, MaxAzureDiskVolumeCountPred, MaxCinderVolumeCountPred, CheckVolumeBindingPred, NoVolumeZoneConflictPred, CheckNodeMemoryPressurePred, CheckNodePIDPressurePred, CheckNodeDiskPressurePred, EvenPodsSpreadPred, MatchInterPodAffinityPred&#125;) 源码分析上节中已经说到调用预选以及优选算法的逻辑在 k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:189中， 123456789101112131415func (g *genericScheduler) Schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (result ScheduleResult, err error) &#123; ...... // 执行 predicates 策略 filteredNodes, failedPredicateMap, filteredNodesStatuses, err := g.findNodesThatFit(pluginContext, pod) ...... // 执行 priorities 策略 priorityList, err := PrioritizeNodes(pod, g.nodeInfoSnapshot.NodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders, g.framework, pluginContext) ...... return&#125; findNodesThatFit() 是 predicates 策略的实际调用方法，其基本流程如下： 设定最多需要检查的节点数，作为预选节点数组的容量，避免总节点过多影响调度效率 通过NodeTree()不断获取下一个节点来判断该节点是否满足 pod 的调度条件 通过之前注册的各种 predicates 函数来判断当前节点是否符合 pod 的调度条件 最后返回满足调度条件的 node 列表，供下一步的优选操作 checkNode()是一个校验 node 是否符合要求的函数，其实际调用到的核心函数是podFitsOnNode()，再通过workqueue() 并发执行checkNode() 函数，workqueue() 会启动 16 个 goroutine 来并行计算需要筛选的 node 列表，其主要流程如下： 通过 cache 中的 NodeTree() 不断获取下一个 node 将当前 node 和 pod 传入podFitsOnNode() 方法中来判断当前 node 是否符合要求 如果当前 node 符合要求就将当前 node 加入预选节点的数组中filtered 如果当前 node 不满足要求，则加入到失败的数组中，并记录原因 通过workqueue.ParallelizeUntil()并发执行checkNode()函数，一旦找到足够的可行节点数后就停止筛选更多节点 若配置了 extender 则再次进行过滤已筛选出的 node k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:464123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657func (g *genericScheduler) findNodesThatFit(pluginContext *framework.PluginContext, pod *v1.Pod) ([]*v1.Node, FailedPredicateMap, framework.NodeToStatusMap, error) &#123; var filtered []*v1.Node failedPredicateMap := FailedPredicateMap&#123;&#125; filteredNodesStatuses := framework.NodeToStatusMap&#123;&#125; if len(g.predicates) == 0 &#123; filtered = g.cache.ListNodes() &#125; else &#123; allNodes := int32(g.cache.NodeTree().NumNodes()) // 1.设定最多需要检查的节点数 numNodesToFind := g.numFeasibleNodesToFind(allNodes) filtered = make([]*v1.Node, numNodesToFind) ...... // 2.获取该 pod 的 meta 值 meta := g.predicateMetaProducer(pod, g.nodeInfoSnapshot.NodeInfoMap) // 3.checkNode 为执行预选算法的函数 checkNode := func(i int) &#123; nodeName := g.cache.NodeTree().Next() // 4.podFitsOnNode 最终执行预选算法的函数 fits, failedPredicates, status, err := g.podFitsOnNode( ...... ) if err != nil &#123; ...... &#125; if fits &#123; length := atomic.AddInt32(&amp;filteredLen, 1) if length &gt; numNodesToFind &#123; cancel() atomic.AddInt32(&amp;filteredLen, -1) &#125; else &#123; filtered[length-1] = g.nodeInfoSnapshot.NodeInfoMap[nodeName].Node() &#125; &#125; else &#123; ...... &#125; &#125; // 5.启动 16 个 goroutine 并发执行 checkNode 函数 workqueue.ParallelizeUntil(ctx, 16, int(allNodes), checkNode) filtered = filtered[:filteredLen] if len(errs) &gt; 0 &#123; ...... &#125; &#125; // 6.若配置了 extender 则再次进行过滤 if len(filtered) &gt; 0 &amp;&amp; len(g.extenders) != 0 &#123; ...... &#125; return filtered, failedPredicateMap, filteredNodesStatuses, nil&#125; 然后继续看如何设定最多需要检查的节点数，此过程由numFeasibleNodesToFind()进行处理，基本流程如下： 如果总的 node 节点小于minFeasibleNodesToFind(默认为100)则直接返回总节点数 如果节点数超过 100，则取指定百分比 percentageOfNodesToScore(默认值为 50)的节点数 ，当该百分比后的数目仍小于minFeasibleNodesToFind，则返回minFeasibleNodesToFind 如果百分比后的数目大于minFeasibleNodesToFind，则返回该百分比的节点数 所以当节点数小于 100 时直接返回，大于 100 时只返回其总数的 50%。percentageOfNodesToScore 参数在 v1.12 引入，默认值为 50，kube-scheduler 在启动时可以设定该参数的值。 k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:4411234567891011121314151617181920func (g *genericScheduler) numFeasibleNodesToFind(numAllNodes int32) (numNodes int32) &#123; if numAllNodes &lt; minFeasibleNodesToFind || g.percentageOfNodesToScore &gt;= 100 &#123; return numAllNodes &#125; adaptivePercentage := g.percentageOfNodesToScore if adaptivePercentage &lt;= 0 &#123; adaptivePercentage = schedulerapi.DefaultPercentageOfNodesToScore - numAllNodes/125 if adaptivePercentage &lt; minFeasibleNodesPercentageToFind &#123; adaptivePercentage = minFeasibleNodesPercentageToFind &#125; &#125; numNodes = numAllNodes * adaptivePercentage / 100 if numNodes &lt; minFeasibleNodesToFind &#123; return minFeasibleNodesToFind &#125; return numNodes&#125; pridicates 调度算法的核心是 podFitsOnNode() ，scheduler 的抢占机制也会执行该函数，podFitsOnNode()基本流程如下： 遍历已经注册好的预选策略predicates.Ordering()，按顺序执行对应的策略函数 遍历执行每个策略函数，并返回是否合适，预选失败的原因和错误 如果预选函数执行失败，则加入预选失败的数组中，直接返回，后面的预选函数不会再执行 如果该 node 上存在 nominated pod 则执行两次预选函数 因为引入了抢占机制，此处主要说明一下执行两次预选函数的原因： 第一次循环，若该 pod 为抢占者(nominatedPods)，调度器会假设该 pod 已经运行在这个节点上，然后更新meta和nodeInfo，nominatedPods是指执行了抢占机制且已经分配到了 node(pod.Status.NominatedNodeName 已被设定) 但是还没有真正运行起来的 pod，然后再执行所有的预选函数。 第二次循环，不将nominatedPods加入到 node 内。 而只有这两遍 predicates 算法都能通过时，这个 pod 和 node 才会被认为是可以绑定(bind)的。这样做是因为考虑到 pod affinity 等策略的执行，如果当前的 pod 与nominatedPods有依赖关系就会有问题，因为nominatedPods不能保证一定可以调度且在已指定的 node 运行成功，也可能出现被其他高优先级的 pod 抢占等问题，关于抢占问题下篇会详细介绍。 k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:61012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func (g *genericScheduler) podFitsOnNode(......) (bool, []predicates.PredicateFailureReason, *framework.Status, error) &#123; var failedPredicates []predicates.PredicateFailureReason var status *framework.Status podsAdded := false for i := 0; i &lt; 2; i++ &#123; metaToUse := meta nodeInfoToUse := info if i == 0 &#123; // 1.第一次循环加入 NominatedPods，计算 meta, nodeInfo podsAdded, metaToUse, nodeInfoToUse = addNominatedPods(pod, meta, info, queue) &#125; else if !podsAdded || len(failedPredicates) != 0 &#123; break &#125; // 2.按顺序执行所有预选函数 for _, predicateKey := range predicates.Ordering() &#123; var ( fit bool reasons []predicates.PredicateFailureReason err error ) if predicate, exist := predicateFuncs[predicateKey]; exist &#123; fit, reasons, err = predicate(pod, metaToUse, nodeInfoToUse) if err != nil &#123; return false, []predicates.PredicateFailureReason&#123;&#125;, nil, err &#125; // 3.任何一个预选函数执行失败则直接返回 if !fit &#123; failedPredicates = append(failedPredicates, reasons...) if !alwaysCheckAllPredicates &#123; klog.V(5).Infoln(&quot;since alwaysCheckAllPredicates has not been set, the predicate &quot; + &quot;evaluation is short circuited and there are chances &quot; + &quot;of other predicates failing as well.&quot;) break &#125; &#125; &#125; &#125; // 4.执行 Filter Plugin status = g.framework.RunFilterPlugins(pluginContext, pod, info.Node().Name) if !status.IsSuccess() &amp;&amp; !status.IsUnschedulable() &#123; return false, failedPredicates, status, status.AsError() &#125; &#125; return len(failedPredicates) == 0 &amp;&amp; status.IsSuccess(), failedPredicates, status, nil&#125; 至此，关于 predicates 调度算法的执行过程已经分析完。 priorities 调度算法源码分析priorities 调度算法是在 pridicates 算法后执行的，主要功能是对已经过滤出的 nodes 进行打分并选出最佳的一个 node。 调度算法说明默认的调度算法在pkg/scheduler/algorithmprovider/defaults/defaults.go中定义了： 123456789101112func defaultPriorities() sets.String &#123; return sets.NewString( priorities.SelectorSpreadPriority, priorities.InterPodAffinityPriority, priorities.LeastRequestedPriority, priorities.BalancedResourceAllocation, priorities.NodePreferAvoidPodsPriority, priorities.NodeAffinityPriority, priorities.TaintTolerationPriority, priorities.ImageLocalityPriority, )&#125; 默认调度算法的一些说明： priorities 算法 说明 SelectorSpreadPriority 按 service，rs，statefulset 归属计算 Node 上分布最少的同类 Pod数量，数量越少得分越高，默认权重为1 InterPodAffinityPriority pod 亲和性选择策略，默认权重为1 LeastRequestedPriority 选择空闲资源（CPU 和 Memory）最多的节点，默认权重为1，其计算方式为：score = (cpu((capacity-sum(requested))10/capacity) + memory((capacity-sum(requested))10/capacity))/2 BalancedResourceAllocation CPU、Memory 以及 Volume 资源分配最均衡的节点，默认权重为1，其计算方式为：score = 10 - variance(cpuFraction,memoryFraction,volumeFraction)*10 NodePreferAvoidPodsPriority 判断 node annotation 是否有scheduler.alpha.kubernetes.io/preferAvoidPods 标签，类似于 taints 机制，过滤标签中定义类型的 pod，默认权重为10000 NodeAffinityPriority 节点亲和性选择策略，默认权重为1 TaintTolerationPriority Pod 是否容忍节点上的 Taint，优先调度到标记了 Taint 的节点，默认权重为1 ImageLocalityPriority 待调度 Pod 需要使用的镜像是否存在于该节点，默认权重为1 源码分析执行 priorities 调度算法的逻辑是在 PrioritizeNodes()函数中，其目的是执行每个 priority 函数为 node 打分，分数为 0-10，其功能主要有： PrioritizeNodes() 通过并行运行各个优先级函数来对节点进行打分 每个优先级函数会给节点打分，打分范围为 0-10 分，0 表示优先级最低的节点，10表示优先级最高的节点 每个优先级函数有各自的权重 优先级函数返回的节点分数乘以权重以获得加权分数 最后计算所有节点的总加权分数 k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:6911234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071func PrioritizeNodes(......) (schedulerapi.HostPriorityList, error) &#123; // 1.检查是否有自定义配置 if len(priorityConfigs) == 0 &amp;&amp; len(extenders) == 0 &#123; result := make(schedulerapi.HostPriorityList, 0, len(nodes)) for i := range nodes &#123; hostPriority, err := EqualPriorityMap(pod, meta, nodeNameToInfo[nodes[i].Name]) if err != nil &#123; return nil, err &#125; result = append(result, hostPriority) &#125; return result, nil &#125; ...... results := make([]schedulerapi.HostPriorityList, len(priorityConfigs), len(priorityConfigs)) ...... // 2.使用 workqueue 启动 16 个 goroutine 并发为 node 打分 workqueue.ParallelizeUntil(context.TODO(), 16, len(nodes), func(index int) &#123; nodeInfo := nodeNameToInfo[nodes[index].Name] for i := range priorityConfigs &#123; if priorityConfigs[i].Function != nil &#123; continue &#125; var err error results[i][index], err = priorityConfigs[i].Map(pod, meta, nodeInfo) if err != nil &#123; appendError(err) results[i][index].Host = nodes[index].Name &#125; &#125; &#125;) // 3.执行自定义配置 for i := range priorityConfigs &#123; ...... &#125; wg.Wait() if len(errs) != 0 &#123; return schedulerapi.HostPriorityList&#123;&#125;, errors.NewAggregate(errs) &#125; // 4.运行 Score plugins scoresMap, scoreStatus := framework.RunScorePlugins(pluginContext, pod, nodes) if !scoreStatus.IsSuccess() &#123; return schedulerapi.HostPriorityList&#123;&#125;, scoreStatus.AsError() &#125; result := make(schedulerapi.HostPriorityList, 0, len(nodes)) // 5.为每个 node 汇总分数 for i := range nodes &#123; result = append(result, schedulerapi.HostPriority&#123;Host: nodes[i].Name, Score: 0&#125;) for j := range priorityConfigs &#123; result[i].Score += results[j][i].Score * priorityConfigs[j].Weight &#125; for j := range scoresMap &#123; result[i].Score += scoresMap[j][i].Score &#125; &#125; // 6.执行 extender if len(extenders) != 0 &amp;&amp; nodes != nil &#123; ...... &#125; ...... return result, nil&#125; 总结本文主要讲述了 kube-scheduler 中的 predicates 调度算法与 priorities 调度算法的执行流程，可以看到 kube-scheduler 中有许多的调度策略，但是想要添加自己的策略并不容易，scheduler 目前已经朝着提升性能与扩展性的方向演进了，其调度部分进行性能优化的一个最根本原则就是尽最大可能将集群信息 cache 化，以便从根本上提高 predicates 和 priorities 调度算法的执行效率。第二个就是在 bind 阶段进行异步处理，只会更新其 cache 里的 pod 和 node 的信息，这种基于“乐观”假设的 API 对象更新方式，在 kubernetes 里被称作 assume，如果这次异步的 bind 过程失败了，其实也没有太大关系，等 scheduler cache 同步之后一切又恢复正常了。除了上述的“cache 化”和“乐观绑定”，还有一个重要的设计，那就是“无锁化”，predicates 调度算法与 priorities 调度算法的执行都是并行的，只有在调度队列和 scheduler cache 进行操作时，才需要加锁，而对调度队列的操作并不影响主流程。 参考： https://kubernetes.io/docs/concepts/configuration/scheduling-framework/ predicates-ordering.md]]></content>
      <tags>
        <tag>kube-scheduler</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-scheduler 源码分析]]></title>
    <url>%2F2019%2F10%2F21%2Fkube_scheduler_process%2F</url>
    <content type="text"><![CDATA[kube-scheduler 的设计Kube-scheduler 是 kubernetes 的核心组件之一，也是所有核心组件之间功能比较单一的，其代码也相对容易理解。kube-scheduler 的目的就是为每一个 pod 选择一个合适的 node，整体流程可以概括为三步，获取未调度的 podList，通过执行一系列调度算法为 pod 选择一个合适的 node，提交数据到 apiserver，其核心则是一系列调度算法的设计与执行。 官方对 kube-scheduler 的调度流程描述 The Kubernetes Scheduler： 1234567891011121314151617181920212223242526272829303132333435363738394041For given pod: +---------------------------------------------+ | Schedulable nodes: | | | | +--------+ +--------+ +--------+ | | | node 1 | | node 2 | | node 3 | | | +--------+ +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ Pred. filters: node 3 doesn&apos;t have enough resource +-------------------+-------------------------+ | | v +-------------------+-------------------------+ | remaining nodes: | | +--------+ +--------+ | | | node 1 | | node 2 | | | +--------+ +--------+ | | | +-------------------+-------------------------+ | | v +-------------------+-------------------------+ Priority function: node 1: p=2 node 2: p=5 +-------------------+-------------------------+ | | v select max&#123;node priority&#125; = node 2 kube-scheduler 目前包含两部分调度算法 predicates 和 priorities，首先执行 predicates 算法过滤部分 node 然后执行 priorities 算法为所有 node 打分，最后从所有 node 中选出分数最高的最为最佳的 node。 kube-scheduler 源码分析 kubernetes 版本: v1.16 kubernetes 中所有组件的启动流程都是类似的，首先会解析命令行参数、添加默认值，kube-scheduler 的默认参数在 k8s.io/kubernetes/pkg/scheduler/apis/config/v1alpha1/defaults.go 中定义的。然后会执行 run 方法启动主逻辑，下面直接看 kube-scheduler 的主逻辑 run 方法执行过程。 Run() 方法主要做了以下工作： 初始化 scheduler 对象 启动 kube-scheduler server，kube-scheduler 监听 10251 和 10259 端口，10251 端口不需要认证，可以获取 healthz metrics 等信息，10259 为安全端口，需要认证 启动所有的 informer 执行 sched.Run() 方法，执行主调度逻辑 k8s.io/kubernetes/cmd/kube-scheduler/app/server.go:160 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455func Run(cc schedulerserverconfig.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;, registryOptions ...Option) error &#123; ...... // 1、初始化 scheduler 对象 sched, err := scheduler.New(......） if err != nil &#123; return err &#125; // 2、启动事件广播 if cc.Broadcaster != nil &amp;&amp; cc.EventClient != nil &#123; cc.Broadcaster.StartRecordingToSink(stopCh) &#125; if cc.LeaderElectionBroadcaster != nil &amp;&amp; cc.CoreEventClient != nil &#123; cc.LeaderElectionBroadcaster.StartRecordingToSink(&amp;corev1.EventSinkImpl&#123;Interface: cc.CoreEventClient.Events(&quot;&quot;)&#125;) &#125; ...... // 3、启动 http server if cc.InsecureServing != nil &#123; separateMetrics := cc.InsecureMetricsServing != nil handler := buildHandlerChain(newHealthzHandler(&amp;cc.ComponentConfig, separateMetrics, checks...), nil, nil) if err := cc.InsecureServing.Serve(handler, 0, stopCh); err != nil &#123; return fmt.Errorf(&quot;failed to start healthz server: %v&quot;, err) &#125; &#125; ...... // 4、启动所有 informer go cc.PodInformer.Informer().Run(stopCh) cc.InformerFactory.Start(stopCh) cc.InformerFactory.WaitForCacheSync(stopCh) run := func(ctx context.Context) &#123; sched.Run() &lt;-ctx.Done() &#125; ctx, cancel := context.WithCancel(context.TODO()) // TODO once Run() accepts a context, it should be used here defer cancel() go func() &#123; select &#123; case &lt;-stopCh: cancel() case &lt;-ctx.Done(): &#125; &#125;() // 5、选举 leader if cc.LeaderElection != nil &#123; ...... &#125; // 6、执行 sched.Run() 方法 run(ctx) return fmt.Errorf(&quot;finished without leader elect&quot;)&#125; 下面看一下 scheduler.New() 方法是如何初始化 scheduler 结构体的，该方法主要的功能是初始化默认的调度算法以及默认的调度器 GenericScheduler。 创建 scheduler 配置文件 根据默认的 DefaultProvider 初始化 schedulerAlgorithmSource 然后加载默认的预选及优选算法，然后初始化 GenericScheduler 若启动参数提供了 policy config 则使用其覆盖默认的预选及优选算法并初始化 GenericScheduler，不过该参数现已被弃用 k8s.io/kubernetes/pkg/scheduler/scheduler.go:16612345678910111213141516171819202122232425262728293031323334func New(......) (*Scheduler, error) &#123; ...... // 1、创建 scheduler 的配置文件 configurator := factory.NewConfigFactory(&amp;factory.ConfigFactoryArgs&#123; ...... &#125;) var config *factory.Config source := schedulerAlgorithmSource // 2、加载默认的调度算法 switch &#123; case source.Provider != nil: // 使用默认的 ”DefaultProvider“ 初始化 config sc, err := configurator.CreateFromProvider(*source.Provider) if err != nil &#123; return nil, fmt.Errorf(&quot;couldn&apos;t create scheduler using provider %q: %v&quot;, *source.Provider, err) &#125; config = sc case source.Policy != nil: // 通过启动时指定的 policy source 加载 config ...... config = sc default: return nil, fmt.Errorf(&quot;unsupported algorithm source: %v&quot;, source) &#125; // Additional tweaks to the config produced by the configurator. config.Recorder = recorder config.DisablePreemption = options.disablePreemption config.StopEverything = stopCh // 3.创建 scheduler 对象 sched := NewFromConfig(config) ...... return sched, nil&#125; 下面是 pod informer 的启动逻辑，只监听 status.phase 不为 succeeded 以及 failed 状态的 pod，即非 terminating 的 pod。 k8s.io/kubernetes/pkg/scheduler/factory/factory.go:527123456789func NewPodInformer(client clientset.Interface, resyncPeriod time.Duration) coreinformers.PodInformer &#123; selector := fields.ParseSelectorOrDie( &quot;status.phase!=&quot; + string(v1.PodSucceeded) + &quot;,status.phase!=&quot; + string(v1.PodFailed)) lw := cache.NewListWatchFromClient(client.CoreV1().RESTClient(), string(v1.ResourcePods), metav1.NamespaceAll, selector) return &amp;podInformer&#123; informer: cache.NewSharedIndexInformer(lw, &amp;v1.Pod&#123;&#125;, resyncPeriod, cache.Indexers&#123;cache.NamespaceIndex: cache.MetaNamespaceIndexFunc&#125;), &#125;&#125; 然后继续看 Run() 方法中最后执行的 sched.Run() 调度循环逻辑，若 informer 中的 cache 同步完成后会启动一个循环逻辑执行 sched.scheduleOne 方法。 k8s.io/kubernetes/pkg/scheduler/scheduler.go:3131234567func (sched *Scheduler) Run() &#123; if !sched.config.WaitForCacheSync() &#123; return &#125; go wait.Until(sched.scheduleOne, 0, sched.config.StopEverything)&#125; scheduleOne() 每次对一个 pod 进行调度，主要有以下步骤： 从 scheduler 调度队列中取出一个 pod，如果该 pod 处于删除状态则跳过 执行调度逻辑 sched.schedule() 返回通过预算及优选算法过滤后选出的最佳 node 如果过滤算法没有选出合适的 node，则返回 core.FitError 若没有合适的 node 会判断是否启用了抢占策略，若启用了则执行抢占机制 判断是否需要 VolumeScheduling 特性 执行 reserve plugin pod 对应的 spec.NodeName 写上 scheduler 最终选择的 node，更新 scheduler cache 请求 apiserver 异步处理最终的绑定操作，写入到 etcd 执行 permit plugin 执行 prebind plugin 执行 postbind plugin k8s.io/kubernetes/pkg/scheduler/scheduler.go:51512345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788func (sched *Scheduler) scheduleOne() &#123; fwk := sched.Framework pod := sched.NextPod() if pod == nil &#123; return &#125; // 1.判断 pod 是否处于删除状态 if pod.DeletionTimestamp != nil &#123; ...... &#125; // 2.执行调度策略选择 node start := time.Now() pluginContext := framework.NewPluginContext() scheduleResult, err := sched.schedule(pod, pluginContext) if err != nil &#123; if fitError, ok := err.(*core.FitError); ok &#123; // 3.若启用抢占机制则执行 if sched.DisablePreemption &#123; ...... &#125; else &#123; preemptionStartTime := time.Now() sched.preempt(pluginContext, fwk, pod, fitError) ...... &#125; ...... metrics.PodScheduleFailures.Inc() &#125; else &#123; klog.Errorf(&quot;error selecting node for pod: %v&quot;, err) metrics.PodScheduleErrors.Inc() &#125; return &#125; ...... assumedPod := pod.DeepCopy() // 4.判断是否需要 VolumeScheduling 特性 allBound, err := sched.assumeVolumes(assumedPod, scheduleResult.SuggestedHost) if err != nil &#123; klog.Errorf(&quot;error assuming volumes: %v&quot;, err) metrics.PodScheduleErrors.Inc() return &#125; // 5.执行 &quot;reserve&quot; plugins if sts := fwk.RunReservePlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() &#123; ..... &#125; // 6.为 pod 设置 NodeName 字段，更新 scheduler 缓存 err = sched.assume(assumedPod, scheduleResult.SuggestedHost) if err != nil &#123; ...... &#125; // 7.异步请求 apiserver go func() &#123; // Bind volumes first before Pod if !allBound &#123; err := sched.bindVolumes(assumedPod) if err != nil &#123; ...... return &#125; &#125; // 8.执行 &quot;permit&quot; plugins permitStatus := fwk.RunPermitPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost) if !permitStatus.IsSuccess() &#123; ...... &#125; // 9.执行 &quot;prebind&quot; plugins preBindStatus := fwk.RunPreBindPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost) if !preBindStatus.IsSuccess() &#123; ...... &#125; err := sched.bind(assumedPod, scheduleResult.SuggestedHost, pluginContext) ...... if err != nil &#123; ...... &#125; else &#123; ...... // 10.执行 &quot;postbind&quot; plugins fwk.RunPostBindPlugins(pluginContext, assumedPod, scheduleResult.SuggestedHost) &#125; &#125;()&#125; scheduleOne() 中通过调用 sched.schedule() 来执行预选与优选算法处理： k8s.io/kubernetes/pkg/scheduler/scheduler.go:3371234567func (sched *Scheduler) schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (core.ScheduleResult, error) &#123; result, err := sched.Algorithm.Schedule(pod, pluginContext) if err != nil &#123; ...... &#125; return result, err&#125; sched.Algorithm 是一个 interface，主要包含四个方法，GenericScheduler 是其具体的实现： k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:131123456type ScheduleAlgorithm interface &#123; Schedule(*v1.Pod, *framework.PluginContext) (scheduleResult ScheduleResult, err error) Preempt(*framework.PluginContext, *v1.Pod, error) (selectedNode *v1.Node, preemptedPods []*v1.Pod, cleanupNominatedPods []*v1.Pod, err error) Predicates() map[string]predicates.FitPredicate Prioritizers() []priorities.PriorityConfig&#125; Schedule()：正常调度逻辑，包含预算与优选算法的执行 Preempt()：抢占策略，在 pod 调度发生失败的时候尝试抢占低优先级的 pod，函数返回发生抢占的 node，被 抢占的 pods 列表，nominated node name 需要被移除的 pods 列表以及 error Predicates()：predicates 算法列表 Prioritizers()：prioritizers 算法列表 kube-scheduler 提供的默认调度为 DefaultProvider，DefaultProvider 配置的 predicates 和 priorities policies 在 k8s.io/kubernetes/pkg/scheduler/algorithmprovider/defaults/defaults.go 中定义，算法具体实现是在 k8s.io/kubernetes/pkg/scheduler/algorithm/predicates/ 和k8s.io/kubernetes/pkg/scheduler/algorithm/priorities/ 中，默认的算法如下所示： pkg/scheduler/algorithmprovider/defaults/defaults.go12345678910111213141516171819202122232425262728293031func defaultPredicates() sets.String &#123; return sets.NewString( predicates.NoVolumeZoneConflictPred, predicates.MaxEBSVolumeCountPred, predicates.MaxGCEPDVolumeCountPred, predicates.MaxAzureDiskVolumeCountPred, predicates.MaxCSIVolumeCountPred, predicates.MatchInterPodAffinityPred, predicates.NoDiskConflictPred, predicates.GeneralPred, predicates.CheckNodeMemoryPressurePred, predicates.CheckNodeDiskPressurePred, predicates.CheckNodePIDPressurePred, predicates.CheckNodeConditionPred, predicates.PodToleratesNodeTaintsPred, predicates.CheckVolumeBindingPred, )&#125;func defaultPriorities() sets.String &#123; return sets.NewString( priorities.SelectorSpreadPriority, priorities.InterPodAffinityPriority, priorities.LeastRequestedPriority, priorities.BalancedResourceAllocation, priorities.NodePreferAvoidPodsPriority, priorities.NodeAffinityPriority, priorities.TaintTolerationPriority, priorities.ImageLocalityPriority, )&#125; 下面继续看 sched.Algorithm.Schedule() 调用具体调度算法的过程： 检查 pod pvc 信息 执行 prefilter plugins 获取 scheduler cache 的快照，每次调度 pod 时都会获取一次快照 执行 g.findNodesThatFit() 预选算法 执行 postfilter plugin 若 node 为 0 直接返回失败的 error，若 node 数为1 直接返回该 node 执行 g.priorityMetaProducer() 获取 metaPrioritiesInterface，计算 pod 的metadata，检查该 node 上是否有相同 meta 的 pod 执行 PrioritizeNodes() 算法 执行 g.selectHost() 通过得分选择一个最佳的 node k8s.io/kubernetes/pkg/scheduler/core/generic_scheduler.go:18612345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667func (g *genericScheduler) Schedule(pod *v1.Pod, pluginContext *framework.PluginContext) (result ScheduleResult, err error) &#123; ...... // 1.检查 pod pvc if err := podPassesBasicChecks(pod, g.pvcLister); err != nil &#123; return result, err &#125; // 2.执行 &quot;prefilter&quot; plugins preFilterStatus := g.framework.RunPreFilterPlugins(pluginContext, pod) if !preFilterStatus.IsSuccess() &#123; return result, preFilterStatus.AsError() &#125; // 3.获取 node 数量 numNodes := g.cache.NodeTree().NumNodes() if numNodes == 0 &#123; return result, ErrNoNodesAvailable &#125; // 4.快照 node 信息 if err := g.snapshot(); err != nil &#123; return result, err &#125; // 5.执行预选算法 startPredicateEvalTime := time.Now() filteredNodes, failedPredicateMap, filteredNodesStatuses, err := g.findNodesThatFit(pluginContext, pod) if err != nil &#123; return result, err &#125; // 6.执行 &quot;postfilter&quot; plugins postfilterStatus := g.framework.RunPostFilterPlugins(pluginContext, pod, filteredNodes, filteredNodesStatuses) if !postfilterStatus.IsSuccess() &#123; return result, postfilterStatus.AsError() &#125; // 7.预选后没有合适的 node 直接返回 if len(filteredNodes) == 0 &#123; ...... &#125; startPriorityEvalTime := time.Now() // 8.若只有一个 node 则直接返回该 node if len(filteredNodes) == 1 &#123; return ScheduleResult&#123; SuggestedHost: filteredNodes[0].Name, EvaluatedNodes: 1 + len(failedPredicateMap), FeasibleNodes: 1, &#125;, nil &#125; // 9.获取 pod meta 信息，执行优选算法 metaPrioritiesInterface := g.priorityMetaProducer(pod, g.nodeInfoSnapshot.NodeInfoMap) priorityList, err := PrioritizeNodes(pod, g.nodeInfoSnapshot.NodeInfoMap, metaPrioritiesInterface, g.prioritizers, filteredNodes, g.extenders, g.framework, pluginContext) if err != nil &#123; return result, err &#125; // 10.根据打分选择最佳的 node host, err := g.selectHost(priorityList) trace.Step(&quot;Selecting host done&quot;) return ScheduleResult&#123; SuggestedHost: host, EvaluatedNodes: len(filteredNodes) + len(failedPredicateMap), FeasibleNodes: len(filteredNodes), &#125;, err&#125; 至此，scheduler 的整个过程分析完毕。 总结本文主要对于 kube-scheduler v1.16 的调度流程进行了分析，但其中有大量的细节都暂未提及，包括预选算法以及优选算法的具体实现、优先级与抢占调度的实现、framework 的使用及实现，因篇幅有限，部分内容会在后文继续说明。 参考： The Kubernetes Scheduler scheduling design proposals]]></content>
      <tags>
        <tag>kube-scheduler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大规模场景下 kubernetes 集群的性能优化]]></title>
    <url>%2F2019%2F10%2F12%2Fk8s_improvements%2F</url>
    <content type="text"><![CDATA[一、etcd 优化 1、etcd 采用本地 ssd 盘作为后端存储存储 2、etcd 独立部署在非 k8s node 上 3、etcd 快照(snap)与预写式日志(wal)分盘存储 etcd 详细的优化操作可以参考上篇文章：etcd 性能测试与调优。 二、apiserver 的优化1、参数调整 --max-mutating-requests-inflight ：在给定时间内的最大 mutating 请求数，调整 apiserver 的流控 qos，可以调整至 3000，默认为 200 --max-requests-inflight：在给定时间内的最大 non-mutating 请求数，默认 400，可以调整至 1000 --watch-cache-sizes：调大 resources 的 watch size，默认为 100，当集群中 node 以及 pod 数量非常多时可以稍微调大，比如： --watch-cache-sizes=node#1000,pod#5000 2、etcd 多实例支持对于不同 object 进行分库存储，首先应该将数据与状态分离，即将 events 放在单独的 etcd 实例中，在 apiserver 的配置中加上--etcd-servers-overrides=/events#https://xxx:3379;https://xxx:3379;https://xxx:3379;https://xxxx:3379;https://xxx:3379，后期可以将 pod、node 等 object 也分离在单独的 etcd 实例中。 3、apiserver 的负载均衡通常为了保证集群的高可用，集群中一般会有多个 master 节点，kubelet 的连接也会被均分到不同的 apiserver，在 k8s v1.10 以前的版本中，kubelet 使用 HTTP/2，HTTP/2 为了提高网络性能，一个主机只建立一个连接，所有的请求都通过该连接进行，默认情况下，即使网络异常，它还是重用这个连接，直到操作系统将连接关闭，而操作系统关闭僵尸连接的时间默认是十几分钟，所以在 v1.10 以前的版本中 kubelet 连接 apiserver 超时之后不会主动 reset 掉连接进行重试，除非主动重启 kubelet 或者等待十多分钟后其进行重试。 此问题在 v1.10 版本中被修复过（track/close kubelet-&gt;API connections on heartbeat failure #63492），代码也被 merge 到了 v1.8 和 v1.9，但是该问题并没有完全修复，直到 v1.14 版本才被完全修复（ kubelet: fix fail to close kubelet-&gt;API connections on heartbeat failure #78016）。 所以为了保证 apiserver 的连接数均衡，请使用 v1.14 及以上版本。 4、使用 pprof 进行性能分析pprof 是 golang 的一大杀器，要想进行源码级别的性能分析，必须使用 pprof。 1234567891011// 安装相关包$ brew install graphviz// 启动 pprof$ go tool pprof http://localhost:8001/debug/pprof/profileFile: kube-apiserverType: cpuTime: Oct 11, 2019 at 11:39am (CST)Duration: 30s, Total samples = 620ms ( 2.07%)Entering interactive mode (type &quot;help&quot; for commands, &quot;o&quot; for options)(pprof) web // 使用 web 命令生成 svg 文件 然后打开 svg 文件： 可以通过 graph 以及交互式界面得到 cpu 耗时、goroutine 阻塞等信息，apiserver 中的对象比较多，序列化会消耗非常大的时间，golang 标准库的 json 也有很严重的性能问题，开源的 json-iter 相比标准库有不少性能上的提升，但 json-iter 有很多标准库不兼容的问题，此前也有相关的 issue 进行反馈但并没有合进主线。 三、kube-controller-manager 的优化1、参数优化 调大 –kube-api-qps 值：可以调整至 100，默认值为 20 调大 –kube-api-burst 值：可以调整至 100，默认值为 30 禁用不需要的 controller：kubernetes v1.14 中已有 35 个 controller，默认启动为--controllers，即启动所有 controller，可以禁用不需要的 controller 调整 controller 同步资源的周期：避免过多的资源同步导致集群资源的消耗，所有带有 --concurrent 前缀的参数 2、kube-controller-manager 升级过程 informer 预加载 参考自 阿里巴巴云原生实践 15 讲 controller-manager 中存储的对象非常多，每次升级过程中从 apiserver 获取这些对象并反序列化的开销是无法忽略的，重启 controller-manager 恢复时可能要花费几分钟才能完成。我们需要尽量的减小 controller-manager 单次升级对系统的中断时间，主要有以下两处改造： 预启动备 controller informer，提前加载 controller 需要的数据 主 controller 升级时，会主动释放 Leader Lease，触发备立即接管工作 通过此方案 controller-manager 中断时间降低到秒级别(升级时 &lt; 2s)，即使在异常宕机时，备仅需等待 leader lease 的过期(默认 15s)，无需要花费几分 钟重新同步数据。通过这个增强，显著的降低了 controller-manager MTTR（平均恢复时间），同时降低了 controller-manager 恢复时对 apiserver 的性能冲击。 此方案需要对 controller-manager 上面两处的代码进行修改，controller-manager 默认的启动方式是先拿到锁然后 callback run 方法，在 run 方法中会启动 informers 然后同步对象，在停止时也要改为主动释放 leader lease。 四、kube-scheduler 优化在 k8s 核心组件中，调度器的功能做的比较通用，大部分公司都不会局限于当前调度器的功能而进行一系列的改造，例如美团就对 kube-scheduler 进行过一些优化，并将预选失败中断机制（详见PR）和将全局最优解改为局部最优解（详见PR1，PR2）等重要 feature 回馈给了社区。 首先还是使用好调度器的基本功能： Pod/Node Affinity &amp; Anti-affinity Taint &amp; Toleration Priority &amp; Preemption Pod Disruption Budget 然后再进行一些必要的优化。 1、参数优化调大--kube-api-qps 值：可以调整至 100，默认值为 50 2、调度器优化 扩展调度器功能：目前可以通过 scheduler_extender 很方便的扩展调度器，比如对于 GPU 的调度，可以通过 scheduler_extender + device-plugins 来支持。 多调度器支持：kubernetes 也支持在集群中运行多个调度器调度不同作业，例如可以在 pod 的 spec.schedulerName 指定对应的调度器，也可以在 job 的 .spec.template.spec.schedulerName 指定调度器。 动态调度支持：由于 kubernetes 的默认调度器只在 pod 创建过程中进行一次性调度，后续不会重新去平衡 pod 在集群中的分布，导致实际的资源使用率不均衡，此时集群中会存在部分热点宿主，为了解决默认调度器的功能缺陷，kubernetes 孵化了一个工具 Descheduler 来对默认调度器的功能进行一些补充，详细说明可以参考官方文档。 3、其他优化策略 根据实际资源使用率进行调度：目前默认的调度仅根据 pod 的 request 值进行调度，对于一些资源使用率非常不均衡的场景可以考虑直接以实际的使用率进行调度。 等价类划分（Equivalence classes）:典型的用户扩容请求为一次扩容多个容器，因此我 们通过将 pending 队列中的请求划分等价类的方式，实现批处理，显著的降 低 Predicates/Priorities 的次数，这是阿里在今年的 kubeCon 上提出的一个优化方式。 五、kubelet 优化1、使用 node lease 减少心跳上报频率在大规模场景下，大量 node 的心跳汇报严重影响了 node 的 watch，apiserver 处理心跳请求也需要非常大的开销。而开启 nodeLease 之后，kubelet 会使用非常轻量的 nodeLease 对象 (0.1 KB) 更新请求替换老的 Update Node Status 方式，这会大大减轻 apiserver 的负担。 2、使用 bookmark 机制kubernetes v1.15 支持 bookmark 机制，bookmark 主要作用是只将特定的事件发送给客户端，从而避免增加 apiserver 的负载。bookmark 的核心思想概括起来就是在 client 与 server 之间保持一个“心跳”， 即使队列中无 client 需要感知的更新，reflector 内部的版本号也需要及时的更新。 比如：每个节点上的 kubelet 仅关注 和自己节点相关的 pods，pod storage 队列是有限的(FIFO)，当 pods 的队列更新时，旧的变更就会从队列中淘汰，当队列中的更新与某个 kubelet client 无关时，kubelet client watch 的 resourceVersion 仍然保持不变，若此时 kubelet client 重连 apiserver 后，这时候 apiserver 无法判断当前队列的最小值与 kubelet client 之间是否存在需要感知的变更，因此返回 client too old version err 触发 kubelet client 重新 list 所有的数据。 3、限制驱逐kubelet 拥有节点自动修复的能力，例如在发现异常容器或不合规容器后，会对它们进行驱逐删除操作，这对于有些场景来说风险太大。例如当 kubelet 发现当前宿主机上容器个数比设置的最大容器个数大时，会挑选驱逐和删除某些容器，虽然正常情况下不会轻易发生这种问题，但是也需要对此进行控制，降低此类风险，配置 kubelet 的参数 ----eviction-hard= 来确保在任何情况 kubelet 都不会驱逐容器。 4、原地升级kubernetes 默认只要 pod 的 spec 信息有改动，例如镜像信息，此时 pod 的 hash 值就会改变，然后会导致 pod 的销毁重建，一个Pod中可能包含了主业务容器，还有不可剥离的依赖业务容器，以及SideCar组件容器等，这在生产环境中代价是很大的，一方面 ip 和 hostname 可能会发生改变，pod 重启也需要一定的时间，另一方面频繁的重建也给集群管理带来了更多的压力，甚至还可能导致无法调度成功。为了解决该问题，就需要支持容器的原地升级。可以开发一个 operator 来实现相关的功能，这种方法需要重新实现一个 resource 对应于 k8s 中的应用，然后当 pod 中的 image 改变后只更新 pod 不重建，kubelet 会重启 container 的，可以参考阿里的 cafeDeployment，或者对原生 deployment/statefulset 中的控制器直接进行修改。 六、kube-proxy 优化1、使用 ipvs 模式由于 iptables 匹配时延和规则更新时延在大规模集群中呈指数增长，增加以及删除规则非常耗时，所以需要转为 ipvs，ipvs 使用 hash 表，其增加或者删除一条规则几乎不受规则基数的影响。iptables 以及 ipvs 详细的介绍会在后面的文章中介绍。 2、独立部署kube-proxy 默认与 kubelet 同时部署在一台 node 上，可以将 kube-proxy 组件独立部署在非 k8s node 上，避免在所有 node 上都产生大量 iptables 规则。 七、镜像优化一个容器的镜像平均 2G 左右，若频繁的拉取镜像可能会将宿主机的带宽打满，甚至影响镜像仓库的使用， 1、限制镜像的大小 2、镜像缓存 3、使用 P2P 进行镜像分发，比如：dragonfly 4、基础镜像预加载：一般镜像会分为三层，第一层基础镜像即 os，第二层环境镜像即带有 nginx、tomcat 等服务的镜像，第三层业务镜像也就是带有业务代码的镜像。基础镜像一般不会频繁更新，可在所有宿主机上预先加载，环境镜像可以定时进行加载，业务镜像则实时拉取。 八、客户端优化在大规模场景下，集群中所有的 daemonset、webhook 以及 operator 等组件非常多，每个客户端都要从 apiserver 中获取资源，此时对 apiserver 的压力非常大，若客户端使用不当很可能导致 apiserver 或者 etcd 崩溃，此时对客户端的行为进行限制就非常有必要了。首先应确保所有客户端都使用 ListWatch 机制而不是只使用 List，并且在使用 ListWatch 机制时尽量不要覆盖 ListOption，即直接从 apiserver 的缓存中获取资源列表，避免请求直接命中 etcd。 k8s.io/apimachinery/pkg/apis/meta/v1/types.go： 12345678910111213141516171819...// ListOptions is the query options to a standard REST list call.type ListOptions struct &#123; ... // When specified with a watch call, shows changes that occur after that particular version of a resource. // Defaults to changes from the beginning of history. // When specified for list: // - if unset, then the result is returned from remote storage based on quorum-read flag; // - if it&apos;s 0, then we simply return what we currently have in cache, no guarantee; // - if set to non zero, then the result is at least as fresh as given rv. // +optional ResourceVersion string `json:&quot;resourceVersion,omitempty&quot; protobuf:&quot;bytes,4,opt,name=resourceVersion&quot;` ...&#125;... 九、资源使用率的提升在大规模场景中，提高资源使用率是非常有必要的，否则会存在严重的资源浪费，资源使用率高即宿主的 cpu 利用率，但是不可能一个宿主上所有容器的资源利用率都非常高，容器和物理机不同，一个服务下容器的平均 cpu idle 一般到 50% 时此服务就该扩容了，但物理机 idle 在 50% 时还是处于稳定运行状态的，而服务一般都会有潮汐现象，所以需要一些其他方法来提高整机的 cpu 使用率。 1、pod 分配资源压缩：为 pod 设置 request 和 limit 值，对应的 pod qos 为 burstable。 2、宿主资源超卖：比如将一个实际只有 48 核的宿主上报资源给 apiserver 时上报为 60 核，以此来对宿主进行资源超卖。第一种方法就是给宿主机打上特定的资源超卖标签，然后直接修改 kubelet 的代码上报时应用指定的超卖系数，或者使用 admission webhook 在 patch node status 时修改其资源中对应的值，这种方法需要对 kubelet 注册 apiserver 的原理有深入了解。 3、在离线业务混部：大部分公司都会做在离线混部，在离线混部需要解决的问题有： 1、在线能及时抢占离线资源（目前内核不支持） 2、让离线高效的利用空闲 CPU ​ 腾讯云对在离线有深入研究，整机资源使用率已达 90%，可以借鉴其一些设计理念，参考：腾讯成本优化黑科技：整机CPU利用率最高提升至90%。 十、动态调整 Pod 资源限制 参考：超大规模商用 K8s 场景下，阿里巴巴如何动态解决容器资源的按需分配问题？ 在大规模集群场景，服务可能会因高峰期资源不足导致响应慢等问题，对于某些应用时间内 HPA 或者 VPA 都不是件容易的事情。先说 HPA，我们或许可以秒级拉起了 Pod，创建新的容器，然而拉起的容器是否真的可用呢。从创建到可用，中间要经过调度、分配ip、拉取镜像、同步白名单等，可能需要比较久的时间，对于大促和抢购秒杀 这种访问量“洪峰”可能仅维持几分钟或者十几分钟的实际场景，如果我们等到 HPA 的副本全部可用，可能市场活动早已经结束了。至于社区目前的 VPA 场景，删掉旧 Pod，创建新 Pod，这样的逻辑更难接受。 目前阿里的 policy engine 支持动态调整 pod 的资源限制，底层使用类似 cadvisor 的一个数据采集组件，直接采集 cgroup 数据，然后对容器做画像，当容器资源不足时会瞬时快速修改容器 cgroup 文件目录下的的参数，如果是 cpu 型的，直接调整低优先级容器的 cgroup 下 cpu quota 的值，首先抑制低优先级的容器对于 cpu 的争抢，然后再适当上调高优先级容器的相关资源值。 十一、其他优化方法1、禁用 kubectl 的 --all 操作，避免误操作导致某一资源全部被删除 十二、总结以上是笔者对 kubernetes 性能优化方法的一些思考及总结，部分方法参考社区的文档。kubernetes 拥有庞大而快速发展的生态系统，以上提及的优化方法仅是冰山一角，性能优化无终点，在生产环境中能发挥价值才是最有用的。 参考：eBay应用程序集群管理器TESS.IO在大规模集群下的性能优化 Meet a Kubernetes Descheduler 网易云基于Kubernetes的深度定制化实践 开放下载《阿里巴巴云原生实践 15 讲》揭秘九年云原生规模化落地 使用 K8S 几年后，这些技术专家有话要说 Kubernetes API 分析 Kubernetes 调度优化–重平衡策略方案整理 探秘金融级云原生发布工作负载 CafeDeployment 腾讯成本优化黑科技：整机CPU利用率最高提升至90% 华为云在 K8S 大规模场景下的 Service 性能优化实践 优化Kubernetes集群负载的技术方案探讨 记一次kubernetes集群异常: kubelet连接apiserver超时]]></content>
      <tags>
        <tag>improvements</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[etcd 性能测试与调优]]></title>
    <url>%2F2019%2F10%2F08%2Fetcd_improvements%2F</url>
    <content type="text"><![CDATA[etcd 是一个分布式一致性键值存储。其主要功能有服务注册与发现、消息发布与订阅、负载均衡、分布式通知与协调、分布式锁、分布式队列、集群监控与leader 选举等。 etcd 性能优化 官方文档原文：https://github.com/etcd-io/etcd/blob/master/Documentation/tuning.md 译文参考：https://skyao.gitbooks.io/learning-etcd3/content/documentation/op-guide/performance.html 理解 etcd 的性能决定 etcd 性能的关键因素，包括： 延迟(latency)：延迟是完成操作的时间。 吞吐量(throughput)：吞吐量是在某个时间期间之内完成操作的总数量。 当 etcd 接收并发客户端请求时，通常平均延迟随着总体吞吐量增加而增加。 在通常的云环境，比如 Google Compute Engine (GCE) 标准的 n-4 或者 AWS 上相当的机器类型，一个三成员 etcd 集群在轻负载下可以在低于1毫秒内完成一个请求，并在重负载下可以每秒完成超过 30000 个请求。 etcd 使用 Raft 一致性算法来在成员之间复制请求并达成一致。一致性性能，特别是提交延迟，受限于两个物理约束：网络IO延迟和磁盘IO延迟。完成一个etcd请求的最小时间是成员之间的网络往返时延(Round Trip Time / RTT)，加需要提交数据到持久化存储的 fdatasync 时间。在一个数据中心内的 RTT 可能有数百毫秒。在美国典型的 RTT 是大概 50ms, 而在大陆之间可以慢到400ms。旋转硬盘(注：指传统机械硬盘)的典型 fdatasync 延迟是大概 10ms。对于 SSD 硬盘, 延迟通常低于 1ms。为了提高吞吐量, etcd 将多个请求打包在一起并提交给 Raft。这个批量策略让 etcd 在重负载试获得高吞吐量。也有其他子系统影响到 etcd 的整体性能。每个序列化的 etcd 请求必须通过 etcd 的 boltdb 支持的(boltdb-backed) MVCC 存储引擎,它通常需要10微秒来完成。etcd 定期递增快照它最近实施的请求，将他们和之前在磁盘上的快照合并。这个过程可能导致延迟尖峰(latency spike)。虽然在SSD上这通常不是问题，在HDD上它可能加倍可观察到的延迟。而且，进行中的压缩可以影响 etcd 的性能。幸运的是，压缩通常无足轻重，因为压缩是错开的，因此它不和常规请求竞争资源。RPC 系统，gRPC，为 etcd 提供定义良好，可扩展的 API，但是它也引入了额外的延迟，尤其是本地读取。 Etcd 的默认配置在本地网络环境（localhost）下通常能够运行的很好，因为延迟很低。然而，当跨数据中心部署 Etcd 或网络延时很高时，etcd 的心跳间隔或选举超时时间等参数需要根据实际情况进行调整。 网络并不是导致延时的唯一来源。不论是 Follower 还是 Leader，其请求和响应都受磁盘 I/O 延时的影响。每个 timeout 都代表从请求发起到成功返回响应的总时间。 时间参数Etcd 底层的分布式一致性协议依赖两个时间参数来保证节点之间能够在部分节点掉钱的情况下依然能够正确处理主节点的选举。第一个参数就是所谓的心跳间隔，即主节点通知从节点它还是领导者的频率。实践数据表明，该参数应该设置成节点之间 RTT 的时间。Etcd 的心跳间隔默认是 100 毫秒。第二个参数是选举超时时间，即从节点等待多久没收到主节点的心跳就尝试去竞选领导者。Etcd 的选举超时时间默认是 1000 毫秒。 调整这些参数值是有条件的，此消波长。心跳间隔值推荐设置为临近节点间 RTT 的最大值，通常是 0.5~1.5 倍 RTT 值。如果心跳间隔设得太短，那么 Etcd 就会发送没必要的心跳信息，从而增加 CPU 和网络资源的消耗；如果设得太长，就会导致选举等待时间的超时。如果选举等待时间设置的过长，就会导致节点异常检测时间过长。评估 RTT 值的最简单的方法是使用 ping 的操作。 选举超时时间应该基于心跳间隔和节点之间的平均 RTT 值。选举超时必须至少是 RTT 10 倍的时间以便对网络波动。例如，如果 RTT 的值是 10 毫秒，那么选举超时时间必须至少是 100 毫秒。选举超时时间的上线是 50000 毫秒（50 秒），这个时间只能只用于全球范围内分布式部署的 Etcd 集群。美国大陆的一个 RTT 的合理时间大约是 130 毫秒，美国和日本的 RTT 大约是 350~400 毫秒。如果算上网络波动和重试的时间，那么 5 秒是一次全球 RTT 的安全上线。因为选举超时时间应该是心跳包广播时间的 10 倍，所以 50 秒的选举超时时间是全局分布式部署 Etcd 的合理上线值。 心跳间隔和选举超时时间的值对同一个 Etcd 集群的所有节点都生效，如果各个节点都不同的话，就会导致集群发生不可预知的不稳定性。Etcd 启动时通过传入启动参数或环境变量覆盖默认值，单位是毫秒。示例代码具体如下： 1234$ etcd --heartbeat-interval=100 --election-timeout=500# 环境变量值$ ETCD_HEARTBEAT_INTERVAL=100 ETCD_ELECTION_TIMEOUT=500 etcd 快照Etcd 总是向日志文件中追加 key，这样一来，日志文件会随着 key 的改动而线性增长。当 Etcd 集群使用较少时，保存完整的日志历史记录是没问题的，但如果 Etcd 集群规模比较大时，那么集群就会携带很大的日志文件。为了避免携带庞大的日志文件，Etcd 需要做周期性的快照。快照提供了一种通过保存系统的当前状态并移除旧日志文件的方式来压缩日志文件。 快照调优为 v2 后端存储创建快照的代价是很高的，所以只用当参数累积到一定的数量时，Etcd 才会创建快照文件。默认情况下，修改数量达到 10000 时才会建立快照。如果 Etcd 的内存使用和磁盘使用过高，那么应该尝试调低快照触发的阈值，具体请参考如下命令。 启动参数： 1$ etcd --snapshot-count=5000 环境变量： 1$ ETCD_SNAPSHOT_COUNT=5000 etcd 磁盘etcd 的存储目录分为 snapshot 和 wal，他们写入的方式是不同的，snapshot 是内存直接 dump file。而 wal 是顺序追加写，对于这两种方式系统调优的方式是不同的，snapshot 可以通过增加 io 平滑写来提高磁盘 io 能力，而 wal 可以通过降低 pagecache 的方式提前写入时序。因此对于不同的场景，可以考虑将 snap 与 wal 进行分盘，放在两块 SSD 盘上，提高整体的 IO 效率，这种方式可以提升etcd 20%左右的性能。 etcd 集群对磁盘 I/O 的延时非常敏感，因为 Etcd 必须持久化它的日志，当其他 I/O 密集型的进程也在占用磁盘 I/O 的带宽时，就会导致 fsync 时延非常高。这将导致 Etcd 丢失心跳包、请求超时或暂时性的 Leader 丢失。这时可以适当为 Etcd 服务赋予更高的磁盘 I/O 权限，让 Etcd 更稳定的运行。在 Linux 系统中，磁盘 I/O 权限可以通过 ionice 命令进行调整。 nux 默认 IO 调度器使用 CFQ 调度算法，支持用 ionice 命令为程序指定 IO 调度策略和优先级，IO 调度策略分为三种： Idle ：其他进程没有磁盘 IO 时，才进行磁盘 IO Best Effort：缺省调度策略，可以设置0-7的优先级，数值越小优先级越高，同优先级的进程采用 round-robin算法调度； Real Time ：立即访问磁盘，无视其它进程 IO None 即Best Effort，进程未指定策略和优先级时显示为none，会使用依据cpu nice设置计算出优先级 Linux 中 etcd 的磁盘优先级可以使用 ionice 配置： 1$ ionice -c2 -n0 -p `pgrep etcd` 网络etcd 中比较复杂的是网络的调优，因此大量的网络请求会在 peer 节点之间转发，而且整体网络吞吐也很大，但是还是再次强调不建议大家调整系统参数，大家可以通过修改 etcd 的 --heartbeat-interval 与 --election-timeout 启动参数来适当提高高吞吐网络下 etcd 的集群鲁棒性，通常同步吞吐在100MB左右的集群可以考虑将 --heartbeat-interval 设置为 300ms-500ms，--election-timeout 可以设置在 5000ms 左右。此外官方还有基于 TC 的网络优先传输方案，也是一个比较适用的调优手段。 如果 etcd 的 Leader 服务大量并发客户端，这就会导致 follower 的请求的处理被延迟因为网络延迟。follower 的send buffer中能看到错误的列表，如下所示： 123dropped MsgProp to 247ae21ff9436b2d since streamMsg&apos;s sending buffer is fulldropped MsgAppResp to 247ae21ff9436b2d since streamMsg&apos;s sending buffer is full 这些错误可以通过提高 Leader 的网络优先级来提高 follower 的请求的响应。可以通过流量控制机制来提高: 12345678910111213141516// 针对 2379、2380 端口放行$ tc qdisc add dev eth0 root handle 1: prio bands 3$ tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip sport 2380 0xffff flowid 1:1$ tc filter add dev eth0 parent 1: protocol ip prio 1 u32 match ip dport 2380 0xffff flowid 1:1$ tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip sport 2379 0xffff flowid 1:1$ tc filter add dev eth0 parent 1: protocol ip prio 2 u32 match ip dport 2379 0xffff flowid 1:1// 查看现有的队列$ tc -s qdisc ls dev enp0s8qdisc prio 1: root refcnt 2 bands 3 priomap 1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1 Sent 258578 bytes 923 pkt (dropped 0, overlimits 0 requeues 0) backlog 0b 0p requeues 0// 删除队列$ tc qdisc del dev enp0s8 root 数据规模etcd 的硬盘存储上限（默认是 2GB）,当 etcd 数据量超过默认 quota 值后便不再接受写请求，可以通过设置 --quota-backend-bytes 参数来增加存储大小,quota-backend-bytes 默认值为 0，即使用默认 quota 为 2GB，上限值为 8 GB，具体说明可参考官方文档：dev-guide/limit.md。 1The default storage size limit is 2GB, configurable with `--quota-backend-bytes` flag. 8GB is a suggested maximum size for normal environments and etcd warns at startup if the configured value exceeds it. 以下摘自 当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？ 阿里进行了深入研究了 etcd 内部的实现原理，并发现了影响 etcd 扩展性的一个关键问题在底层 bbolt db 的 page 页面分配算法上：随着 etcd 中存储的数据量的增长，bbolt db 中线性查找“连续长度为 n 的 page 存储页面”的性能显著下降。 为了解决该问题，我们设计了基于 segregrated hashmap 的空闲页面管理算法，hashmap 以连续 page 大小为 key, 连续页面起始 page id 为 value。通过查这个 segregrated hashmap 实现 O(1) 的空闲 page 查找，极大地提高了性能。在释放块时，新算法尝试和地址相邻的 page 合并，并更新 segregrated hashmap。更详细的算法分析可以见已发表在CNCF 博客的博文。 通过这个算法改进，我们可以将 etcd 的存储空间从推荐的 2GB 扩展到 100GB，极大地提高了 etcd 存储数据的规模，并且读写无显著延迟增长。 pull request ： https://github.com/etcd-io/bbolt/pull/141 目前社区已发布的 v3.4 系列版本并没有说明支持数据规模可达 100 G。 etcd 性能测试 测试环境：本机 mac 使用 virtualbox 安装 vm，所有 etcd 实例都是运行在在 vm 中的 docker 上 参考官方文档：https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/performance.md 安装 etcd 压测工具 benchmark： 1234$ go get go.etcd.io/etcd/tools/benchmark# GOPATH should be set$ ls $GOPATH/binbenchmark 本文仅对 etcd v3.3.10 以及 v3.4.1 进行压测。 部署 etcd 集群以下为脚本示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/bin/bashdocker ps -a | grep etcd | grep -v k8sdocker rm -f etcdETCD_VERSION=3.3.10TOKEN=my-etcd-tokenCLUSTER_STATE=newNAME_1=etcd-node-0NAME_2=etcd-node-1NAME_3=etcd-node-2HOST_1=192.168.74.36HOST_2=192.168.74.36HOST_3=192.168.74.36CLUSTER=$&#123;NAME_1&#125;=http://$&#123;HOST_1&#125;:23801,$&#123;NAME_2&#125;=http://$&#123;HOST_2&#125;:23802,$&#123;NAME_3&#125;=http://$&#123;HOST_3&#125;:23803# 对于节点1THIS_NAME=$&#123;NAME_1&#125;THIS_IP=$&#123;HOST_1&#125;sudo docker run -d --net=host --name $&#123;THIS_NAME&#125; k8s.gcr.io/etcd:$&#123;ETCD_VERSION&#125; \ /usr/local/bin/etcd \ --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \ --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:23801 --listen-peer-urls http://$&#123;THIS_IP&#125;:23801 \ --advertise-client-urls http://$&#123;THIS_IP&#125;:23791 --listen-client-urls http://$&#123;THIS_IP&#125;:23791 \ --initial-cluster $&#123;CLUSTER&#125; \ --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;# 对于节点2THIS_NAME=$&#123;NAME_2&#125;THIS_IP=$&#123;HOST_2&#125;sudo docker run -d --net=host --name $&#123;THIS_NAME&#125; k8s.gcr.io/etcd:$&#123;ETCD_VERSION&#125; \ /usr/local/bin/etcd \ --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \ --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:23802 --listen-peer-urls http://$&#123;THIS_IP&#125;:23802 \ --advertise-client-urls http://$&#123;THIS_IP&#125;:23792 --listen-client-urls http://$&#123;THIS_IP&#125;:23792 \ --initial-cluster $&#123;CLUSTER&#125; \ --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125;# 对于节点3THIS_NAME=$&#123;NAME_3&#125;THIS_IP=$&#123;HOST_3&#125;sudo docker run -d --net=host --name $&#123;THIS_NAME&#125; k8s.gcr.io/etcd:$&#123;ETCD_VERSION&#125; \ /usr/local/bin/etcd \ --data-dir=data.etcd --name $&#123;THIS_NAME&#125; \ --initial-advertise-peer-urls http://$&#123;THIS_IP&#125;:23803 --listen-peer-urls http://$&#123;THIS_IP&#125;:23803 \ --advertise-client-urls http://$&#123;THIS_IP&#125;:23793 --listen-client-urls http://$&#123;THIS_IP&#125;:23793 \ --initial-cluster $&#123;CLUSTER&#125; \ --initial-cluster-state $&#123;CLUSTER_STATE&#125; --initial-cluster-token $&#123;TOKEN&#125; 压测本文主要对不同场景下 etcd 的读写操作进行测试，尽管环境有限，但在不同场景下 etcd 的表现还是有区别的。对于写入测试，按照官方文档的测试方法指定不同数量的客户端和连接数以及 key 的大小，对于读取操作，分别测试了线性化读取以及串行化读取，由于 etcd 是强一致性的，其默认读取测试就是线性化读取。 etcd v3.3.10写入测试 12345678910111213// 查看 leader$ etcdctl member list// leader$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256// 所有 members$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256 key 数量 Key 大小 Value的大小 连接数量 客户端数量 目标 etcd 服务器 平均写入 QPS 每请求平均延迟 Average server RSS 调整磁盘IO优先级 调整网络带宽和优先级 10,000 8 256 1 1 只有主 75 50.0ms - 否 否 10,000 8 256 1 1 只有主 77 46.5ms - 是 否 10,000 8 256 1 1 只有主 - - - 是 是 100,000 8 256 100 1000 只有主 1144 1697.5ms - 否 否 100,000 8 256 100 1000 只有主 1185 1541.8ms - 是 否 100,000 8 256 100 1000 只有主 - - - 是 是 10,000 8 256 1 1 所有 members 73 49.6ms - 否 否 10,000 8 256 1 1 所有 members 80 48.5ms - 是 否 10,000 8 256 1 1 所有 members - - - 是 是 100,000 8 256 100 1000 all members 1132 1649.1ms - 否 否 100,000 8 256 100 1000 all members 1198 1536.8ms - 是 否 100,000 8 256 100 1000 all members - - - 是 是 读取测试 1234567$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --conns=1 --clients=1 range foo --consistency=l --total=10000$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --conns=1 --clients=1 range foo --consistency=s --total=10000$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --conns=100 --clients=1000 range foo --consistency=l --total=100000$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --conns=100 --clients=1000 range foo --consistency=s --total=100000 key 数量 Key 大小 Value的大小 连接数量 客户端数量 一致性(线性化/串行化) 每请求平均延迟 平均读取 QPS 10,000 8 256 1 1 Linearizable 11.5ms 740 10,000 8 256 1 1 Serializable 3.5ms 2146 100,000 8 256 100 1000 Linearizable 647.3ms 3376 100,000 8 256 100 1000 Serializable 546.9ms 4060 etcd v3.4.1写入测试 12345678910111213// 查看 etcd leader$ etcdctl --write-out=table --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23801&quot; endpoint status// leader$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256$ benchmark --endpoints=&quot;http://192.168.74.36:23791&quot; --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256// 所有 members$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --target-leader --conns=1 --clients=1 put --key-size=8 --sequential-keys --total=10000 --val-size=256$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --target-leader --conns=100 --clients=1000 put --key-size=8 --sequential-keys --total=100000 --val-size=256 key 数量 Key 大小 Value的大小 连接数量 客户端数量 目标 etcd 服务器 平均写入 QPS 每请求平均延迟 Average server RSS 调整磁盘IO优先级 调整网络带宽和优先级 10,000 8 256 1 1 只有主 75 50.0ms - 否 否 10,000 8 256 1 1 只有主 322 13.2ms - 是 否 10,000 8 256 1 1 只有主 - - - 是 是 100,000 8 256 100 1000 只有主 1871 1207.7ms - 否 否 100,000 8 256 100 1000 只有主 2239 992.4ms - 是 否 100,000 8 256 100 1000 只有主 - - - 是 是 10,000 8 256 1 1 所有 members 326 13.4ms - 否 否 100,000 8 256 1 1 所有 members 352 12.6ms - 是 否 10,000 8 256 1 1 所有 members - - - 是 是 100,000 8 256 100 1000 all members 1132 1649.1ms - 否 否 100,000 8 256 100 1000 all members 1198 1536.8ms - 是 否 100,000 8 256 100 1000 all members - - - 是 是 读取测试 1234567$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --conns=1 --clients=1 range foo --consistency=l --total=10000$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --conns=1 --clients=1 range foo --consistency=s --total=10000$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --conns=100 --clients=1000 range foo --consistency=l --total=100000$ benchmark --endpoints=&quot;http://192.168.74.36:23791,http://192.168.74.36:23792,http://192.168.74.36:23793&quot; --conns=100 --clients=1000 range foo --consistency=s --total=100000 key 数量 Key 大小 Value的大小 连接数量 客户端数量 一致性(线性化/串行化) 每请求平均延迟(99%) 平均读取 QPS 10,000 8 256 1 1 Linearizable 36.2ms 319 10,000 8 256 1 1 Serializable 34.4ms 916 100,000 8 256 100 1000 Linearizable 1302.7ms 1680 100,000 8 256 100 1000 Serializable 1097.6ms 2401 由于仅在本地进行测试，所受网络带宽影响不大，所以仅调整 io。 分析可以看到，测试结果中写入操作与以上列出的几种因素关联比较大。读取指标的时候，串行化要比线性化要好，但为了一致性，线性化(Linearizable)读取请求要通过集群成员的法定人数来获取最新的数据。串行化(Serializable)读取请求比线性化读取要廉价一些，因为他们是通过任意单台 etcd 服务器来提供服务，而不是成员的法定人数，代价是可能提供过期数据。 本文在力所能及的范围内对 etcd 的性能进行了一定的评估，所得到的数据并不能作为最终的参考数据，应当根据自己的环境进行评估，结合以上性能优化的方法得到最终的结论。 参考： Raft一致性算法论文的中文翻译 etcd 在超大规模数据场景下的性能优化 当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？ Everything you should know about etcd etcd2 与 etcd3 相比 etcd使用经验总结 Understanding performance]]></content>
      <tags>
        <tag>etcd</tag>
        <tag>improvements</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 版本多久该升级一次]]></title>
    <url>%2F2019%2F09%2F26%2Fk8s_release_version%2F</url>
    <content type="text"><![CDATA[kubernetes 社区每三个月发布一个新版本，可以说发布新版本的速度非常快，当然，在生产环境中版本升级的速度可能跟不上新版本发布的速度，那么确保目前使用的版本还处于社区的维护阶段就非常重要了，kubernetes 官方对各个版本支持的时间是多长呢？ kubernetes 发行版通常支持9个月，在此期间，如果发现严重的bug或安全问题，会在对应的分支发布补丁版本。比如，当前版本为 v1.10.1，当社区修复一些 bug 后，就会发布 v1.10.2 版本。 官方支持时间说明如下： Kubernetes version Release month End-of-life-month v1.6.x March 2017 December 2017 v1.7.x June 2017 March 2018 v1.8.x September 2017 June 2018 v1.9.x December 2017 September 2018 v1.10.x March 2018 December 2018 v1.11.x June 2018 March 2019 v1.12.x September 2018 June 2019 v1.13.x December 2018 September 2019 v1.14.x March 2019 December 2019 v1.15.x June 2019 March 2020 v1.16.x September 2019 June 2020 到目前为止，v1.13.x 以前的版本已经停止支持了，请尽快升级至高版本。 kubernetes 版本发布流程 翻译自官方文档：Kubernetes Release Versioning 说明：Kube X.Y.Z 代表 kubernetes 已经发布的版本（git tag），这个版本包含所有的组件：apiserver, kubelet, kubectl, etc. (X 表示主版本号, Y 是此版本号, Z 是补丁版本。) 版本发布时间次版本发布计划与时间表 Kube X.Y.0-alpha.W, W &gt; 0 ( 分支：master) Alpha 版本大约每两周直接从 master 分支发布一次。 没有 cherrypick 版本。如有有严重的 bug 被修复，可以基于 master 分支提前创建一个新版本。 Kube X.Y.Z-beta.W (分支: release-X.Y) 当 master 完成 Kube X.Y 的功能后，在距 X.Y.0 发布前两周会停掉 release-X.Y 分支，只将一些比较重要的 PR cherry-pick 到 X.Y。 该分支会被标记为 X.Y.0-beta.0，master 分支会被移到 X.Y+1.0-alpha.0。 如果 X.Y.0-beta.0 的功能有缺陷，还会发布其他的 beta 版本 (X.Y.0-beta.W | W &gt; 0) 。 Kube X.Y.0 (分支: release-X.Y) 最终的 release 版本会提前两周从 release-X.Y 分支上产生。 在同一分支的同一 commit 处也会被标记为 X.Y.1-beta.0。 在 X.Y.0 发布 3-4 个月后会发布 X.(Y-1).0。 Kube X.Y.Z, Z &gt; 0 (分支: release-X.Y) 当 cherrypick commits 到 release-X.Y 分支时，若有需要，也会发布相应的补丁版本 （X.Y.Z-beta.W）。 X.Y.Z 是直接从 release-X.Y 分支上产生的，当使用 beta 版本在更新 pkg/version/base.go 后会被标记为 X.Y.Z+1-beta.0。 Kube X.Y.Z, Z &gt; 0 (分支: release-X.Y.Z) 这是一个特殊的 tag，如果在上一个 release 分支后有重大的 bug 被修复，会有一个 X.Y.Z tag。 release-X.Y.Z 分支会被停掉以确保补丁版本是最新的。 如果还有重要 bug 被修复会再有一个补丁版本 X.Y.(Z+1)。 一般不会有补丁版本，补丁版本仅用于一些重大 bug 的修复。 可以参考#19849看看补丁版本的作用。 主版本时间线主版本暂时没有预期发布的时间点，也没有公布 2.0.0 的标准。到目前为止，我们还没有对任何类型的不兼容更改(例如，组件参数更改)。之前讨论过在发布 2.0.0 后 删除 v1 API group/version，但目前没有这样做的计划。 支持的组件版本与兼容版本我们希望用户在生产中使用 kubernetes 最稳定的版本，但升级版本需要一些时间，尤其是对于生产环境中的关键组件。我们也希望用户更新到最新的补丁版本，补丁版本中包含一些重要的 bugfix，希望用户尽快升级。 kubernetes 对各组件的版本也有一定的兼容性。具体的兼容策略是： slave组件可以与master组件最多延迟两个版本(minor version)，但是不能比 master 组件新。client 不能与 master 组件落后一个次版本，但是可以高一个版本，也就是说： v1.3 的 master 可以与 v1.1，v1.2，v1.3 的 slave 组件一起使用，与 v1.2，v1.3，v1.4 client 一起使用。 此外，我们希望一次“支持”三个次版本，“支持”意味着我们希望用户在生产环境中运行该版本，虽然我们可能对于不在支持的版本进行 bugfix。例如，当 v1.3 发布时，将不再支持 v1.0。此外新版本每三个月发行一次，也就是说一个版本仅支持 9 个月。 升级策略用户可以使用滚动方式升级，一次升级一个小版本，不建议直接跨度两个及以上小版本，升级时先升级 master 再升级 node 节点。 以下是在实际升级过程中的一些经验： 金丝雀部署：即灰度升级，若使用二进制部署，则在原有集群直接替换二进制进行升级，运维代价小，不会导致服务中断；若以 pod 方式部署的 master 组件直接替换镜像进行升级，若以 deployment 方式部署 master 组件，对于 apiserver 可以参考阿里的经验，设置 maxSurge=3 的方式升级，以避免升级过程带来的性能抖动，但所有的 node 组件依然需要替换二进制升级。 蓝绿部署：搭建一套新的集群，这种方式升级方式比较麻烦，涉及到数据迁移，IP 更换操作，对于部分业务不适用，风险不可控。 可以看到，kubernetes 社区的更新速度非常快，坚决不建议自己维护一套 kubernetes 版本，每次升级巨麻烦，将所有修改过的 commit cherry-pick 到每个新版本上，也容易出错，有些新版本的改动也比较大，之前修改过的地方在新版中有可能已经被移除或放在别的位置了。 详细的升级策略可以参考：kubernetes集群升级的正确姿势。 结论kubernetes 每三个月发布一个版本，社区仅维护最新的三个版本，一个版本的维护时间为 9 个月，请尽量保持生产环境的版本在社区维护范围内，版本升级时尽量保持小版本滚动升级，不建议跨多个版本升级。 参考：https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md]]></content>
      <tags>
        <tag>release version</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 kind 部署单机版 kubernetes 集群]]></title>
    <url>%2F2019%2F09%2F06%2Fkind_deploy%2F</url>
    <content type="text"><![CDATA[kubernetes 从一发布开始其学习门槛就比较高，首先就是部署难，用户要想学习 kubernetes 必须要过部署这一关，社区也推出了多个部署工具帮助简化集群的部署，社区中推出的部署工具主要目标有两大类，部署测试环境与生产环境，本节主要讲述测试环境的部署，目前社区已经有多套部署方案了： https://github.com/bsycorp/kind https://github.com/ubuntu/microk8s https://github.com/kinvolk/kube-spawn https://github.com/kubernetes/minikube https://github.com/danderson/virtuakube https://github.com/kubernetes-sigs/kubeadm-dind-cluster 而本文主要讲述使用 kind（Kubernetes In Docker）部署 k8s 集群，因为 kind 使用起来实在太简单了，特别适用于在本机部署测试环境。 kind 的原理就是将 k8s 所需要的所有组件，全部部署在一个 docker 容器中，只需要一个镜像即可部署一套 k8s 环境，其底层是使用 kubeadm 进行部署，CRI 使用 Containerd，CNI 使用 weave。下面就来看看如何使用 kind 部署一套 kubernetes 环境，在使用 kind 前你需要确保目标机器已经安装了 docker 服务。 一、使用 kind 部署 k8s 集群 以下安装环境为 mac os。 安装 kind ： 123$ wget https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-darwin-amd64$ chmod +x kind-darwin-amd64$ mv kind-darwin-amd64 /usr/local/bin/kind 使用 kind 部署 kubernetes 集群： 12345678910// 默认的 cluster name 为 kind，可以使用 --name 指定$ kind create clusterCreating cluster &quot;kind&quot; ... ✓ Ensuring node image (kindest/node:v1.15.3) 🖼 ✓ Preparing nodes 📦 ✓ Creating kubeadm config 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾Cluster creation complete. You can now use the cluster with: 使用 kind create cluster 安装，是没有指定任何配置文件的安装方式。从安装打印出的输出来看，分为 6 步： 安装基础镜像 kindest/node:v1.15.4，这个镜像里面包含了所需要的二进制文件、配置文件以及 k8s 左右组件镜像的 tar 包 准备 node，检查环境、启动镜像等工作 生成 kubeadm 的配置，然后使用 kubeadm 安装，和直接使用 kubeadm 的步骤类似 启动服务 部署 CNI 插件，kind 默认使用 weave。 创建 StorageClass。 12345// 查看 kubeconfig path$ kind get kubeconfig-path/Users/feiyu/.kube/kind-config-kind$ export KUBECONFIG=&quot;$(kind get kubeconfig-path --name=&quot;kind&quot;)&quot; kind 还有多个子命令，此处不再一一详解。 123456789101112// 查看集群信息，$ kubectl cluster-infoKubernetes master is running at https://127.0.0.1:55387KubeDNS is running at https://127.0.0.1:55387/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use &apos;kubectl cluster-info dump&apos;.// 查看本地的 kind 容器$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe26545538cc7 kindest/node:v1.15.3 &quot;/usr/local/bin/entr…&quot; 15 minutes ago Up 15 minutes 55387/tcp, 127.0.0.1:55387-&gt;6443/tcp kind-control-plane 可以看到，kind 容器暴露的 6443 端口映射在本机的一个随机端口(55387)上。 1234567891011121314151617181920212223242526272829303132333435// 查看 node 的详细信息，可以看到 cni 为 containerd$ kubectl describe node kind-control-plane... Container Runtime Version: containerd://1.2.6-0ubuntu1 Kubelet Version: v1.15.3 Kube-Proxy Version: v1.15.3PodCIDR: 10.244.0.0/24ExternalID: kind-control-plane...# 进入 kind 容器查看 k8s 的配置，和单独使用 kubeadm 时一致$ docker exec -it e26545538cc bashroot@kind-control-plane:~# ls /etc/kubernetes/admin.conf controller-manager.conf kubelet.conf manifests pki scheduler.confroot@kind-control-plane:~# ls /etc/kubernetes/manifests/etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml# 查看 cni 配置root@kind-control-plane:/etc/kubernetes# cat /var/lib/kubelet/kubeadm-flags.envKUBELET_KUBEADM_ARGS=&quot;--container-runtime=remote --container-runtime-endpoint=/run/containerd/containerd.sock --fail-swap-on=false --node-ip=172.17.0.2&quot;# 查看容器的状态root@kind-control-plane:~# crictl podsPOD ID CREATED STATE NAME NAMESPACE ATTEMPTfc8700af77ca2 About an hour ago Ready coredns-5c98db65d4-bjxl2 kube-system 06378297d32811 About an hour ago Ready coredns-5c98db65d4-q2drh kube-system 0124b42a35e0d1 About an hour ago Ready kube-proxy-99nc9 kube-system 054b9511069534 About an hour ago Ready kindnet-xz8dp kube-system 061cb720ddece8 About an hour ago Ready etcd-kind-control-plane kube-system 04514b98de1a44 About an hour ago Ready kube-scheduler-kind-control-plane kube-system 09a29dbebc8dd1 About an hour ago Ready kube-controller-manager-kind-control-plane kube-system 0ab028c5f5a3e5 About an hour ago Ready kube-apiserver-kind-control-plane kube-system 0 删除集群： 1$ kind delete cluster kind 也支持创建多 master 以及多 work 节点的集群，需要自定义 yaml 配置： 12345678910111213# a cluster with 3 control-plane nodes and 3 workerskind: ClusterapiVersion: kind.sigs.k8s.io/v1alpha3nodes:- role: control-plane- role: control-plane- role: control-plane- role: worker- role: worker- role: worker// 创建集群指定 config$ kind create cluster --config kind.yaml kind 还支持自定义映射的端口号、支持使用自定义镜像仓库、支持启用 Feature Gates 等多个功能，详细的使用请参考官方文档 quick-start。 二、本地测试既然 kind 不能用作生产环境，那怎么在本地测试时使用呢？由于 k8s 的新版已经全面启用了 TLS，不再支持非安全端口，访问 APIServer 的接口都需要认证，但是本地测试不需要那么麻烦，如下所示，为匿名用户设置访问权限即可。 1234567891011121314// 为匿名用户关联 RBAC 规则$ kubectl create clusterrolebinding system:anonymous --clusterrole=cluster-admin --user=system:anonymous// 请求相关的 API$ curl -k https://127.0.0.1:55387/api/v1/nodes&#123; &quot;kind&quot;: &quot;NodeList&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: &#123; &quot;selfLink&quot;: &quot;/api/v1/nodes&quot;, &quot;resourceVersion&quot;: &quot;11844&quot; &#125;, &quot;items&quot;: [ ...]]></content>
      <tags>
        <tag>kind</tag>
        <tag>deploy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-on-kube-operator 开发(三)]]></title>
    <url>%2F2019%2F09%2F01%2Fkube_on_kube_operator_3%2F</url>
    <content type="text"><![CDATA[kube-on-kube-operator 开发(一) kube-on-kube-operator 开发(二) 本文是介绍 kubernetes-operator 开发的第三篇，前几篇已经提到过 kubernetes-operator 的主要目标是实现以下三种场景中的集群管理： kube-on-kube kube-to-kube kube-to-cloud-kube 目前笔者主要在开发 kube-to-kube，这一节会介绍 kube-to-kube 中如何使用二进制方式部署一个集群，问什么要先支持部署二进制集群呢，可以参考之前的文章。目前 kubernetes-operator 中部署集群是通过 ansible 调用笔者写的一些脚本部署的，由于 kubernetes 二进制文件比较大，暂时仅支持离线部署，部署前请下载好所需的二进制文件，笔者也提供了部署 v1.14 需要的所有二进制文件、镜像、yaml 等。 二进制安装 kubernetes 最困难的地方就在于其复杂的认证(Authentication)及鉴权(Authorization)机制，上篇文章已经介绍了 kubernetes 中的认证与鉴权机制以及其中的证书链，若安装过程中有疑问请参考 浅析 kubernetes 的认证与鉴权机制。 使用 kubernetes-operator 管理集群时首选需要有一个元集群，元集群可以使用 minkube 或者 kind 部署一个单机版集群，然后将 kubernetes-operator 部署到该集群中再通过创建 CR 来部署一个业务集群，最后使用该业务集群作为元集群即可，或者也可以使用 kubernetes-operator 中部署业务集群的方式来部署元集群。 部署集群前请先克隆 https://github.com/gosoon/kubernetes-operator 和 https://github.com/gosoon/kubernetes-utils 项目，部署集群所需要的一些工具、配置以及 bin 文件都存放在这两个项目中，你也可以使用自己的配置。 准备环境禁用防火墙： 12$ systemctl stop firewalld$ systemctl disable firewalld 禁用 SELinux： 12$ setenforce 0$ sed -i &apos;s/^SELINUX=enforcing$/SELINUX=permissive/&apos; /etc/selinux/config 关闭 swap： 1swapoff -a 修改内核参数： 12345cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 配置 CA 及创建 TLS 证书安装证书生成工具，本文使用 cfssl 1$ cp kubernetes-utils/scripts/bin/certs/* /usr/bin/ etcd123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475$ cat &lt;&lt; EOF &gt; etcd-root-ca-csr.json&#123; &quot;CN&quot;: &quot;etcd-root-ca&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 4096 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;ca&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;&#125;EOF$ cat &lt;&lt; EOF &gt; etcd-gencert.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125;&#125;EOF$ cat &lt;&lt; EOF &gt; etcd-csr.json&#123; &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;CN&quot;: &quot;etcd&quot;&#125;EOF$ cat &lt;&lt; EOF &gt; config-etcd-peer.json&#123; &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;O&quot;: &quot;etcd&quot;, &quot;OU&quot;: &quot;etcd Security&quot;, &quot;L&quot;: &quot;Beijing&quot;, &quot;ST&quot;: &quot;Beijing&quot;, &quot;C&quot;: &quot;CN&quot; &#125; ], &quot;CN&quot;: &quot;etcd&quot;&#125;EOF 1234567891011121314151617181920$ cfssl gencert --initca=true etcd-root-ca-csr.json | cfssljson -bare output/ca// 指定 etcd hosts，etcd server 和 etcd peer 中必须包含所有 etcd 的 hosts，eg：ETCD_HOSTS=&quot;10.0.4.15，10.0.2.15&quot;# etcd server$ cfssl gencert \ -ca=output/ca.pem \ -ca-key=output/ca-key.pem \ -config=ca-config.json \ -hostname=127.0.0.1,$&#123;ETCD_HOSTS&#125; \ -profile=server \ server.json | cfssljson -bare output/etcd-server # etcd peer$ cfssl gencert \ -ca=output/ca.pem \ -ca-key=output/ca-key.pem \ -config=ca-config.json \ -hostname=127.0.0.1,$&#123;ETCD_HOSTS&#125; \ -profile=peer \ server.json | cfssljson -bare output/etcd-peer 生成证书后反解 etcd server 和 peer 证书校验 ip 是否正确： 1$ cfssl certinfo -cert etcd-peer.pem master由于 master 组件的 CSR 配置与 kubernetes 中的认证与鉴权相关联，需要严格按照 kubernetes 中默认的 RBAC 进行配置，每个组件都有默认的 user 或者 group。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209$ cat &lt;&lt; EOF &gt; ca-csr.json&#123; &quot;CN&quot;: &quot;Kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;Kubernetes&quot;, &quot;OU&quot;: &quot;Shanghai&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF$ cat &lt;&lt; EOF &gt; ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [&quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot;], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125;EOF// kube-apiserver csr$ cat &lt;&lt; EOF &gt; kube-apiserver-csr.json&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;Kubernetes&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// kube-controller-manager csr$ cat &lt;&lt; EOF &gt; kube-controller-manager-csr.json&#123; &quot;CN&quot;: &quot;system:kube-controller-manager&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:kube-controller-manager&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// kube-scheduler csr$ cat &lt;&lt; EOF &gt; kube-scheduler-csr.json&#123; &quot;CN&quot;: &quot;system:kube-scheduler&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:kube-scheduler&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// kubelet csr，请替换 nodeName$ cat &lt;&lt; EOF &gt; kubelet-csr.json&#123; &quot;CN&quot;: &quot;system:node:&lt;nodeName&gt;&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:nodes&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// apiserver client csr$ cat &lt;&lt; EOF &gt; apiserver-kubelet-client-csr.json&#123; &quot;CN&quot;: &quot;system:kubelet-api-admin&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// kube-proxy csr$ cat &lt;&lt; EOF &gt; kube-proxy-csr.json&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:node-proxier&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF// kubectl csr$ cat &lt;&lt; EOF &gt; admin-csr.json&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125;EOF$ cfssl gencert -initca ca-csr.json | cfssljson -bare output/ca为了保证客户端与 Kubernetes API 的认证，Kubernetes API Server 凭证中必需包含 master 的静态 IP 地址,在 hostname 中指定# apiserver$ cfssl gencert \ -ca=output/ca.pem \ -ca-key=output/ca-key.pem \ -config=ca-config.json \ -hostname=10.250.0.1,$&#123;MASTER_HOSTS&#125;,$&#123;MASTER_VIP&#125;,127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc \ -profile=kubernetes \ kube-apiserver-csr.json | cfssljson -bare output/kube-apiserver# kubeletfor node in `echo $&#123;NODE_HOSTS&#125; | tr &apos;,&apos; &apos; &apos;`;do cfssl gencert \ -ca=output/ca.pem \ -ca-key=output/ca-key.pem \ -config=ca-config.json \ -hostname=$&#123;NODE_HOSTS&#125; \ -profile=kubernetes \ kubelet-csr.json | cfssljson -bare output/kubeletdone# other componentfor component in kube-controller-manager kube-scheduler kube-proxy apiserver-kubelet-client admin service-account;do cfssl gencert \ -ca=output/ca.pem \ -ca-key=output/ca-key.pem \ -config=ca-config.json \ -profile=kubernetes \ $&#123;component&#125;-csr.json | cfssljson -bare output/$&#123;component&#125;done 生成 kubeconfig12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697// 替换 apiserver KUBE_APISERVER=&quot;https://10.0.4.15:6443&quot;CERTS_DIR=&quot;/etc/kubernetes/ssl&quot;# 生成 kubectl 配置文件echo &quot;Create kubectl kubeconfig...&quot;kubectl config set-cluster kubernetes \ --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=output/kubectl.kubeconfigkubectl config set-credentials &quot;system:masters&quot; \ --client-certificate=$&#123;CERTS_DIR&#125;/admin.pem \ --client-key=$&#123;CERTS_DIR&#125;/admin-key.pem \ --embed-certs=true \ --kubeconfig=output/kubectl.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=system:masters \ --kubeconfig=output/kubectl.kubeconfigkubectl config use-context default --kubeconfig=output/kubectl.kubeconfig# 生成 kube-controller-manager 配置文件echo &quot;Create kube-controller-manager kubeconfig...&quot;kubectl config set-cluster kubernetes \ --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=output/kube-controller-manager.kubeconfigkubectl config set-credentials &quot;system:kube-controller-manager&quot; \ --client-certificate=$&#123;CERTS_DIR&#125;/kube-controller-manager.pem \ --client-key=$&#123;CERTS_DIR&#125;/kube-controller-manager-key.pem \ --embed-certs=true \ --kubeconfig=output/kube-controller-manager.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=system:kube-controller-manager \ --kubeconfig=output/kube-controller-manager.kubeconfigkubectl config use-context default --kubeconfig=output/kube-controller-manager.kubeconfig# 生成 kube-scheduler 配置文件echo &quot;Create kube-scheduler kubeconfig...&quot;kubectl config set-cluster kubernetes \ --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=output/kube-scheduler.kubeconfigkubectl config set-credentials &quot;system:kube-scheduler&quot; \ --client-certificate=$&#123;CERTS_DIR&#125;/kube-scheduler.pem \ --client-key=$&#123;CERTS_DIR&#125;/kube-scheduler-key.pem \ --embed-certs=true \ --kubeconfig=output/kube-scheduler.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=system:kube-scheduler \ --kubeconfig=output/kube-scheduler.kubeconfigkubectl config use-context default --kubeconfig=output/kube-scheduler.kubeconfig# 生成 kubelet 配置文件,需要添加对应的 nodeNameecho &quot;Create kubelet kubeconfig...&quot;kubectl config set-cluster kubernetes \ --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfigkubectl config set-credentials system:node:$&#123;node&#125; \ --client-certificate=$&#123;CERTS_DIR&#125;/kubelet.pem \ --client-key=$&#123;CERTS_DIR&#125;/kubelet-key.pem \ --embed-certs=true \ --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=system:node:$&#123;node&#125; \ --kubeconfig=$&#123;CERTS_DIR&#125;/kubelet-$&#123;node&#125;.kubeconfigkubectl config use-context default --kubeconfig=output/kubelet-$&#123;node&#125;.kubeconfig# 生成 kube-proxy 配置文件echo &quot;Create kube-proxy kubeconfig...&quot;kubectl config set-cluster kubernetes \ --certificate-authority=$&#123;CERTS_DIR&#125;/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=output/kube-proxy.kubeconfigkubectl config set-credentials &quot;system:kube-proxy&quot; \ --client-certificate=$&#123;CERTS_DIR&#125;/kube-proxy.pem \ --client-key=$&#123;CERTS_DIR&#125;/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=output/kube-proxy.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=system:kube-proxy \ --kubeconfig=output/kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=output/kube-proxy.kubeconfig 部署部署 etcd拷贝证书文件： 1$ cp output/* /etc/etcd/ssl/ 拷贝 bin 文件： 1$ cp kubernetes-utils/scripts/bin/etcd_v3.3.13/* /usr/bin/ 拷贝配置文件，配置文件中的 ip 需要手动替换掉： 1$ cp kubernetes-operator/scripts/config/etcd/etcd.conf /etc/etcd/ 部署 k8s master 组件拷贝证书文件： 1$ cp output/* /etc/kubernetes/ssl/ 拷贝 bin 文件： 1$ cp kubernetes-utils/scripts/bin/kubernetes_v1.14.0/* /usr/bin/ 拷贝配置文件，配置文件中的 ip 需要手动替换掉： 1$ cp kubernetes-operator/scripts/config/master/* /etc/kubernetes/ 部署 k8s node 组件部署 docker，拷贝 bin 文件： 1$ cp kubernetes-utils/scripts/bin/docker-ce-18.06.1.ce/* /usr/bin/ 拷贝证书文件： 1$ cp output/* /etc/kubernetes/ssl/ 拷贝配置文件，配置文件中的 ip 需要手动替换掉： 1$ cp kubernetes-operator/scripts/config/node/* /etc/kubernetes/ 创建 systemd 文件拷贝所有服务的 systemd 文件： 拷贝配置文件，配置文件中的 ip 需要手动替换掉： 1$ cp kubernetes-operator/scripts/systemd/* /usr/lib/systemd/system/ 启动服务首先启动 etcd 服务，etcd 所部署的几个节点需要同时启动，否则服务会启动失败。 然后依次启动 master 上的组件和 node 上的组件。 总结本文主要讲述了 kubernetes-operator 中 kube-to-kube 部署集群的方式，介绍了主要的部署步骤，文中部署集群所有的操作都提供了脚本的方式：https://github.com/gosoon/kubernetes-operator/tree/master/scripts。 kube-to-kube 的部署方式暂时是以 ansible + 自定义脚本的方式部署，部署方式也在持续更新与完善中。接下来会继续开发 kube-on-kube 的部署方式，kube-on-kube 会将业务集群的 master 组件部署在元集群中，kube-on-kube 方式暂时会采用对 kubeadm 封装的形式进行部署。]]></content>
      <tags>
        <tag>operator</tag>
        <tag>kube-on-kube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅析 kubernetes 的认证与鉴权机制]]></title>
    <url>%2F2019%2F08%2F18%2Fk8s_auth_rbac%2F</url>
    <content type="text"><![CDATA[笔者最初接触 kubernetes 时使用的是 v1.4 版本，集群间的通信仅使用 8080 端口，认证与鉴权机制还未得到完善，到后来开始使用 static token 作为认证机制，直到 v1.6 时才开始使用 TLS 认证。随着社区的发展，kubernetes 的认证与鉴权机制已经越来越完善，新版本已经全面趋于 TLS + RBAC 配置，但其认证与鉴权机制也极其复杂，本文将会带你一步步了解。 kubernetes 集群的所有操作基本上都是通过 apiserver 这个组件进行的，它提供 HTTP RESTful 形式的 API 供集群内外客户端调用。kubernetes 对于访问 API 来说提供了三个步骤的安全措施：认证、授权、准入控制，当用户使用 kubectl，client-go 或者 REST API 请求 apiserver 时，都要经过以上三个步骤的校验。认证解决的问题是识别用户的身份，鉴权是为了解决用户有哪些权限，准入控制是作用于 kubernetes 中的对象，通过合理的权限管理，能够保证系统的安全可靠。认证授权过程只存在 HTTPS 形式的 API 中，也就是说，如果客户端使用 HTTP 连接到 apiserver，是不会进行认证授权的，然而 apiserver 的非安全认证端口 8080 已经在 v1.12 中废弃了，未来将全面使用 HTTPS。 首先来看一下 kubernetes 中的认证、授权以及访问控制机制。 kubernetes 的认证机制(Authentication)kubernetes 目前所有的认证策略如下所示： X509 client certs Static Token File Bootstrap Tokens Static Password File Service Account Tokens OpenId Connect Tokens Webhook Token Authentication Authticating Proxy Anonymous requests User impersonation Client-go credential plugins 可以看到，kubernetes 的认证机制非常多，要想一个个搞清楚也绝非易事，本文仅分析几个比较重要且使用广泛的认证机制。 X509 client certsX509是一种数字证书的格式标准，现在 HTTPS 依赖的 SSL 证书使用的就是使用的 X509 格式。X509 客户端证书认证方式是 kubernetes 所有认证中使用最多的一种，相对来说也是最安全的一种，kubernetes 的一些部署工具 kubeadm、minkube 等都是基于证书的认证方式。客户端证书认证叫作 TLS 双向认证，也就是服务器客户端互相验证证书的正确性，在都正确的情况下协调通信加密方案。目前最常用的 X509 证书制作工具有 openssl、cfssl 等。 Service Account Tokens有些情况下，我们希望在 pod 内部访问 apiserver，获取集群的信息，甚至对集群进行改动。针对这种情况，kubernetes 提供了一种特殊的认证方式：serviceaccounts。 serviceaccounts 是面向 namespace 的，每个 namespace 创建的时候，kubernetes 会自动在这个 namespace 下面创建一个默认的 serviceaccounts；并且这个 serviceaccounts 只能访问该 namespace 的资源。serviceaccounts 和 pod、service、deployment 一样是 kubernetes 集群中的一种资源，用户也可以创建自己的 serviceaccounts。 serviceaccounts 主要包含了三个内容：namespace、token 和 ca，每个 serviceaccounts 中都对应一个 secrets，namespace、token 和 ca 信息都是保存在 secrets 中且都通过 base64 编码的。namespace 指定了 pod 所在的 namespace，ca 用于验证 apiserver 的证书，token 用作身份验证，它们都通过 mount 的方式保存在 pod 的文件系统中，其三者都是保存在 /var/run/secrets/kubernetes.io/serviceaccount/目录下。 关于 serviceaccounts 的配置可以参考官方的 Configure Service Accounts for Pods 文档。 认证机制的官方文档，请参考：https://kubernetes.io/docs/reference/access-authn-authz/authentication/ 小结：kubernetes 中有多种认证方式，上面讲了最常使用的两种认证方式，X509 client certs 认证方式是用在一些客户端访问 apiserver 以及集群组件之间访问时使用，比如 kubectl 请求 apiserver 时。serviceaccounts 是用在 pod 中访问 apiserver 时进行认证的，比如使用自定义 controller 时。 认证解决的问题是识别用户的身份，那 kubernetes 中都有哪几种用户？目前 kubernetes 中的用户分为内部用户和外部用户，内部用户指在 kubernetes 集群中的 pod 要访问 apiserver 时所使用的，也就是 serviceaccounts，内部用户需要在 kubernetes 中创建。外部用户指 kubectl 以及一些客户端工具访问 apiserver 时所需要认证的用户，此类用户嵌入在客户端的证书中。 kubernetes 的鉴权机制(Authorization)kubernetes 目前支持如下四种鉴权机制： Node ABAC RBAC Webhook 下面仅介绍两种最常使用的鉴权机制： Node仅 v1.7 版本以上支持 Node 授权，配合 NodeRestriction 准入控制来限制 kubelet，使其仅可访问 node、endpoint、pod、service 以及 secret、configmap、pv、pvc 等相关的资源，在 apiserver 中使用以下配置来开启 node 的鉴权机制： 123KUBE_ADMISSION_CONTROL=&quot;...,NodeRestriction,...&quot;KUBE_API_ARGS=&quot;...,--authorization-mode=Node,...&quot; RBACRBAC（Role-Based Access Control）是 kubernetes 中负责完成授权，是基于角色的访问控制，通过自定义角色并将角色和特定的 user，group，serviceaccounts 关联起来已达到权限控制的目的。 RBAC 中有三个比较重要的概念： Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限； Subject：被作用者，包括 user，group，serviceaccounts，通俗来讲就是认证机制中所识别的用户； RoleBinding：定义了“被作用者”和“角色”的绑定关系，也就是将用户以及操作权限进行绑定； RBAC 其实就是通过创建角色(Role），通过 RoleBinding 将被作用者（subject）和角色（Role）进行绑定。下图是 RBAC 中的几种绑定关系： 鉴权机制的官方文档，请参考：https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authorization-modules 准入控制(Admission Control)准入控制是请求的最后一个步骤，准入控制有许多内置的模块，可以作用于对象的 “CREATE”、”UPDATE”、”DELETE”、”CONNECT” 四个阶段。在这一过程中，如果任一准入控制模块拒绝，那么请求立刻被拒绝。一旦请求通过所有的准入控制器后就会写入对象存储中。 准入控制是在 apiserver 中进行配置的： 1KUBE_ADMISSION_CONTROL=&quot;--enable-admission-plugins=NamespaceLifecycle,LimitRanger,...MutatingAdmissionWebhook,ValidatingAdmissionWebhook,NodeRestriction...&quot; 准入控制的配置是有序的，不同的顺序会影响 kubernetes 的性能，建议使用官方的配置。 若需要对 kubernetes 中的对象做一些扩展，可以使用准入控制，比如：创建 pod 时添加 initContainer 或者校验字段等。准入控制最常使用的扩展方式就是 admission webhooks，以前写过一篇类似的文章，可以参考：http://blog.tianfeiyu.com/2019/07/02/k8s_crd_verify/。 准入控制更详细的文档，请参考：https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ 小结：上文已经说了 kubernetes 中有两种用户，一种是内置用户被称为 serviceaccounts，一种外部用户，嵌入在客户端的证书中，那么 kubernetes 中有哪些证书链以及内嵌的用户如何与 RBAC 结合呢？ kubernetes 中的证书链笔者通过自己的研究及实践经验发现，在目前主流版本的 kubernetes 集群中，有四条重要的 CA 证书链，而在大多数生产环境中，则至少需要两条 CA 证书链。 apiserver CA 证书链：主要用于 kubernetes 内部组件互相访问以及外部客户端访问 apiserver 使用 etcd CA 证书链：主要用于 etcd 节点之间的访问以及 apiserver 访问 etcd 使用 extension apiserver CA 证书链：用于访问 extension apiserver 使用，比如 metrics-server kubelet CA 信任链：用于 apiserver 访问 kubelet 时使用 其他证书链：admission webhook 证书链、audit webhook 证书链，用于 apiserver 访问 webhook 时使用 以上这几套 CA 证书链中，apiserver CA 证书链和 etcd CA 证书链是必要的。extension apiserver 的 CA 证书链只有在使用时才会用到，且不可与 apiserver CA 证书链相同。kubelet 的 CA 证书链不是必要的，根据部署的实际情况可以和 apiserver CA 证书链公用。 证书中的内嵌用户如何与 RBAC 配置进行结合证书中的内嵌用户以下是 kubelet 的证书请求文件（CSR）： 12345678910111213141516&#123; &quot;CN&quot;: &quot;system:node:&lt;nodeName&gt;&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;China&quot;, &quot;L&quot;: &quot;Shanghai&quot;, &quot;O&quot;: &quot;system:nodes&quot;, &quot;OU&quot;: &quot;Kubernetes&quot;, &quot;ST&quot;: &quot;Shanghai&quot; &#125; ]&#125; “CN”：Common Name，从证书中提取该字段作为请求的用户名 (User Name)； “O”：Organization，从证书中提取该字段作为请求用户所属的组 (Group)； kubernetes 使用 X509 证书中 CN(Common Name) 以及 O(Organization) 字段对应 kubernetes 中的 user 和 group，即 RBAC 中的 subject，而 kubernetes 也为多个组件内置了 Role 以及 RoleBinding，巧妙的将 Authentication 和 RBAC Authorization 结合到了一起。 查看 kubernetes 中内置的 RBAC： 123$ kubectl get clusterrole$ kubectl get clusterrolebinding 下面是 kubernetes 中核心组件内置的 user 和 group，在为每个组件生成证书时需要在其 CSR 中使用对应的 CN 和 O 字段。 访问 apiserver 的几种方式通过上文可以知道访问 apiserver 时需要通过认证、鉴权以及访问控制三个步骤，认证的方式可以使用 serviceaccounts 和 X509 证书，鉴权的方式使用 RBAC，访问控制若没有特殊需求可以不使用。 serviceaccounts 是 kubernetes 针对 pod 内访问 apiserver 提供的认证方式，那可以用在外部 client 端吗？答案是可以的，serviceaccounts 最终是通过 ca + token 的方式访问的，你只要创建一个 serviceaccounts 并从对应的 secrets 中获取 ca + token 即可访问 apiserver。那使用证书认证的方式可以在 pod 内访问 apiserver 吗？当然也可以，不过创建证书比 serviceaccounts 麻烦，证书默认是用于内置组件访问 apiserver 使用的。不论哪种方式，你都需要为其创建 RBAC 配置。 所以在 TLS +RBAC 模式下，访问 apiserver 目前有两种方式： 使用 serviceaccounts + RBAC ：需要创建 serviceaccounts 以及关联对应的 RBAC(ca + token + RBAC) 使用证书 + RBAC：需要用到 ca、client、client-key 以及关联对应的 RBAC(ca + client-key + client-cert + RBAC) 总结本文主要讲述了 kubernetes 中的认证(Authentication)以及鉴权(Authorization)机制，其复杂性主要体现在部署 kubernetes 集群时组件之间的认证以及在集群中为附加组件配置正确的权限，希望通过本节你可以了解到 kubernetes 中的组件需要哪些权限认证以及如何为相关组件配置正确的权限。 参考： Controlling Access to the Kubernetes API：https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/ admission controllers：https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/ kubelet 配置权限认证：https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/ master-node communication：https://kubernetes.io/docs/concepts/architecture/master-node-communication/ kubernetes 数字证书体系浅析：https://mp.weixin.qq.com/s/iXuDbPKjSc65t_9Y1--bog]]></content>
      <tags>
        <tag>Authentication</tag>
        <tag>Authorization</tag>
        <tag>RBAC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-on-kube-operator 开发(二)]]></title>
    <url>%2F2019%2F08%2F07%2Fkube_on_kube_operator_2%2F</url>
    <content type="text"><![CDATA[本文主要讲述 kubernetes-operator 的开发过程，kubernetes-operator 已经开发了一个多月，其核心功能已经实现，其中的架构以及功能设计主要来自于一些生产环境的经验以及自己从事 kubernetes 运维开发两年多的一些工作经验，如有问题望指正。 kubernetes-operator 组件介绍kubernetes-operator 中主要包含一个自定义的 controller 和一个 HTTP Server，如下图所示，controller 主要是监听 CRD 的变化以及使其达到终态，HTTP Server 提供了多个 RESTful API，用于操作 CRD(创建、删除、扩缩容、接收回调等)。 除此之外还有其他的组件，ansibleinit、precheck、admission-webhook，ansibleinit 是一个二进制文件用来作为容器内的 1 号进程，会调用 ansible 相关的命令以及处理信号、子进程收割等。precheck 主要用于在对集群操作前检查目标宿主机的环境，由于对集群的操作需要耗费数十秒，为了保证成功率需要在部署前检查宿主的环境。admission-webhook 暂时用于校验 CR 中字段，比如集群执行扩容操作时，master 等字段的值肯定是不能改变的。 kubernetes-operator 的开发下面主要讲 kubernetes-operator 中核心组件的开发，主要有以下几步： 定义 CRD 生成代码 开发 controller 开发 RESTful API 定义 CRD下面是 CRD 的定义，kubernetes-operator 中的自定义资源为 KubernetesCluster，项目中简称为 ecs。 123456789101112131415161718192021apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: kubernetesclusters.ecs.yun.comspec: group: ecs.yun.com names: kind: KubernetesCluster listKind: KubernetesClusterList plural: kubernetesclusters singular: kubernetescluster shortNames: - ecs scope: Namespaced subresources: status: &#123;&#125; version: v1 versions: - name: v1 served: true storage: true 将 CRD 部署到 kubernetes 集群中，CRD 中的自定义资源KubernetesCluster(CR) 就成为了 kubernetes 中的一种资源，和 pod、deployment 等类似。 生成代码生成代码可以参考上一篇文章使用 code-generator 为 CustomResources 生成代码，此处不再详解。 开发 controller如下所示是 controller 最简单的一个声明： 12345for &#123; desired := getDesiredState() current := getCurrentState() makeChanges(desired, current)&#125; 所有 controller 也都是以此进行演变的，controller 的代码模式或者套路可以参考sample-controller 或者 kube-controller-manager 中所有 controller 的实现。 下面是 kubernetes-operator 中 controller 实现的一个流程图： 更新 CR 都是客户端的操作，所以在设计时客户端都是操作 annotation 中的字段，然后 operator 监听到相关的时间后会进行处理。例如，当用户要创建一个集群时，首先客户端将 app.kubernetes.io/operation设置为 creating，此时 operator watch 到 CR 变化后会处理新建集群的操作，operator 会创建一个用来部署集群的 job，以及创建 configmap 来保存本次的操作记录以及关联对应的 job，也能用来查询本次操作的日志，然后会更新 CR 中 status.phase 中的 Creating (新创建的 CR status.phase 为 “”)，接下来为 CR 设置 finalizers，最后会启动一个 goroutine 检测 job 的状态。此时需要等待 job 的完成以及回调，若 job 失败或者超时都会被最后启动的 goroutine 检测到，job 成功与否都会触发更新 CR status.phase 的操作。若 job 执行完成成功回调，客户端会更新 app.kubernetes.io/operation为 create-finished，客户端更新完成后会触发一次事件，然后 operator 会将 status.phase 更新为 Running 状态，否则 job 异常 operator 会直接更新 status.phase 为 Failed。 关于 CR 中 app.kubernetes.io/operation 字段以及 status.phase 中所有的定义请参见 kubernetes-operator/pkg/enum/task.go。 开发 RESTful API在前后端分离的场景中，RESTful API 的开发仅需要一个 route 框架即可，kubernetes-operator 中用的是 mux，具体的代码在 kubernetes-operator/pkg/server 下。 总结本文主要讲述了 kubernetes-operator 中主要的模块以及 controller 的具体实现，其中许多细节暂未提及到，详细的实现请参考代码，该项目只是笔者利用业余时间进行开发的，毕竟个人精力有限，笔者当前主要集中在 kube-on-kube 的开发上，在阅读文章或者代码的过程中如有问题可以随时留言，笔者会持续迭代版本。下一篇文章会讲述如何使用二进制文件部署 kubernetes 集群。 参考： https://github.com/kubernetes/community/blob/8decfe4/contributors/devel/controllers.md https://github.com/kubernetes/sample-controller https://engineering.bitnami.com/articles/a-deep-dive-into-kubernetes-controllers.html https://www.cnblogs.com/gaorong/p/8854934.html https://yucs.github.io/2017/12/21/2017-12-21-operator/]]></content>
      <tags>
        <tag>operator</tag>
        <tag>kube-on-kube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 code-generator 为 CustomResources 生成代码]]></title>
    <url>%2F2019%2F08%2F06%2Fcode_generator%2F</url>
    <content type="text"><![CDATA[kubernetes 项目中有相当一部分代码是自动生成的，主要是 API 的定义和调用方法，kubernetes 项目下 k8s.io/kubernetes/hack/ 目录中以 update 开头的大部分脚本都是用来生成代码的。code-generator 是官方提供的代码生成工具，在实现自定义 controller 的时候需要用到 CRD，也需要使用该工具生成对 CRD 操作的代码。 要生成哪些代码在自定义 controller 时需要用到 typed clientsets，informers，listers 和 deep-copy 等函数，这些函数都可以使用 code-generator 来生成，具体的作用可以参考：kubernetes 中 informer 的使用。 code-generator 里面包含多个生成代码的工具，下面是需要用到的几个： deepcopy-gen：为每种类型T生成方法： func (t* T) DeepCopy() *T，CustomResources 必须实现runtime.Object 接口且要有 DeepCopy 方法 client-gen：为 CustomResource APIGroups 生成 typed clientsets informer-gen：为 CustomResources 创建 informers，用来 watch 对应 CRD 所触发的事件，以便对 CustomResources 的变化进行对应的处理 lister-gen：为 CustomResources 创建 listers，用来对 GET/List 请求提供只读的缓存层 除了上面几个工具外，code-generator 中还提供了 conversion-gen、defaulter-gen、register-gen、set-gen，这些生成器可以应用在其他场景，比如构建聚合 API 服务时会用到一些内部的类型，conversion-gen 会为这些内部和外部类型之间创建转换函数，defaulter-gen 会处理某些字段的默认值。 代码生成步骤使用 code-generator 生成代码还需要以下几步： 创建指定的目录格式 在代码中使用 tag 标注要生成哪些代码 首先要创建指定的目录格式，目录的格式可以参考官方提供的示例项目：sample-controller，下文也会讲到，目录中需要包含对应 CustomResources 的定义以及 group 和 version 信息。 其次要在在代码中使用 tag 标注要生成哪些代码，tag 有两钟类型，全局的和局部的，所有类型的 deepcopy tag 会默认启用，更多关于 tag 的使用方法可以参考：Kubernetes Deep Dive: Code Generation for CustomResources，也可以参考官方的示例 code-generator/_example 。 开始生成代码本文以该 CRD 为例子进行演示，group 为ecs.yun.com ，version 为 v1： 123456789101112131415161718192021apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: kubernetesclusters.ecs.yun.comspec: group: ecs.yun.com names: kind: KubernetesCluster listKind: KubernetesClusterList plural: kubernetesclusters singular: kubernetescluster shortNames: - ecs scope: Namespaced subresources: status: &#123;&#125; version: v1 versions: - name: v1 served: true storage: true 创建指定目录结构 pkg/apis/${group}/${version}，group 可以定义一个 shortNames，也就是 CRD 中的 shortNames 1$ mkdir -pv pkg/apis/ecs/v1 创建 doc.go：123456$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/doc.go// Package v1 contains API Schema definitions for the ecs v1 API group// +k8s:deepcopy-gen=package,register// +groupName=ecs.yun.compackage v1EOF 创建 register.go： 123456789101112131415161718192021222324252627282930313233343536373839$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/register.gopackage v1import ( &quot;github.com/gosoon/kubernetes-operator/pkg/apis/ecs&quot; metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot; &quot;k8s.io/apimachinery/pkg/runtime&quot; &quot;k8s.io/apimachinery/pkg/runtime/schema&quot;)// SchemeGroupVersion is group version used to register these objectsvar SchemeGroupVersion = schema.GroupVersion&#123;Group: ecs.GroupName, Version: &quot;v1&quot;&#125;// Kind takes an unqualified kind and returns back a Group qualified GroupKindfunc Kind(kind string) schema.GroupKind &#123; return SchemeGroupVersion.WithKind(kind).GroupKind()&#125;// Resource takes an unqualified resource and returns a Group qualified GroupResourcefunc Resource(resource string) schema.GroupResource &#123; return SchemeGroupVersion.WithResource(resource).GroupResource()&#125;var ( SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) AddToScheme = SchemeBuilder.AddToScheme)// Adds the list of known types to Scheme.func addKnownTypes(scheme *runtime.Scheme) error &#123; scheme.AddKnownTypes(SchemeGroupVersion, &amp;KubernetesCluster&#123;&#125;, &amp;KubernetesClusterList&#123;&#125;, ) metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil&#125;EOF 创建 types.go，该文件中会定义多个 tag 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475$ cat &lt;&lt; EOF &gt; pkg/apis/ecs/v1/types.gopackage v1import ( metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;)// +genclient// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// KubernetesCluster is the Schema for the kubernetesclusters APItype KubernetesCluster struct &#123; metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;` Spec KubernetesClusterSpec `json:&quot;spec,omitempty&quot;` Status KubernetesClusterStatus `json:&quot;status,omitempty&quot;`&#125;// KubernetesClusterSpec defines the desired state of KubernetesClustertype KubernetesClusterSpec struct &#123; // Add custom validation using kubebuilder tags: // https://book.kubebuilder.io/beyond_basics/generating_crd.html TimeoutMins string `json:&quot;timeout_mins,omitempty&quot;` ClusterType string `json:&quot;clusterType,omitempty&quot;` ContainerCIDR string `json:&quot;containerCIDR,omitempty&quot;` ServiceCIDR string `json:&quot;serviceCIDR,omitempty&quot;` MasterList []Node `json:&quot;masterList&quot; tag:&quot;required&quot;` MasterVIP string `json:&quot;masterVIP,omitempty&quot;` NodeList []Node `json:&quot;nodeList&quot; tag:&quot;required&quot;` EtcdList []Node `json:&quot;etcdList,omitempty&quot;` Region string `json:&quot;region,omitempty&quot;` AuthConfig AuthConfig `json:&quot;authConfig,omitempty&quot;`&#125;// AuthConfig defines the nodes peer authenticationtype AuthConfig struct &#123; Username string `json:&quot;username,omitempty&quot;` Password string `json:&quot;password,omitempty&quot;` PrivateSSHKey string `json:&quot;privateSSHKey,omitempty&quot;`&#125;// KubernetesClusterStatus defines the observed state of KubernetesClustertype KubernetesClusterStatus struct &#123; // Add custom validation using kubebuilder tags: https://book.kubebuilder.io/beyond_basics/generating_crd.html Phase KubernetesOperatorPhase `json:&quot;phase,omitempty&quot;` // when job failed callback or job timeout used Reason string `json:&quot;reason,omitempty&quot;` // JobName is store each job name JobName string `json:&quot;jobName,omitempty&quot;` // Last time the condition transitioned from one status to another. LastTransitionTime metav1.Time `json:&quot;lastTransitionTime,omitempty&quot;`&#125;// +genclient:nonNamespaced// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// KubernetesClusterList contains a list of KubernetesClustertype KubernetesClusterList struct &#123; metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ListMeta `json:&quot;metadata,omitempty&quot;` Items []KubernetesCluster `json:&quot;items&quot;`&#125;// users// &quot;None,Creating,Running,Failed,Scaling&quot;type KubernetesOperatorPhase stringtype Node struct &#123; IP string `json:&quot;ip,omitempty&quot;`&#125;EOF 执行命令生成代码： 1$ $GOPATH/src/k8s.io/code-generator/generate-groups.sh all github.com/gosoon/kubernetes-operator/pkg/client github.com/gosoon/kubernetes-operator/pkg/apis ecs:v1 generate-groups.sh 需要四个参数： 第一个 参数：all，也就是要生成所有的模块，clientset，informers，listers 第二个参数：github.com/gosoon/test/pkg/client 这个是你要生成代码的目录，目录的名称一般定义为 client 第三个参数：github.com/gosoon/test/pkg/apis 这个目录是已经创建好的源目录 第四个参数：”ecs:v1” 是 group 和 version 信息，ecs 是 apis 下的目录，v1 是 ecs 下面的目录 生成的代码如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950.└── pkg ├── apis │ └── ecs │ └── v1 │ ├── doc.go │ ├── register.go │ ├── types.go │ └── zz_generated.deepcopy.go └── client ├── clientset │ └── versioned │ ├── clientset.go │ ├── doc.go │ ├── fake │ │ ├── clientset_generated.go │ │ ├── doc.go │ │ └── register.go │ ├── scheme │ │ ├── doc.go │ │ └── register.go │ └── typed │ └── ecs │ └── v1 │ ├── doc.go │ ├── ecs_client.go │ ├── fake │ │ ├── doc.go │ │ ├── fake_ecs_client.go │ │ └── fake_kubernetescluster.go │ ├── generated_expansion.go │ └── kubernetescluster.go ├── informers │ └── externalversions │ ├── ecs │ │ ├── interface.go │ │ └── v1 │ │ ├── interface.go │ │ └── kubernetescluster.go │ ├── factory.go │ ├── generic.go │ └── internalinterfaces │ └── factory_interfaces.go └── listers └── ecs └── v1 ├── expansion_generated.go └── kubernetescluster.go21 directories, 26 files CRD 以及生成的代码见：kubernetes-operator。 总结本问讲述了如何使用 code-generator 生成代码，要使用自定义 controller 代码生成是最开始的一步，下文会继续讲述自定义 controller 的详细步骤，感兴趣的可以关注笔者 github 的项目 kubernetes-operator。 参考： https://github.com/kubernetes/sample-controller https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/ https://hexo.do1618.com/2018/04/04/Kubernetes-Deep-Dive-Code-Generation-for-CustomResources/]]></content>
      <tags>
        <tag>code-generator</tag>
        <tag>crd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kube-on-kube-operator 开发(一)]]></title>
    <url>%2F2019%2F08%2F05%2Fkube_on_kube_operator_1%2F</url>
    <content type="text"><![CDATA[kubernetes 已经成为容器时代的分布式操作系统内核，目前也是所有公有云提供商的标配，在国内，阿里云、腾讯云、华为云这样的公有云大厂商都支持一键部署 kubernetes 集群，而 kubernetes 集群自动化管理则是迫切需要解决的问题。对于大部分不熟悉 kubernetes 而要上云的小白用户就强烈需要一个被托管及能自动化运维的集群，他们平时只是进行业务的部署与变更，只需要对 kubernetes 中部分概念了解即可。同样在私有云场景下，笔者所待过的几个大小公司一般都会维护多套集群，集群的运维工作就是一个很大的挑战，反观各大厂同样要有效可靠的管理大规模集群，kube-on-kube-operator 是一个很好的解决方案。 所谓 kube-on-kube-operator，就是将 kubernetes 运行在 kubernetes 上，用 kubernetes 托管 kubernetes 方式来自动化管理集群。和所有 operator 的功能类似，系统会定时检测集群当前状态，判断是否与目标状态一致，出现不一致时，operator 会发起一系列操作，驱动集群达到目标状态。今年 kubeCon 上，雅虎日本也分享了其管理大规模 kubernetes 集群的方法，4000 节点构建了 400 个 kubernetes 集群，同样采用的是 kube-on-kube-operator 架构，以 kubernetes as a service 的形式使用。 kubernetes-operator 设计参考记得 kube-on-kube-operator 的概念最初是在去年的 kubeCon China 上蚂蚁金服提出来的，先看看蚂蚁金服以及腾讯云 kube-on-kube-operator 的设计思路，其实腾讯云的架构和蚂蚁金服的是类似的。以下是蚂蚁金服的架构设计图： 首先部署一套 kubernetes 元集群，通过元集群部署业务集群，业务集群的 master 组件都分布在一台宿主上，该宿主以 node 节点的方式挂载在元集群中。在私有云场景下，这样的部署方式有一个很明显的问题就是元集群节点跨机房，虽然业务集群的 master 与 node 都是在同一个机房，但是元集群中的 node 节点大部分是分布在不同的机房，有些公司在不同机房之间会有网络上的限制，有可能网络不通或者只能使用专线连接。在公有云场景下，元集群自己有一套独立的 vpc 网络，它要怎么和用户的 vpc 结点进行通信呢？腾讯云的做法是利用 vpc 提供的弹性网卡能力，将这个弹性网卡直接绑定到运行 apiserver 的 pod 中，运行 master 的这个pod 既加入了元集群的 vpc，又加入了用户的 vpc，也就是一个 pod 同时在两个网络中，这样就可以很好的去实现和用户 node 相关的互通。这种方式都是通过 kubernetes API 去管理 master 组件的，master 组件的升级以及故障自愈都可以通过 kubernetes 提供的方式实现。 kubernetes-operator 设计 kubernetes-operator 项目地址：https://github.com/gosoon/kubernetes-operator 目前该项目的主要目标是实现以下三种场景中的集群管理： kube-on-kube kube-to-kube kube-to-cloud-kube kubernetes-operator 不仅是要实现 kube-on-kube 架构，还有 kube-to-kube，kube-to-cloud-kube，kube-to-kube 即 kubernetes 集群管理业务独立的 kubernetes 集群，两个集群相互独立。kube-to-cloud-kube 即 kubernetes 集群管理多云环境上的 kubernetes 集群。 上面是项目的架构图，红色的线段表示对集群生命周期管理的一个操作，涉及集群的创建、删除、扩缩容、升级等，蓝色线段是对集群应用的操作，集群中应用的创建、删除、发布更新等，kubernetes-proxy 是一个 API 代理，所有涉及 API 的调用都要通过 kubernetes-proxy。左边部署有 kubernetes-operator 的是元集群，kubernetes-operator 使用 etcd 仅存储部分配置信息，其管理业务集群的生命周期，支持三种集群的创建方式，第一种方式就是可以创建出类似蚂蚁金服这种直接将业务集群 master 运行在元集群，node 节点在业务集群，第二种是以二进制方式创建业务集群，其中业务集群的 master 以及 node 都是在业务集群所在的机房，第三种方式就是在各种公有云厂商创建集群，以一种统一的方式管理公有云上的集群，也可以称作融合云。 项目结构总体来说，项目暂时分为三大块： kubernetes proxy：支持 API 透传、访问控制等功能； 控制器：也就是 kubernetes-operator，管理业务集群的生命周期； 集群部署模块：用来部署业务集群，目前主要在开发第二种方式使用二进制部署业务集群； kubernetes 应用安装模块：在新建完成的集群中部署监控、日志采集、镜像仓库、helm 等组件； 控制器控制器也就是 Operator + CR，目前开发 operator 的方式已知的有三种： 自定义 controller 的方式：kube-controller-manager 中所有的 controller 就是以自定义 controller 的方式，这种方式是最原生的方式，需要开发者了解 kubernetes 中的代码生成，informer 的使用等。 operator-sdk 的方式：一个开发 Operator 的框架，对于一些不熟悉 kubernetes 的可以使用 operator-sdk 的方式，这种方式让开发者更注重业务的实现，但是不够灵活。 kubebuilder 的方式：kubebuilder 是开发 controller manager 的框架，controller manager 会管理一个或者多个 operator。 kubebuilder, operator-sdk 都是对controller-runtime做了封装, controller runtime又是对client-go shardInfromer 做的封装，本质上其实都一样的。kubernetes-operator 使用的是自定义 controller 的方式，如果想要更深入的学习 kubernetes，非自定义 controller 方式莫属了，kube-controller-manager 组件中的各种 controller 都是使用这种方式开发的，完全可以按照官方这种套路来开发。在 kubernetes 中，目前有两种方式可以定义一个新对象，一是 CustomResourceDefinition（CRD）、二是 Aggregation ApiServer（AA），其中 CRD 是相对简单也是目前应用比较广的方法。kubernetes-operator 采用 CRD 的方式。 集群部署其实项目中最难的是集群部署这一部分，部署集群目前有两种方式，二进制部署和容器化部署，但是都有一些开源工具的支持。手动部署一个二进制集群需要熟悉 docker 的部署、etcd 的部署、角色证书的创建、RBAC 授权、网络配置、yaml 文件编写、kubernetes 集群运维等等，总之手动部署一个二进制集群是非常麻烦的，但是要真正会用 kubernetes 是逃不了部署这一步的。第二种方式就是以容器化的方式部署，这种部署方式相对来说比较简单，有现成的工具直接傻瓜式操作就能部署成功。但是我目前选择的是使用二进制的部署方式，由于自己运维过二进制的 kubernetes 集群，对于私有云场景一般都是直接将集群部署在物理机上，作为生产环境，自己认为容器化的方式部署还不是非常成熟的，目前工作过的大小公司中，生产环境暂时没有以容器化的方式运行集群。所以 kubernetes-operator 中目前主要支持的就是使用二进制部署集群。 目前比较成熟的用于生产环境的 kubernetes 集群部署工具有：kubeadm、kubespary、kops、rancher、kubeasz 等。kubeadm、kubespary、kops 都是官方开源的产品，kubeadm 使用容器化的方式部署，需要手动执行一些部署命令，暂时无法完全自动化部署。kubespary 是对 kubeadm 的一层封装，使用 ansible + kubeadm 的方式自动化进行部署，据说阿里云就是使用 kubespary 部署集群的。在公有云的环境(GCP、AWS)通常使用 kops 部署起来更方便些。kubeasz 是使用 ansible 自动化的方式部署二进制集群，目前也已经比较成熟了。 应用安装 监控：当然是使用 promethus； 日志采集：使用 filebeat 或者基于 filebeat 封装的一些组件如 logpilot，其他的还有 logkit 等都可以尝试使用； 镜像仓库：当然是使用 harbor； HPA：组件以及应用的自动扩缩容； 应用安装使用 helm 的方式进行安装。 集群升级若以二进制部署最好是替换二进制文件的方式进行升级，若使用容器化部署，master 部署在元集群中可以使用 kubernetes 的滚动方式升级否则要以修改 manifest 文件的方式。 集群升级包括配置和版本的升级，集群部署完成后，master 的配置改动不会很频繁，由于要进行性能上的优化以及业务的支持，对于 node 组件上的配置升级还是比较多的。对于集群的版本升级，升级的难度系数随着版本的跨度增大而增大，若按照官方的升级流程，一般不会出现异常。升级操作一般都是先升 master 再升 node，在工作中经历的几次版本升级中，每次升级完 master 后理论上不会再回退了，除非升级过程中有问题，否则升级完成后已经很难回退了，master 升级完成后 APIServer 的一些 API 还有 pod 的字段都有可能改变，master 版本回退后一些已存在的应用可能会异常，或者还可以参考 openshift 的蓝绿升级方式。二进制部署的集群尽量以替换二进制文件的方式进行升级，对于容器化部署的集群，可以直接使用 kubernetes 的滚动方式升级或者是修改 manifest 文件的方式。 目前蚂蚁金服 kube-on-kube-operator 架构中在业务集群中会部署一个 node-operator，node-operator 会记录 master 组件的镜像、默认启动参数等信息，其作用就是节点配置管理、集群组件升级以及节点故障自愈，未来在项目中也会实现基于此的方式。 后期计划 支持部署 k3s、kubeedge：5G 时代，边缘计算将是非常火的，目前各大厂商也都在此布局，所以支持部署 k3s、kubeedge 这些专门支持边缘计算的产品还是非常有必要的。 支持使用 kops 部署 支持部署多版本 k8s node-operator 开发，支持集群的配置管理、自动化升级、故障自愈等功能 用户及权限管理：操作集群用户的权限和 kubernetes 中 RBAC 规则绑定 Kubernetes-operator 一些功能的扩展和完善 参考： 腾讯云容器服务TKE：一键部署实践 一年时间打造全球最大规模之一的Kubernetes集群，蚂蚁金服怎么做到的？ https://github.com/gosoon/kubernetes-operator]]></content>
      <tags>
        <tag>operator</tag>
        <tag>kube-on-kube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署高可用 kubernetes 集群]]></title>
    <url>%2F2019%2F07%2F12%2Fk8s_components_ha%2F</url>
    <content type="text"><![CDATA[kubernetes 虽然具有故障自愈和容错能力，但某些组件的异常会导致整个集群不可用，生产环境中将其部署为高可用还是非常有必要的，本文会介绍如何构建一个高可用的 Kubernetes 集群。kuber-controller-manager 和 kube-scheduler 的高可用官方已经实现了，都是通过 etcd 全局锁进行选举实现的，etcd 是一个分布式，强一致的（满足 CAP 的 CP）KV 存储系统，其天然具备高可用。而 apiserver 作为整个系统的核心，所有对数据的修改操作都是通过 apiserver 间接操作 etcd 的，所以 apiserver 的高可用实现是比较关键的。 kube-apiserver 的高可用配置apiserver 本身是无状态的，可以横向扩展，其借助外部负载均衡软件配置高可用也相对容易，实现方案比较多，但一般会采用外部组件 LVS 或 HAProxy 的方式实现，我们生产环境是通过 LVS 实现的。apiserver 的高可用可以分为集群外高可用和集群内高可用。集群外高可用指对于直接调用 k8s API 的外部用户（例如 kubectl 、kubelet），客户端需要调用 apiserver 的 VIP 以达到高可用，此处 LVS 的部署以及 VIP 的配置不再详细说明。 集群内的高可用配置是指对于部署到集群中的 pod 访问 kubernetes，kubernetes 集群创建完成后默认会启动一个kubernetes的 service 供集群内的 pod 访问，service 的 ClusterIP 默认值为 172.0.0.1 ，每一个 service 对象生成时，都会生成一个用于暴露该对象后端对应 pod 的对象 endpoints，endpoints 中可以看到 apiserver 的实例。访问 kubernetes 的 service，service 会将请求转发到 endpoints 中的 ip 上，此时若 service 中的 endpoints 中没有 IP，则表示 apiserver 无法访问。 1234567$ kubectl get svc kubernetesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 172.0.0.1 &lt;none&gt; 443/TCP 21d$ kubectl get endpoints kubernetesNAME ENDPOINTS AGEkubernetes 10.0.2.15:6443, 10.0.2.16:6443 21d kubernetes v1.9 之前 kube-apiserver service 的高可用也就是 master ip 要加入到 kubernetes service 的 endpoints 中必须要在参数中指定 --apiserver-count 的值，v1.9 出现了另外一个参数 --endpoint-reconciler-type 要取代以前的 --apiserver-count，但是此时该参数默认是禁用的（Alpha 版本），v1.10 也是默认禁用的。v1.11 中 --endpoint-reconciler-type 参数默认开启了，默认值是 lease。--apiserver-count 参数会在 v1.13 中被移除。v1.11 和 v1.12 中还可以使用 --apiserver-count，但前提是需要设置 --endpoint-reconciler-type=master-count。也就是说在 v1.11 以及之后的版本中 apiserver 中不需要进行配置了，启用了几个 apiserver 实例默认都会加到 对应的 endpoints 中。 kube-controller-manager 和 kube-scheduler 的高可用配置kube-controller-manager 和 kube-scheduler 是由 leader election 实现高可用的，通过向 apiserver 中的 endpoint 加锁的方式来进行 leader election， 启用 leader election 需要在组件的配置中加入以下几个参数： 12345--leader-elect=true--leader-elect-lease-duration=15s--leader-elect-renew-deadline=10s--leader-elect-resource-lock=endpoints--leader-elect-retry-period=2s 组件当前的 leader 会写在 endpoints 的 holderIdentity 字段中， 使用以下命令查看组件当前的 leader: 123$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml $ kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml 关于 kube-controller-manager 和 kube-scheduler 高可用的实现细节可以参考之前写的一篇文章：kubernets 中组件高可用的实现方式。 etcd 的高可用配置etcd 是一个分布式集群，也是一个有状态的服务，其天生就是高可用的架构。为了防止 etcd 脑裂，其组成 etcd 集群的个数一般为奇数个(3 或 5 个节点) 。若使用物理机搭建 k8s 集群，理论上集群的规模也会比较大，此时 etcd 也应该使用 3 个或者5 个节点部署一套独立运行的集群。若想要对 etcd 做到自动化运维，可以考虑使用 etcd-operator 将 etcd 集群部署在 k8s 中。 kubernetes 中组件高可用部署的一个架构图： 总结本文主要介绍如何配置一个高可用 kubernetes 集群，kubernetes 新版本已经越来越趋近全面 TLS + RBAC 配置，若 kubernetes 集群还在使用 8080 端口，此时每个 master 节点上的 kube-controller-manager 和 kube-scheduler 都是通过 8080 端口连接 apiserver，若节点上的 apiserver 挂掉，则 kube-controller-manager 和 kube-scheduler 也会随之挂掉。apiserver 作为集群的核心组件，其必须高可用部署，其他组件实现高可用相对容易。 参考： https://k8smeetup.github.io/docs/admin/high-availability/]]></content>
      <tags>
        <tag>kubernetes</tag>
        <tag>HA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 自定义资源（CRD）的校验]]></title>
    <url>%2F2019%2F07%2F02%2Fk8s_crd_verify%2F</url>
    <content type="text"><![CDATA[在以前的版本若要对 apiserver 的请求做一些访问控制，必须修改 apiserver 的源代码然后重新编译部署，非常麻烦也不灵活，apiserver 也支持一些动态的准入控制器，在 apiserver 配置中看到的ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota 等都是 apiserver 的准入控制器，但这些都是 kubernetes 中默认内置的。在 v1.9 中，kubernetes 的动态准入控制器功能中支持了 Admission Webhooks，即用户可以以插件的方式对 apiserver 的请求做一些访问控制，要使用该功能需要自己写一个 admission webhook，apiserver 会在请求通过认证和授权之后、对象被持久化之前拦截该请求，然后调用 webhook 已达到准入控制，比如 Istio 中 sidecar 的注入就是通过这种方式实现的，在创建 Pod 阶段 apiserver 会回调 webhook 然后将 Sidecar 代理注入至用户 Pod。 本文主要介绍如何使用 AdmissionWebhook 对 CR 的校验，一般在开发 operator 过程中，都是通过对 CR 的操作实现某个功能的，若 CR 不规范可能会导致某些问题，所以对提交 CR 的校验是不可避免的一个步骤。 kubernetes 目前提供了两种方式来对 CR 的校验，语法校验(OpenAPI v3 schema） 和语义校验(validatingadmissionwebhook）。 CRD 的一个示例： 1234567891011121314151617181920212223242526272829apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt; name: kubernetesclusters.ecs.yun.comspec: # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt; group: ecs.yun.com # list of versions supported by this CustomResourceDefinition versions: - name: v1 # Each version can be enabled/disabled by Served flag. served: true # One and only one version must be marked as the storage version. storage: true # either Namespaced or Cluster scope: Namespaced names: # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt; plural: kubernetesclusters # singular name to be used as an alias on the CLI and for display singular: kubernetescluster # kind is normally the CamelCased singular type. Your resource manifests use this. kind: KubernetesCluster # listKind listKind: KubernetesClusterList # shortNames allow shorter string to match your resource on the CLI shortNames: - ecs CRD 的一个对象： 1234567891011121314apiVersion: ecs.yun.com/v1kind: KubernetesClustermetadata: name: test-clusterspec: clusterType: kubernetes serviceCIDR: &apos;&apos; masterList: - ip: 192.168.1.10 nodeList: - ip: 192.168.1.11 privateSSHKey: &apos;&apos; scaleUp: 0 scaleDown: 0 一、OpenAPI v3 schemaOpenAPI 是针对 REST API 的 API 描述格式，也是一种规范。 1234567891011121314151617181920212223242526272829303132333435apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: kubernetesclusters.ecs.yun.comspec: group: ecs.yun.com versions: - name: v1 served: true storage: true scope: Namespaced names: plural: kubernetesclusters singular: kubernetescluster kind: KubernetesCluster listKind: KubernetesClusterList shortNames: - ecs validation: openAPIV3Schema: properties: spec: type: object required: - clusterType - masterList - nodeList properties: clusterType: type: string scaleUp: type: integer scaleDown: type: integer minimum: 0 上面是使用 OpenAPI v3 检验的一个例子，OpenAPI v3 仅支持一些简单的校验规则，可以校验参数的类型，参数值的类型(支持正则)，是否为必要参数等，但若要使用与、或、非等操作对多个字段同时校验还是做不到的，所以针对一些特定场景的校验需要使用 admission webhook。 二、Admission Webhooksadmission control 在 apiserver 中进行配置的，使用--enable-admission-plugins 或 --admission-control进行启用，admission control 配置的控制器列表是有顺序的，越靠前的越先执行，一旦某个控制器返回的结果是reject 的，那么整个准入控制阶段立刻结束，所以这里的配置顺序是有序的，建议使用官方的顺序配置。 在 v1.9 中，admission webhook 是通过在 --admission-control 中配置 ValidatingAdmissionWebhook 或 MutatingAdmissionWebhook 来支持使用的，两者区别如下： MutatingAdmissionWebhook：允许在 webhook 中对 object 进行 mutate 修改，但匹配到的 webhook 串行执行，因为每个 webhook 都可能会 mutate object。 ValidatingAdmissionWebhook: 不允许在 webhook 中对 Object 进行 mutate 修改，仅返回 true 或 false。 启用 admission webhook 后，每次对 CR 做 CRUD 操作时，请求就会被 apiserver 拦住，至于 CRUD 中哪些请求被拦住都是提前在 WebhookConfiguration 中配置的，然后会调用 AdmissionWebhook 进行检查是否 Admit 通过。 三、启用 Admission Webhooks 功能 kubernetes 版本 &gt;= v1.9 1、在 apiserver 中开启 admission webhooks 在 v1.9 版本中使用的是： 1--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota 在 v1.10 以后会弃用 --admission-control，取而代之的是 --enable-admission-plugins： 1--enable-admission-plugins=NodeRestriction,NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota 启用之后在 api-resources 可以看到： 123# kubectl api-resources | grep admissionregistrationmutatingwebhookconfigurations admissionregistration.k8s.io false MutatingWebhookConfigurationvalidatingwebhookconfigurations admissionregistration.k8s.io false ValidatingWebhookConfiguration 2、启用 admissionregistration.k8s.io/v1alpha1 API 12// 检查 API 是否已启用$ kubectl api-versions | grep admissionregistration.k8s.io 若不存在则需要在 apiserver 的配置中添加--runtime-config=admissionregistration.k8s.io/v1alpha1。 四、编写 Admission Webhook Serverwebhook 其实就是一个 RESTful API 里面加上自己的一些校验逻辑。 可以参考官方的示例：https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/images/webhook/main.go或者https://github.com/kubernetes/kubernetes/blob/v1.13.0/test/e2e/apimachinery/webhook.go 完整代码参考：https://github.com/gosoon/admission-webhook 五、部署 Admission Webhook Service由于 apiserver 调用 webhook 时强制使用 TLS 认证，所以 WebhookConfiguration 中一定要配置 caBundle，也就是需要自己生成一套私有证书。 生成证书的方式比较多，以下使用 openssl 生成，脚本如下所示： 1234567891011121314#!/bin/bash# Generate the CA cert and private keyopenssl req -nodes -new -x509 -days 365 -keyout ca.key -out ca.crt -subj &quot;/CN=admission-webhook CA&quot;# Generate the private key for the webhook serveropenssl genrsa -out admission-webhook-tls.key 2048# Generate a Certificate Signing Request (CSR) for the private key, and sign it with the private key of the CA.openssl req -new -key admission-webhook-tls.key -subj &quot;/CN=admission-webhook.ecs-system.svc&quot; \ | openssl x509 -days 365 -req -CA ca.crt -CAkey ca.key -CAcreateserial -out admission-webhook-tls.crt# Generate pemopenssl base64 -A &lt; ca.crt &gt; ca.pem 生成证书后将 ca.pem 中的内容复制到 caBundle 处。 ValidatingWebhook yaml 文件如下： 1234567891011121314151617181920212223apiVersion: admissionregistration.k8s.io/v1beta1kind: ValidatingWebhookConfigurationmetadata: name: admission-webhookwebhooks: - name: admission-webhook.ecs-system.svc # 必须为 &lt;svc_name&gt;.&lt;svc_namespace&gt;.svc. failurePolicy: Ignore clientConfig: service: name: admission-webhook namespace: ecs-system path: /ecs/operator/cluster # webhook controller caBundle: xxx rules: - operations: # 需要校验的方法 - CREATE - UPDATE apiGroups: # api group - ecs.yun.com apiVersions: # version - v1 resources: # resource - kubernetesclusters 注意 failurePolicy 可以为 Ignore或者Fail，意味着如果和 webhook 通信出现问题导致调用失败，将根据 failurePolicy决定忽略失败（admit）还是准入失败(reject)。 最后将 webhook 部署在集群中。 参考：https://github.com/gosoon/admission-webhookhttps://banzaicloud.com/blog/k8s-admission-webhooks/http://blog.fatedier.com/2019/03/20/k8s-crd/https://my.oschina.net/jxcdwangtao/blog/1591681https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#is-there-a-recommended-set-of-admission-controllers-to-usehttps://istio.io/zh/help/ops/setup/validation/https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/]]></content>
      <tags>
        <tag>crd</tag>
        <tag>admission control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Go Modules 管理依赖]]></title>
    <url>%2F2019%2F06%2F22%2Fgolang_modules%2F</url>
    <content type="text"><![CDATA[Go Modules 是 Go 语言的一种依赖管理方式，该 feature 是在 Go 1.11 版本中出现的，由于最近在做的项目中，团队都开始使用 go module 来替代以前的 Godep，Kubernetes 也从 v1.15 开始采用 go module 来进行包管理，所以有必要了解一下 go module。go module 相比于原来的 Godep，go module 在打包、编译等多个环节上有着明显的速度优势，并且能够在任意操作系统上方便的复现依赖包，更重要的是 go module 本身的设计使得自身被其他项目引用变得更加容易，这也是 Kubernetes 项目向框架化演进的又一个重要体现。 使用 go module 管理依赖后会在项目根目录下生成两个文件 go.mod 和 go.sum。 go.mod 中会记录当前项目的所依赖，文件格式如下所示： 123456789module github.com/gosoon/audit-webhookgo 1.12require ( github.com/elastic/go-elasticsearch v0.0.0 github.com/gorilla/mux v1.7.2 github.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81) go.sum记录每个依赖库的版本和哈希值，文件格式如下所示： 123456github.com/elastic/go-elasticsearch v0.0.0 h1:Pd5fqOuBxKxv83b0+xOAJDAkziWYwFinWnBO0y+TZaA=github.com/elastic/go-elasticsearch v0.0.0/go.mod h1:TkBSJBuTyFdBnrNqoPc54FN0vKf5c04IdM4zuStJ7xg=github.com/gorilla/mux v1.7.2 h1:zoNxOV7WjqXptQOVngLmcSQgXmgk4NMz1HibBchjl/I=github.com/gorilla/mux v1.7.2/go.mod h1:1lud6UwP+6orDFRuTfBEV8e9/aOM/c4fVVCaMa2zaAs=github.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81 h1:JP0LU0ajeawW2xySrbhDqtSUfVWohZ505Q4LXo+hCmg=github.com/gosoon/glog v0.0.0-20180521124921-a5fbfb162a81/go.mod h1:1e0N9vBl2wPF6qYa+JCRNIZnhxSkXkOJfD2iFw3eOfg= 一、如何启用 go module 功能(1) go 版本 &gt;= v1.11 (2) 设置GO111MODULE环境变量 要使用go module 首先要设置GO111MODULE=on，GO111MODULE 有三个值，off、on、auto，off 和 on 即关闭和开启，auto 则会根据当前目录下是否有 go.mod 文件来判断是否使用 modules 功能。无论使用哪种模式，module 功能默认不在 GOPATH 目录下查找依赖文件，所以使用 modules 功能时请设置好代理。 在使用 go module 时，将 GO111MODULE 全局环境变量设置为 off，在需要使用的时候再开启，避免在已有项目中意外引入 go module。 123$ echo export GO111MODULE=off &gt;&gt; ~/.zshrcor$ echo export GO111MODULE=off &gt;&gt; ~/.bashrc go mod 命令的使用： 12345678download download modules to local cache (下载依赖的module到本地cache))edit edit go.mod from tools or scripts (编辑go.mod文件)graph print module requirement graph (打印模块依赖图))init initialize new module in current directory (在当前文件夹下初始化一个新的module, 创建go.mod文件))tidy add missing and remove unused modules (增加丢失的module，去掉未使用的module)vendor make vendored copy of dependencies (将依赖复制到vendor下)verify verify dependencies have expected content (校验依赖)why explain why packages or modules are needed (解释为什么需要依赖) 二、使用 go module 功能对于新建项目使用 go module： 1234567$ export GO111MODULE=on $ go mod init github.com/you/hello ...// go build 会将项目的依赖添加到 go.mod 中$ go build 对于已有项目要改为使用 go module： 1234567$ export GO111MODULE=on// 创建一个空的 go.mod 文件$ go mod init .// 查找依赖并记录在 go.mod 文件中$ go get ./... go.mod 文件必须要提交到 git 仓库，但 go.sum 文件可以不用提交到 git 仓库(gi t忽略文件 .gitignore 中设置一下)。 三、项目的打包首先需要使用 go mod vendor 将项目所有的依赖下载到本地 vendor 目录中然后进行编译，下面是一个参考： 12345678910#!/bin/bashexport GO111MODULE=&quot;on&quot;export GOPROXY=&quot;https://goproxy.io&quot;export CGO_ENABLED=&quot;0&quot;export GOOS=&quot;linux&quot;export GOARCH=amd64go mod vendorgo build -ldflags &quot;-s -w&quot; -a -installsuffix cgo -o audit-webhook . 四、注意事项1、依赖下载 go module 默认不在 GOPATH 目录下查找依赖文件，其首先会在$GOPATH/pkg/mod中查找有没有所需要的依赖，没有的直接会进行下载。可以使用 go mod download下载好所需要的依赖，依赖默认会下载到$GOPATH/pkg/mod中，其他项目也会使用缓存的 module。 2、国内无法访问的依赖 使用 Go 的其他包管理工具 godep、govendor、glide、dep 等都避免不了翻墙的问题，Go Modules 也是一样，但在go.mod中可以使用replace将特定的库替换成其他库： 123replace ( golang.org/x/text v0.3.0 =&gt; github.com/golang/text v0.3.0) 或者也可以在其他机器上使用 go mod download下载好所需要的依赖，然后再传输到本机。 参考： https://github.com/kubernetes/enhancements/blob/master/keps/sig-architecture/2019-03-19-go-modules.md https://blog.golang.org/using-go-modules]]></content>
      <tags>
        <tag>go module</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet 状态上报的方式]]></title>
    <url>%2F2019%2F06%2F09%2Fnode_status%2F</url>
    <content type="text"><![CDATA[分布式系统中服务端会通过心跳机制确认客户端是否存活，在 k8s 中，kubelet 也会定时上报心跳到 apiserver，以此判断该 node 是否存活，若 node 超过一定时间没有上报心跳，其状态会被置为 NotReady，宿主上容器的状态也会被置为 Nodelost 或者 Unknown 状态。kubelet 自身会定期更新状态到 apiserver，通过参数 --node-status-update-frequency 指定上报频率，默认是 10s 上报一次，kubelet 不止上报心跳信息还会上报自身的一些数据信息。 一、kubelet 上报哪些状态 在 k8s 中，一个 node 的状态包含以下几个信息： Addresses Condition Capacity Info 1、Addresses主要包含以下几个字段： HostName：Hostname 。可以通过 kubelet 的 --hostname-override 参数进行覆盖。 ExternalIP：通常是可以外部路由的 node IP 地址（从集群外可访问）。 InternalIP：通常是仅可在集群内部路由的 node IP 地址。 2、Conditionconditions 字段描述了所有 Running nodes 的状态。 3、Capacity描述 node 上的可用资源：CPU、内存和可以调度到该 node 上的最大 pod 数量。 4、Info描述 node 的一些通用信息，例如内核版本、Kubernetes 版本（kubelet 和 kube-proxy 版本）、Docker 版本 （如果使用了）和系统版本，这些信息由 kubelet 从 node 上获取到。 使用 kubectl get node xxx -o yaml 可以看到 node 所有的状态的信息，其中 status 中的信息都是 kubelet 需要上报的，所以 kubelet 不止上报心跳信息还上报节点信息、节点 OOD 信息、内存磁盘压力状态、节点监控状态、是否调度等。 二、kubelet 状态异常时的影响如果一个 node 处于非 Ready 状态超过 pod-eviction-timeout的值(默认为 5 分钟，在 kube-controller-manager 中定义)，在 v1.5 之前的版本中 kube-controller-manager 会 force delete pod 然后调度该宿主上的 pods 到其他宿主，在 v1.5 之后的版本中，kube-controller-manager 不会 force delete pod，pod 会一直处于Terminating 或Unknown 状态直到 node 被从 master 中删除或 kubelet 状态变为 Ready。在 node NotReady 期间，Daemonset 的 Pod 状态变为 Nodelost，Deployment、Statefulset 和 Static Pod 的状态先变为 NodeLost，然后马上变为 Unknown。Deployment 的 pod 会 recreate，Static Pod 和 Statefulset 的 Pod 会一直处于 Unknown 状态。 当 kubelet 变为 Ready 状态时，Daemonset的pod不会recreate，旧pod状态直接变为Running，Deployment的则是将kubelet进程停止的Node删除，Statefulset的Pod会重新recreate，Staic Pod 会被删除。 三、kubelet 状态上报的实现kubelet 有两种上报状态的方式，第一种定期向 apiserver 发送心跳消息，简单理解就是启动一个 goroutine 然后定期向 APIServer 发送消息。 第二中被称为 NodeLease，在 v1.13 之前的版本中，节点的心跳只有 NodeStatus，从 v1.13 开始，NodeLease feature 作为 alpha 特性引入。当启用 NodeLease feature 时，每个节点在“kube-node-lease”名称空间中都有一个关联的“Lease”对象，该对象由节点定期更新，NodeStatus 和 NodeLease 都被视为来自节点的心跳。NodeLease 会频繁更新，而只有在 NodeStatus 发生改变或者超过了一定时间(默认值为1分钟，node-monitor-grace-period 的默认值为 40s)，才会将 NodeStatus 上报给 master。由于 NodeLease 比 NodeStatus 更轻量级，该特性在集群规模扩展性和性能上有明显提升。本文主要分析第一种上报方式的实现。 kubernetes 版本 ：v1.13 kubelet 上报状态的代码大部分在 kubernetes/pkg/kubelet/kubelet_node_status.go 中实现。状态上报的功能是在 kubernetes/pkg/kubelet/kubelet.go#Run 方法以 goroutine 形式中启动的，kubelet 中多个重要的功能都是在该方法中启动的。 kubernetes/pkg/kubelet/kubelet.go#Run 123456789101112131415func (kl *Kubelet) Run(updates &lt;-chan kubetypes.PodUpdate) &#123; // ... if kl.kubeClient != nil &#123; // Start syncing node status immediately, this may set up things the runtime needs to run. go wait.Until(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, wait.NeverStop) go kl.fastStatusUpdateOnce() // 一种新的状态上报方式 // start syncing lease if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &#123; go kl.nodeLeaseController.Run(wait.NeverStop) &#125; &#125; // ... &#125; kl.syncNodeStatus 便是上报状态的，此处 kl.nodeStatusUpdateFrequency 使用的是默认设置的 10s，也就是说节点间同步状态的函数 kl.syncNodeStatus 每 10s 执行一次。 syncNodeStatus 是状态上报的入口函数，其后所调用的多个函数也都是在同一个文件中实现的。 kubernetes/pkg/kubelet/kubelet_node_status.go#syncNodeStatus 1234567891011121314151617func (kl *Kubelet) syncNodeStatus() &#123; kl.syncNodeStatusMux.Lock() defer kl.syncNodeStatusMux.Unlock() if kl.kubeClient == nil || kl.heartbeatClient == nil &#123; return &#125; // 是否为注册节点 if kl.registerNode &#123; // This will exit immediately if it doesn&apos;t need to do anything. kl.registerWithAPIServer() &#125; if err := kl.updateNodeStatus(); err != nil &#123; klog.Errorf(&quot;Unable to update node status: %v&quot;, err) &#125;&#125; syncNodeStatus 调用 updateNodeStatus， 然后又调用 tryUpdateNodeStatus 来进行上报操作，而最终调用的是 setNodeStatus。这里还进行了同步状态判断，如果是注册节点，则执行 registerWithAPIServer，否则，执行 updateNodeStatus。 updateNodeStatus 主要是调用 tryUpdateNodeStatus 进行后续的操作，该函数中定义了状态上报重试的次数，nodeStatusUpdateRetry 默认定义为 5 次。 kubernetes/pkg/kubelet/kubelet_node_status.go#updateNodeStatus 1234567891011121314func (kl *Kubelet) updateNodeStatus() error &#123; klog.V(5).Infof(&quot;Updating node status&quot;) for i := 0; i &lt; nodeStatusUpdateRetry; i++ &#123; if err := kl.tryUpdateNodeStatus(i); err != nil &#123; if i &gt; 0 &amp;&amp; kl.onRepeatedHeartbeatFailure != nil &#123; kl.onRepeatedHeartbeatFailure() &#125; klog.Errorf(&quot;Error updating node status, will retry: %v&quot;, err) &#125; else &#123; return nil &#125; &#125; return fmt.Errorf(&quot;update node status exceeds retry count&quot;)&#125; tryUpdateNodeStatus 是主要的上报逻辑，先给 node 设置状态，然后上报 node 的状态到 master。 kubernetes/pkg/kubelet/kubelet_node_status.go#tryUpdateNodeStatus 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func (kl *Kubelet) tryUpdateNodeStatus(tryNumber int) error &#123; opts := metav1.GetOptions&#123;&#125; if tryNumber == 0 &#123; util.FromApiserverCache(&amp;opts) &#125; // 获取 node 信息 node, err := kl.heartbeatClient.CoreV1().Nodes().Get(string(kl.nodeName), opts) if err != nil &#123; return fmt.Errorf(&quot;error getting node %q: %v&quot;, kl.nodeName, err) &#125; originalNode := node.DeepCopy() if originalNode == nil &#123; return fmt.Errorf(&quot;nil %q node object&quot;, kl.nodeName) &#125; podCIDRChanged := false if node.Spec.PodCIDR != &quot;&quot; &#123; if podCIDRChanged, err = kl.updatePodCIDR(node.Spec.PodCIDR); err != nil &#123; klog.Errorf(err.Error()) &#125; &#125; // 设置 node 状态 kl.setNodeStatus(node) now := kl.clock.Now() if utilfeature.DefaultFeatureGate.Enabled(features.NodeLease) &amp;&amp; now.Before(kl.lastStatusReportTime.Add(kl.nodeStatusReportFrequency)) &#123; if !podCIDRChanged &amp;&amp; !nodeStatusHasChanged(&amp;originalNode.Status, &amp;node.Status) &#123; kl.volumeManager.MarkVolumesAsReportedInUse(node.Status.VolumesInUse) return nil &#125; &#125; // 更新 node 信息到 master // Patch the current status on the API server updatedNode, _, err := nodeutil.PatchNodeStatus(kl.heartbeatClient.CoreV1(), types.NodeName(kl.nodeName), originalNode, node) if err != nil &#123; return err &#125; kl.lastStatusReportTime = now kl.setLastObservedNodeAddresses(updatedNode.Status.Addresses) // If update finishes successfully, mark the volumeInUse as reportedInUse to indicate // those volumes are already updated in the node&apos;s status kl.volumeManager.MarkVolumesAsReportedInUse(updatedNode.Status.VolumesInUse) return nil&#125; tryUpdateNodeStatus 中调用 setNodeStatus 设置 node 的状态。setNodeStatus 会获取一次 node 的所有状态，然后会将 kubelet 中保存的所有状态改为最新的值，也就是会重置 node status 中的所有字段。 kubernetes/pkg/kubelet/kubelet_node_status.go#setNodeStatus 12345678func (kl *Kubelet) setNodeStatus(node *v1.Node) &#123; for i, f := range kl.setNodeStatusFuncs &#123; klog.V(5).Infof(&quot;Setting node status at position %v&quot;, i) if err := f(node); err != nil &#123; klog.Warningf(&quot;Failed to set some node status fields: %s&quot;, err) &#125; &#125;&#125; setNodeStatus 通过 setNodeStatusFuncs 方法覆盖 node 结构体中所有的字段，setNodeStatusFuncs 是在 NewMainKubelet(pkg/kubelet/kubelet.go) 中初始化的。 kubernetes/pkg/kubelet/kubelet.go#NewMainKubelet 1234567 func NewMainKubelet(kubeCfg *kubeletconfiginternal.KubeletConfiguration, // ... // Generating the status funcs should be the last thing we do, klet.setNodeStatusFuncs = klet.defaultNodeStatusFuncs() return klet, nil&#125; defaultNodeStatusFuncs 是生成状态的函数，通过获取 node 的所有状态指标后使用工厂函数生成状态 kubernetes/pkg/kubelet/kubelet_node_status.go#defaultNodeStatusFuncs 1234567891011121314151617181920212223242526272829303132333435363738func (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error &#123; // if cloud is not nil, we expect the cloud resource sync manager to exist var nodeAddressesFunc func() ([]v1.NodeAddress, error) if kl.cloud != nil &#123; nodeAddressesFunc = kl.cloudResourceSyncManager.NodeAddresses &#125; var validateHostFunc func() error if kl.appArmorValidator != nil &#123; validateHostFunc = kl.appArmorValidator.ValidateHost &#125; var setters []func(n *v1.Node) error setters = append(setters, nodestatus.NodeAddress(kl.nodeIP, kl.nodeIPValidator, kl.hostname, kl.hostnameOverridden, kl.externalCloudProvider, kl.cloud, nodeAddressesFunc), nodestatus.MachineInfo(string(kl.nodeName), kl.maxPods, kl.podsPerCore, kl.GetCachedMachineInfo, kl.containerManager.GetCapacity, kl.containerManager.GetDevicePluginResourceCapacity, kl.containerManager.GetNodeAllocatableReservation, kl.recordEvent), nodestatus.VersionInfo(kl.cadvisor.VersionInfo, kl.containerRuntime.Type, kl.containerRuntime.Version), nodestatus.DaemonEndpoints(kl.daemonEndpoints), nodestatus.Images(kl.nodeStatusMaxImages, kl.imageManager.GetImageList), nodestatus.GoRuntime(), ) if utilfeature.DefaultFeatureGate.Enabled(features.AttachVolumeLimit) &#123; setters = append(setters, nodestatus.VolumeLimits(kl.volumePluginMgr.ListVolumePluginWithLimits)) &#125; setters = append(setters, nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent), nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent), nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent), nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, kl.runtimeState.storageErrors, validateHostFunc, kl.containerManager. Status, kl.recordNodeStatusEvent), nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse), nodestatus.RemoveOutOfDiskCondition(), // TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event // and record state back to the Kubelet runtime object. In the future, I&apos;d like to isolate // these side-effects by decoupling the decisions to send events and partial status recording // from the Node setters. kl.recordNodeSchedulableEvent, ) return setters&#125; defaultNodeStatusFuncs 可以看到 node 上报的所有信息，主要有 MemoryPressureCondition、DiskPressureCondition、PIDPressureCondition、ReadyCondition 等。每一种 nodestatus 都返回一个 setters，所有 setters 的定义在 pkg/kubelet/nodestatus/setters.go 文件中。 对于二次开发而言，如果我们需要 APIServer 掌握更多的 Node 信息，可以在此处添加自定义函数，例如，上报磁盘信息等。 tryUpdateNodeStatus 中最后调用 PatchNodeStatus 上报 node 的状态到 master。 kubernetes/pkg/util/node/node.go#PatchNodeStatus 1234567891011121314// PatchNodeStatus patches node status.func PatchNodeStatus(c v1core.CoreV1Interface, nodeName types.NodeName, oldNode *v1.Node, newNode *v1.Node) (*v1.Node, []byte, error) &#123; // 计算 patch patchBytes, err := preparePatchBytesforNodeStatus(nodeName, oldNode, newNode) if err != nil &#123; return nil, nil, err &#125; updatedNode, err := c.Nodes().Patch(string(nodeName), types.StrategicMergePatchType, patchBytes, &quot;status&quot;) if err != nil &#123; return nil, nil, fmt.Errorf(&quot;failed to patch status %q for node %q: %v&quot;, patchBytes, nodeName, err) &#125; return updatedNode, patchBytes, nil&#125; 在 PatchNodeStatus 会调用已注册的那些方法将状态把状态发给 APIServer。 四、总结本文主要讲述了 kubelet 上报状态的方式及其实现，node 状态上报的方式目前有两种，本文仅分析了第一种状态上报的方式。在大规模集群中由于节点数量比较多，所有 node 都频繁报状态对 etcd 会有一定的压力，当 node 与 master 通信时由于网络导致心跳上报失败也会影响 node 的状态，为了避免类似问题的出现才有 NodeLease 方式，对于该功能的实现后文会继续进行分析。 参考：https://www.qikqiak.com/post/kubelet-sync-node-status/https://www.jianshu.com/p/054450557818https://blog.csdn.net/shida_csdn/article/details/84286058https://kubernetes.io/docs/concepts/architecture/nodes/]]></content>
      <tags>
        <tag>kubelet</tag>
        <tag>node status</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 中 informer 的使用]]></title>
    <url>%2F2019%2F05%2F17%2Fclient-go_informer%2F</url>
    <content type="text"><![CDATA[一、kubernetes 集群的几种访问方式在实际开发过程中，若想要获取 kubernetes 中某个资源（比如 pod）的所有对象，可以使用 kubectl、k8s REST API、client-go(ClientSet、Dynamic Client、RESTClient 三种方式) 等多种方式访问 k8s 集群获取资源。在笔者的开发过程中，最初都是直接调用 k8s 的 REST API 来获取的，使用 kubectl get pod -v=9 可以直接看到调用 k8s 的接口，然后在程序中直接访问还是比较方便的。但是随着集群规模的增长或者从国内获取海外 k8s 集群的数据，直接调用 k8s 接口获取所有 pod 还是比较耗时，这个问题有多种解决方法，最初是直接使用 k8s 原生的 watch 接口来获取的，下面是一个伪代码： 12345678910111213141516171819202122232425262728293031323334const ( ADDED string = &quot;ADDED&quot; MODIFIED string = &quot;MODIFIED&quot; DELETED string = &quot;DELETED&quot; ERROR string = &quot;ERROR&quot;)type Event struct &#123; Type string `json:&quot;type&quot;` Object json.RawMessage `json:&quot;object&quot;`&#125;func main() &#123; resp, err := http.Get(&quot;http://apiserver:8080/api/v1/watch/pods?watch=yes&quot;) if err != nil &#123; // ... &#125; decoder := json.NewDecoder(resp.Body) for &#123; var event Event err = decoder.Decode(&amp;event) if err != nil &#123; // ... &#125; switch event.Type &#123; case ADDED, MODIFIED: // ... case DELETED: // ... case ERROR: // ... &#125; &#125;&#125; 调用 watch 接口后会先将所有的对象 list 一次，然后 apiserver 会将变化的数据推送到 client 端，可以看到每次对于 watch 到的事件都需要判断后进行处理，然后将处理后的结果写入到本地的缓存中，原生的 watch 操作还是非常麻烦的。后来了解到官方推出一个客户端工具 client-go ，client-go 中的 Informer 对 watch 操作做了封装，使用起来非常方便，下面会主要介绍一下 client-go 的使用。 二、Informer 的机制cient-go 是从 k8s 代码中抽出来的一个客户端工具，Informer 是 client-go 中的核心工具包，已经被 kubernetes 中众多组件所使用。所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client，本地缓存被称为 Store，索引被称为 Index。使用 informer 的目的是为了减轻 apiserver 数据交互的压力而抽象出来的一个 cache 层, 客户端对 apiserver 数据的 “读取” 和 “监听” 操作都通过本地 informer 进行。Informer 实例的Lister()方法可以直接查找缓存在本地内存中的数据。 Informer 的主要功能： 同步数据到本地缓存 根据对应的事件类型，触发事先注册好的 ResourceEventHandler 1、Informer 中几个组件的作用Informer 中主要有 Reflector、Delta FIFO Queue、Local Store、WorkQueue 几个组件。以下是 Informer 的工作流程图。 根据流程图来解释一下 Informer 中几个组件的作用： Reflector：称之为反射器，实现对 apiserver 指定类型对象的监控(ListAndWatch)，其中反射实现的就是把监控的结果实例化成具体的对象，最终也是调用 Kubernetes 的 List/Watch API； DeltaIFIFO Queue：一个增量队列，将 Reflector 监控变化的对象形成一个 FIFO 队列，此处的 Delta 就是变化； LocalStore：就是 informer 的 cache，这里面缓存的是 apiserver 中的对象(其中有一部分可能还在DeltaFIFO 中)，此时使用者再查询对象的时候就直接从 cache 中查找，减少了 apiserver 的压力，LocalStore 只会被 Lister 的 List/Get 方法访问。 WorkQueue：DeltaIFIFO 收到时间后会先将时间存储在自己的数据结构中，然后直接操作 Store 中存储的数据，更新完 store 后 DeltaIFIFO 会将该事件 pop 到 WorkQueue 中，Controller 收到 WorkQueue 中的事件会根据对应的类型触发对应的回调函数。 2、Informer 的工作流程 Informer 首先会 list/watch apiserver，Informer 所使用的 Reflector 包负责与 apiserver 建立连接，Reflector 使用 ListAndWatch 的方法，会先从 apiserver 中 list 该资源的所有实例，list 会拿到该对象最新的 resourceVersion，然后使用 watch 方法监听该 resourceVersion 之后的所有变化，若中途出现异常，reflector 则会从断开的 resourceVersion 处重现尝试监听所有变化，一旦该对象的实例有创建、删除、更新动作，Reflector 都会收到”事件通知”，这时，该事件及它对应的 API 对象这个组合，被称为增量（Delta），它会被放进 DeltaFIFO 中。 Informer 会不断地从这个 DeltaFIFO 中读取增量，每拿出一个对象，Informer 就会判断这个增量的时间类型，然后创建或更新本地的缓存，也就是 store。 如果事件类型是 Added（添加对象），那么 Informer 会通过 Indexer 的库把这个增量里的 API 对象保存到本地的缓存中，并为它创建索引，若为删除操作，则在本地缓存中删除该对象。 DeltaFIFO 再 pop 这个事件到 controller 中，controller 会调用事先注册的 ResourceEventHandler 回调函数进行处理。 在 ResourceEventHandler 回调函数中，其实只是做了一些很简单的过滤，然后将关心变更的 Object 放到 workqueue 里面。 Controller 从 workqueue 里面取出 Object，启动一个 worker 来执行自己的业务逻辑，业务逻辑通常是计算目前集群的状态和用户希望达到的状态有多大的区别，然后孜孜不倦地让 apiserver 将状态演化到用户希望达到的状态，比如为 deployment 创建新的 pods，或者是扩容/缩容 deployment。 在worker中就可以使用 lister 来获取 resource，而不用频繁的访问 apiserver，因为 apiserver 中 resource 的变更都会反映到本地的 cache 中。 Informer 在使用时需要先初始化一个 InformerFactory，目前主要推荐使用的是 SharedInformerFactory，Shared 指的是在多个 Informer 中共享一个本地 cache。 Informer 中的 ResourceEventHandler 函数有三种： 12345678// ResourceEventHandlerFuncs is an adaptor to let you easily specify as many or// as few of the notification functions as you want while still implementing// ResourceEventHandler.type ResourceEventHandlerFuncs struct &#123; AddFunc func(obj interface&#123;&#125;) UpdateFunc func(oldObj, newObj interface&#123;&#125;) DeleteFunc func(obj interface&#123;&#125;)&#125; 这三种函数的处理逻辑是用户自定义的，在初始化 controller 时注册完 ResourceEventHandler 后，一旦该对象的实例有创建、删除、更新三中操作后就会触发对应的 ResourceEventHandler。 三、Informer 使用示例在实际的开发工作中，Informer 主要用在两处： 在访问 k8s apiserver 的客户端作为一个 client 缓存对象使用； 在一些自定义 controller 中使用，比如 operator 的开发； 1、下面是一个作为 client 的使用示例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package mainimport ( &quot;flag&quot; &quot;fmt&quot; &quot;log&quot; &quot;path/filepath&quot; corev1 &quot;k8s.io/api/core/v1&quot; &quot;k8s.io/apimachinery/pkg/labels&quot; &quot;k8s.io/apimachinery/pkg/util/runtime&quot; &quot;k8s.io/client-go/informers&quot; &quot;k8s.io/client-go/kubernetes&quot; &quot;k8s.io/client-go/tools/cache&quot; &quot;k8s.io/client-go/tools/clientcmd&quot; &quot;k8s.io/client-go/util/homedir&quot;)func main() &#123; var kubeconfig *string if home := homedir.HomeDir(); home != &quot;&quot; &#123; kubeconfig = flag.String(&quot;kubeconfig&quot;, filepath.Join(home, &quot;.kube&quot;, &quot;config&quot;), &quot;(optional) absolute path to the kubeconfig file&quot;) &#125; else &#123; kubeconfig = flag.String(&quot;kubeconfig&quot;, &quot;&quot;, &quot;absolute path to the kubeconfig file&quot;) &#125; flag.Parse() config, err := clientcmd.BuildConfigFromFlags(&quot;&quot;, *kubeconfig) if err != nil &#123; panic(err) &#125; // 初始化 client clientset, err := kubernetes.NewForConfig(config) if err != nil &#123; log.Panic(err.Error()) &#125; stopper := make(chan struct&#123;&#125;) defer close(stopper) // 初始化 informer factory := informers.NewSharedInformerFactory(clientset, 0) nodeInformer := factory.Core().V1().Nodes() informer := nodeInformer.Informer() defer runtime.HandleCrash() // 启动 informer，list &amp; watch go factory.Start(stopper) // 从 apiserver 同步资源，即 list if !cache.WaitForCacheSync(stopper, informer.HasSynced) &#123; runtime.HandleError(fmt.Errorf(&quot;Timed out waiting for caches to sync&quot;)) return &#125; // 使用自定义 handler informer.AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: onAdd, UpdateFunc: func(interface&#123;&#125;, interface&#123;&#125;) &#123; fmt.Println(&quot;update not implemented&quot;) &#125;, // 此处省略 workqueue 的使用 DeleteFunc: func(interface&#123;&#125;) &#123; fmt.Println(&quot;delete not implemented&quot;) &#125;, &#125;) // 创建 lister nodeLister := nodeInformer.Lister() // 从 lister 中获取所有 items nodeList, err := nodeLister.List(labels.Everything()) if err != nil &#123; fmt.Println(err) &#125; fmt.Println(&quot;nodelist:&quot;, nodeList) &lt;-stopper&#125;func onAdd(obj interface&#123;&#125;) &#123; node := obj.(*corev1.Node) fmt.Println(&quot;add a node:&quot;, node.Name)&#125; Shared指的是多个 lister 共享同一个cache，而且资源的变化会同时通知到cache和 listers。这个解释和上面图所展示的内容的是一致的，cache我们在Indexer的介绍中已经分析过了，lister 指的就是OnAdd、OnUpdate、OnDelete 这些回调函数背后的对象。 2、以下是作为 controller 使用的一个整体工作流程(1) 创建一个控制器 为控制器创建 workqueue 创建 informer, 为 informer 添加 callback 函数，创建 lister (2) 启动控制器 启动 informer 等待本地 cache sync 完成后， 启动 workers (3) 当收到变更事件后，执行 callback 等待事件触发 从事件中获取变更的 Object 做一些必要的检查 生成 object key，一般是 namespace/name 的形式 将 key 放入 workqueue 中 (4) worker loop 等待从 workqueue 中获取到 item，一般为 object key 用 object key 通过 lister 从本地 cache 中获取到真正的 object 对象 做一些检查 执行真正的业务逻辑 处理下一个 item 下面是自定义 controller 使用的一个参考： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485var ( masterURL string kubeconfig string)func init() &#123; flag.StringVar(&amp;kubeconfig, &quot;kubeconfig&quot;, &quot;&quot;, &quot;Path to a kubeconfig. Only required if out-of-cluster.&quot;) flag.StringVar(&amp;masterURL, &quot;master&quot;, &quot;&quot;, &quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.&quot;)&#125;func main() &#123; flag.Parse() stopCh := signals.SetupSignalHandler() cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) if err != nil &#123; glog.Fatalf(&quot;Error building kubeconfig: %s&quot;, err.Error()) &#125; kubeClient, err := kubernetes.NewForConfig(cfg) if err != nil &#123; glog.Fatalf(&quot;Error building kubernetes clientset: %s&quot;, err.Error()) &#125; // 所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client // informer watch apiserver,每隔 30 秒 resync 一次(list) kubeInformerFactory := informers.NewSharedInformerFactory(kubeClient, time.Second*30) controller := controller.NewController(kubeClient, kubeInformerFactory.Core().V1().Nodes()) // 启动 informer go kubeInformerFactory.Start(stopCh) // start controller if err = controller.Run(2, stopCh); err != nil &#123; glog.Fatalf(&quot;Error running controller: %s&quot;, err.Error()) &#125;&#125;// NewController returns a new network controllerfunc NewController( kubeclientset kubernetes.Interface, networkclientset clientset.Interface, networkInformer informers.NetworkInformer) *Controller &#123; // Create event broadcaster // Add sample-controller types to the default Kubernetes Scheme so Events can be // logged for sample-controller types. utilruntime.Must(networkscheme.AddToScheme(scheme.Scheme)) glog.V(4).Info(&quot;Creating event broadcaster&quot;) eventBroadcaster := record.NewBroadcaster() eventBroadcaster.StartLogging(glog.Infof) eventBroadcaster.StartRecordingToSink(&amp;typedcorev1.EventSinkImpl&#123;Interface: kubeclientset.CoreV1().Events(&quot;&quot;)&#125;) recorder := eventBroadcaster.NewRecorder(scheme.Scheme, corev1.EventSource&#123;Component: controllerAgentName&#125;) controller := &amp;Controller&#123; kubeclientset: kubeclientset, networkclientset: networkclientset, networksLister: networkInformer.Lister(), networksSynced: networkInformer.Informer().HasSynced, workqueue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), &quot;Networks&quot;), recorder: recorder, &#125; glog.Info(&quot;Setting up event handlers&quot;) // Set up an event handler for when Network resources change networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: controller.enqueueNetwork, UpdateFunc: func(old, new interface&#123;&#125;) &#123; oldNetwork := old.(*samplecrdv1.Network) newNetwork := new.(*samplecrdv1.Network) if oldNetwork.ResourceVersion == newNetwork.ResourceVersion &#123; // Periodic resync will send update events for all known Networks. // Two different versions of the same Network will always have different RVs. return &#125; controller.enqueueNetwork(new) &#125;, DeleteFunc: controller.enqueueNetworkForDelete, &#125;) return controller&#125; 自定义 controller 的详细使用方法可以参考：k8s-controller-custom-resource 四、使用中的一些问题1、Informer 二级缓存中的同步问题虽然 Informer 和 Kubernetes 之间没有 resync 机制，但 Informer 内部的这两级缓存 DeltaIFIFO 和 LocalStore 之间会存在 resync 机制，k8s 中 kube-controller-manager 的 StatefulSetController 中使用了两级缓存的 resync 机制（如下图所示），我们在生产环境中发现 sts 创建后过了很久 pod 才会创建，主要是由于 StatefulSetController 的两级缓存之间 30s 会同步一次，由于 StatefulSetController watch 到变化后就会把对应的 sts 放入 DeltaIFIFO 中，且每隔30s会把 LocalStore 中全部的 sts 重新入一遍 DeltaIFIFO，入队时会做一些处理，过滤掉一些不需要重复入队列的 sts，若间隔的 30s 内没有处理完队列中所有的 sts，则待处理队列中始终存在未处理完的 sts，并且在同步过程中产生的 sts 会加的队列的尾部，新加入队尾的 sts 只能等到前面的 sts 处理完成（也就是 resync 完成）才会被处理，所以导致的现象就是 sts 创建后过了很久 pod 才会创建。 优化的方法就是去掉二级缓存的同步策略（将 setInformer.Informer().AddEventHandlerWithResyncPeriod() 改为 informer.AddEventHandler()）或者调大同步周期，但是在研究 kube-controller-manager 其他 controller 时发现并不是所有的 controller 都有同步策略，社区也有相关的 issue 反馈了这一问题，Remove resync period for sset controller，社区也会在以后的版本中去掉两级缓存之间的 resync 策略。 k8s.io/kubernetes/pkg/controller/statefulset/stateful_set.go 2、使用 Informer 如何监听所有资源对象？一个 Informer 实例只能监听一种 resource，每个 resource 需要创建对应的 Informer 实例。 3、为什么不是使用 workqueue？建议使用 RateLimitingQueue，它相比普通的 workqueue 多了以下的功能: 限流：可以限制一个 item 被 reenqueued 的次数。 防止 hot loop：它保证了一个 item 被 reenqueued 后，不会马上被处理。 五、总结本文介绍了 client-go 包中核心组件 Informer 的原理以及使用方法，Informer 主要功能是缓存对象到本地以及根据对应的事件类型触发已注册好的 ResourceEventHandler，其主要用在访问 k8s apiserver 的客户端和 operator 中。 参考： 如何用 client-go 拓展 Kubernetes 的 API https://www.kubernetes.org.cn/2693.html Kubernetes 大咖秀徐超《使用 client-go 控制原生及拓展的 Kubernetes API》 Use prometheus conventions for workqueue metrics 深入浅出kubernetes之client-go的workqueue https://gianarb.it/blog/kubernetes-shared-informer 理解 K8S 的设计精髓之 List-Watch机制和Informer模块 https://ranler.org/notes/file/528 Kubernetes Client-go Informer 源码分析]]></content>
      <tags>
        <tag>client-go</tag>
        <tag>informer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用插件扩展 kubectl]]></title>
    <url>%2F2019%2F05%2F16%2Fkubectl_plugin%2F</url>
    <content type="text"><![CDATA[由于笔者所维护的集群规模较大，经常需要使用 kubectl 来排查一些问题，但是 kubectl 功能有限，有些操作还是需要写一个脚本对 kubectl 做一些封装才能达到目的。比如我经常做的一个操作就是排查一下线上哪些宿主的 cpu/memory request 使用率超过某个阈值，kubectl 并不能直接看到一个 master 下所有宿主的 request 使用率，但可以使用 kubectl describe node xxx查看某个宿主机的 request 使用率，所以只好写一个脚本来扫一遍了。 123456789#!/bin/bashecho -e &quot;node\tcpu_requets memory_requets&quot;for i in `kubectl get node | grep -v NAME | awk &apos;&#123;print $1&#125;&apos;`;do res=$(kubectl describe node $i | grep -A 3 &quot;Resource&quot;) cpu_requets=$(echo $&#123;res&#125; | awk &apos;&#123;print $9&#125;&apos; | awk -F &apos;%&apos; &apos;&#123;print $1&#125;&apos; | awk -F &apos;(&apos; &apos;&#123;print $2&#125;&apos;) memory_requets=$(echo $&#123;res&#125; | awk &apos;&#123;print $14&#125;&apos; | awk -F &apos;%&apos; &apos;&#123;print $1&#125;&apos; | awk -F &apos;(&apos; &apos;&#123;print $2&#125;&apos;) echo -e &quot;$i\t$&#123;cpu_requets&#125; \t$&#123;memory_requets&#125;&quot;done 类似的需求比较多，此处不一一列举，这种操作经常需要做，虽然写一个脚本也能完全搞定，但确实比较 low，也不便提供给别人使用，基于此了解到目前官方对 kubectl 的插件机制做了一些改进，对 kubectl 的扩展也比较容易，所以下文会带你了解一下 kubectl 的扩展功能。 一、编写 kubectl 插件kubectl 命令从 v1.8.0 版本开始支持插件机制，之后的版本中我们都可以对 kubectl 命令进行扩展，kubernetes 在 v1.12 以后插件可以直接是以 kubectl- 开头命令的一个二进制文件，插件机制在 v1.14 进入 GA 状态，这种改进是希望用户以二进制文件形式可以扩展自己的 kubectl 子命令。当然，kubectl 插件机制是与语言无关的，也就是说你可以用任何语言编写插件。 如 kubernetes 官方文档中描述，只要将二进制文件放在系统 PATH 下，kubectl 即可识别，二进制文件类似 kubectl-foo-bar，并且在使用时 kubectl 会匹配最长的二进制文件。 官方建议使用 k8s.io/cli-runtime 库进行编写，若你的插件需要支持一些命令行参数，可以参考使用，官方也给了一个例子 sample-cli-plugin。 还是回到最初的问题，对于获取一个集群写所有 node 的资源使用率，笔者基于也编写了一个简单的插件。 12345// 安装插件$ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o bin/kubectl-view-node-resource cmd/view-node-resource/main.go$ mv bin/kubectl-view-node-resource /usr/bin/ 使用 kubectl plugin list 查看 PATH 下有哪些可用的插件。 123456// 查看插件$ kubectl plugin listThe following kubectl-compatible plugins are available:/usr/bin/kubectl-view-node-resource 123456789101112131415161718192021// 使用插件$ kubectl view node taints --helpA longer description that spans multiple lines and likely containsexamples and usage of using your application. For example:Cobra is a CLI library for Go that empowers applications.This application is a tool to generate the needed filesto quickly create a Cobra application.Usage: view-node-taints [flags]Flags: --config string config file (default is $HOME/.view-node-taints.yaml) -h, --help help for view-node-taints -t, --toggle Help message for toggle$ kubectl view node resource Name PodCount CPURequests MemoryRequests CPULimits MemoryLimits 192.168.1.110 4 0 (0.00%) 6.4 (41.26%) 8 (100.00%) 16.0 (103.14%) 此外，还开发一个查看集群下所有 node taints 的插件，kubectl 支持查看宿主的 label，但是没有直接查看所有宿主 taints 的命令，插件效果如下： 123$ kubectl view node taints Name Status Age Version Taints 192.168.1.110 Ready,SchedulingDisabled 49d v1.8.1-35+9406f9d9909c61-dirty enabledDiskSchedule=true:NoSchedule 插件代码地址：kubectl-plugin 二、kubectl 插件管理工具 krew上文讲了如何编写一个插件，但是官方也提供一个插件库并提供了一个插件管理工具 krew ，krew 是 kubectl 插件的管理器，使用 krew 可以轻松的查找、安装和管理 kubectl 插件，它类似于 yum、apt、 dnf，krew 也可以帮助你将已写好的插件在多个平台上打包和分发，krew 自己也作为一个 kubectl 插件存在。 krew 仅支持在 v1.12 及之后的版本中使用。 1、安装 krew 123456789$ ( set -x; cd &quot;$(mktemp -d)&quot; &amp;&amp; curl -fsSLO &quot;https://storage.googleapis.com/krew/v0.2.1/krew.&#123;tar.gz,yaml&#125;&quot; &amp;&amp; tar zxvf krew.tar.gz &amp;&amp; ./krew-&quot;$(uname | tr &apos;[:upper:]&apos; &apos;[:lower:]&apos;)_amd64&quot; install \ --manifest=krew.yaml --archive=krew.tar.gz)$ export PATH=&quot;$&#123;KREW_ROOT:-$HOME/.krew&#125;/bin:$PATH&quot; 2、krew 的使用 12345$ kubectl krew search # show all plugins$ kubectl krew install view-secret # install a plugin named &quot;view-secret&quot;$ kubectl view-secret default-token-4cwvh # use the plugin$ kubectl krew upgrade # upgrade installed plugins$ kubectl krew remove view-secret # uninstall a plugin 若想让你自己的插件加入到 krew 的索引中，可以参考：how to package and publish a plugin for krew。 参考： kubectl 插件命明规范 https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/ https://github.com/gosoon/kubectl-plugin]]></content>
      <tags>
        <tag>kubectl plugin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署 kubernetes 可视化监控组件]]></title>
    <url>%2F2019%2F04%2F22%2Fk8s_dashboard_prometheus%2F</url>
    <content type="text"><![CDATA[随着 kubernetes 的大规模使用，对 kubernetes 组件及其上运行服务的监控也是非常重要的一个环节，目前开源的监控组件有很多种，例如 cAdvisor、Heapster、metrics-server、kube-state-metrics、Prometheus 等，对监控数据的可视化查看组件有 Dashboard、 Prometheus、Grafana 等，本文会介绍 kube-dashboard 和基于 prometheus 搭建数据可视化监控。 kubernetes 版本：v1.12 一、kubernetes-dashboard 的部署1、创建 kubernetes-dashboard1234567$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml$ kubectl get svc -n kube-system | grep kubernetes-dashboardkubernetes-dashboard ClusterIP 10.101.203.44 &lt;none&gt; 443/TCP 2h$ kubectl get pod -n kube-system | grep kubernetes-dashboardkubernetes-dashboard-65c76f6c97-8npsv 1/1 Running 0 2h 所需镜像下载地址：k8s-system-images 2、使用 nodePort 方式访问 kubernetes-dashboardnodeport 的访问方式虽然有性能损失但是比较简单，kubernetes-dashboard 默认使用 clusterIP 的方式暴露服务，修改 kubernetes-dashboard svc 使用 nodePort 方式： 123456789101112131415$ kubectl edit svc -n kube-system ... spec: clusterIP: 10.101.203.44 externalTrafficPolicy: Cluster ports: - nodePort: 8004 // 添加 nodeport 端口 port: 443 protocol: TCP targetPort: 8443 selector: k8s-app: kubernetes-dashboard sessionAffinity: None type: NodePort // 将 ClusterIP 修改为 NodePort ... nodePort 端口默认为 30000-32767，若使用其他端口，需要修改 apiserver 的启动参数 --service-node-port-range 来指定 nodePort 范围，如：--service-node-port-range 8000-9000。 3、创建 kubernetes-dashboard 管理员角色 kubernetes-dashboard-admin.yaml： 123456789101112131415161718apiVersion: v1kind: ServiceAccountmetadata: name: dashboard-admin namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: dashboard-adminsubjects: - kind: ServiceAccount name: dashboard-admin namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io 创建角色并获取 token： 1234567891011121314151617$ kubectl apply -f kubernetes-dashboard-admin.yaml$ kubectl describe secrets `kubectl get secret -n kube-system | grep dashboard-admin | awk &apos;&#123;print $1&#125;&apos;` -n kube-systemName: dashboard-admin-token-hrhfdNamespace: kube-systemLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: 76805bdb-6047-11e9-ba0d-525400c322d9Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 11 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4taHJoZmQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNzY4MDViZGItNjA0Ny0xMWU5LWJhMGQtNTI1NDAwYzMyMmQ5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.hJJRyp_O4sGvIULj3BhqidCkkPnD4A2AtnpkXJoEPCALaaQHC8zhCA5-nDNlo2fiEggZ02UZPwiyGxKKFPC57UlKhjTf5zYcMIhELVXlj5FdBmjzCZcCHVFF4tj_rCoOFlZi6fQ3vNCcX8CtLxX_OsH1YXaFVuUmR1gYm97hbyuO382_k3tFIPXFP3QG8zUtc_7QMkeMNEakJZLCvkW8xdlaCuC-GVAMhZl5Kq1MSthuF-8HY7KaXhvqQzfD4DQZrdQ7vf_7NG3rdvhsj8nQ__TTe1W0RjqwkQuxg5YdE4gbAsxwJjkek-N0K9HfnZhkS9WosaUaUe9pZaGZ9akqyQ token 是访问 dashboard 需要用的。 若没有安装 kube-proxy，可以参考官方提供使用 kubectl proxy 的方式访问： 1$ kubectl proxy --address=IP --disable-filter=true 访问 http://IP:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login 已部署 kube-proxy 的可直接访问 https://IP:nodePort 选择令牌方式使用上面生成的 token 登录。 Dashboard 可以使用 Ingress、Let’s Encrypt 等多种方式配置 ssl，关于 ssl 的详细配置此处不进行详解。 二、部署 prometheusprometheus 作为 CNCF 生态圈中的重要一员，其活跃度仅次于 Kubernetes, 现已广泛用于 Kubernetes 集群的监控系统中。prometheus 的部署相对比较简单，社区已经有了 kube-prometheus，kube-prometheus 会部署包含 prometheus-operator、grafana、kube-state-metrics 等多个组件。 123$ git clone https://github.com/coreos/kube-prometheus$ kubectl apply -f manifests/ 为了使用简单，我也会将 prometheus 和 grafana 的端口修改为 nodePort 的方式进行暴露： 12345678910111213141516171819202122232425262728$ kubectl edit svc prometheus-k8s -n monitoring$ kubectl edit svc grafana -n monitoring$ kubectl get svc -n monitoringNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEalertmanager-main NodePort 10.102.81.118 &lt;none&gt; 9093:8007/TCP 5d1halertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,6783/TCP 5d1hgrafana NodePort 10.96.19.82 &lt;none&gt; 3000:8006/TCP 5d1hkube-state-metrics ClusterIP None &lt;none&gt; 8443/TCP,9443/TCP 5d1hnode-exporter ClusterIP None &lt;none&gt; 9100/TCP 5d1hprometheus-adapter ClusterIP 10.107.103.58 &lt;none&gt; 443/TCP 5d1hprometheus-k8s NodePort 10.110.222.41 &lt;none&gt; 9090:8005/TCP 5d1hprometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 5d1hprometheus-operator ClusterIP None &lt;none&gt; 8080/TCP 5d1h$ kubectl get pod -n monitoringNAME READY STATUS RESTARTS AGEalertmanager-main-0 2/2 Running 0 4dalertmanager-main-1 2/2 Running 0 4dalertmanager-main-2 2/2 Running 0 4dgrafana-9d97dfdc7-qfjts 1/1 Running 0 4dkube-state-metrics-74d7dcd7dc-qfz5m 4/4 Running 0 3d11hnode-exporter-5cdl2 2/2 Running 0 4dprometheus-adapter-b7d894c9c-dvzzq 1/1 Running 0 4dprometheus-k8s-0 3/3 Running 1 2d2hprometheus-k8s-1 3/3 Running 1 4dprometheus-operator-77b8b97459-7qfxj 1/1 Running 0 4d 上面几个组件成功运行后就可以在页面访问 prometheus 和 ganfana ： 进入 grafana 的 web 端，默认用户名和密码均为 admin： grafana 支持导入其他的 Dashboard，在 grafana 官方网站可以搜到大量与 k8s 相关的 dashboard。 三、总结本文介绍了对 kubernetes 和容器监控比较成熟的两个方案，虽然目前开源的方案比较多，但是要形成采集、存储、展示、报警一个完成的体系还需要在使用过程中不断探索与完善。]]></content>
      <tags>
        <tag>kube-dashboard</tag>
        <tag>prometheus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 指标采集组件 metrics-server 的部署]]></title>
    <url>%2F2019%2F04%2F14%2Fk8s_metrics_server%2F</url>
    <content type="text"><![CDATA[metrics-server 是一个采集集群中指标的组件，类似于 cadvisor，在 v1.8 版本中引入，官方将其作为 heapster 的替代者，metric-server 属于 core metrics(核心指标)，提供 API metrics.k8s.io，仅可以查看 node、pod 当前 CPU/Memory/Storage 的资源使用情况，也支持通过 Metrics API 的形式获取，以此数据提供给 Dashboard、HPA、scheduler 等使用。 一、开启 API Aggregation由于 metrics-server 需要暴露 API，但 k8s 的 API 要统一管理，如何将 apiserver 的请求转发给 metrics-server ，解决方案就是使用 kube-aggregator ，所以在部署 metrics-server 之前，需要在 kube-apiserver 中开启 API Aggregation，即增加以下配置： 1234567--proxy-client-cert-file=/etc/kubernetes/certs/proxy.crt--proxy-client-key-file=/etc/kubernetes/certs/proxy.key--requestheader-client-ca-file=/etc/kubernetes/certs/proxy-ca.crt--requestheader-allowed-names=aggregator--requestheader-extra-headers-prefix=X-Remote-Extra---requestheader-group-headers=X-Remote-Group--requestheader-username-headers=X-Remote-User 如果kube-proxy没有在Master上面运行，还需要配置 1--enable-aggregator-routing=true kube-aggregator 的详细设计文档请参考：configure-aggregation-layer 二、部署 metrics-server1、获取配置文件12$ git clone https://github.com/kubernetes/kubernetes$ cd kubernetes/cluster/addons/metrics-server/ 2、修改 metrics-server 配置参数修改 resource-reader.yaml 文件： 123456789101112rules:- apiGroups: - &quot;&quot; resources: - pods - nodes - nodes/stats #新增这一行 - namespaces verbs: - get - list - watch 修改 metrics-server-deployment.yaml文件: 123456789101112131415161718192021222324252627282930313233 ...... # metrics-server containers 启动参数作如下修改： containers: - name: metrics-server image: k8s.gcr.io/metrics-server-amd64:v0.3.1 command: - /metrics-server - --metric-resolution=30s - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP # These are needed for GKE, which doesn&apos;t support secure communication yet. # Remove these lines for non-GKE clusters, and when GKE supports token-based auth. #- --kubelet-port=10255 #- --deprecated-kubelet-completely-insecure=true ...... # 修改启动参数： command: - /pod_nanny - --config-dir=/etc/config - --cpu=80m - --extra-cpu=0.5m - --memory=80Mi - --extra-memory=8Mi - --threshold=5 - --deployment=metrics-server-v0.3.1 - --container=metrics-server - --poll-period=300000 - --estimator=exponential # Specifies the smallest cluster (defined in number of nodes) # resources will be scaled to. #- --minClusterSize=&#123;&#123; metrics_server_min_cluster_size &#125;&#125; 3、部署1kubectl apply -f . metrics-server 的资源占用量会随着集群中的 Pod 数量的不断增长而不断上升，因此需要 addon-resizer 垂直扩缩 metrics-server。addon-resizer 依据集群中节点的数量线性地扩展 metrics-server，以保证其能够有能力提供完整的metrics API 服务，具体参考：addon-resizer。 所需要的镜像可以在 k8s-system-images 中下载。 检查是否部署成功： 12345$ kubectl get apiservices | grep metricsv1beta1.metrics.k8s.io kube-system/metrics-server True 2m$ kubectl get pod -n kube-systemmetrics-server-v0.3.1-65b6db6945-rpqwf 2/2 Running 0 20h 三、metrics-server 的使用由于采集数据间隔为1分钟，等待数分钟后查看数据： 123456789101112131415$ kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%node1 108m 2% 1532Mi 40%$ kubectl top pod -n kube-systemNAME CPU(cores) MEMORY(bytes)coredns-576cbf47c7-8v6n8 2m 14Micoredns-576cbf47c7-qk7rk 2m 10Mietcd-node1 11m 80Mikube-apiserver-node1 17m 566Mikube-controller-manager-node1 17m 67Mikube-flannel-ds-amd64-8lvs2 2m 13Mikube-proxy-85lhl 3m 19Mikube-scheduler-node1 5m 16Mimetrics-server-v0.3.1-65b6db6945-rpqwf 2m 19Mi Metrics-server 可用 API 列表如下： http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes/&lt;node-name&gt; http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/pods http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespace/&lt;namespace-name&gt;/pods/&lt;pod-name&gt; 由于 k8s 在 v1.10 后废弃了 8080 端口，可以通过代理或者使用认证的方式访问这些 API：12$ kubectl proxy$ curl http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/nodes 也可以直接通过 kubectl 命令来访问这些 API，比如：1234$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/&lt;node-name&gt;$ kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespace/&lt;namespace-name&gt;/pods/&lt;pod-name&gt;]]></content>
      <tags>
        <tag>metrics-server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernets 中组件高可用的实现方式]]></title>
    <url>%2F2019%2F03%2F13%2Fk8s_leader_election%2F</url>
    <content type="text"><![CDATA[生产环境中为了保障业务的稳定性，集群都需要高可用部署，k8s 中 apiserver 是无状态的，可以横向扩容保证其高可用，kube-controller-manager 和 kube-scheduler 两个组件通过 leader 选举保障高可用，即正常情况下 kube-scheduler 或 kube-manager-controller 组件的多个副本只有一个是处于业务逻辑运行状态，其它副本则不断的尝试去获取锁，去竞争 leader，直到自己成为leader。如果正在运行的 leader 因某种原因导致当前进程退出，或者锁丢失，则由其它副本去竞争新的 leader，获取 leader 继而执行业务逻辑。 kubernetes 版本： v1.12 组件高可用的使用k8s 中已经为 kube-controller-manager、kube-scheduler 组件实现了高可用，只需在每个组件的配置文件中添加 --leader-elect=true 参数即可启用。在每个组件的日志中可以看到 HA 相关参数的默认值： 12345I0306 19:17:14.109511 161798 flags.go:33] FLAG: --leader-elect=&quot;true&quot;I0306 19:17:14.109513 161798 flags.go:33] FLAG: --leader-elect-lease-duration=&quot;15s&quot;I0306 19:17:14.109516 161798 flags.go:33] FLAG: --leader-elect-renew-deadline=&quot;10s&quot;I0306 19:17:14.109518 161798 flags.go:33] FLAG: --leader-elect-resource-lock=&quot;endpoints&quot;I0306 19:17:14.109520 161798 flags.go:33] FLAG: --leader-elect-retry-period=&quot;2s&quot; kubernetes 中查看组件 leader 的方法： 12$ kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml &amp;&amp; kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml 当前组件 leader 的 hostname 会写在 annotation 的 control-plane.alpha.kubernetes.io/leader 字段里。 Leader Election 的实现Leader Election 的过程本质上是一个竞争分布式锁的过程。在 Kubernetes 中，这个分布式锁是以创建 Endpoint 资源的形式进行，谁先创建了该资源，谁就先获得锁，之后会对该资源不断更新以保持锁的拥有权。 下面开始讲述 kube-controller-manager 中 leader 的竞争过程，cm 在加载及配置完参数后就开始执行 run 方法了。代码在 k8s.io/kubernetes/cmd/kube-controller-manager/app/controllermanager.go 中： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586// Run runs the KubeControllerManagerOptions. This should never exit.func Run(c *config.CompletedConfig, stopCh &lt;-chan struct&#123;&#125;) error &#123; ... // kube-controller-manager 的核心 run := func(ctx context.Context) &#123; rootClientBuilder := controller.SimpleControllerClientBuilder&#123; ClientConfig: c.Kubeconfig, &#125; var clientBuilder controller.ControllerClientBuilder if c.ComponentConfig.KubeCloudShared.UseServiceAccountCredentials &#123; if len(c.ComponentConfig.SAController.ServiceAccountKeyFile) == 0 &#123; // It&apos;c possible another controller process is creating the tokens for us. // If one isn&apos;t, we&apos;ll timeout and exit when our client builder is unable to create the tokens. glog.Warningf(&quot;--use-service-account-credentials was specified without providing a --service-account-private-key-file&quot;) &#125; clientBuilder = controller.SAControllerClientBuilder&#123; ClientConfig: restclient.AnonymousClientConfig(c.Kubeconfig), CoreClient: c.Client.CoreV1(), AuthenticationClient: c.Client.AuthenticationV1(), Namespace: &quot;kube-system&quot;, &#125; &#125; else &#123; clientBuilder = rootClientBuilder &#125; controllerContext, err := CreateControllerContext(c, rootClientBuilder, clientBuilder, ctx.Done()) if err != nil &#123; glog.Fatalf(&quot;error building controller context: %v&quot;, err) &#125; saTokenControllerInitFunc := serviceAccountTokenControllerStarter&#123;rootClientBuilder: rootClientBuilder&#125;.startServiceAccountTokenController // 初始化及启动所有的 controller if err := StartControllers(controllerContext, saTokenControllerInitFunc, NewControllerInitializers(controllerContext.LoopMode), unsecuredMux); err != nil &#123; glog.Fatalf(&quot;error starting controllers: %v&quot;, err) &#125; controllerContext.InformerFactory.Start(controllerContext.Stop) close(controllerContext.InformersStarted) select &#123;&#125; &#125; // 如果 LeaderElect 参数未配置,说明 controller-manager 是单点启动的， // 则直接调用 run 方法来启动需要被启动的控制器即可。 if !c.ComponentConfig.Generic.LeaderElection.LeaderElect &#123; run(context.TODO()) panic(&quot;unreachable&quot;) &#125; // 如果 LeaderElect 参数配置为 true,说明 controller-manager 是以 HA 方式启动的， // 则执行下面的代码进行 leader 选举，选举出的 leader 会回调 run 方法。 id, err := os.Hostname() if err != nil &#123; return err &#125; // add a uniquifier so that two processes on the same host don&apos;t accidentally both become active id = id + &quot;_&quot; + string(uuid.NewUUID()) // 初始化资源锁 rl, err := resourcelock.New(c.ComponentConfig.Generic.LeaderElection.ResourceLock, &quot;kube-system&quot;, &quot;kube-controller-manager&quot;, c.LeaderElectionClient.CoreV1(), resourcelock.ResourceLockConfig&#123; Identity: id, EventRecorder: c.EventRecorder, &#125;) if err != nil &#123; glog.Fatalf(&quot;error creating lock: %v&quot;, err) &#125; // 进入到选举的流程 leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig&#123; Lock: rl, LeaseDuration: c.ComponentConfig.Generic.LeaderElection.LeaseDuration.Duration, RenewDeadline: c.ComponentConfig.Generic.LeaderElection.RenewDeadline.Duration, RetryPeriod: c.ComponentConfig.Generic.LeaderElection.RetryPeriod.Duration, Callbacks: leaderelection.LeaderCallbacks&#123; OnStartedLeading: run, OnStoppedLeading: func() &#123; glog.Fatalf(&quot;leaderelection lost&quot;) &#125;, &#125;, WatchDog: electionChecker, Name: &quot;kube-controller-manager&quot;, &#125;) panic(&quot;unreachable&quot;)&#125; 1、初始化资源锁，kubernetes 中默认的资源锁使用 endpoints，也就是 c.ComponentConfig.Generic.LeaderElection.ResourceLock 的值为 “endpoints”，在代码中我并没有找到对 ResourceLock 初始化的地方，只看到了对该参数的说明以及日志中配置的默认值： ​在初始化资源锁的时候还传入了 EventRecorder，其作用是当 leader 发生变化的时候会将对应的 events 发送到 apiserver。 2、rl 资源锁被用于 controller-manager 进行 leader 的选举，RunOrDie 方法中就是 leader 的选举过程了。 3、Callbacks 中定义了在切换状态后需要执行的操作，当成为 leader 后会执行 OnStartedLeading 中的 run 方法，run 方法是 controller-manager 的核心，run 方法中会初始化并启动所包含资源的 controller，以下是 kube-controller-manager 中所有的 controller： 123456789101112131415161718192021222324252627282930313233343536373839func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc &#123; controllers := map[string]InitFunc&#123;&#125; controllers[&quot;endpoint&quot;] = startEndpointController controllers[&quot;replicationcontroller&quot;] = startReplicationController controllers[&quot;podgc&quot;] = startPodGCController controllers[&quot;resourcequota&quot;] = startResourceQuotaController controllers[&quot;namespace&quot;] = startNamespaceController controllers[&quot;serviceaccount&quot;] = startServiceAccountController controllers[&quot;garbagecollector&quot;] = startGarbageCollectorController controllers[&quot;daemonset&quot;] = startDaemonSetController controllers[&quot;job&quot;] = startJobController controllers[&quot;deployment&quot;] = startDeploymentController controllers[&quot;replicaset&quot;] = startReplicaSetController controllers[&quot;horizontalpodautoscaling&quot;] = startHPAController controllers[&quot;disruption&quot;] = startDisruptionController controllers[&quot;statefulset&quot;] = startStatefulSetController controllers[&quot;cronjob&quot;] = startCronJobController controllers[&quot;csrsigning&quot;] = startCSRSigningController controllers[&quot;csrapproving&quot;] = startCSRApprovingController controllers[&quot;csrcleaner&quot;] = startCSRCleanerController controllers[&quot;ttl&quot;] = startTTLController controllers[&quot;bootstrapsigner&quot;] = startBootstrapSignerController controllers[&quot;tokencleaner&quot;] = startTokenCleanerController controllers[&quot;nodeipam&quot;] = startNodeIpamController if loopMode == IncludeCloudLoops &#123; controllers[&quot;service&quot;] = startServiceController controllers[&quot;route&quot;] = startRouteController &#125; controllers[&quot;nodelifecycle&quot;] = startNodeLifecycleController controllers[&quot;persistentvolume-binder&quot;] = startPersistentVolumeBinderController controllers[&quot;attachdetach&quot;] = startAttachDetachController controllers[&quot;persistentvolume-expander&quot;] = startVolumeExpandController controllers[&quot;clusterrole-aggregation&quot;] = startClusterRoleAggregrationController controllers[&quot;pvc-protection&quot;] = startPVCProtectionController controllers[&quot;pv-protection&quot;] = startPVProtectionController controllers[&quot;ttl-after-finished&quot;] = startTTLAfterFinishedController return controllers&#125; OnStoppedLeading 是从 leader 状态切换为 slave 要执行的操作，此方法仅打印了一条日志。 12345678910func RunOrDie(ctx context.Context, lec LeaderElectionConfig) &#123; le, err := NewLeaderElector(lec) if err != nil &#123; panic(err) &#125; if lec.WatchDog != nil &#123; lec.WatchDog.SetLeaderElection(le) &#125; le.Run(ctx)&#125; 在 RunOrDie 中首先调用 NewLeaderElector 初始化了一个 LeaderElector 对象，然后执行 LeaderElector 的 run 方法进行选举。 12345678910111213func (le *LeaderElector) Run(ctx context.Context) &#123; defer func() &#123; runtime.HandleCrash() le.config.Callbacks.OnStoppedLeading() &#125;() if !le.acquire(ctx) &#123; return // ctx signalled done &#125; ctx, cancel := context.WithCancel(ctx) defer cancel() go le.config.Callbacks.OnStartedLeading(ctx) le.renew(ctx)&#125; Run 中首先会执行 acquire 尝试获取锁，获取到锁之后会回调 OnStartedLeading 启动所需要的 controller，然后会执行 renew 方法定期更新锁，保持 leader 的状态。 12345678910111213141516171819202122func (le *LeaderElector) acquire(ctx context.Context) bool &#123; ctx, cancel := context.WithCancel(ctx) defer cancel() succeeded := false desc := le.config.Lock.Describe() glog.Infof(&quot;attempting to acquire leader lease %v...&quot;, desc) wait.JitterUntil(func() &#123; // 尝试创建或者续约资源锁 succeeded = le.tryAcquireOrRenew() // leader 可能发生了改变，在 maybeReportTransition 方法中会 // 执行相应的 OnNewLeader() 回调函数,代码中对 OnNewLeader() 并没有初始化 le.maybeReportTransition() if !succeeded &#123; glog.V(4).Infof(&quot;failed to acquire lease %v&quot;, desc) return &#125; le.config.Lock.RecordEvent(&quot;became leader&quot;) glog.Infof(&quot;successfully acquired lease %v&quot;, desc) cancel() &#125;, le.config.RetryPeriod, JitterFactor, true, ctx.Done()) return succeeded&#125; 在 acquire 中首先初始化了一个 ctx，通过 wait.JitterUntil 周期性的去调用 le.tryAcquireOrRenew 方法来获取资源锁，直到获取为止。如果获取不到锁，则会以 RetryPeriod 为间隔不断尝试。如果获取到锁，就会关闭 ctx 通知 wait.JitterUntil 停止尝试，tryAcquireOrRenew 是最核心的方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758func (le *LeaderElector) tryAcquireOrRenew() bool &#123; now := metav1.Now() leaderElectionRecord := rl.LeaderElectionRecord&#123; HolderIdentity: le.config.Lock.Identity(), LeaseDurationSeconds: int(le.config.LeaseDuration / time.Second), RenewTime: now, AcquireTime: now, &#125; // 1、获取当前的资源锁 oldLeaderElectionRecord, err := le.config.Lock.Get() if err != nil &#123; if !errors.IsNotFound(err) &#123; glog.Errorf(&quot;error retrieving resource lock %v: %v&quot;, le.config.Lock.Describe(), err) return false &#125; // 没有获取到资源锁，开始创建资源锁，若创建成功则成为 leader if err = le.config.Lock.Create(leaderElectionRecord); err != nil &#123; glog.Errorf(&quot;error initially creating leader election record: %v&quot;, err) return false &#125; le.observedRecord = leaderElectionRecord le.observedTime = le.clock.Now() return true &#125; // 2、获取资源锁后检查当前 id 是不是 leader if !reflect.DeepEqual(le.observedRecord, *oldLeaderElectionRecord) &#123; le.observedRecord = *oldLeaderElectionRecord le.observedTime = le.clock.Now() &#125; // 如果资源锁没有过期且当前 id 不是 Leader，直接返回 if le.observedTime.Add(le.config.LeaseDuration).After(now.Time) &amp;&amp; !le.IsLeader() &#123; glog.V(4).Infof(&quot;lock is held by %v and has not yet expired&quot;, oldLeaderElectionRecord.HolderIdentity) return false &#125; // 3、如果当前 id 是 Leader，将对应字段的时间改成当前时间，准备续租 // 如果是非 Leader 节点则抢夺资源锁 if le.IsLeader() &#123; leaderElectionRecord.AcquireTime = oldLeaderElectionRecord.AcquireTime leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions &#125; else &#123; leaderElectionRecord.LeaderTransitions = oldLeaderElectionRecord.LeaderTransitions + 1 &#125; // 更新资源 // 对于 Leader 来说，这是一个续租的过程 // 对于非 Leader 节点（仅在上一个资源锁已经过期），这是一个更新锁所有权的过程 if err = le.config.Lock.Update(leaderElectionRecord); err != nil &#123; glog.Errorf(&quot;Failed to update lock: %v&quot;, err) return false &#125; le.observedRecord = leaderElectionRecord le.observedTime = le.clock.Now() return true&#125; 上面的这个函数的主要逻辑： 1、获取 ElectionRecord 记录，如果没有则创建一条新的 ElectionRecord 记录，创建成功则表示获取到锁并成为 leader 了。 2、当获取到资源锁后开始检查其中的信息，比较当前 id 是不是 leader 以及资源锁有没有过期，如果资源锁没有过期且当前 id 不是 Leader，则直接返回。 3、如果当前 id 是 Leader，将对应字段的时间改成当前时间，更新资源锁进行续租。 4、如果当前 id 不是 Leader 但是资源锁已经过期了，则抢夺资源锁，抢夺成功则成为 leader 否则返回。 最后是 renew 方法： 1234567891011121314151617181920212223242526272829303132333435func (le *LeaderElector) renew(ctx context.Context) &#123; ctx, cancel := context.WithCancel(ctx) defer cancel() wait.Until(func() &#123; timeoutCtx, timeoutCancel := context.WithTimeout(ctx, le.config.RenewDeadline) defer timeoutCancel() // 每间隔 RetryPeriod 就执行 tryAcquireOrRenew() // 如果 tryAcquireOrRenew() 返回 false 说明续租失败 err := wait.PollImmediateUntil(le.config.RetryPeriod, func() (bool, error) &#123; done := make(chan bool, 1) go func() &#123; defer close(done) done &lt;- le.tryAcquireOrRenew() &#125;() select &#123; case &lt;-timeoutCtx.Done(): return false, fmt.Errorf(&quot;failed to tryAcquireOrRenew %s&quot;, timeoutCtx.Err()) case result := &lt;-done: return result, nil &#125; &#125;, timeoutCtx.Done()) le.maybeReportTransition() desc := le.config.Lock.Describe() if err == nil &#123; glog.V(4).Infof(&quot;successfully renewed lease %v&quot;, desc) return &#125; // 续租失败，说明已经不是 Leader，然后程序 panic le.config.Lock.RecordEvent(&quot;stopped leading&quot;) glog.Infof(&quot;failed to renew lease %v: %v&quot;, desc, err) cancel() &#125;, le.config.RetryPeriod, ctx.Done())&#125; 获取到锁之后定期进行更新，renew 只有在获取锁之后才会调用，它会通过持续更新资源锁的数据，来确保继续持有已获得的锁，保持自己的 leader 状态。 Leader Election 功能的使用以下是一个 demo，使用 k8s 中 k8s.io/client-go/tools/leaderelection 进行一个演示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package mainimport ( &quot;context&quot; &quot;flag&quot; &quot;fmt&quot; &quot;os&quot; &quot;time&quot; &quot;github.com/golang/glog&quot; &quot;k8s.io/api/core/v1&quot; &quot;k8s.io/client-go/kubernetes&quot; &quot;k8s.io/client-go/kubernetes/scheme&quot; v1core &quot;k8s.io/client-go/kubernetes/typed/core/v1&quot; &quot;k8s.io/client-go/tools/clientcmd&quot; &quot;k8s.io/client-go/tools/leaderelection&quot; &quot;k8s.io/client-go/tools/leaderelection/resourcelock&quot; &quot;k8s.io/client-go/tools/record&quot;)var ( masterURL string kubeconfig string)func init() &#123; flag.StringVar(&amp;kubeconfig, &quot;kubeconfig&quot;, &quot;&quot;, &quot;Path to a kubeconfig. Only required if out-of-cluster.&quot;) flag.StringVar(&amp;masterURL, &quot;master&quot;, &quot;&quot;, &quot;The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.&quot;) flag.Set(&quot;logtostderr&quot;, &quot;true&quot;)&#125;func main() &#123; flag.Parse() defer glog.Flush() id, err := os.Hostname() if err != nil &#123; panic(err) &#125; // 加载 kubeconfig 配置 cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) if err != nil &#123; glog.Fatalf(&quot;Error building kubeconfig: %s&quot;, err.Error()) &#125; // 创建 kubeclient kubeClient, err := kubernetes.NewForConfig(cfg) if err != nil &#123; glog.Fatalf(&quot;Error building kubernetes clientset: %s&quot;, err.Error()) &#125; // 初始化 eventRecorder eventBroadcaster := record.NewBroadcaster() eventRecorder := eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource&#123;Component: &quot;test-1&quot;&#125;) eventBroadcaster.StartLogging(glog.Infof) eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeClient.CoreV1().Events(&quot;&quot;)&#125;) run := func(ctx context.Context) &#123; fmt.Println(&quot;run.........&quot;) select &#123;&#125; &#125; id = id + &quot;_&quot; + &quot;1&quot; rl, err := resourcelock.New(&quot;endpoints&quot;, &quot;kube-system&quot;, &quot;test&quot;, kubeClient.CoreV1(), resourcelock.ResourceLockConfig&#123; Identity: id, EventRecorder: eventRecorder, &#125;) if err != nil &#123; glog.Fatalf(&quot;error creating lock: %v&quot;, err) &#125; leaderelection.RunOrDie(context.TODO(), leaderelection.LeaderElectionConfig&#123; Lock: rl, LeaseDuration: 15 * time.Second, RenewDeadline: 10 * time.Second, RetryPeriod: 2 * time.Second, Callbacks: leaderelection.LeaderCallbacks&#123; OnStartedLeading: run, OnStoppedLeading: func() &#123; glog.Info(&quot;leaderelection lost&quot;) &#125;, &#125;, Name: &quot;test-1&quot;, &#125;)&#125; 分别使用多个 hostname 同时运行后并测试 leader 切换，可以在 events 中看到 leader 切换的记录： 1234567891011# kubectl describe endpoints test -n kube-systemName: testNamespace: kube-systemLabels: &lt;none&gt;Annotations: control-plane.alpha.kubernetes.io/leader=&#123;&quot;holderIdentity&quot;:&quot;localhost_2&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2019-03-10T08:47:42Z&quot;,&quot;renewTime&quot;:&quot;2019-03-10T08:47:44Z&quot;,&quot;leaderTransitions&quot;:2&#125;Subsets:Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal LeaderElection 50s test-1 localhost_1 became leader Normal LeaderElection 5s test-2 localhost_2 became leader 总结本文讲述了 kube-controller-manager 使用 HA 的方式启动后 leader 选举过程的实现说明，k8s 中通过创建 endpoints 资源以及对该资源的持续更新来实现资源锁轮转的过程。但是相对于其他分布式锁的实现，普遍是直接基于现有的中间件实现，比如 redis、zookeeper、etcd 等，其所有对锁的操作都是原子性的，那 k8s 选举过程中的原子操作是如何实现的？k8s 中的原子操作最终也是通过 etcd 实现的，其在做 update 更新锁的操作时采用的是乐观锁，通过对比 resourceVersion 实现的，详细的实现下节再讲。 参考文档：API OVERVIEWSimple leader election with Kubernetes and Docker]]></content>
      <tags>
        <tag>leader-election</tag>
        <tag>component</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 集群升级至 v1.12 需要注意的几个问题]]></title>
    <url>%2F2019%2F03%2F05%2Fk8s_v1.12%2F</url>
    <content type="text"><![CDATA[最近我们生产环境的集群开始升级至 v1.12 版本了，之前的版本是 v1.8，由于跨了多个版本，风险还是比较大的，官方的建议也是一个一个版本升级，k8s 每三个月出一个版本，集群上了规模后升级太麻烦，鉴于我们真正使用 k8s 中的功能还是比较少的，耦合性没有那么大，所以风险还是相对可控，测试环境运行 v1.12 一段时间后发现问题不大，于是开始升级。此处记录几个升级过程要注意的问题： 1、注意 k8s 中 resource version 的变化k8s 中许多 resouce 都是随着 k8s 的版本变化而变化的，例如，statefulset 在 v1.8 版本中 apiVersion 是 apps/v1beta1，在 v1.12 中变为了 apps/v1。k8s 有接口可以获取到当前版本所有的 OpenAPI ： 参考文档：The Kubernetes API 虽然 k8s 中 resource version 都是向下兼容的，但是在升级完成后尽量使用当前版本的 resource version 避免不必要的麻烦。 2、kubelet 配置文件格式v1.8 中 kubelet 的配置是在 /etc/kubernetes/kubelet 文件中的 KUBELET_ARGS 后面指定，但是在 v1.12 中开始使用 config.yaml 文件，即所有的配置都可以放在 yaml 文件中，由于配置是兼容的，所以暂时也可以继续用以前的方式，其中有些参数仅支持在 config.yaml 文件中指定。 config.yaml 文件的官方说明：Set Kubelet parameters via a config file 一个例子： 12345678910111213141516171819202122232425262728apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0- podseventBurst: 10eventRecordQPS: 5evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0sfailSwapOn: truefileCheckFrequency: 20shealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20simageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0smaxOpenFiles: 1000000maxPods: 110nodeLeaseDurationSeconds: 40nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1port: 10250staticPodPath: /etc/kubernetes/manifests 将 kubelet 配置文件中的 LOG_LEVEL 参数改为大于等于 5 可以看到 config.yaml 中配置的定义，以方便排查问题： 对应的日志输出： 123456789I0228 16:14:14.064292 191819 server.go:260] KubeletConfiguration: config.KubeletConfiguration&#123;TypeMeta:v1.TypeMeta&#123;Kind:&quot;&quot;, APIVersion:&quot;&quot;&#125;, StaticPodPath:&quot;&quot;, Sync Frequency:v1.Duration&#123;Duration:60000000000&#125;, FileCheckFrequency:v1.Duration&#123;Duration:20000000000&#125;,HTTPCheckFrequency:v1.Duration&#123;Duration:20000000000&#125;, StaticPodURL:&quot;&quot;, StaticPodURLHeader:map[string][]string(nil),Address:&quot;0.0.0.0&quot;, Port:10250, ... 注意：kubelet 配置文件中 ARGS 中定义的参数会覆盖 config.yaml 中的定义。 3、feature-gates 中功能的使用v1.12 中 feature-gates 中许多功能默认为开启状态，需要根据实际场景选择，不必要的功能在配置文件中将其关闭。 k8s 各版本中的 Feature 列表以及是否启用状态可以在 Feature Gates 中查看。 4、cadvisor 的使用k8s 在 1.12 中将 cadvisor 从 kubelet 中移除了，若要使用 cadvisor，官方建议使用 DaemonSet 进行部署。由于我们一直从 cadvisor 获取容器的监控数据然后推送到自有的监控系统中进行展示，所以 cadvisor 还得继续使用。 这是官方推荐的 cadvisor 部署方法，cAdvisor Kubernetes Daemonset，其中用了 k8s.gcr.io/cadvisor:v0.30.2 镜像，在我们的测试环境中，该镜像无法启动，报错 /sys/fs/cgroup/cpuacct,cpu: no such file or directory, 经查 cadvisor v0.30.2 版本的镜像使用 cgroup v2，v2 版本中已经没有了 cpuacct subsystem，而 linux kernel 4.5 以上的版本才支持 cgroup v2，与我们的实际场景不太相符，最后测试发现 v0.28.0 的镜像可以正常使用。 由于要兼容之前的使用方式，cadvisor 在宿主机上需要启动 4194 端口，但是创建容器又要结合自身的网络方案，最终我们使用 hostnetwork 的方式部署。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566apiVersion: apps/v1kind: DaemonSetmetadata: name: cadvisor namespace: kube-system labels: app: cadvisorspec: selector: matchLabels: name: cadvisor template: metadata: labels: name: cadvisor spec: hostNetwork: true tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule key: enabledDiskSchedule value: &quot;true&quot; effect: NoSchedule containers: - name: cadvisor image: k8s.gcr.io/cadvisor:v0.28.0 imagePullPolicy: IfNotPresent volumeMounts: - name: rootfs mountPath: /rootfs readOnly: true - name: var-run mountPath: /var/run readOnly: false - name: sys mountPath: /sys readOnly: true - name: docker mountPath: /var/lib/docker readOnly: true ports: - name: http containerPort: 4194 protocol: TCP readinessProbe: tcpSocket: port: 4194 initialDelaySeconds: 5 periodSeconds: 10 args: - --housekeeping_interval=10s - --port=4194 terminationGracePeriodSeconds: 30 volumes: - name: rootfs hostPath: path: / - name: var-run hostPath: path: /var/run - name: sys hostPath: path: /sys - name: docker hostPath: path: /var/lib/docker 官方建议使用 kustomize 进行部署，kustomize 是 k8s 的一个配置管理工具，此处暂不详细解释。 注意：若集群中有打 taint 的宿主，需要在 yaml 文件中加上对应的 tolerations。]]></content>
      <tags>
        <tag>kubernetes v1.12</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernets 中事件处理机制]]></title>
    <url>%2F2019%2F02%2F26%2Fk8s_events%2F</url>
    <content type="text"><![CDATA[当集群中的 node 或 pod 异常时，大部分用户会使用 kubectl 查看对应的 events，那么 events 是从何而来的？其实 k8s 中的各个组件会将运行时产生的各种事件汇报到 apiserver，对于 k8s 中的可描述资源，使用 kubectl describe 都可以看到其相关的 events，那 k8s 中又有哪几个组件都上报 events 呢？ 只要在 k8s.io/kubernetes/cmd 目录下暴力搜索一下就能知道哪些组件会产生 events：1$ grep -R -n -i &quot;EventRecorder&quot; . 可以看出，controller-manage、kube-proxy、kube-scheduler、kubelet 都使用了 EventRecorder，本文只讲述 kubelet 中对 Events 的使用。 1、Events 的定义events 在 k8s.io/api/core/v1/types.go 中进行定义,结构体如下所示： 12345678910111213141516171819type Event struct &#123; metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ObjectMeta `json:&quot;metadata&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;` InvolvedObject ObjectReference `json:&quot;involvedObject&quot; protobuf:&quot;bytes,2,opt,name=involvedObject&quot;` Reason string `json:&quot;reason,omitempty&quot; protobuf:&quot;bytes,3,opt,name=reason&quot;` Message string `json:&quot;message,omitempty&quot; protobuf:&quot;bytes,4,opt,name=message&quot;` Source EventSource `json:&quot;source,omitempty&quot; protobuf:&quot;bytes,5,opt,name=source&quot;` FirstTimestamp metav1.Time `json:&quot;firstTimestamp,omitempty&quot; protobuf:&quot;bytes,6,opt,name=firstTimestamp&quot;` LastTimestamp metav1.Time `json:&quot;lastTimestamp,omitempty&quot; protobuf:&quot;bytes,7,opt,name=lastTimestamp&quot;` Count int32 `json:&quot;count,omitempty&quot; protobuf:&quot;varint,8,opt,name=count&quot;` Type string `json:&quot;type,omitempty&quot; protobuf:&quot;bytes,9,opt,name=type&quot;` EventTime metav1.MicroTime `json:&quot;eventTime,omitempty&quot; protobuf:&quot;bytes,10,opt,name=eventTime&quot;` Series *EventSeries `json:&quot;series,omitempty&quot; protobuf:&quot;bytes,11,opt,name=series&quot;` Action string `json:&quot;action,omitempty&quot; protobuf:&quot;bytes,12,opt,name=action&quot;` Related *ObjectReference `json:&quot;related,omitempty&quot; protobuf:&quot;bytes,13,opt,name=related&quot;` ReportingController string `json:&quot;reportingComponent&quot; protobuf:&quot;bytes,14,opt,name=reportingComponent&quot;` ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;` ReportingInstance string `json:&quot;reportingInstance&quot; protobuf:&quot;bytes,15,opt,name=reportingInstance&quot;`&#125; 其中 InvolvedObject 代表和事件关联的对象，source 代表事件源，使用 kubectl 看到的事件一般包含 Type、Reason、Age、From、Message 几个字段。 k8s 中 events 目前只有两种类型：”Normal” 和 “Warning”： 2、EventBroadcaster 的初始化events 的整个生命周期都与 EventBroadcaster 有关，kubelet 中对 EventBroadcaster 的初始化在k8s.io/kubernetes/cmd/kubelet/app/server.go中： 1234567891011121314151617181920212223242526func RunKubelet(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce bool) error &#123; ... // event 初始化 makeEventRecorder(kubeDeps, nodeName) ...&#125;func makeEventRecorder(kubeDeps *kubelet.Dependencies, nodeName types.NodeName) &#123; if kubeDeps.Recorder != nil &#123; return &#125; // 初始化 EventBroadcaster eventBroadcaster := record.NewBroadcaster() // 初始化 EventRecorder kubeDeps.Recorder = eventBroadcaster.NewRecorder(legacyscheme.Scheme, v1.EventSource&#123;Component: componentKubelet, Host: string(nodeName)&#125;) // 记录 events 到本地日志 eventBroadcaster.StartLogging(glog.V(3).Infof) if kubeDeps.EventClient != nil &#123; glog.V(4).Infof(&quot;Sending events to api server.&quot;) // 上报 events 到 apiserver eventBroadcaster.StartRecordingToSink(&amp;v1core.EventSinkImpl&#123;Interface: kubeDeps.EventClient.Events(&quot;&quot;)&#125;) &#125; else &#123; glog.Warning(&quot;No api server defined - no events will be sent to API server.&quot;) &#125;&#125; Kubelet 在启动的时候会初始化一个 EventBroadcaster，它主要是对接收到的 events 做一些后续的处理(保存、上报等），EventBroadcaster 也会被 kubelet 中的其他模块使用，以下是相关的定义，对 events 生成和处理的函数都定义在 k8s.io/client-go/tools/record/event.go 中： 123456789101112131415type eventBroadcasterImpl struct &#123; *watch.Broadcaster sleepDuration time.Duration&#125;// EventBroadcaster knows how to receive events and send them to any EventSink, watcher, or log.type EventBroadcaster interface &#123; StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface StartRecordingToSink(sink EventSink) watch.Interface StartLogging(logf func(format string, args ...interface&#123;&#125;)) watch.Interface NewRecorder(scheme *runtime.Scheme, source v1.EventSource) EventRecorder&#125; EventBroadcaster 是个接口类型，该接口有以下四个方法： StartEventWatcher() ： EventBroadcaster 中的核心方法，接收各模块产生的 events，参数为一个处理 events 的函数，用户可以使用 StartEventWatcher() 接收 events 然后使用自定义的 handle 进行处理 StartRecordingToSink() ： 调用 StartEventWatcher() 接收 events，并将收到的 events 发送到 apiserver StartLogging() ：也是调用 StartEventWatcher() 接收 events，然后保存 events 到日志 NewRecorder() ：会创建一个指定 EventSource 的 EventRecorder，EventSource 指明了哪个节点的哪个组件 eventBroadcasterImpl 是 eventBroadcaster 实际的对象，初始化 EventBroadcaster 对象的时候会初始化一个 Broadcaster，Broadcaster 会启动一个 goroutine 接收各组件产生的 events 并广播到每一个 watcher。 123func NewBroadcaster() EventBroadcaster &#123; return &amp;eventBroadcasterImpl&#123;watch.NewBroadcaster(maxQueuedEvents, watch.DropIfChannelFull), defaultSleepDuration&#125;&#125; 可以看到，kubelet 在初始化完 EventBroadcaster 后会调用 StartRecordingToSink() 和 StartLogging() 两个方法，StartRecordingToSink() 处理函数会将收到的 events 进行缓存、过滤、聚合而后发送到 apiserver，StartLogging() 仅将 events 保存到 kubelet 的日志中。 3、Events 的生成从初始化 EventBroadcaster 的代码中可以看到 kubelet 在初始化完 EventBroadcaster 后紧接着初始化了 EventRecorder，并将已经初始化的 Broadcaster 对象作为参数传给了 EventRecorder，至此，EventBroadcaster、EventRecorder、Broadcaster 三个对象产生了关联。EventRecorder 的主要功能是生成指定格式的 events，以下是相关的定义： 12345678910111213141516type recorderImpl struct &#123; scheme *runtime.Scheme source v1.EventSource *watch.Broadcaster clock clock.Clock&#125;type EventRecorder interface &#123; Event(object runtime.Object, eventtype, reason, message string) Eventf(object runtime.Object, eventtype, reason, messageFmt string, args ...interface&#123;&#125;) PastEventf(object runtime.Object, timestamp metav1.Time, eventtype, reason, messageFmt string, args ...interface&#123;&#125;) AnnotatedEventf(object runtime.Object, annotations map[string]string, eventtype, reason, messageFmt string, args ...interface&#123;&#125;)&#125; EventRecorder 中包含的几个方法都是产生指定格式的 events，Event() 和 Eventf() 的功能类似 fmt.Println() 和 fmt.Printf()，kubelet 中的各个模块会调用 EventRecorder 生成 events。recorderImpl 是 EventRecorder 实际的对象。EventRecorder 的每个方法会调用 generateEvent，在 generateEvent 中初始化 events 。 以下是生成 events 的函数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344func (recorder *recorderImpl) generateEvent(object runtime.Object, annotations map[string]string, timestamp metav1.Time, eventtype, reason, message string) &#123; ref, err := ref.GetReference(recorder.scheme, object) if err != nil &#123; glog.Errorf(&quot;Could not construct reference to: &apos;%#v&apos; due to: &apos;%v&apos;. Will not report event: &apos;%v&apos; &apos;%v&apos; &apos;%v&apos;&quot;, object, err, eventtype, reason, message) return &#125; if !validateEventType(eventtype) &#123; glog.Errorf(&quot;Unsupported event type: &apos;%v&apos;&quot;, eventtype) return &#125; event := recorder.makeEvent(ref, annotations, eventtype, reason, message) event.Source = recorder.source go func() &#123; // NOTE: events should be a non-blocking operation defer utilruntime.HandleCrash() // 发送事件 recorder.Action(watch.Added, event) &#125;()&#125;func (recorder *recorderImpl) makeEvent(ref *v1.ObjectReference, annotations map[string]string, eventtype, reason, message string) *v1.Event &#123; t := metav1.Time&#123;Time: recorder.clock.Now()&#125; namespace := ref.Namespace if namespace == &quot;&quot; &#123; namespace = metav1.NamespaceDefault &#125; return &amp;v1.Event&#123; ObjectMeta: metav1.ObjectMeta&#123; Name: fmt.Sprintf(&quot;%v.%x&quot;, ref.Name, t.UnixNano()), Namespace: namespace, Annotations: annotations, &#125;, InvolvedObject: *ref, Reason: reason, Message: message, FirstTimestamp: t, LastTimestamp: t, Count: 1, Type: eventtype, &#125;&#125; 初始化完 events 后会调用 recorder.Action() 将 events 发送到 Broadcaster 的事件接收队列中, Action() 是 Broadcaster 中的方法。 以下是 Action() 方法的实现： 123func (m *Broadcaster) Action(action EventType, obj runtime.Object) &#123; m.incoming &lt;- Event&#123;action, obj&#125;&#125; 4、Events 的广播上面已经说了，EventBroadcaster 初始化时会初始化一个 Broadcaster，Broadcaster 的作用就是接收所有的 events 并进行广播，Broadcaster 的实现在 k8s.io/apimachinery/pkg/watch/mux.go 中，Broadcaster 初始化完成后会在后台启动一个 goroutine，然后接收所有从 EventRecorder 发送过来的 events，Broadcaster 中有一个 map 会保存每一个注册的 watcher， 接着将 events 广播给所有的 watcher，每个 watcher 都有一个接收消息的 channel，watcher 可以通过它的 ResultChan() 方法从 channel 中读取数据进行消费。 以下是 Broadcaster 广播 events 的实现：123456789101112131415161718192021222324252627282930313233func (m *Broadcaster) loop() &#123; for event := range m.incoming &#123; if event.Type == internalRunFunctionMarker &#123; event.Object.(functionFakeRuntimeObject)() continue &#125; m.distribute(event) &#125; m.closeAll() m.distributing.Done()&#125;// distribute sends event to all watchers. Blocking.func (m *Broadcaster) distribute(event Event) &#123; m.lock.Lock() defer m.lock.Unlock() if m.fullChannelBehavior == DropIfChannelFull &#123; for _, w := range m.watchers &#123; select &#123; case w.result &lt;- event: case &lt;-w.stopped: default: // Don&apos;t block if the event can&apos;t be queued. &#125; &#125; &#125; else &#123; for _, w := range m.watchers &#123; select &#123; case w.result &lt;- event: case &lt;-w.stopped: &#125; &#125; &#125;&#125; 5、Events 的处理那么 watcher 是从何而来呢？每一个要处理 events 的 client 都需要初始化一个 watcher，处理 events 的方法是在 EventBroadcaster 中定义的，以下是 EventBroadcaster 中对 events 处理的三个函数： 12345678910111213141516func (eventBroadcaster *eventBroadcasterImpl) StartEventWatcher(eventHandler func(*v1.Event)) watch.Interface &#123; watcher := eventBroadcaster.Watch() go func() &#123; defer utilruntime.HandleCrash() for watchEvent := range watcher.ResultChan() &#123; event, ok := watchEvent.Object.(*v1.Event) if !ok &#123; // This is all local, so there&apos;s no reason this should // ever happen. continue &#125; eventHandler(event) &#125; &#125;() return watcher&#125; StartEventWatcher() 首先实例化一个 watcher，每个 watcher 都会被塞入到 Broadcaster 的 watcher 列表中，watcher 从 Broadcaster 提供的 channel 中读取 events，然后再调用 eventHandler 进行处理，StartLogging() 和 StartRecordingToSink() 都是对 StartEventWatcher() 的封装，都会传入自己的处理函数。 123456func (eventBroadcaster *eventBroadcasterImpl) StartLogging(logf func(format string, args ...interface&#123;&#125;)) watch.Interface &#123; return eventBroadcaster.StartEventWatcher( func(e *v1.Event) &#123; logf(&quot;Event(%#v): type: &apos;%v&apos; reason: &apos;%v&apos; %v&quot;, e.InvolvedObject, e.Type, e.Reason, e.Message) &#125;)&#125; StartLogging() 传入的 eventHandler 仅将 events 保存到日志中。 123456789101112131415161718192021222324252627282930313233343536373839func (eventBroadcaster *eventBroadcasterImpl) StartRecordingToSink(sink EventSink) watch.Interface &#123; // The default math/rand package functions aren&apos;t thread safe, so create a // new Rand object for each StartRecording call. randGen := rand.New(rand.NewSource(time.Now().UnixNano())) eventCorrelator := NewEventCorrelator(clock.RealClock&#123;&#125;) return eventBroadcaster.StartEventWatcher( func(event *v1.Event) &#123; recordToSink(sink, event, eventCorrelator, randGen, eventBroadcaster.sleepDuration) &#125;)&#125;func recordToSink(sink EventSink, event *v1.Event, eventCorrelator *EventCorrelator, randGen *rand.Rand, sleepDuration time.Duration) &#123; eventCopy := *event event = &amp;eventCopy result, err := eventCorrelator.EventCorrelate(event) if err != nil &#123; utilruntime.HandleError(err) &#125; if result.Skip &#123; return &#125; tries := 0 for &#123; if recordEvent(sink, result.Event, result.Patch, result.Event.Count &gt; 1, eventCorrelator) &#123; break &#125; tries++ if tries &gt;= maxTriesPerEvent &#123; glog.Errorf(&quot;Unable to write event &apos;%#v&apos; (retry limit exceeded!)&quot;, event) break &#125; // 第一次重试增加随机性，防止 apiserver 重启的时候所有的事件都在同一时间发送事件 if tries == 1 &#123; time.Sleep(time.Duration(float64(sleepDuration) * randGen.Float64())) &#125; else &#123; time.Sleep(sleepDuration) &#125; &#125;&#125; StartRecordingToSink() 方法先根据当前时间生成一个随机数发生器 randGen，增加随机数是为了在重试时增加随机性，防止 apiserver 重启的时候所有的事件都在同一时间发送事件，接着实例化一个EventCorrelator，EventCorrelator 会对事件做一些预处理的工作，其中包括过滤、聚合、缓存等操作，具体代码不做详细分析，最后将 recordToSink() 函数作为处理函数，recordToSink() 会将处理后的 events 发送到 apiserver，这是 StartEventWatcher() 的整个工作流程。 6、Events 简单实现了解完 events 的整个处理流程后，可以参考其实现方式写一个 demo，要实现一个完整的 events 需要包含以下几个功能： 1、事件的产生 2、事件的发送 3、事件广播 4、事件缓存 5、事件过滤和聚合 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201package mainimport ( &quot;fmt&quot; &quot;sync&quot; &quot;time&quot;)// watcher queueconst queueLength = int64(1)// Events xxxtype Events struct &#123; Reason string Message string Source string Type string Count int64 Timestamp time.Time&#125;// EventBroadcaster xxxtype EventBroadcaster interface &#123; Event(etype, reason, message string) StartLogging() Interface Stop()&#125;// eventBroadcaster xxxtype eventBroadcasterImpl struct &#123; *Broadcaster&#125;func NewEventBroadcaster() EventBroadcaster &#123; return &amp;eventBroadcasterImpl&#123;NewBroadcaster(queueLength)&#125;&#125;func (eventBroadcaster *eventBroadcasterImpl) Stop() &#123; eventBroadcaster.Shutdown()&#125;// generate eventfunc (eventBroadcaster *eventBroadcasterImpl) Event(etype, reason, message string) &#123; events := &amp;Events&#123;Type: etype, Reason: reason, Message: message&#125; // send event to broadcast eventBroadcaster.Action(events)&#125;// 仅实现 StartLogging() 的功能，将日志打印func (eventBroadcaster *eventBroadcasterImpl) StartLogging() Interface &#123; // register a watcher watcher := eventBroadcaster.Watch() go func() &#123; for watchEvent := range watcher.ResultChan() &#123; fmt.Printf(&quot;%v\n&quot;, watchEvent) &#125; &#125;() go func() &#123; time.Sleep(time.Second * 4) watcher.Stop() &#125;() return watcher&#125;// --------------------// Broadcaster 定义与实现// 接收 events channel 的长度const incomingQueuLength = 100type Broadcaster struct &#123; lock sync.Mutex incoming chan Events watchers map[int64]*broadcasterWatcher watchersQueue int64 watchQueueLength int64 distributing sync.WaitGroup&#125;func NewBroadcaster(queueLength int64) *Broadcaster &#123; m := &amp;Broadcaster&#123; incoming: make(chan Events, incomingQueuLength), watchers: map[int64]*broadcasterWatcher&#123;&#125;, watchQueueLength: queueLength, &#125; m.distributing.Add(1) // 后台启动一个 goroutine 广播 events go m.loop() return m&#125;// Broadcaster 接收所产生的 eventsfunc (m *Broadcaster) Action(event *Events) &#123; m.incoming &lt;- *event&#125;// 广播 events 到每个 watcherfunc (m *Broadcaster) loop() &#123; // 从 incoming channel 中读取所接收到的 events for event := range m.incoming &#123; // 发送 events 到每一个 watcher for _, w := range m.watchers &#123; select &#123; case w.result &lt;- event: case &lt;-w.stopped: default: &#125; &#125; &#125; m.closeAll() m.distributing.Done()&#125;func (m *Broadcaster) Shutdown() &#123; close(m.incoming) m.distributing.Wait()&#125;func (m *Broadcaster) closeAll() &#123; // TODO m.lock.Lock() defer m.lock.Unlock() for _, w := range m.watchers &#123; close(w.result) &#125; m.watchers = map[int64]*broadcasterWatcher&#123;&#125;&#125;func (m *Broadcaster) stopWatching(id int64) &#123; m.lock.Lock() defer m.lock.Unlock() w, ok := m.watchers[id] if !ok &#123; return &#125; delete(m.watchers, id) close(w.result)&#125;// 调用 Watch(）方法注册一个 watcherfunc (m *Broadcaster) Watch() Interface &#123; watcher := &amp;broadcasterWatcher&#123; result: make(chan Events, incomingQueuLength), stopped: make(chan struct&#123;&#125;), id: m.watchQueueLength, m: m, &#125; m.watchers[m.watchersQueue] = watcher m.watchQueueLength++ return watcher&#125;// watcher 实现type Interface interface &#123; Stop() ResultChan() &lt;-chan Events&#125;type broadcasterWatcher struct &#123; result chan Events stopped chan struct&#123;&#125; stop sync.Once id int64 m *Broadcaster&#125;// 每个 watcher 通过该方法读取 channel 中广播的 eventsfunc (b *broadcasterWatcher) ResultChan() &lt;-chan Events &#123; return b.result&#125;func (b *broadcasterWatcher) Stop() &#123; b.stop.Do(func() &#123; close(b.stopped) b.m.stopWatching(b.id) &#125;)&#125;// --------------------func main() &#123; eventBroadcast := NewEventBroadcaster() var wg sync.WaitGroup wg.Add(1) // producer event go func() &#123; defer wg.Done() time.Sleep(time.Second) eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;1&quot;) time.Sleep(time.Second * 2) eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;2&quot;) time.Sleep(time.Second * 3) eventBroadcast.Event(&quot;add&quot;, &quot;test&quot;, &quot;3&quot;) //eventBroadcast.Stop() &#125;() eventBroadcast.StartLogging() wg.Wait()&#125; 此处仅简单实现，将 EventRecorder 处理 events 的功能直接放在了 EventBroadcaster 中实现，对 events 的处理方法仅实现了 StartLogging()，Broadcaster 中的部分功能是直接复制 k8s 中的代码，有一定的精简，其实现值得学习，此处对 EventCorrelator 并没有进行实现。 代码请参考：https://github.com/gosoon/k8s-learning-notes/tree/master/k8s-package/events 7、总结本文讲述了 k8s 中 events 从产生到展示的一个完整过程，最后也实现了一个简单的 demo，在此将 kubelet 对 events 的整个处理过程再梳理下，其中主要有三个对象 EventBroadcaster、EventRecorder、Broadcaster： 1、kubelet 首先会初始化 EventBroadcaster 对象，同时会初始化一个 Broadcaster 对象。 2、kubelet 通过 EventBroadcaster 对象的 NewRecorder() 方法初始化 EventRecorder 对象，EventRecorder 对象提供的几个方法会生成 events 并通过 Action() 方法发送 events 到 Broadcaster 的 channel 队列中。 3、Broadcaster 的作用就是接收所有的 events 并进行广播，Broadcaster 初始化后会在后台启动一个 goroutine，然后接收所有从 EventRecorder 发来的 events。 4、EventBroadcaster 对 events 有三个处理方法：StartEventWatcher()、StartRecordingToSink()、StartLogging()，StartEventWatcher() 是其中的核心方法，会初始化一个 watcher 注册到 Broadcaster，其余两个处理函数对 StartEventWatcher() 进行了封装，并实现了自己的处理函数。 5、 Broadcaster 中有一个 map 会保存每一个注册的 watcher，其会将所有的 events 广播给每一个 watcher，每个 watcher 通过它的 ResultChan() 方法从 channel 接收 events。 6、kubelet 会使用 StartRecordingToSink() 和 StartLogging() 对 events 进行处理，StartRecordingToSink() 处理函数收到 events 后会进行缓存、过滤、聚合而后发送到 apiserver，apiserver 会将 events 保存到 etcd 中，使用 kubectl 或其他客户端可以查看。StartLogging() 仅将 events 保存到 kubelet 的日志中。]]></content>
      <tags>
        <tag>events</tag>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s 中定时任务的实现]]></title>
    <url>%2F2019%2F02%2F16%2Fk8s-crontab%2F</url>
    <content type="text"><![CDATA[k8s 中有许多优秀的包都可以在平时的开发中借鉴与使用，比如，任务的定时轮询、高可用的实现、日志处理、缓存使用等都是独立的包，可以直接引用。本篇文章会介绍 k8s 中定时任务的实现，k8s 中定时任务都是通过 wait 包实现的，wait 包在 k8s 的多个组件中都有用到，以下是 wait 包在 kubelet 中的几处使用： 123456789101112131415161718192021222324func run(s *options.KubeletServer, kubeDeps *kubelet.Dependencies, stopCh &lt;-chan struct&#123;&#125;) (err error) &#123; ... // kubelet 每5分钟一次从 apiserver 获取证书 closeAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute) if err != nil &#123; return err &#125; closeAllConns, err := kubeletcertificate.UpdateTransport(wait.NeverStop, clientConfig, clientCertificateManager, 5*time.Minute) if err != nil &#123; return err &#125; ...&#125;...func startKubelet(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableServer bool) &#123; // 持续监听 pod 的变化 go wait.Until(func() &#123; k.Run(podCfg.Updates()) &#125;, 0, wait.NeverStop) ...&#125; golang 中可以通过 time.Ticker 实现定时任务的执行，但在 k8s 中用了更原生的方式，使用 time.Timer 实现的。time.Ticker 和 time.Timer 的使用区别如下： ticker 只要定义完成，从此刻开始计时，不需要任何其他的操作，每隔固定时间都会自动触发。 timer 定时器是到了固定时间后会执行一次，仅执行一次 如果 timer 定时器要每隔间隔的时间执行，实现 ticker 的效果，使用 func (t *Timer) Reset(d Duration) bool 一个示例： 1234567891011121314151617181920212223242526272829303132333435package mainimport ( &quot;fmt&quot; &quot;sync&quot; &quot;time&quot;)func main() &#123; var wg sync.WaitGroup timer1 := time.NewTimer(2 * time.Second) ticker1 := time.NewTicker(2 * time.Second) wg.Add(1) go func(t *time.Ticker) &#123; defer wg.Done() for &#123; &lt;-t.C fmt.Println(&quot;exec ticker&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;)) &#125; &#125;(ticker1) wg.Add(1) go func(t *time.Timer) &#123; defer wg.Done() for &#123; &lt;-t.C fmt.Println(&quot;exec timer&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;)) t.Reset(2 * time.Second) &#125; &#125;(timer1) wg.Wait()&#125; 一、wait 包中的核心代码核心代码（k8s.io/apimachinery/pkg/util/wait/wait.go）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func JitterUntil(f func(), period time.Duration, jitterFactor float64, sliding bool, stopCh &lt;-chan struct&#123;&#125;) &#123; var t *time.Timer var sawTimeout bool for &#123; select &#123; case &lt;-stopCh: return default: &#125; jitteredPeriod := period if jitterFactor &gt; 0.0 &#123; jitteredPeriod = Jitter(period, jitterFactor) &#125; if !sliding &#123; t = resetOrReuseTimer(t, jitteredPeriod, sawTimeout) &#125; func() &#123; defer runtime.HandleCrash() f() &#125;() if sliding &#123; t = resetOrReuseTimer(t, jitteredPeriod, sawTimeout) &#125; select &#123; case &lt;-stopCh: return case &lt;-t.C: sawTimeout = true &#125; &#125;&#125;...func resetOrReuseTimer(t *time.Timer, d time.Duration, sawTimeout bool) *time.Timer &#123; if t == nil &#123; return time.NewTimer(d) &#125; if !t.Stop() &amp;&amp; !sawTimeout &#123; &lt;-t.C &#125; t.Reset(d) return t&#125; 几个关键点的说明： 1、如果 sliding 为 true，则在 f() 运行之后计算周期。如果为 false，那么 period 包含 f() 的执行时间。 2、在 golang 中 select 没有优先级选择，为了避免额外执行 f(),在每次循环开始后会先判断 stopCh chan。 k8s 中 wait 包其实是对 time.Timer 做了一层封装实现。 二、wait 包常用的方法1、定期执行一个函数，永不停止，可以使用 Forever 方法：func Forever(f func(), period time.Duration) 2、在需要的时候停止循环，那么可以使用下面的方法，增加一个用于停止的 chan 即可，方法定义如下：func Until(f func(), period time.Duration, stopCh &lt;-chan struct{}) 上面的第三个参数 stopCh 就是用于退出无限循环的标志，停止的时候我们 close 掉这个 chan 就可以了。 3、有时候，我们还会需要在运行前去检查先决条件，在条件满足的时候才去运行某一任务，这时候可以使用 Poll 方法：func Poll(interval, timeout time.Duration, condition ConditionFunc) 这个函数会以 interval 为间隔，不断去检查 condition 条件是否为真，如果为真则可以继续后续处理；如果指定了 timeout 参数，则该函数也可以只常识指定的时间。 4、PollUntil 方法和上面的类似，但是没有 timeout 参数，多了一个 stopCh 参数，如下所示：PollUntil(interval time.Duration, condition ConditionFunc, stopCh &lt;-chan struct{}) error 此外还有 PollImmediate 、 PollInfinite 和 PollImmediateInfinite 方法。 三、总结本篇文章主要讲了 k8s 中定时任务的实现与对应包（wait）中方法的使用。通过阅读 k8s 的源代码，可以发现 k8s 中许多功能的实现也都是我们需要在平时工作中用的，其大部分包的性能都是经过大规模考验的，通过使用其相关的工具包不仅能学到大量的编程技巧也能避免自己造轮子。]]></content>
      <tags>
        <tag>crontab</tag>
        <tag>wait</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 审计日志功能]]></title>
    <url>%2F2019%2F01%2F30%2Fk8s-audit-webhook%2F</url>
    <content type="text"><![CDATA[审计日志可以记录所有对 apiserver 接口的调用，让我们能够非常清晰的知道集群到底发生了什么事情，通过记录的日志可以查到所发生的事件、操作的用户和时间。kubernetes 在 v1.7 中支持了日志审计功能（Alpha），在 v1.8 中为 Beta 版本，v1.12 为 GA 版本。 kubernetes feature-gates 中的功能 Alpha 版本默认为 false，到 Beta 版本时默认为 true，所以 v1.8 会默认启用审计日志的功能。 一、审计日志的策略1、日志记录阶段kube-apiserver 是负责接收及相应用户请求的一个组件，每一个请求都会有几个阶段，每个阶段都有对应的日志，当前支持的阶段有： RequestReceived - apiserver 在接收到请求后且在将该请求下发之前会生成对应的审计日志。 ResponseStarted - 在响应 header 发送后并在响应 body 发送前生成日志。这个阶段仅为长时间运行的请求生成（例如 watch）。 ResponseComplete - 当响应 body 发送完并且不再发送数据。 Panic - 当有 panic 发生时生成。 也就是说对 apiserver 的每一个请求理论上会有三个阶段的审计日志生成。 2、日志记录级别当前支持的日志记录级别有： None - 不记录日志。 Metadata - 只记录 Request 的一些 metadata (例如 user, timestamp, resource, verb 等)，但不记录 Request 或 Response 的body。 Request - 记录 Request 的 metadata 和 body。 RequestResponse - 最全记录方式，会记录所有的 metadata、Request 和 Response 的 body。 3、日志记录策略在记录日志的时候尽量只记录所需要的信息，不需要的日志尽可能不记录，避免造成系统资源的浪费。 一个请求不要重复记录，每个请求有三个阶段，只记录其中需要的阶段 不要记录所有的资源，不要记录一个资源的所有子资源 系统的请求不需要记录，kubelet、kube-proxy、kube-scheduler、kube-controller-manager 等对 kube-apiserver 的请求不需要记录 对一些认证信息（secerts、configmaps、token 等）的 body 不记录 k8s 审计日志的一个示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;kind&quot;: &quot;EventList&quot;, &quot;apiVersion&quot;: &quot;audit.k8s.io/v1beta1&quot;, &quot;Items&quot;: [ &#123; &quot;Level&quot;: &quot;Request&quot;, &quot;AuditID&quot;: &quot;793e7ae2-5ca7-4ad3-a632-19708d2f8265&quot;, &quot;Stage&quot;: &quot;RequestReceived&quot;, &quot;RequestURI&quot;: &quot;/api/v1/namespaces/default/pods/test-pre-sf-de7cc-0&quot;, &quot;Verb&quot;: &quot;get&quot;, &quot;User&quot;: &#123; &quot;Username&quot;: &quot;system:unsecured&quot;, &quot;UID&quot;: &quot;&quot;, &quot;Groups&quot;: [ &quot;system:masters&quot;, &quot;system:authenticated&quot; ], &quot;Extra&quot;: null &#125;, &quot;ImpersonatedUser&quot;: null, &quot;SourceIPs&quot;: [ &quot;192.168.1.11&quot; ], &quot;UserAgent&quot;: &quot;kube-scheduler/v1.12.2 (linux/amd64) kubernetes/73f3294/scheduler&quot;, &quot;ObjectRef&quot;: &#123; &quot;Resource&quot;: &quot;pods&quot;, &quot;Namespace&quot;: &quot;default&quot;, &quot;Name&quot;: &quot;test-pre-sf-de7cc-0&quot;, &quot;UID&quot;: &quot;&quot;, &quot;APIGroup&quot;: &quot;&quot;, &quot;APIVersion&quot;: &quot;v1&quot;, &quot;ResourceVersion&quot;: &quot;&quot;, &quot;Subresource&quot;: &quot;&quot; &#125;, &quot;ResponseStatus&quot;: null, &quot;RequestObject&quot;: null, &quot;ResponseObject&quot;: null, &quot;RequestReceivedTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;, &quot;StageTimestamp&quot;: &quot;2019-01-11T06:51:43.528703Z&quot;, &quot;Annotations&quot;: null &#125; ]&#125; 二、启用审计日志当前的审计日志支持两种收集方式：保存为日志文件和调用自定义的 webhook，在 v1.13 中还支持动态的 webhook。 1、将审计日志以 json 格式保存到本地文件apiserver 配置文件的 KUBE_API_ARGS 中需要添加如下参数：1--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/var/log/kube-audit --audit-log-format=json 日志保存到本地后再通过 fluentd 等其他组件进行收集。还有其他几个选项可以指定保留审计日志文件的最大天数、文件的最大数量、文件的大小等。 2、将审计日志打到后端指定的 webhook1--audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-webhook-config-file=/etc/kubernetes/audit-webhook-kubeconfig webhook 配置文件实际上是一个 kubeconfig，apiserver 会将审计日志发送 到指定的 webhook 后，webhook 接收到日志后可以再分发到 kafka 或其他组件进行收集。 audit-webhook-kubeconfig 示例：1234567891011121314apiVersion: v1clusters:- cluster: server: http://127.0.0.1:8081/audit/webhook name: metriccontexts:- context: cluster: metric user: &quot;&quot; name: default-contextcurrent-context: default-contextkind: Configpreferences: &#123;&#125;users: [] 前面提到过，apiserver 的每一个请求会记录三个阶段的审计日志，但是在实际中并不是需要所有的审计日志，官方也说明了启用审计日志会增加 apiserver 对内存的使用量。 Note: The audit logging feature increases the memory consumption of the API server because some context required for auditing is stored for each request. Additionally, memory consumption depends on the audit logging configuration. audit-policy.yaml 配置示例： 123456789101112131415161718192021222324apiVersion: audit.k8s.io/v1kind: Policy# ResponseStarted 阶段不记录omitStages: - &quot;ResponseStarted&quot;rules: # 记录用户对 pod 和 statefulset 的操作 - level: RequestResponse resources: - group: &quot;&quot; resources: [&quot;pods&quot;,&quot;pods/status&quot;] - group: &quot;apps&quot; resources: [&quot;statefulsets&quot;,&quot;statefulsets/scale&quot;] # kube-controller-manager、kube-scheduler 等已经认证过身份的请求不需要记录 - level: None userGroups: [&quot;system:authenticated&quot;] nonResourceURLs: - &quot;/api*&quot; - &quot;/version&quot; # 对 config、secret、token 等认证信息不记录请求体和返回体 - level: Metadata resources: - group: &quot;&quot; # core API group resources: [&quot;secrets&quot;, &quot;configmaps&quot;] 官方提供两个参考示例： Use fluentd to collect and distribute audit events from log file Use logstash to collect and distribute audit events from webhook backend 3、subresource 说明kubernetes 每个资源对象都有 subresource,通过调用 master 的 api 可以获取 kubernetes 中所有的 resource 以及对应的 subresource,比如 pod 有 logs、exec 等 subresource。 12获取所有 resource（ 1.10 之后使用）：$ curl 127.0.0.1:8080/openapi/v2 参考：https://kubernetes.io/docs/concepts/overview/kubernetes-api/ 三、webhook 的一个简单示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package mainimport ( &quot;encoding/json&quot; &quot;io/ioutil&quot; &quot;log&quot; &quot;net/http&quot; &quot;github.com/emicklei/go-restful&quot; &quot;github.com/gosoon/glog&quot; &quot;k8s.io/apiserver/pkg/apis/audit&quot;)func main() &#123; // NewContainer creates a new Container using a new ServeMux and default router (CurlyRouter) container := restful.NewContainer() ws := new(restful.WebService) ws.Path(&quot;/audit&quot;). Consumes(restful.MIME_JSON). Produces(restful.MIME_JSON) ws.Route(ws.POST(&quot;/webhook&quot;).To(AuditWebhook)) //WebService ws2被添加到container2中 container.Add(ws) server := &amp;http.Server&#123; Addr: &quot;:8081&quot;, Handler: container, &#125; //go consumer() log.Fatal(server.ListenAndServe())&#125;func AuditWebhook(req *restful.Request, resp *restful.Response) &#123; body, err := ioutil.ReadAll(req.Request.Body) if err != nil &#123; glog.Errorf(&quot;read body err is: %v&quot;, err) &#125; var eventList audit.EventList err = json.Unmarshal(body, &amp;eventList) if err != nil &#123; glog.Errorf(&quot;unmarshal failed with:%v,body is :\n&quot;, err, string(body)) return &#125; for _, event := range eventList.Items &#123; jsonBytes, err := json.Marshal(event) if err != nil &#123; glog.Infof(&quot;marshal failed with:%v,event is \n %+v&quot;, err, event) &#125; // 消费日志 asyncProducer(string(jsonBytes)) &#125; resp.AddHeader(&quot;Content-Type&quot;, &quot;application/json&quot;) resp.WriteEntity(&quot;success&quot;)&#125; 完整代码请参考：https://github.com/gosoon/k8s-audit-webhook 四、总结本文主要介绍了 kubernetes 的日志审计功能，kubernetes 最近也被爆出多个安全漏洞，安全问题是每个团队不可忽视的，kubernetes 虽然被多数公司用作私有云，但日志审计也是不可或缺的。 参考：https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/ttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/阿里云 Kubernetes 审计日志方案]]></content>
      <tags>
        <tag>audit</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubeadm 安装 kubernetes]]></title>
    <url>%2F2019%2F01%2F17%2Fkubeadm%2F</url>
    <content type="text"><![CDATA[kubeadm 是 Kubernetes 主推的部署工具之一，正在快速迭代开发中，当前版本为 GA，暂不建议用于部署生产环境，其先进的设计理念可以借鉴。 一、kubeadm 原理介绍kubeadm 会在初始化的机器上首先部署 kubelet 服务，kubelet 创建 pod 的方式有三种，其中一种就是监控指定目下（/etc/kubernetes/manifests）容器状态的变化然后进行相应的操作。kubeadm 启动 kubelet 后会在 /etc/kubernetes/manifests 目录下创建出 etcd、kube-apiserver、kube-controller-manager、kube-scheduler 四个组件 static pod 的 yaml 文件，此时 kubelet 监测到该目录下有 yaml 文件便会将其创建为对应的 pod，最终 kube-apiserver、kube-controller-manager、kube-scheduler 以及 etcd 会以 static pod 的方式运行。 本次安装 kubernetes 版本：v1.12.0 当前宿主机系统与内核版本：12345$ uname -r3.10.0-514.16.1.el7.x86_64$ cat /etc/redhat-releaseCentOS Linux release 7.2.1511 (Core) 二、安装前的准备工作1234567891011121314151617# 关闭swap$ sudo swapoff -a# 关闭selinux$ sed -i &apos;s/SELINUX=permissive/SELINUX=disabled/&apos; /etc/sysconfig/selinux $ setenforce 0# 关闭防火墙$ systemctl disable firewalld.service &amp;&amp; systemctl stop firewalld.service# 配置转发相关参数$ cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOF $ sysctl -p 三、安装 Docker CE 本次安装的 docker 版本：docker-ce-18.06.1.ce 123456789101112131415161718192021222324252627282930313233343536# Install Docker CE## Set up the repository### Install required packages.yum install yum-utils device-mapper-persistent-data lvm2### Add docker repository.yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo## Install docker ce.yum update &amp;&amp; yum install docker-ce-18.06.1.ce## Create /etc/docker directory.mkdir /etc/docker# Setup daemon.cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;100m&quot; &#125;, &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; ]&#125;EOFmkdir -p /etc/systemd/system/docker.service.d# Restart docker.systemctl daemon-reloadsystemctl restart docker 参考：https://kubernetes.io/docs/setup/cri/ 四、安装 kubernetes master 组件使用 kubeadm 初始化集群：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758$ kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16[init] using Kubernetes version: v1.12.0[preflight] running pre-flight checks[preflight/images] Pulling images required for setting up a Kubernetes cluster[preflight/images] This might take a minute or two, depending on the speed of your internet connection[preflight/images] You can also perform this action in beforehand using &apos;kubeadm config images pull&apos;[kubelet] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;[kubelet] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;[preflight] Activating the kubelet service[certificates] Using the existing front-proxy-client certificate and key.[certificates] Using the existing etcd/server certificate and key.[certificates] Using the existing etcd/peer certificate and key.[certificates] Using the existing etcd/healthcheck-client certificate and key.[certificates] Using the existing apiserver-etcd-client certificate and key.[certificates] Using the existing apiserver certificate and key.[certificates] Using the existing apiserver-kubelet-client certificate and key.[certificates] valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;[certificates] Using the existing sa key.[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/admin.conf&quot;[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/kubelet.conf&quot;[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/controller-manager.conf&quot;[kubeconfig] Using existing up-to-date KubeConfig file: &quot;/etc/kubernetes/scheduler.conf&quot;[controlplane] wrote Static Pod manifest for component kube-apiserver to &quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;[controlplane] wrote Static Pod manifest for component kube-controller-manager to &quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;[controlplane] wrote Static Pod manifest for component kube-scheduler to &quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;[etcd] Wrote Static Pod manifest for a local etcd instance to &quot;/etc/kubernetes/manifests/etcd.yaml&quot;[init] waiting for the kubelet to boot up the control plane as Static Pods from directory &quot;/etc/kubernetes/manifests&quot;[init] this might take a minute or longer if the control plane images have to be pulled[apiclient] All control plane components are healthy after 14.002350 seconds[uploadconfig] storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace[kubelet] Creating a ConfigMap &quot;kubelet-config-1.12&quot; in namespace kube-system with the configuration for the kubelets in the cluster[markmaster] Marking the node 192.168.1.110 as master by adding the label &quot;node-role.kubernetes.io/master=&apos;&apos;&quot;[markmaster] Marking the node 192.168.1.110 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule][patchnode] Uploading the CRI Socket information &quot;/var/run/dockershim.sock&quot; to the Node API object &quot;192.168.1.110&quot; as an annotation[bootstraptoken] using token: wu5hfy.lkuz9fih6hlqe1jt[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash sha256:e8d2649fceae9d7f6de94af0b7e294680b87f7d1e207c75c3cb496841b12ec23 这个命令会自动执行以下步骤： 系统状态检查 生成 token 生成自签名 CA 和 client 端证书 生成 kubeconfig 用于 kubelet 连接 API server 为 Master 组件生成 Static Pod manifests，并放到 /etc/kubernetes/manifests 目录中 配置 RBAC 并设置 Master node 只运行控制平面组件 创建附加服务，比如 kube-proxy 和 CoreDNS 配置 kubetl 认证信息：123$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config 将本机作为 node 加入到 master 中：1$ kubeadm join 192.168.1.110:6443 --token wu5hfy.lkuz9fih6hlqe1jt --discovery-token-ca-cert-hash kubeadm 默认 master 节点不作为 node 节点使用，初始化完成后会给 master 节点打上 taint 标签，若单机部署，使用以下命令去掉 taint 标签：1$ kubectl taint nodes --all node-role.kubernetes.io/master- 查看各组件是否正常运行： 123456789$ kubectl get pod -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-99b9bb8bd-pgh5t 1/1 Running 0 48metcd 1/1 Running 2 48mkube-apiserver 1/1 Running 1 48mkube-controller-manager 1/1 Running 0 49mkube-flannel-ds-amd64-b5rjg 1/1 Running 0 31mkube-proxy-c8ktg 1/1 Running 0 48mkube-scheduler 1/1 Running 2 48m 五、安装 kubernetes 网络kubernetes 本身是不提供网络方案的，但是有很多开源组件可以帮助我们打通容器和容器之间的网络，实现 Kubernetes 要求的网络模型。从实现原理上来说大致分为以下两种： overlay 网络，通过封包解包的方式构造一个隧道，代表的方案有 flannel(udp/vxlan）、weave、calico(ipip)，openvswitch 等 通过路由来实现(更改 iptables 等手段)，flannel(host-gw)，calico(bgp)，macvlan 等 当然每种方案都有自己适合的场景，flannel 和 calico 是两种最常见的网络方案，我们要根据自己的实际需要进行选择。此次安装选择 flannel 网络： 此操作也会为 flannel 创建对应的 RBAC 规则，flannel 会以 daemonset 的方式创建出来：1$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 创建一个 pod 验证集群是否正常： 123456789101112apiVersion: v1kind: Podmetadata: name: nginx labels: name: nginxspec: containers: - name: nginx image: nginx ports: - containerPort: 80 六、kubeadm 其他相关的操作1、删除安装:1$ kubeadm reset 2、版本升级12345# 查看可升级的版本$ kubeadm upgrade plan# 升级至指定版本$ kubeadm upgrade apply [version] 要执行升级，需要先将 kubeadm 升级到对应的版本； kubeadm 并不负责 kubelet 的升级，需要在升级完 master 组件后，手工对 kubelet 进行升级。 七、创建过程中的一些 case 记录1、flannel 容器启动报错：pod cidr not assgned需要在 /etc/kubernetes/manifests/kube-controller-manager.yaml 文件中添加以下配置： –allocate-node-cidrs=true–cluster-cidr=10.244.0.0/16 参考：https://github.com/coreos/flannel/issues/728 2、coredns 容器启动失败报错：/proc/sys/net/ipv6/conf/eth0/accept_dad: no such file or directory123$ vim /etc/default/grub and change the value of kernel parameter ipv6.disable from 1 to 0 in line$ grub2-mkconfig -o /boot/grub2/grub.cfg$ shutdown -r now 参考：https://github.com/containernetworking/cni/issues/569 3、kubeadm 证书有效期问题默认情况下，kubeadm 会生成集群运行所需的所有证书，我们也可以通过提供自己的证书来覆盖此行为。要做到这一点，必须把它们放在 –cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录（默认目录为 /etc/kubernetes/pki），如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。所以只要我们自己提供一个有效期很长的证书去覆盖掉默认的证书就可以来避免这个的问题。 4、kubeadm join 时 token 无法生效token 的失效为24小时，若忘记或者 token 过期可以使用 kubeadm token create 重新生成 token。 八、总结本篇文章讲述了使用 kubeadm 来搭建一个 kubernetes 集群，kubeadm 暂时还不建议用于生产环境，若部署生产环境请使用二进制文件。kubeadm 搭建出的集群还是有很多不完善的地方，比如，集群 master 组件的参数配置问题，官方默认的并不会满足需求，有许多参数需要根据实际情况进行修改。 参考：Creating a single master cluster with kubeadmkubeadm 工作原理DockOne微信分享（一六三）：Kubernetes官方集群部署工具kubeadm原理解析centos7.2 安装k8s v1.11.0]]></content>
      <tags>
        <tag>kubeadm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 中 kubeconfig 的用法]]></title>
    <url>%2F2019%2F01%2F09%2Fkubeconfig%2F</url>
    <content type="text"><![CDATA[用于配置集群访问信息的文件叫作 kubeconfig 文件，在开启了 TLS 的集群中，每次与集群交互时都需要身份认证，生产环境一般使用证书进行认证，其认证所需要的信息会放在 kubeconfig 文件中。此外，k8s 的组件都可以使用 kubeconfig 连接 apiserver，client-go 、operator、helm 等其他组件也使用 kubeconfig 访问 apiserver。 一、kubeconfig 配置文件的生成kubeconfig 的一个示例：123456789101112131415161718192021222324252627apiVersion: v1clusters:- cluster: certificate-authority-data: xxx server: https://xxx:6443 name: cluster1- cluster: certificate-authority-data: xxx server: https://xxx:6443 name: cluster2contexts:- context: cluster: cluster1 user: kubelet name: cluster1-context- context: cluster: cluster2 user: kubelet name: cluster2-contextcurrent-context: cluster1-contextkind: Configpreferences: &#123;&#125;users:- name: kubelet user: client-certificate-data: xxx client-key-data: xxx apiVersion 和 kind 标识客户端解析器的版本和模式，不应手动编辑。 preferences 指定可选（和当前未使用）的 kubectl 首选项。 1、clusters模块cluster中包含 kubernetes 集群的端点数据，包括 kubernetes apiserver 的完整 url 以及集群的证书颁发机构。 可以使用 kubectl config set-cluster 添加或修改 cluster 条目。 2、users 模块user 定义用于向 kubernetes 集群进行身份验证的客户端凭据。 可用凭证有 client-certificate、client-key、token 和 username/password。username/password 和 token 是二者只能选择一个，但 client-certificate 和 client-key 可以分别与它们组合。 可以使用 kubectl config set-credentials 添加或者修改 user 条目。 3、contexts 模块context 定义了一个命名的cluster、user、namespace元组，用于使用提供的认证信息和命名空间将请求发送到指定的集群。 三个都是可选的；仅使用 cluster、user、namespace 之一指定上下文，或指定none。 未指定的值或在加载的 kubeconfig 中没有相应条目的命名值将被替换为默认值。加载和合并 kubeconfig 文件的规则很简单，但有很多，具体可以查看加载和合并kubeconfig规则。 可以使用kubectl config set-context添加或修改上下文条目。 4、current-context 模块current-context 是作为cluster、user、namespace元组的 key，当 kubectl 从该文件中加载配置的时候会被默认使用。 可以在 kubectl 命令行里覆盖这些值，通过分别传入--context=CONTEXT、--cluster=CLUSTER、--user=USER 和 --namespace=NAMESPACE。以上示例中若不指定 context 则默认使用 cluster1-context。1kubectl get node --kubeconfig=./kubeconfig --context=cluster2-context 可以使用 kubectl config use-context 更改 current-context。 5、kubectl 生成 kubeconfig 的示例kubectl 可以快速生成 kubeconfig，以下是一个示例：123456$ kubectl config set-credentials myself --username=admin --password=secret$ kubectl config set-cluster local-server --server=http://localhost:8080$ kubectl config set-context default-context --cluster=local-server --user=myself$ kubectl config use-context cluster-context$ kubectl config set contexts.default-context.namespace the-right-prefix$ kubectl config view 若使用手写 kubeconfig 的方式，推荐一个工具 kubeval，可以校验 kubernetes yaml 或 json 格式的配置文件是否正确。 二、使用 kubeconfig 文件配置 kuebctl 跨集群认证kubectl 作为操作 k8s 的一个客户端工具，只要为 kubectl 提供连接 apiserver 的配置(kubeconfig)，kubectl 可以在任何地方操作该集群，当然，若 kubeconfig 文件中配置多个集群，kubectl 也可以轻松地在多个集群之间切换。 kubectl 加载配置文件的顺序：1、kubectl 默认连接本机的 8080 端口2、从 $HOME/.kube 目录下查找文件名为 config 的文件3、通过设置环境变量 KUBECONFIG 或者通过设置去指定其它 kubeconfig 文件123456# 设置 KUBECONFIG 的环境变量export KUBECONFIG=/etc/kubernetes/kubeconfig/kubelet.kubeconfig# 指定 kubeconfig 文件kubectl get node --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig# 使用不同的 context 在多个集群之间切换kubectl get node --kubeconfig=./kubeconfig --context=cluster1-context 开篇的示例就是多集群认证方式配置的一种。 参考：https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/]]></content>
      <tags>
        <tag>kubeconfig</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet 创建 pod 的流程]]></title>
    <url>%2F2019%2F01%2F03%2Fkubelet_create_pod%2F</url>
    <content type="text"><![CDATA[上篇文章介绍了 kubelet 的启动流程，本篇文章主要介绍 kubelet 创建 pod 的流程。 kubernetes 版本： v1.12 kubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。 本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。 kubelet 创建 pod 的流程 1、kubelet 的控制循环（syncLoop）syncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。 12345678910111213141516171819202122232425262728293031323334func (kl *Kubelet) syncLoop(updates &lt;-chan kubetypes.PodUpdate, handler SyncHandler) &#123; glog.Info(&quot;Starting kubelet main sync loop.&quot;) // syncTicker 每秒检测一次是否有需要同步的 pod workers syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() // 每两秒检测一次是否有需要清理的 pod housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() // pod 的生命周期变化 plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for &#123; if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 &#123; time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue &#125; ... kl.syncLoopMonitor.Store(kl.clock.Now()) // 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface， // 在该文件开头处定义 if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) &#123; break &#125; kl.syncLoopMonitor.Store(kl.clock.Now()) &#125;&#125; 2、监听 pod 变化（syncLoopIteration）syncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息： configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。 syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态 houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作 plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。 livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作 12345678910111213141516171819202122232425262728293031323334353637func (kl *Kubelet) syncLoopIteration(configCh &lt;-chan kubetypes.PodUpdate, handler SyncHandler, syncCh &lt;-chan time.Time, housekeepingCh &lt;-chan time.Time, plegCh &lt;-chan *pleg.PodLifecycleEvent) bool &#123; select &#123; case u, open := &lt;-configCh: if !open &#123; glog.Errorf(&quot;Update channel is closed. Exiting the sync loop.&quot;) return false &#125; switch u.Op &#123; case kubetypes.ADD: ... case kubetypes.UPDATE: ... case kubetypes.REMOVE: ... case kubetypes.RECONCILE: ... case kubetypes.DELETE: ... case kubetypes.RESTORE: ... case kubetypes.SET: ... &#125; ... case e := &lt;-plegCh: ... case &lt;-syncCh: ... case update := &lt;-kl.livenessManager.Updates(): ... case &lt;-housekeepingCh: ... &#125; return true&#125; 3、处理新增 pod（HandlePodAddtions）对于事件中的每个 pod，执行以下操作： 1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理 2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了 3、如果是 mirror pod 调用其单独的方法 4、验证 pod 是否能在该节点运行，如果不可以直接拒绝 5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理 6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测 1234567891011121314151617181920212223242526272829303132333435func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) &#123; start := kl.clock.Now() // 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理 sort.Sort(sliceutils.PodsByCreationTime(pods)) for _, pod := range pods &#123; if kl.dnsConfigurer != nil &amp;&amp; kl.dnsConfigurer.ResolverConfig != &quot;&quot; &#123; kl.dnsConfigurer.CheckLimitsForResolvConf() &#125; existingPods := kl.podManager.GetPods() // 把 pod 加入到 podManager 中 kl.podManager.AddPod(pod) // 判断是否是 mirror pod（即 static pod） if kubepod.IsMirrorPod(pod) &#123; kl.handleMirrorPod(pod, start) continue &#125; if !kl.podIsTerminated(pod) &#123; activePods := kl.filterOutTerminatedPods(existingPods) // 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间) // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok &#123; kl.rejectPod(pod, reason, message) continue &#125; &#125; mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) // 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法. kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start) // 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测 kl.probeManager.AddPod(pod) &#125;&#125; static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。 4、下发任务（dispatchWork）dispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。 12345678910111213141516171819202122func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) &#123; if kl.podIsTerminated(pod) &#123; if pod.DeletionTimestamp != nil &#123; kl.statusManager.TerminatePod(pod) &#125; return &#125; // 落实在 podWorkers 中 kl.podWorkers.UpdatePod(&amp;UpdatePodOptions&#123; Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, OnCompleteFunc: func(err error) &#123; if err != nil &#123; metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start)) &#125; &#125;, &#125;) if syncType == kubetypes.SyncPodCreate &#123; metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) &#125;&#125; 5、更新事件的 channel（UpdatePod）podWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。 1234567891011121314151617181920212223242526272829303132func (p *podWorkers) UpdatePod(options *UpdatePodOptions) &#123; pod := options.Pod uid := pod.UID var podUpdates chan UpdatePodOptions var exists bool p.podLock.Lock() defer p.podLock.Unlock() // 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel if podUpdates, exists = p.podUpdates[uid]; !exists &#123; // 创建 channel podUpdates = make(chan UpdatePodOptions, 1) p.podUpdates[uid] = podUpdates // 启动 goroutine go func() &#123; defer runtime.HandleCrash() p.managePodLoop(podUpdates) &#125;() &#125; // 下发更新事件 if !p.isWorking[pod.UID] &#123; p.isWorking[pod.UID] = true podUpdates &lt;- *options &#125; else &#123; update, found := p.lastUndeliveredWorkUpdate[pod.UID] if !found || update.UpdateType != kubetypes.SyncPodKill &#123; p.lastUndeliveredWorkUpdate[pod.UID] = *options &#125; &#125;&#125; 6、调用 syncPodFn 方法同步 pod（managePodLoop）managePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情: 将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync 将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。 12345678910111213141516171819202122232425262728func (p *podWorkers) managePodLoop(podUpdates &lt;-chan UpdatePodOptions) &#123; var lastSyncTime time.Time for update := range podUpdates &#123; err := func() error &#123; podUID := update.Pod.UID status, err := p.podCache.GetNewerThan(podUID, lastSyncTime) if err != nil &#123; ... &#125; err = p.syncPodFn(syncPodOptions&#123; mirrorPod: update.MirrorPod, pod: update.Pod, podStatus: status, killPodOptions: update.KillPodOptions, updateType: update.UpdateType, &#125;) lastSyncTime = time.Now() return err &#125;() if update.OnCompleteFunc != nil &#123; update.OnCompleteFunc(err) &#125; if err != nil &#123; ... &#125; p.wrapUp(update.Pod.UID, err) &#125;&#125; 7、完成创建容器前的准备工作（SyncPod）在这个方法中，主要完成以下几件事情： 如果是删除 pod，立即执行并返回 同步 podStatus 到 kubelet.statusManager 检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息 创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup 如果是 static Pod，就创建或者更新对应的 mirrorPod 创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据 然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。 调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑 这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071func (kl *Kubelet) syncPod(o syncPodOptions) error &#123; // pull out the required options pod := o.pod mirrorPod := o.mirrorPod podStatus := o.podStatus updateType := o.updateType // 是否为 删除 pod if updateType == kubetypes.SyncPodKill &#123; ... &#125; ... // 检查 pod 是否能运行在本节点 runnable := kl.canRunPod(pod) if !runnable.Admit &#123; ... &#125; // 更新 pod 状态 kl.statusManager.SetPodStatus(pod, apiPodStatus) // 如果 pod 非 running 状态则直接 kill 掉 if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed &#123; ... &#125; // 加载网络插件 if rs := kl.runtimeState.networkErrors(); len(rs) != 0 &amp;&amp; !kubecontainer.IsHostNetworkPod(pod) &#123; ... &#125; pcm := kl.containerManager.NewPodContainerManager() if !kl.podIsTerminated(pod) &#123; ... // 创建并更新 pod 的 cgroups if !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) &#123; if !pcm.Exists(pod) &#123; ... &#125; &#125; &#125; // 为 static pod 创建对应的 mirror pod if kubepod.IsStaticPod(pod) &#123; ... &#125; // 创建数据目录 if err := kl.makePodDataDirs(pod); err != nil &#123; ... &#125; // 挂载 volume if !kl.podIsTerminated(pod) &#123; if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil &#123; ... &#125; &#125; // 获取 secret 信息 pullSecrets := kl.getPullSecretsForPod(pod) // 调用 containerRuntime 的 SyncPod 方法开始创建容器 result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil &#123; ... &#125; return nil&#125; 8、创建容器containerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。syncPod 主要执行以下几个操作： 1、计算 sandbox 和 container 是否发生变化 2、创建 sandbox 容器 3、启动 init 容器 4、启动业务容器 initContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) &#123; // 1、计算 sandbox 和 container 是否发生变化 podContainerChanges := m.computePodActions(pod, podStatus) if podContainerChanges.CreateSandbox &#123; ref, err := ref.GetReference(legacyscheme.Scheme, pod) if err != nil &#123; glog.Errorf(&quot;Couldn&apos;t make a ref to pod %q: &apos;%v&apos;&quot;, format.Pod(pod), err) &#125; ... &#125; // 2、kill 掉 sandbox 已经改变的 pod if podContainerChanges.KillPod &#123; ... &#125; else &#123; // 3、kill 掉非 running 状态的 containers ... for containerID, containerInfo := range podContainerChanges.ContainersToKill &#123; ... if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil &#123; ... &#125; &#125; &#125; m.pruneInitContainersBeforeStart(pod, podStatus) podIP := &quot;&quot; if podStatus != nil &#123; podIP = podStatus.IP &#125; // 4、创建 sandbox podSandboxID := podContainerChanges.SandboxID if podContainerChanges.CreateSandbox &#123; podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) if err != nil &#123; ... &#125; ... podSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID) if err != nil &#123; ... &#125; // 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置 if !kubecontainer.IsHostNetworkPod(pod) &#123; podIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus) glog.V(4).Infof(&quot;Determined the ip %q for pod %q after sandbox changed&quot;, podIP, format.Pod(pod)) &#125; &#125; configPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID) result.AddSyncResult(configPodSandboxResult) // 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等) podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt) ... // 5、启动 init container if container := podContainerChanges.NextInitContainerToStart; container != nil &#123; ... if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil &#123; ... &#125; &#125; // 6、启动业务容器 for _, idx := range podContainerChanges.ContainersToStart &#123; ... if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil &#123; ... &#125; &#125; return&#125; 9、启动容器最终由 startContainer 完成容器的启动，其主要有以下几个步骤： 1、拉取镜像 2、生成业务容器的配置信息 3、调用 docker api 创建容器 4、启动容器 5、执行 post start hook 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) &#123; // 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。 imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets) if err != nil &#123; ... &#125; ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil &#123; ... &#125; // 设置 RestartCount restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil &#123; restartCount = containerStatus.RestartCount + 1 &#125; // 2、生成业务容器的配置信息 containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType) if cleanupAction != nil &#123; defer cleanupAction() &#125; ... // 3、通过 client.CreateContainer 调用 docker api 创建业务容器 containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil &#123; ... &#125; err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil &#123; ... &#125; ... // 3、启动业务容器 err = m.runtimeService.StartContainer(containerID) if err != nil &#123; ... &#125; containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) &#123; if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil &#123; glog.Errorf(&quot;Failed to create legacy symbolic link %q to container %q log %q: %v&quot;, legacySymlink, containerID, containerLog, err) &#125; &#125; // 4、执行 post start hook if container.Lifecycle != nil &amp;&amp; container.Lifecycle.PostStart != nil &#123; kubeContainerID := kubecontainer.ContainerID&#123; Type: m.runtimeName, ID: containerID, &#125; // runner.Run 这个方法的主要作用就是在业务容器起来的时候， // 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。 // 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。 msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil &#123; ... &#125; &#125; return &quot;&quot;, nil&#125; 总结本文主要讲述了 kubelet 从监听到容器调度至本节点再到创建容器的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。 参考：k8s源码分析-kubeletKubelet源码分析(一):启动流程分析kubelet 源码分析：pod 新建流程kubelet创建Pod流程解析Kubelet: Pod Lifecycle Event Generator (PLEG) Design- proposals]]></content>
      <tags>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubelet 架构浅析]]></title>
    <url>%2F2018%2F12%2F16%2Fkubelet-modules%2F</url>
    <content type="text"><![CDATA[一、概要kubelet 是运行在每个节点上的主要的“节点代理”，每个节点都会启动 kubelet进程，用来处理 Master 节点下发到本节点的任务，按照 PodSpec 描述来管理Pod 和其中的容器（PodSpec 是用来描述一个 pod 的 YAML 或者 JSON 对象）。 kubelet 通过各种机制（主要通过 apiserver ）获取一组 PodSpec 并保证在这些 PodSpec 中描述的容器健康运行。 二、kubelet 的主要功能1、kubelet 默认监听四个端口，分别为 10250 、10255、10248、4194。 1234LISTEN 0 128 *:10250 *:* users:((&quot;kubelet&quot;,pid=48500,fd=28))LISTEN 0 128 *:10255 *:* users:((&quot;kubelet&quot;,pid=48500,fd=26))LISTEN 0 128 *:4194 *:* users:((&quot;kubelet&quot;,pid=48500,fd=13))LISTEN 0 128 127.0.0.1:10248 *:* users:((&quot;kubelet&quot;,pid=48500,fd=23)) 10250（kubelet API）：kubelet server 与 apiserver 通信的端口，定期请求 apiserver 获取自己所应当处理的任务，通过该端口可以访问获取 node 资源以及状态。 10248（健康检查端口）：通过访问该端口可以判断 kubelet 是否正常工作, 通过 kubelet 的启动参数 --healthz-port 和 --healthz-bind-address 来指定监听的地址和端口。 12$ curl http://127.0.0.1:10248/healthzok 4194（cAdvisor 监听）：kublet 通过该端口可以获取到该节点的环境信息以及 node 上运行的容器状态等内容，访问 http://localhost:4194 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参数 --cadvisor-port 可以指定启动的端口。 1$ curl http://127.0.0.1:4194/metrics 10255 （readonly API）：提供了 pod 和 node 的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。123456// 获取 pod 的接口，与 apiserver 的 // http://127.0.0.1:8080/api/v1/pods?fieldSelector=spec.nodeName= 接口类似$ curl http://127.0.0.1:10255/pods// 节点信息接口,提供磁盘、网络、CPU、内存等信息$ curl http://127.0.0.1:10255/spec/ 2、kubelet 主要功能： pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理。 容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要。 三、kubelet 组件中的模块 上图展示了 kubelet 组件中的模块以及模块间的划分。 1、PLEG(Pod Lifecycle Event Generator）PLEG 是 kubelet 的核心模块,PLEG 会一直调用 container runtime 获取本节点 containers/sandboxes 的信息，并与自身维护的 pods cache 信息进行对比，生成对应的 PodLifecycleEvent，然后输出到 eventChannel 中，通过 eventChannel 发送到 kubelet syncLoop 进行消费，然后由 kubelet syncPod 来触发 pod 同步处理过程，最终达到用户的期望状态。 2、cAdvisorcAdvisor（https://github.com/google/cadvisor）是 google 开发的容器监控工具，集成在 kubelet 中，起到收集本节点和容器的监控信息，大部分公司对容器的监控数据都是从 cAdvisor 中获取的 ，cAvisor 模块对外提供了 interface 接口，该接口也被 imageManager，OOMWatcher，containerManager 等所使用。 3、OOMWatcher系统 OOM 的监听器，会与 cadvisor 模块之间建立 SystemOOM,通过 Watch方式从 cadvisor 那里收到的 OOM 信号，并产生相关事件。 4、probeManagerprobeManager 依赖于 statusManager,livenessManager,containerRefManager，会定时去监控 pod 中容器的健康状况，当前支持两种类型的探针：livenessProbe 和readinessProbe。livenessProbe：用于判断容器是否存活，如果探测失败，kubelet 会 kill 掉该容器，并根据容器的重启策略做相应的处理。readinessProbe：用于判断容器是否启动完成，将探测成功的容器加入到该 pod 所在 service 的 endpoints 中，反之则移除。readinessProbe 和 livenessProbe 有三种实现方式：http、tcp 以及 cmd。 5、statusManagerstatusManager 负责维护状态信息，并把 pod 状态更新到 apiserver，但是它并不负责监控 pod 状态的变化，而是提供对应的接口供其他组件调用，比如 probeManager。 6、containerRefManager容器引用的管理，相对简单的Manager，用来报告容器的创建，失败等事件，通过定义 map 来实现了 containerID 与 v1.ObjectReferece 容器引用的映射。 7、evictionManager当节点的内存、磁盘或 inode 等资源不足时，达到了配置的 evict 策略， node 会变为 pressure 状态，此时 kubelet 会按照 qosClass 顺序来驱赶 pod，以此来保证节点的稳定性。可以通过配置 kubelet 启动参数 --eviction-hard= 来决定 evict 的策略值。 8、imageGCimageGC 负责 node 节点的镜像回收，当本地的存放镜像的本地磁盘空间达到某阈值的时候，会触发镜像的回收，删除掉不被 pod 所使用的镜像，回收镜像的阈值可以通过 kubelet 的启动参数 --image-gc-high-threshold 和 --image-gc-low-threshold 来设置。 9、containerGCcontainerGC 负责清理 node 节点上已消亡的 container，具体的 GC 操作由runtime 来实现。 10、imageManager调用 kubecontainer 提供的PullImage/GetImageRef/ListImages/RemoveImage/ImageStates 方法来保证pod 运行所需要的镜像。 11、volumeManager负责 node 节点上 pod 所使用 volume 的管理，volume 与 pod 的生命周期关联，负责 pod 创建删除过程中 volume 的 mount/umount/attach/detach 流程，kubernetes 采用 volume Plugins 的方式，实现存储卷的挂载等操作，内置几十种存储插件。 12、containerManager负责 node 节点上运行的容器的 cgroup 配置信息，kubelet 启动参数如果指定 --cgroups-per-qos 的时候，kubelet 会启动 goroutine 来周期性的更新 pod 的 cgroup 信息，维护其正确性，该参数默认为 true，实现了 pod 的Guaranteed/BestEffort/Burstable 三种级别的 Qos。 13、runtimeManagercontainerRuntime 负责 kubelet 与不同的 runtime 实现进行对接，实现对于底层 container 的操作，初始化之后得到的 runtime 实例将会被之前描述的组件所使用。可以通过 kubelet 的启动参数 --container-runtime 来定义是使用docker 还是 rkt，默认是 docker。 14、podManagerpodManager 提供了接口来存储和访问 pod 的信息，维持 static pod 和 mirror pods 的关系，podManager 会被statusManager/volumeManager/runtimeManager 所调用，podManager 的接口处理流程里面会调用 secretManager 以及 configMapManager。 在 v1.12 中，kubelet 组件有18个 manager： 123456789101112131415161718certificateManagercgroupManagercontainerManagercpuManagernodeContainerManagerconfigmapManagercontainerReferenceManagerevictionManagernvidiaGpuManagerimageGCManagerkuberuntimeManagerhostportManagerpodManagerproberManagersecretManagerstatusManagervolumeManager tokenManager 其中比较重要的模块后面会进行一一分析。 参考：微软资深工程师详解 K8S 容器运行时kubernetes 简介： kubelet 和 podKubelet 组件解析]]></content>
      <tags>
        <tag>kubelet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 架构中的几个核心概念]]></title>
    <url>%2F2018%2F12%2F05%2Fdocker-introduces%2F</url>
    <content type="text"><![CDATA[一、Docker 开源之路2015 年 6 月 ，docker 公司将 libcontainer 捐出并改名为 runC 项目，交由一个完全中立的基金会管理，然后以 runC 为依据，大家共同制定一套容器和镜像的标准和规范 OCI。 2016 年 4 月，docker 1.11 版本之后开始引入了 containerd 和 runC，Docker 开始依赖于 containerd 和 runC 来管理容器，containerd 也可以操作满足 OCI 标准规范的其他容器工具，之后只要是按照 OCI 标准规范开发的容器工具，都可以被 containerd 使用起来。 从 2017 年开始，Docker 公司先是将 Docker项目的容器运行时部分 Containerd 捐赠给CNCF 社区，紧接着，Docker 公司宣布将 Docker 项目改名为 Moby。 二、Docker 架构 三、核心概念docker 1.13 版本中包含以下几个二进制文件。123456$ docker --versionDocker version 1.13.1, build 092cba3$ dockerdocker docker-containerd-ctr dockerd docker-proxydocker-containerd docker-containerd-shim docker-init docker-runc 1、dockerdocker 的命令行工具，是给用户和 docker daemon 建立通信的客户端。 2、dockerddockerd 是 docker 架构中一个常驻在后台的系统进程，称为 docker daemon，dockerd 实际调用的还是 containerd 的 api 接口（rpc 方式实现）,docker daemon 的作用主要有以下两方面： 接收并处理 docker client 发送的请求 管理所有的 docker 容器 有了 containerd 之后，dockerd 可以独立升级，以此避免之前 dockerd 升级会导致所有容器不可用的问题。 3、containerdcontainerd 是 dockerd 和 runc 之间的一个中间交流组件，docker 对容器的管理和操作基本都是通过 containerd 完成的。containerd 的主要功能有： 容器生命周期管理 日志管理 镜像管理 存储管理 容器网络接口及网络管理 4、containerd-shimcontainerd-shim 是一个真实运行容器的载体，每启动一个容器都会起一个新的containerd-shim的一个进程， 它直接通过指定的三个参数：容器id，boundle目录（containerd 对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID，其中包括了容器配置和标准输入、标准输出、标准错误三个管道文件），运行时二进制（默认为runC）来调用 runc 的 api 创建一个容器，上面的 docker 进程图中可以直观的显示。其主要作用是： 它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC) 即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的 向 containerd 报告容器的退出状态 有了它就可以在不中断容器运行的情况下升级或重启 dockerd，对于生产环境来说意义重大。 5、runCrunC 是 Docker 公司按照 OCI 标准规范编写的一个操作容器的命令行工具，其前身是 libcontainer 项目演化而来，runC 实际上就是 libcontainer 配上了一个轻型的客户端，是一个命令行工具端，根据 OCI（开放容器组织）的标准来创建和运行容器，实现了容器启停、资源隔离等功能。 一个例子，使用 runC 运行 busybox 容器:12345678910111213141516171819# mkdir /container# cd /container/# mkdir rootfs准备容器镜像的文件系统,从 busybox 镜像中提取# docker export $(docker create busybox) | tar -C rootfs -xvf - # ls rootfs/bin dev etc home proc root sys tmp usr var有了rootfs之后，我们还要按照 OCI 标准有一个配置文件 config.json 说明如何运行容器，包括要运行的命令、权限、环境变量等等内容，runc 提供了一个命令可以自动帮我们生成# docker-runc spec# lsconfig.json rootfs# docker-runc run simplebusybox #启动容器/ # lsbin dev etc home proc root sys tmp usr var/ # hostnamerunc 参考：Use of containerd-shim in docker-architecture从 docker 到 runCOCI 和 runc：容器标准化和 dockerOpen Container Initiative]]></content>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 常用 API]]></title>
    <url>%2F2018%2F09%2F02%2Fkubernetes-api%2F</url>
    <content type="text"><![CDATA[kubectl 的所有操作都是调用 kube-apisever 的 API 实现的，所以其子命令都有相应的 API，每次在调用 kubectl 时使用参数 -v=9 可以看调用的相关 API，例： $ kubectl get node -v=9 以下为 kubernetes 开发中常用的 API： Markdown 表格显示过大，此仅以图片格式展示。]]></content>
  </entry>
  <entry>
    <title><![CDATA[etcd 启用 https]]></title>
    <url>%2F2017%2F03%2F15%2Fetcd-enable-https%2F</url>
    <content type="text"><![CDATA[1， 生成 TLS 秘钥对 2，拷贝密钥对到所有节点 3，配置 etcd 使用证书 4，测试 etcd 是否正常 5，配置 kube-apiserver 使用 CA 连接 etcd 6，测试 kube-apiserver 7，未解决的问题 SSL/TSL 认证分单向认证和双向认证两种方式。简单说就是单向认证只是客户端对服务端的身份进行验证，双向认证是客户端和服务端互相进行身份认证。就比如，我们登录淘宝买东西，为了防止我们登录的是假淘宝网站，此时我们通过浏览器打开淘宝买东西时，浏览器会验证我们登录的网站是否是真的淘宝的网站，而淘宝网站不关心我们是否“合法”，这就是单向认证。而双向认证是服务端也需要对客户端做出认证。 因为大部分 kubernetes 基于内网部署，而内网应该都会采用私有 IP 地址通讯，权威 CA 好像只能签署域名证书，对于签署到 IP 可能无法实现。所以我们需要预先自建 CA 签发证书。 Generate self-signed certificates 官方参考文档 官方推荐使用 cfssl 来自建 CA 签发证书，当然你也可以用众人熟知的 OpenSSL 或者 easy-rsa。以下步骤遵循官方文档： 1， 生成 TLS 秘钥对生成步骤： 1，下载 cfssl 2，初始化证书颁发机构 3，配置 CA 选项 4，生成服务器端证书 5，生成对等证书 6，生成客户端证书 想深入了解 HTTPS 的看这里： 聊聊HTTPS和SSL/TLS协议 数字证书CA及扫盲 互联网加密及OpenSSL介绍和简单使用 SSL双向认证和单向认证的区别 1，下载 cfsslmkdir ~/bin curl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x ~/bin/{cfssl,cfssljson} export PATH=$PATH:~/bin 2，初始化证书颁发机构1234mkdir ~/cfsslcd ~/cfsslcfssl print-defaults config &gt; ca-config.jsoncfssl print-defaults csr &gt; ca-csr.json 证书类型介绍： client certificate 用于通过服务器验证客户端。例如etcdctl，etcd proxy，fleetctl或docker客户端。 server certificate 由服务器使用，并由客户端验证服务器身份。例如docker服务器或kube-apiserver。 peer certificate 由 etcd 集群成员使用，供它们彼此之间通信使用。 3，配置 CA 选项123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566$ cat &lt;&lt; EOF &gt; ca-config.json&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;server&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot; ] &#125;, &quot;client&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;client auth&quot; ] &#125;, &quot;peer&quot;: &#123; &quot;expiry&quot;: &quot;43800h&quot;, &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ] &#125; &#125; &#125;&#125;$ cat &lt;&lt; EOF &gt; ca-csr.json&#123; &quot;CN&quot;: &quot;My own CA&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;US&quot;, &quot;L&quot;: &quot;CA&quot;, &quot;O&quot;: &quot;My Company Name&quot;, &quot;ST&quot;: &quot;San Francisco&quot;, &quot;OU&quot;: &quot;Org Unit 1&quot;, &quot;OU&quot;: &quot;Org Unit 2&quot; &#125; ]&#125;生成 CA 证书：$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca -将会生成以下几个文件：ca-key.pemca.csrca.pem 请务必保证 ca-key.pem 文件的安全，*.csr 文件在整个过程中不会使用。 4，生成服务器端证书12345678$ echo &apos;&#123;&quot;CN&quot;:&quot;coreos1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=&quot;10.93.81.17,127.0.0.1,server&quot; - | cfssljson -bare serverhosts 字段需要自定义。然后将得到以下几个文件：server-key.pemserver.csrserver.pem 5，生成对等证书1234567891011$ echo &apos;&#123;&quot;CN&quot;:&quot;member1&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=&quot;10.93.81.17,127.0.0.1,server,member1&quot; - | cfssljson -bare member1hosts 字段需要自定义。然后将得到以下几个文件：member1-key.pemmember1.csrmember1.pem如果有多个 etcd 成员，重复此步为每个成员生成对等证书。 6，生成客户端证书123456789$ echo &apos;&#123;&quot;CN&quot;:&quot;client&quot;,&quot;hosts&quot;:[&quot;10.93.81.17&quot;,&quot;127.0.0.1&quot;],&quot;key&quot;:&#123;&quot;algo&quot;:&quot;rsa&quot;,&quot;size&quot;:2048&#125;&#125;&apos; | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare clienthosts 字段需要自定义。然后将得到以下几个文件：client-key.pemclient.csrclient.pem 至此，所有证书都已生成完毕。 2，拷贝密钥对到所有节点 1，拷贝密钥对到所有节点 2，更新系统证书库 1，拷贝密钥对到所有节点12345$ mkdir -pv /etc/ssl/etcd/$ cp ~/cfssl/* /etc/ssl/etcd/$ chown -R etcd:etcd /etc/ssl/etcd$ chmod 600 /etc/ssl/etcd/*-key.pem$ cp ~/cfssl/ca.pem /etc/ssl/certs/ 2，更新系统证书库123$ yum install ca-certificates -y $ update-ca-trust 3，配置 etcd 使用证书12345678910111213141516171819202122232425262728293031323334353637$ etcdctl versionetcdctl version: 3.1.3API version: 3.1$ cat /etc/etcd/etcd.confETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;#监听URL，用于与其他节点通讯ETCD_LISTEN_PEER_URLS=&quot;https://10.93.81.17:2380&quot;#告知客户端的URL, 也就是服务的URLETCD_LISTEN_CLIENT_URLS=&quot;https://10.93.81.17:2379,https://10.93.81.17:4001&quot;#表示监听其他节点同步信号的地址ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://10.93.81.17:2380&quot;#–advertise-client-urls 告知客户端的URL, 也就是服务的URL，tcp2379端口用于监听客户端请求ETCD_ADVERTISE_CLIENT_URLS=&quot;https://10.93.81.17:2379&quot;#启动参数配置ETCD_NAME=&quot;node1&quot;ETCD_INITIAL_CLUSTER=&quot;node1=https://10.93.81.17:2380&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;#[security]ETCD_CERT_FILE=&quot;/etc/ssl/etcd/server.pem&quot;ETCD_KEY_FILE=&quot;/etc/ssl/etcd/server-key.pem&quot;ETCD_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;ETCD_CLIENT_CERT_AUTH=&quot;true&quot;ETCD_PEER_CERT_FILE=&quot;/etc/ssl/etcd/member1.pem&quot;ETCD_PEER_KEY_FILE=&quot;/etc/ssl/etcd/member1-key.pem&quot;ETCD_PEER_TRUSTED_CA_FILE=&quot;/etc/ssl/etcd/ca.pem&quot;ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;#[logging]ETCD_DEBUG=&quot;true&quot;ETCD_LOG_PACKAGE_LEVELS=&quot;etcdserver=WARNING,security=DEBUG&quot; 4，测试 etcd 是否正常123456789101112$ systemctl restart etcd如果报错，使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题。$ curl --cacert /etc/ssl/etcd/ca.pem --cert /etc/ssl/etcd/client.pem --key /etc/ssl/etcd/client-key.pem https://10.93.81.17:2379/health&#123;&quot;health&quot;: &quot;true&quot;&#125;$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list $ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem put /foo/bar &quot;hello world&quot; $ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem get /foo/bar 5，配置 kube-apiserver 使用 CA 连接 etcd1234567$ cp /etc/ssl/etcd/* /var/run/kubernetes/ $ chown -R kube.kube /var/run/kubernetes/在 /etc/kubernetes/apiserver 中 KUBE_API_ARGS 新加一下几个参数：--cert-dir=&apos;/var/run/kubernetes/&apos; --etcd-cafile=&apos;/var/run/kubernetes/ca.pem&apos; --etcd-certfile=&apos;/var/run/kubernetes/client.pem&apos; --etcd-keyfile=&apos;/var/run/kubernetes/client-key.pem&apos; 6，测试 kube-apiserver12345678910111213141516$ systemctl restart kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy$ systemctl status -l kube-apiserver kube-controller-manager kube-scheduler kubelet kube-proxy$ kubectl get node$ kubectl get csNAME STATUS MESSAGE ERRORscheduler Healthy okcontroller-manager Healthy oketcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate$ ./version.shetcdctl version: 3.1.3API version: 3.1Kubernetes v1.6.0-beta.1 7，未解决的问题1，使用 kubectl get cs 查看会出现如上面所示的报错：1etcd-0 Unhealthy Get https://10.93.81.17:2379/health: remote error: tls: bad certificate 此问题有人提交 pr 但尚未被 merge，etcd component status check should include credentials 2，使用以下命令查看到的 2380 端口是未加密的1234$ etcdctl --endpoints=[10.93.81.17:2379] --cacert=/etc/ssl/etcd/ca.pem --cert=/etc/ssl/etcd/client.pem --key=/etc/ssl/etcd/client-key.pem member list 2017-03-15 15:02:05.611564 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated145b401ad8709f51, started, node1, http://10.93.81.17:2380, https://10.93.81.17:2379 参考文档： kubernetes + etcd ssl 支持 Security model Enabling HTTPS in an existing etcd cluster]]></content>
  </entry>
  <entry>
    <title><![CDATA[etcd 备份与恢复]]></title>
    <url>%2F2017%2F03%2F02%2Fetcd-backup%2F</url>
    <content type="text"><![CDATA[etcd 是一款开源的分布式一致性键值存储,由 CoreOS 公司进行维护，详细的介绍请参考官方文档。 etcd 目前最新的版本的 v3.1.1，但它的 API 又有 v3 和 v2 之分，社区通常所说的 v3 与 v2 都是指 API 的版本号。从 etcd 2.3 版本开始推出了一个实验性的全新 v3 版本 API 的实现，v2 与 v3 API 使用了不同的存储引擎，所以客户端命令也完全不同。 # etcdctl --version etcdctl version: 3.0.4 API version: 2 官方指出 etcd v2 和 v3 的数据不能混合存放，support backup of v2 and v3 stores 。 特别提醒：若使用 v3 备份数据时存在 v2 的数据则不影响恢复若使用 v2 备份数据时存在 v3 的数据则恢复失败 对于 API 2 备份与恢复方法官方 v2 admin guide etcd的数据默认会存放在我们的命令工作目录中，我们发现数据所在的目录，会被分为两个文件夹中： snap: 存放快照数据,etcd防止WAL文件过多而设置的快照，存储etcd数据状态。 wal: 存放预写式日志,最大的作用是记录了整个数据变化的全部历程。在etcd中，所有数据的修改在提交前，都要先写入到WAL中。 # etcdctl backup --data-dir /home/etcd/ --backup-dir /home/etcd_backup # etcd -data-dir=/home/etcd_backup/ -force-new-cluster 恢复时会覆盖 snapshot 的元数据(member ID 和 cluster ID)，所以需要启动一个新的集群。 对于 API 3 备份与恢复方法官方 v3 admin guide 在使用 API 3 时需要使用环境变量 ETCDCTL_API 明确指定。 在命令行设置： # export ETCDCTL_API=3 备份数据： # etcdctl --endpoints localhost:2379 snapshot save snapshot.db 恢复： # etcdctl snapshot restore snapshot.db --name m3 --data-dir=/home/etcd_data 恢复后的文件需要修改权限为 etcd:etcd–name:重新指定一个数据目录，可以不指定，默认为 default.etcd–data-dir：指定数据目录建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir etcd 集群都是至少 3 台机器，官方也说明了集群容错为 (N-1)/2，所以备份数据一般都是用不到，但是鉴上次 gitlab 出现的问题，对于备份数据也要非常重视。 官方文档翻译]]></content>
  </entry>
  <entry>
    <title><![CDATA[kubernetes 学习笔记]]></title>
    <url>%2F2017%2F02%2F12%2Fkubernetes-learn%2F</url>
    <content type="text"><![CDATA[1 月初办理了入职手续，所在的团队是搞私有云的，目前只有小规模的应用，所采用 kubernetes + docker 技术栈，年前所做的事情也不算多，熟悉了 kubernetes 的架构，自己搭建单机版的 kubernetes，以及在程序中调用 kubernetes 的 API 进行某些操作。 1，kubernetes 搭建kubernetes 是 google 的一个开源软件，其社区活跃量远超 Mesos，Coreos 的，若想深入学习建议参考《kubernetes 权威指南》，我们团队的人都是从这本书学起的，作为一个新技术，会踩到的坑非常多，以下提及的是我学习过程中整理的部分资料。 kubernetes 是一个分布式系统，所以它有多个组件，并且需要安装在多个节点，一般来说有三个节点，etcd，master 和 minion，但是每个节点却又有多台机器，etcd 作为高性能存储服务，一般独立为一个节点，当然容错是必不可少的，官方建议集群使用奇数个节点，我们的线下集群使用 3 个节点。etcd 的学习可以参考 gitbook 上面某大神的一本书 一 etcd3学习笔记。master 端需要安装 kube-apiserver、kube-controller-manager和kube-scheduler 组件，minion 节点需要部署 kubelet、kube-proxy、docker 组件。 注意：内核版本 &gt; 3.10 的系统才支持 kubernetes，所以一般安装在centos 7 上。 etcd 节点： # yum install -y etcd # systemctl start etcd master 节点： # yum install -y kubernetes-master # systemctl start kube-apiserver # systemctl start kube-controller-manager # systemctl start kube-scheduler minion 节点： # yum install -y kubernetes docker # systemctl start kubelet # systemctl start kube-proxy # systemctl start docker 2，kubernetes 版本升级以前一直以为公司会追求稳定性，在软件和系统的选取方便会优先考虑稳定的版本。但是来了公司才发现，某些软件出了新版本后，若有期待的功能并且在掌控范围内都会及时更新，所以也协助过导师更新了线下集群的 minion 节点。 下面是 minion 节点的升级操作，master 节点的操作类似。首先需要下载 kubernetes-server-linux-amd64.tar.gz 这个包，下载你所要更新到的版本。 升级步骤： 1，先关掉 docker 服务。docker 关闭后，当前节点的 pod 随之会被调度到其他节点上 2，备份二进制程序（kubectl,kube-proxy） 3，将解压后的二进制程序覆盖以前的版本 4，最后重新启动服务 # systemctl stop docker # which kubectl kube-proxy /usr/bin/kubectl /usr/bin/kube-proxy # cp /usr/bin/{kubectl,kube-proxy} /tmp/ # yes | cp bin/{kubectl,kube-proxy} /usr/bin/ # systemctl status {kubectl,kube-proxy} # systemctl start docker 3，kubeconfig 使用若你使用的 kubelet 版本为 1.4，使用 systemctl status kubelet 会看到这样一句话： --api-servers option is deprecated for kubelet, so I am now trying to deploy with simply using --kubeconfig=/etc/kubernetes/node-kubeconfig.yaml 使用 kuconfig 是为了将所有的命令行启动选项放在一个文件中方便使用。由于我们已经升级到了 1.5，所以也得升级此功能，首先需要写一个 kubeconfig 的 yaml 文件，其 官方文档 有格式说明， 本人已将其翻译，翻译文档见下文。 kubeconfig 文件示例： apiVersion: v1 clusters: - cluster: server: http://localhost:8080 name: local-server contexts: - context: cluster: local-server namespace: the-right-prefix user: myself name: default-context current-context: default-context kind: Config preferences: {} users: - name: myself user: password: secret username: admin # kubelet --kubeconfig=/etc/kubernetes/config --require-kubeconfig=true kubeconfig 参数：设置 kubelet 配置文件路径，这个配置文件用来告诉 kubelet 组件 api-server 组件的位置，默认路径是。 require-kubeconfig 参数：这是一个布尔类型参数，可以设置成true 或者 false，如果设置成 true，那么表示启用 kubeconfig 参数，从 kubeconfig 参数设置的配置文件中查找 api-server 组件，如果设置成 false，那么表示使用 kubelet 另外一个参数 “api-servers” 来查找 api-server 组件位置。 关于 kubeconfig 的一个 issue，Kubelet won’t read apiserver from kubeconfig。 升级步骤，当然前提是你的 kubelet 版本已经到了 1.5： 1，关闭 kubelet、kube-proxy 服务； 2，注释掉 /etc/kubernetes/kubelet 文件中下面这一行: KUBELET_API_SERVER=&quot;--api-servers=http://127.0.0.1:8080&quot; 然后在 KUBELET_ARGS 中添加： --kubeconfig=/etc/kubernetes/kubeconfig --require-kubeconfig=true 这里的路径是你 yaml 文件放置的路径。 3，重新启动刚关掉的两个服务 4，以下为 kubeconfig 配置官方文档的翻译kubernetes 中的验证对于不同的群体可以使用不同的方法. 运行 kubelet 可能有的一种认证方式（即证书）。 用户可能有不同的认证方式（即 token）。 管理员可以为每个用户提供一个证书列表。 可能会有多个集群，但我们想在一个地方定义它们 - 使用户能够用自己的证书并重用相同的全局配置。 因此为了在多个集群之间轻松切换，对于多个用户，定义了一个 kubeconfig 文件。 此文件包含一系列认证机制和与 nicknames 有关的群集连接信息。它还引入了认证信息元组（用户）和集群连接信息的概念，被称为上下文也与 nickname 相关联。 如果明确指定，也可以允许使用多个 kubeconfig 文件。在运行时，它们被合并加载并覆盖从命令行指定的选项（参见下面的规则）。 相关讨论http://issue.k8s.io/1755 kubeconfig 文件的组件kubeconfig 文件示例： current-context: federal-context apiVersion: v1 clusters: - cluster: api-version: v1 server: http://cow.org:8080 name: cow-cluster - cluster: certificate-authority: path/to/my/cafile server: https://horse.org:4443 name: horse-cluster - cluster: insecure-skip-tls-verify: true server: https://pig.org:443 name: pig-cluster contexts: - context: cluster: horse-cluster namespace: chisel-ns user: green-user name: federal-context - context: cluster: pig-cluster namespace: saw-ns user: black-user name: queen-anne-context kind: Config preferences: colors: true users: - name: blue-user user: token: blue-token - name: green-user user: client-certificate: path/to/my/client/cert client-key: path/to/my/client/key 组件的解释clusterclusters: - cluster: certificate-authority: path/to/my/cafile server: https://horse.org:4443 name: horse-cluster - cluster: insecure-skip-tls-verify: true server: https://pig.org:443 name: pig-cluster cluster 包含 kubernetes 集群的 endpoint 数据。它包括 kubernetes apiserver 完全限定的 URL，以及集群的证书颁发机构或 insecure-skip-tls-verify：true，如果集群的服务证书未由系统信任的证书颁发机构签名。集群有一个名称（nickname），该名称用作此 kubeconfig 文件中的字典键。你可以使用 kubectl config set-cluster 添加或修改集群条目。 userusers: - name: blue-user user: token: blue-token - name: green-user user: client-certificate: path/to/my/client/cert client-key: path/to/my/client/key 用户定义用于向 Kubernetes 集群进行身份验证的客户端凭证。在 kubeconfig 被加载/合并之后，用户具有在用户条目列表中充当其键的名称（nickname）。可用的凭证是客户端证书，客户端密钥，令牌和用户名/密码。用户名/密码和令牌是互斥的，但客户端证书和密钥可以与它们组合。你可以使用 kubectl config set-credentials 添加或修改用户条目。 contextcontexts: - context: cluster: horse-cluster namespace: chisel-ns user: green-user name: federal-context context 定义 cluster,user,namespace 元组的名称，用来向指定的集群使用提供的认证信息和命名空间向指定的集群发送请求。三个都是可选的，仅指定 cluster，user，namespace 中的一个也是可用的，或者指定为 none。未指定的值或命名值，在加载的 kubeconfig 中没有对应的条目（例如，如果context 在上面的 kubeconfig 文件指定为 pink-user ）将被替换为默认值。有关覆盖/合并行为，请参阅下面的加载/合并规则。你可以使用 kubectl config set-context 添加或修改上下文条目。 current-contextcurrent-context: federal-context current-context 是 cluster,user,namespace 中的 nickname 或者 ‘key’，kubectl 在从此文件加载配置时将使用默认值。通过给 kubelett 传递 –context=CONTEXT, –cluster=CLUSTER, –user=USER, and/or –namespace=NAMESPACE 可以从命令行覆盖任何值。你可以使用 kubectl config use-context 更改当前上下文。 杂项apiVersion: v1 kind: Config preferences: colors: true apiVersion 和 kind 标识客户端要解析的版本和模式，不应手动编辑。preferences 指定选项(和当前未使用的) kubectl preferences. 查看 kubeconfig 文件kubectl config view 会显示当前的 kubeconfig 配置。默认情况下，它会显示所有加载的 kubeconfig 配置， 你可以通过 –minify 选项来过滤与 current-context 相关的设置。请参见 kubectl config view 的其他选项。 创建你的 kubeconfig 文件注意，如果你通过 kube-up.sh 部署 k8s，则不需要创建 kubeconfig 文件，脚本将为你创建。 在任何情况下，可以轻松地使用此文件作为模板来创建自己的 kubeconfig 文件。 因此，让我们快速浏览上述文件的基础知识，以便可以根据需要轻松修改… 以上文件可能对应于使用–token-auth-file = tokens.csv 选项启动的 api 服务器，其中 tokens.csv文件看起来像这样： blue-user,blue-user,1 mister-red,mister-red,2 此外，由于不同用户使用不同的验证机制，api-server 可能已经启动其他的身份验证选项（有许多这样的选项，在制作 kubeconfig 文件之前确保你理解所关心的，因为没有人需要实现所有可能的认证方案）。 由于 current-context 的用户是 “green-user”，因此任何使用此 kubeconfig 文件的客户端自然都能够成功登录 api-server，因为我们提供了 “green-user” 的客户端凭据。 类似地，我们也可以选择改变 current-context 的值为 “blue-user”。 在上述情况下，“green-user” 将必须通过提供证书登录，而 “blue-user” 只需提供 token。所有的这些信息将由我们处理通过 加载和合并规则加载和合并 kubeconfig 文件的规则很简单，但有很多。最终配置按照以下顺序构建： 1，从磁盘获取 kubeconfig。通过以下层次结构和合并规则完成：如果设置了 CommandLineLocation（kubeconfig 命令行选项的值），则仅使用此文件，不合并。只允许此标志的一个实例。 否则，如果 EnvVarLocation（$KUBECONFIG 的值）可用，将其用作应合并的文件列表。根据以下规则将文件合并在一起。将忽略空文件名。文件内容不能反序列化则产生错误。设置特定值或映射密钥的第一个文件将被使用，并且值或映射密钥永远不会更改。这意味着设置CurrentContext 的第一个文件将保留其 context。也意味着如果两个文件指定 “red-user”,，则仅使用来自第一个文件的 “red-user” 的值。来自第二个 “red-user” 文件的非冲突条目也将被丢弃。 对于其他的，使用 HomeDirectoryLocation（~/.kube/config）也不会被合并。 2，此链中第一个被匹配的 context 将被使用： 1，命令行参数 - 命令行选项中 context 的值 2，合并文件中的 current-context 3，此段允许为空 3，确定要使用的集群信息和用户。在此处，也可能没有 context。这个链中第一次使用的会被构建。（运行两次，一次为用户，一次为集群）： 1，命令行参数 - user 是用户名，cluster 是集群名 2，如果存在 context 则使用 3，允许为空 4，确定要使用的实际集群信息。在此处，也可能没有集群信息。基于链构建每个集群信息（首次使用的）： 1，命令行参数 - server，api-version，certificate-authority 和 insecure-skip-tls-verify 2，如果存在集群信息并且该属性的值存在，则使用它。 3，如果没有 server 位置则出错。 5，确定要使用的实际用户信息。用户构建使用与集群信息相同的规则，但每个用户只能具有一种认证方法： 1，加载优先级为 1）命令行参数，2） kubeconfig 的用户字段 2，命令行参数：客户端证书，客户端密钥，用户名，密码和 token。 3，如果两者有冲突则失败 6，对于仍然缺失的信息，使用默认值并尽可能提示输入身份验证信息。 7，kubeconfig 文件中的所有文件引用都是相对于 kubeconfig 文件本身的位置解析的。当文件引用显示在命令行上时，它们被视为相对于当前工作目录。当路径保存在 ~/.kube/config 中时，相对路径和绝对路径被分别存储。 kubeconfig 文件中的任何路径都是相对于 kubeconfig 文件本身的位置解析的。 通过 kubectl config 操作 kubeconfig为了更容易地操作 kubeconfig 文件，可以使用 kubectl config 的子命令。请参见 kubectl/kubectl_config.md 获取帮助。 例如： $ kubectl config set-credentials myself --username=admin --password=secret $ kubectl config set-cluster local-server --server=http://localhost:8080 $ kubectl config set-context default-context --cluster=local-server --user=myself $ kubectl config use-context default-context $ kubectl config set contexts.default-context.namespace the-right-prefix $ kubectl config view 输出： apiVersion: v1 clusters: - cluster: server: http://localhost:8080 name: local-server contexts: - context: cluster: local-server namespace: the-right-prefix user: myself name: default-context current-context: default-context kind: Config preferences: {} users: - name: myself user: password: secret username: admin 一个 kubeconfig 文件类似这样： apiVersion: v1 clusters: - cluster: server: http://localhost:8080 name: local-server contexts: - context: cluster: local-server namespace: the-right-prefix user: myself name: default-context current-context: default-context kind: Config preferences: {} users: - name: myself user: password: secret username: admin 示例文件的命令操作： $ kubectl config set preferences.colors true $ kubectl config set-cluster cow-cluster --server=http://cow.org:8080 --api-version=v1 $ kubectl config set-cluster horse-cluster --server=https://horse.org:4443 --certificate-authority=path/to/my/cafile $ kubectl config set-cluster pig-cluster --server=https://pig.org:443 --insecure-skip-tls-verify=true $ kubectl config set-credentials blue-user --token=blue-token $ kubectl config set-credentials green-user --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key $ kubectl config set-context queen-anne-context --cluster=pig-cluster --user=black-user --namespace=saw-ns $ kubectl config set-context federal-context --cluster=horse-cluster --user=green-user --namespace=chisel-ns $ kubectl config use-context federal-context 最后的总结： 所以，看完这些，你就可以快速开始创建自己的 kubeconfig 文件了： 仔细查看并了解 api-server 如何启动：了解你的安全策略后，然后才能设计 kubeconfig 文件以便于身份验证 将上面的代码段替换为你集群的 api-server endpoint 的信息。 确保 api-server 已启动，以至少向其提供一个用户（例如：green-user）凭证。当然，你必须查看 api-server 文档，以确定以目前最好的技术提供详细的身份验证信息。]]></content>
  </entry>
</search>
